0 __________________________________________________
A Waymo robotaxi operating in autonomous mode struck and killed a small dog last month in San Francisco, according to an incident report filed with the California Department of Motor Vehicles. The incident appears to have been unavoidable, based on information provided in the report. According to the report, one of Waymo’s self-driving Jaguar I-Pace cars was traveling on Toland Street, a low-speed street near Waymo’s depot, and the dog ran into the street. The vehicle was operating in autonomous mode, and a human safety operator was in the driver’s seat at the time of the accident. The human operator didn’t see the dog, but the vehicle’s autonomous system did. However, a number of factors, including the speed and trajectory of the dog’s path, made the collision unavoidable, according to Waymo. Neither the safety operator nor the autonomous system braked to avoid collision, according to Waymo. In both cases, that’s because of the “unusual path” the dog took at “a high rate of speed directly towards the side of the vehicle,” said a Waymo spokesperson. A Waymo spokesperson told TechCrunch that the company reconstructed last month’s event in simulation against the NIEON model, but the analysis showed a collision in this case was unavoidable.
1 __________________________________________________
The back-up driver of an Uber self-driving car that killed a pedestrian has been charged with negligent homicide. Elaine Herzberg, aged 49, was hit by the car as she wheeled a bicycle across the road in Tempe, Arizona, in 2018. Investigators said the car's safety driver, Rafael Vasquez, had been streaming an episode of the television show The Voice at the time. The accident was the first death on record involving a self-driving car, and resulted in Uber ending its testing of the technology in Arizona. Lengthy investigations by police and the US National Transportation Safety Board (NTSB) found that human error was mostly to blame for the crash. Dash-cam footage released by police showed Ms Vasquez looking down, away from the road, for several seconds immediately before the crash, while the car was travelling at 39mph (63km/h). The NTSB identified the probable cause of the accident as failure of the operator to monitor their surroundings, and the automated system, "because she was visually distracted throughout the trip by her personal cell phone". Following the crash, authorities in Arizona suspended Uber's ability to test self-driving cars on the state's public roads, and Uber ended its tests in the state.
2 __________________________________________________
A driver reported that their Tesla’s “full-self-driving” software braked unexpectedly, triggering an eight-car pileup in the San Francisco Bay Area last month. The accident resulted in nine people being treated for minor injuries, including one juvenile who was hospitalized, according to a California Highway Patrol traffic crash report. The report detailed that the Tesla vehicle was seen changing lanes and slowing to a stop. However, it could not confirm if “full self-driving” was active at the time of the crash. The crash occurred on Thanksgiving, causing traffic on Interstate 80 east of the Bay Bridge to be disrupted for about 90 minutes. The report states that the Tesla Model S was traveling at about 55 mph and shifted into the far left-hand lane, but then braked abruptly, slowing the car to about 20 mph. This led to a chain reaction that ultimately involved eight vehicles to crash. Tesla’s driver-assist technologies, Autopilot and “full self-driving” are already being investigated by the National Highway Traffic Safety Administration following reports of unexpected braking that occurs “without warning, at random, and often repeatedly in a single drive.”
3 __________________________________________________
Four out of the 48 self-driving cars on public roads in California have been involved in accidents in the last eight months, according to the state's Department of Motor Vehicles. The agency began issuing permits for the testing of autonomous vehicles in September 2014. Three of the four cars belonged to Google, the fourth to parts supplier Delphi. Both firms denied their vehicles had been at fault. Google said its driverless cars had never been the cause of an accident, and that the majority of "minor fender-benders" had been in the form of rear-end collisions from other drivers. Delphi told the BBC its vehicle was hit while stationary at a crossroads and was in manual driving mode at the time. An anonymous source told the Associated Press that two of the accidents occurred while the vehicles were occupied by human drivers, and all four vehicles were going very slowly at the time of the collisions. Chris Urmson, director of Google's self driving car programme, wrote in a blog post that there have been 11 accidents involving Google cars since the project began six years ago but not one has been caused by one of its vehicles.
4 __________________________________________________
When an autonomous Uber test vehicle killed a pedestrian in 2017, the human safety operator in the vehicle was charged with negligent homicide with a dangerous instrument. Uber faced no charges. And after a deadly crash in 2019 involving a Tesla vehicle operating in Autopilot, the driver, not Tesla, was charged with two felony counts of vehicular manslaughter with gross negligence.
5 __________________________________________________
A leaked email from Robbie Miller, operations manager for Uber's self-driving trucks, warned Uber's executives that its self-driving cars were "routinely in accidents". This was partly due to faults with the technology and partly because of the "poor behaviour" of operators. The email was sent on 13 March, only days before a fatal collision in Tempe, Arizona in which an autonomous Uber car hit and killed pedestrian Elaine Herzberg. Uber suspended all its tests following the accident, which is still under investigation by the National Transportation Safety Board. In June it was revealed that the safety operator of the car was watching TV just before the accident. Mr Miller recommended that Uber reduce its fleet size by up to 85%, stop tests after all accidents and review any incident, even minor ones "immediately".
6 __________________________________________________
Police have released two videos showing the moments leading up to a fatal crash involving a self-driving Uber car in Tempe, Arizona, on Sunday. In the 14-second video, the autonomous vehicle is seen failing to slow down before hitting Elaine Herzberg, 49, who is walking her bike across the road. One video shows dashcam footage of the impact. The other, an Uber operator monitoring the car's controls. Uber has suspended self-driving tests in North America following the crash. In footage released on Wednesday by the Tempe police department, the human Uber operator sitting inside the Volvo appears to be looking down at something while the vehicle is travelling in autonomous mode. Moments later, the woman appears visibly shocked as she looks up to see Ms Herzberg crossing the highway in their path seconds before impact. Police said the accident happened on Sunday night, adding that Ms Herzberg had not been using a pedestrian crossing. Ms Herzberg was taken to a local hospital following the collision but died of her injuries. The US National Highway Traffic Safety Administration and the National Transportation Safety Board earlier said they would be investigating the incident in Tempe. While self-driving cars have been involved in multiple accidents, it is thought to be the first time an autonomous car has been involved in a fatal collision with a pedestrian.
7 __________________________________________________
Volvo has stated that it will accept full liability for accidents involving its driverless cars, making it one of the first car companies to do so. This move is seen as an attempt to expedite regulation in the US, where a lack of consistent rules is seen as holding back the industry. The uncertainty over who is liable in a driverless car crash is seen as one of the biggest barriers to the adoption of this technology. Volvo's president, Hakan Samuelsson, has stated that the US is currently the most progressive country in the world in autonomous driving, but it risks losing its leading position due to the lack of Federal guidelines for the testing and certification of autonomous vehicles.
8 __________________________________________________
The National Highway Traffic Safety Administration (NHTSA) released data indicating that Teslas using driver-assist systems were involved in 273 crashes over the past nine months. The data included crashes from vehicles using driver-assist technologies like Tesla Autopilot and fully autonomous vehicles. Of the 367 crashes involving vehicles using driver-assist technologies, 273 involved a Tesla system, either its “full self-driving” software or its precursor, Tesla Autopilot. NHTSA found that of the 367 driver-assist crashes reported, there were six fatalities and five serious injuries. The safety risks of these new technologies have drawn the attention of safety advocates for years. NHTSA’s investigation into Teslas rear-ending first responders’ vehicles was expanded last week and could lead to a recall. The National Transportation Safety Board has investigated fatal crashes involving Autopilot and called for the automaker to make changes. NHTSA said that its investigation into Teslas rear-ending emergency vehicles while using Autopilot found that in 37 of 43 crashes with detailed car log data available, drivers had their hands on the wheel in the last second prior to the collision.
9 __________________________________________________
Toyota is resuming operations of its autonomous vehicles at the Paralympic Games village in Tokyo following an accident. Services of the e-Palette pods were halted after a vehicle hit a visually impaired athlete last week. The athlete was not seriously injured, but he had to pull out of an event because of cuts and bruises. The vehicles will now have more operator control and extra staff to ensure they do not hit any more people. Aramitsu Kitazono, a member of Japan's judo team, was hit as he was walking across a pedestrian crossing on Thursday. Mr Kitazono was unable to compete in his 81kg category because of the accident. In a statement late on Monday, Toyota said: "The vehicle's sensor detected the pedestrian crossing and activated the automatic brake, and the operator also activated the emergency brake. The vehicle and pedestrians, however, came into contact before it came to a complete halt." The company said that operators would now be given control over how fast the vehicles travel, with two members of safety staff on board, rather than one, to help look out for pedestrians. New safety features will also include louder warning sounds, while pedestrian guides at busy crossings in the Paralympic village will be increased to 20 from six. Toyota also said that it would continue to make safety improvements "on a daily basis" until the village closes. The company also said it was co-operating with a local police investigation to determine the cause of the accident. On Friday, Toyota chief executive Akio Toyoda made a public apology after the incident. Mr Toyoda said the accident illustrated just how difficult it was to operate self-driving vehicles in the special circumstances of the village during the Paralympics, with people there who are visually impaired or have other disabilities. "It shows that autonomous vehicles are not yet realistic for normal roads," he added.
10 __________________________________________________
Google’s self-driving car project has achieved a milestone with its fleet of about a dozen autonomous cars having driven 300,000 miles without a single accident under computer control. However, Google warns that there are still challenges to overcome, such as handling snow-covered roads and interpreting temporary construction signs. In the future, Google plans to let some team members drive the cars solo for their daily commutes. There have been some accidents involving Google’s self-driving cars in the past, but all of these occurred while humans were in control of the cars.
11 __________________________________________________
California authorities have asked General Motors to take some of its Cruise robotaxis off the road after autonomous vehicles were involved in two collisions in San Francisco last week. One of the collisions involved an active fire truck. The Department of Motor Vehicles is investigating these incidents and has requested Cruise to reduce its active fleet of operating vehicles by 50% until the investigation is complete and corrective actions are taken to improve road safety. The self-driving subsidiary of General Motors, Cruise, can now have no more than 50 driverless cars in operation during the day, and 150 at night. The firetruck crash occurred when an emergency vehicle moved into an oncoming lane of traffic to bypass a red light. Cruise's driverless car identified the risk but was unable to avoid the collision, resulting in one passenger being taken to the hospital for minor injuries. The other crash took place when another car ran a red light at a high speed. The autonomous vehicle detected the vehicle and braked, but the other vehicle made contact with the autonomous vehicle. There were no passengers in the autonomous vehicle and the driver of the other vehicle was treated and released at the scene.
12 __________________________________________________
A distracted safety operator in an Uber self-driving car was primarily to blame for a fatal crash in 2018, a US regulator has ruled. The National Transportation Safety Board (NTSB) said an “inadequate safety culture” at Uber was also a major contributing factor. Elaine Herzberg, 49, was killed when an Uber-owned self-driving car - operating in autonomous mode - struck her as she crossed a road in Tempe, Arizona, on 18 March 2018. In the car was safety driver Rafaela Vasquez who, according to investigators, had been streaming a TV show on her mobile phone while behind the wheel. Dashcam footage showed Ms Vasquez spent 36% of the journey that evening looking at the device. Uber’s computers detected Ms Herzberg 5.6 seconds before impact, the NTSB said, but did not correctly identify her as a person. The report said Ms Herzberg was acting unsafely in attempting to cross the road where she did - investigators said toxicology reports suggested she had taken drugs that may have impaired her judgement. Nevertheless, the NTSB said Uber had an "inadequate safety culture, exhibited by a lack of risk assessment mechanisms, of oversight of vehicle operators, and of personnel with backgrounds in safety management”. It acknowledged on Tuesday that the company had made significant changes since the accident. Uber said it welcomed the recommendations. "We deeply regret the March 2018 crash that resulted in the loss of Elaine Herzberg’s life, and we remain committed to improving the safety of our self-driving program,” said Nat Beuse, head of safety at Uber’s advanced technologies group.
13 __________________________________________________
The text does not provide information related to an autopilot accident.
14 __________________________________________________
Some companies are closer than others to completely driverless vehicles, but the edge case driving situations are still a challenge, as we sadly found not long ago when an AV hit a pedestrian walking her bicycle across a dark highway in Arizona. Although there was a driver present who could have taken the controls, she didn’t. One can hardly blame her inattention, for the whole point of autonomous driving technology is to allow for, if not encourage, drivers to disengage from the task. The “autonomous vehicle paradox” of inducing drivers to disconnect because they are not needed most of the time is confounding. At least in the interim, until autonomous systems can reliably achieve better than a 98% safety rate (roughly the rate of human drivers), autonomous systems will need to be supplemented by a human driver for emergencies and other unexpected situations.
C:\anaconda\python.exe C:/Users/Morning/Desktop/hiwi/heart/paper/selenium_test.py
15 __________________________________________________
The text does not contain information related to an autopilot accident.
16 __________________________________________________
The Uber test driver, Rafaela Vasquez, who was responsible for monitoring one of the company’s self-driving cars that hit and killed a pedestrian in 2018 was charged with negligent homicide this week. Vasquez was watching television on her smartphone when the Uber self-driving vehicle struck Elaine Herzberg, who was crossing a road in Tempe, Arizona. The NTSB investigation concluded that the crash was caused because Vasquez was distracted by her phone, and that Uber’s inadequate safety culture contributed to the crash. The NTSB found that Uber’s system could not correctly classify and predict the path of a pedestrian crossing midblock, which occurred in the incident involving Vasquez. Uber had also deactivated a forward collision warning and automatic emergency braking system on the Volvo. A grand jury in Maricopa County, Arizona charged Vasquez with the felony long after Uber had been cleared of any potential charges. In March 2019, the Yavapai County Attorney’s Office concluded there was no basis for criminal liability against Uber.
17 __________________________________________________
One of Google’s self-driving cars has gotten into a minor accident while the AI was in control. This is the first time an accident has occurred while the vehicle was in autonomous mode. The Google Lexus-model autonomous vehicle was traveling in autonomous mode eastbound on El Camino Real in Mountain View when it signaled its intent to make a right turn on red onto Castro St. The car had to stop and go around sandbags positioned around a storm drain that were blocking its path. When the light turned green, the car began to proceed back into the center lane to pass the sand bags. A public transit bus was approaching from behind. The Google AV test driver saw the bus approaching but believed the bus would stop or slow to allow the Google AV to continue. Approximately three seconds later, as the Google AV was reentering the center of the lane it made contact with the side of the bus. The Google AV was operating in autonomous mode and traveling at less than 2 mph, and the bus was travelling at about 15 mph at the time of contact. The Google AV sustained bus damage to the left front fender, the left front wheel and one of its driver’s-side sensors. There were no injuries reported at the scene.
18 __________________________________________________
Toyota has suspended US tests of driverless cars on public roads following a fatal accident in Arizona involving one of Uber Technologies' self-driving vehicles. Toyota said it was concerned about the "emotional effect" the incident might have on its test drivers. The carmaker said it did not have a timeline for re-starting the trials. The Arizona accident has revived debate about whether autonomous vehicles are being put into use prematurely. The accident in Tempe, Arizona on Sunday is believed to be the first fatality involving a fully autonomous vehicle. Police and federal officials are investigating the details of the incident, in which a 49-year-old woman was killed by an Uber car operating in autonomous mode. A human monitor was also behind the wheel. On Tuesday, Tempe police said they had reviewed video of the crash and repeated that fault had not been determined. Uber said after the accident that it would temporarily halt driverless car tests. Toyota has also been performing trials of its Chauffeur mode on public roads in Michigan and California. The firm previously said it expected some of its cars to be equipped with automated driving technology by 2020. "Because we feel the incident may have an emotional effect on our test drivers, we have decided to temporarily pause our Chauffeur mode testing on public roads," a spokesman said.
19 __________________________________________________
Uber halted testing of its self-driving cars following a fatal accident in Tempe, Arizona this March that left a pedestrian dead. An autonomous Uber SUV accompanied by a safety driver was driving northbound when it struck a woman, who was taken to the hospital where she later died as a result of her injuries. Investigators later determined the driver, Rafaela Vasquez, had looked down at a phone 204 times during a 43-minute test drive, according to a 318-page police report released by the Tempe Police Department. In the aftermath of the accident, Uber paused all of its AV testing operations in Pittsburgh, Toronto, San Francisco and Phoenix. Moving forward, Uber will test its self-driving cars more cautiously, per a recently released Uber safety report. The company will require that two employees are in the front seat of its cars at all times, that an automatic braking system is enabled and that its safety employees are more strictly monitored.
20 __________________________________________________
Apple’s secretive self-driving vehicle program has disclosed its first accident, according to a report filed with the California Department of Motor Vehicles. The low speed accident, which occurred August 24, is a milestone of sorts for the company, albeit not one that is being celebrated. The Apple test car was attempting to merge onto an expressway near its headquarters in Cupertino, California, and traveling about 1 mile per hour, when it was rear-ended by a Nissan Leaf, according to the report. There were no injuries reported. Both parties reported moderate damage to their vehicles. The self-driving test vehicle involved in the accident was a 2016i Lexus RX450H.
21 __________________________________________________
Automaker Toyota has temporarily ceased its public road testing of its fully autonomous ‘Chauffeur’ system in the U.S. after an accident earlier this week saw an Uber self-driving test vehicle strike a pedestrian, which ultimately resulted in her death. Police have stated that initial findings suggest the accident would’ve been extremely difficult to avoid regardless of whether a human or an AV system was in control at the time, because of how quickly the victim crossed in front of the moving vehicle (outside of a crosswalk). Toyota has indicated to Bloomberg that it’s stopping testing for now due to the potential “emotional effect on [its] test drivers.” Toyota spokesperson Brian Lyons noted that the automaker couldn’t speculate on the cause of the crash or its implications for the future of the self-driving industry.
22 __________________________________________________
The robot cars, one made by Delphi Automotive and one by Google, met on a Californian road in Palo Alto. The Google car pulled in front of the Delphi vehicle making it abandon a planned lane change. The incident occurred as the Delphi car, an Audi Q5 crossover, was preparing to change lanes. As it did so the Google car, a Lexus RX400h crossover, abruptly moved in front of it forcing the Audi to abandon its manoeuvre. The Delphi car coped well with the incident, said Mr Absmeier, and "took appropriate action". A Delphi spokeswoman clarified the incident with tech news site Ars Technica saying there was nothing amiss in the encounter and that its vehicle behaved "admirably". Google played down the the incident, saying early reports that the cars were involved in a "near miss" were inaccurate. It said the cars treated each other as they would any other vehicle and neither was in danger of colliding with the other. Delphi and Google's autonomous vehicles have been involved in several minor accidents and incidents during testing. However, before now all of those have involved the robot cars and human-driven vehicles. In almost all cases, the firms have said, the fault lay with human drivers.
23 __________________________________________________
Less than a week before an Uber self-driving SUV prototype struck and killed a pedestrian in Tempe, Arizona last March, a manager sent executives an email cautioning that its autonomous vehicle unit needed to “work on establishing a culture rooted in safety,”. Robbie Miller, then a manager in the unit’s testing operations, warned that “cars are routinely in accidents resulting in damage” and backup drivers, who sit behind the wheel in self-driving cars for safety reasons, weren’t properly trained or fired even if they made repeated mistakes. In his March 13 email, Miller wrote that “the cars are routinely in accidents resulting in damage. This is usually the result of poor behavior of the operator or the AV technology. A car was damaged nearly every other day in February. We shouldn’t be hitting things every 15,000 miles. Repeated infractions for poor driving rarely results in termination. Several of the drivers appear to not have been properly vetted or trained.” Five days later, on March 18, the Tempe collision occurred, resulting in the death of 49-year-old Elaine Herzberg, who was crossing a street when she was hit by the SUV. Uber immediately halted testing of its autonomous cars on public roads and reached a settlement with Herzberg’s family two weeks later. It was later revealed in a police report that the car’s backup safety driver was watching videos on her phone when the crash occurred. The incident also raised serious questions about the safety of Uber’s self-driving technology system. Uber resumed testing of its self-driving cars in July, but in manual mode and with new safety standards in place.
24 __________________________________________________
Two vehicles reportedly engaged in self-drive modes - a Tesla Model S and a General Motors Chevy Bolt - have been involved in separate road accidents in California. Culver City's fire service said the Tesla had "ploughed into the rear" of one of its fire engines parked at the scene of an accident on Monday. The car's owner subsequently claimed it had been in Autopilot mode at the time. The GM incident resulted in a collision with a motorbike in San Francisco. The rider says the car - which was using GM's Cruise Automation technology - caused him serious injury and is now suing GM, according to local newspaper The Mercury News. GM has alleged the motorcyclist was at fault. The event dates back to December, but has come to light only now. The US National Transportation Board (NTSB) has said it will investigate the Tesla crash. According to a tweet by the Culver City Firefighters, the Model S was travelling at 65mph (105km/h) when the impact occurred. Tesla has the ability to analyse data gathered by its vehicles' on-board computers to determine the cause of crashes, and has shared information with the press about previous high-profile accidents. However, for now the car company has limited itself to saying that "Autopilot is intended for use only with a fully attentive driver" and that it has instructed drivers to keep their hands on the steering wheel while employing it. The NTSB previously investigated Tesla after a Model S crash in 2016 in which the driver died. It held the company partly accountable saying the Autopilot system had given the victim "leeway... to divert his attention to something other than driving". Since the accident, Tesla has introduced an update that brings its cars to a halt if they detect a driver's hands are not on the wheel.
25 __________________________________________________
Uber has suspended self-driving car tests in all North American cities after a fatal accident in Tempe, Arizona. A 49-year-old woman was hit by a car and killed as she crossed the street. This is thought to be the first time an autonomous car has been involved in a fatal collision. The accident happened Sunday night while the car was in autonomous mode, with a human monitor also behind the wheel. The woman, Elaine Herzberg, had not been using a pedestrian crossing and was taken to a local hospital where she died. The US National Highway Traffic Safety Administration and the National Transportation Safety Board are sending teams to Tempe to investigate.
26 __________________________________________________
On May 5, a Class 8 Waymo Via truck operating in autonomous mode with a human safety operator behind the wheel was hauling a trailer northbound on Interstate 45 toward Dallas, Texas. At 3:11 p.m., just outside Ennis, the modified Peterbilt was traveling in the far right lane when a passing truck and trailer combo entered its lane. The driver of the Waymo Via truck told police that the other semi truck continued to maneuver into the lane, forcing Waymo’s truck and trailer off the roadway. She was later taken to a hospital for injuries that Waymo described in its report to the National Highway Traffic Safety Administration as “moderate.” The other truck drove off without stopping. While Waymo’s autonomous semi truck was not at fault in the hit and run, the incident highlights gaps in reporting mechanisms, and raises questions about how ready the public and law enforcement are to cope with heavy, fast-moving vehicles that have no human driver. Waymo reported that its truck was driving in autonomous mode at 62 miles per hour, slightly below the speed limit, when the other truck entered its lane and forced it off the road. Waymo told TechCrunch that the safety operator did not take control of the truck from its autonomous system. “The technology was not a factor, as this collision was caused by a human driver of another vehicle when they crossed the lane line and collided with the cab of Waymo’s vehicle and continued driving,” spokesperson Katherine Barna wrote in an email. The driver, however, was taken to a nearby hospital with unspecified, moderate injuries. The attending officer classified the incident as a hit and run. Waymo told TechCrunch that it understands the driver is doing well, following their injury. The driver did not respond to a request from TechCrunch for comment.
27 __________________________________________________
An Uber self-driving test vehicle that hit and killed a woman in 2018 had software problems, according to US safety investigators. Elaine Herzberg, 49, was hit by the car as she was crossing a road in Tempe, Arizona. The US National Transportation Safety Board (NTSB) found the car failed to identify her properly as a pedestrian. The fatal crash occurred in March 2018, and involved a Volvo XC90 that Uber had been using to test its self-driving technology. Just before the crash, Ms Herzberg had been walking with a bicycle across a poorly lit stretch of a multi-lane road. According to the NTSB, Uber's test vehicle failed to correctly identify the bicycle as an imminent collision until just before impact. By that time, it was too late for the vehicle to avoid the crash. The report also said there were 37 crashes of Uber vehicles in self-driving mode between September 2016 and March 2018. Following the crash, authorities in Arizona suspended Uber's ability to test self-driving cars on the state's public roads. The company subsequently pulled the plug on its autonomous car operation in Arizona, although the company later resumed tests in Pennsylvania.
28 __________________________________________________
"A series of accidents have left some consumers wary of driverless cars. Uber briefly suspended self-driving cars tests in March after a fatal accident in the US, while a self-driving car owned by Apple was involved in an accident this month."
29 __________________________________________________
An autonomous Uber test SUV driving in Tempe, Arizona was involved in a fatal collision last night. The Uber vehicle was in autonomous mode at the time the accident occurred. The Uber SUV was driving northbound, and a woman crossed in its path outside of a crosswalk, at which point she was struck by the vehicle. She was later taken to hospital, where she died as a result of her injuries. The AV had a safety driver at the wheel, who is in place in order to be able to take control of the autonomous test vehicle in case the self-driving system should fail or appear to be at risk of endangering others on the road. No other passengers were in the vehicle at the time of this accident. Uber has paused all of its AV testing operations as a result of this accident, across all cities where it operates, including Pittsburgh, Toronto, San Francisco and Phoenix. This is the first time an autonomous vehicle operating in self-driving mode has resulted in a human death. Uber CEO Dara Khosrowshahi expressed sympathy for the victim’s family on Twitter, and reiterated that Uber is working with local authorities to determine what exactly occurred.
30 __________________________________________________
A year after the first fatality caused by a fully self-driving car, questions about liability in the event of a death involving the cars are still completely up in the air. Officials announced earlier this week that Uber won’t face criminal charges in the death of a pedestrian struck and killed by one of its self-driving cars nearly a year ago in Tempe, Arizona. The Yavapai County Attorney’s Office said it conducted a thorough review of the evidence and determined there was no basis for criminal liability against Uber. The pedestrian was walking a bicycle across a road at night. Uber’s self-driving software system initially classified the pedestrian as an unknown object, then as a vehicle, then as a bicycle, but never braked. However, the Uber employee who was behind the wheel of the SUV could still face criminal charges. Companies working on self-driving cars, such as Uber, have test drivers who are supposed to intervene if the car fails to act properly. The Uber incident had unusual circumstances, so it’s not a model case for setting law and policy around fully self-driving cars, according to Walters. Video showed the test driver was distracted and not watching the road. Uber isn’t facing criminal charges but has dealt with adverse effects on its business as a result of the incident. The company temporarily halted its vehicle test program and shut down its self-driving operations in Arizona, laying off 300 workers. In December, Uber resumed testing vehicles in autonomous mode. But the fatality increased public skepticism of self-driving vehicles, and slowed efforts to pass autonomous vehicle legislation on Capitol Hill. Uber also faced the risk of a civil lawsuit. Edwards said this is why the company moved quickly to settle with the family of Elaine Herzberg, the pedestrian who was killed, shortly after her death. Uber and Herzberg’s family settled fewer than two weeks after the crash. Details of the agreement weren’t revealed.
31 __________________________________________________
The Uber test driver, Rafaela Vasquez, who was behind the wheel of one of the company’s self-driving cars when it hit and killed a pedestrian in 2018, pleaded guilty to endangerment and was sentenced to three years of supervised probation. Vasquez was watching television on her smartphone in March 2018 when the Uber self-driving vehicle fatally struck Elaine Herzberg, 49, who was crossing a road in Tempe, Arizona. The National Transportation Safety Board’s 2019 investigation found that Vasquez was looking away from the road for over a third of the trip. The board concluded that the crash was “avoidable” if the safety driver had been alert and also found that an inadequate safety culture at Uber contributed to the crash. The company’s self-driving software wasn’t designed to expect that pedestrians outside crosswalks may be crossing the street. Uber reached a settlement with the victim’s family less than two weeks after her death. The company did not face criminal charges.
32 __________________________________________________
The US National Highway Traffic Safety Administration (NHTSA) has opened an official investigation into Tesla's "self-driving" Autopilot system following 11 Tesla crashes since 2018 involving emergency vehicles. In some cases, the Tesla vehicles "crashed directly into the vehicles of first responders". The investigation will cover roughly 765,000 Tesla cars made since 2014, including those in the Model Y, Model X, Model S and Model 3. The NHTSA is primarily concerned with an apparent inability of Tesla vehicles to cope with vehicles stopped in the road, specifically emergency vehicles attending an incident. The NHTSA is opening its preliminary investigation into "the technologies and methods used to monitor, assist, and enforce the driver's engagement", while using Autopilot. It said that in the 11 crashes that prompted its investigation, either Autopilot or a system called Traffic Aware Cruise Control had been active "just prior" to the collisions.
33 __________________________________________________
A Google self-driving Lexus RX 450h was involved in a crash with a van in Mountain View, Calif. on Friday afternoon, according to local police. Another driver ran a red light and crashed into the car. Thankfully, nobody was injured in the accident. Google issued the following statement with details about the crash: “A Google vehicle was traveling northbound on Phyllis Ave. in Mountain View when a car heading westbound on El Camino Real ran a red light and collided with the right side of our vehicle. Our light was green for at least six seconds before our car entered the intersection. Thousands of crashes happen everyday on U.S. roads, and red-light running is the leading cause of urban crashes in the U.S. Human error plays a role in 94% of these crashes, which is why we’re developing fully self-driving technology to make our roads safer.” Two local TV stations, KRON and KPIX, reported that the Google autonomous vehicle had been “in control,” or in its self-driving mode at the time of the crash. However, the self-driving car was reportedly manned by a Google employee who took over its operation, and applied the brakes when the other car’s driver began crossing an intersection, apparently running a red light and colliding with Google’s vehicle. The crash comes just after the U.S. Department of Transportation made some major autonomous vehicle policy announcements earlier this week.
34 __________________________________________________
A driverless car was involved in a traffic accident on a California city street earlier this year. No one was hurt in the small fender bender, but the accident does signal we are making incredible leaps forward on the road toward driverless cars. This wasn’t really an accident in the traditional sense — intentional software changes implemented just weeks earlier were likely a contributing factor. The incident was partially caused by a subtle software update, implemented a few weeks prior in all of Google’s autonomous cars, that enabled them to “hug the rightmost side of the lane,” a common social norm that allows other drivers to pass on the left. According to the accident report, the Google autonomous vehicle was shifting within its lane to bypass an obstacle in its path when it made contact with a bus approaching from behind. The car was traveling slower than two miles per hour at the time of impact — the bus, about 15 mph. Accidents like this are vital learning exercises. Google’s driverless vehicles cover more than 10,000 miles a day, in addition to the three million miles of computer-simulated driving taking place daily. But these real-life tests are crucial.
35 __________________________________________________
The text does not provide information about an autopilot accident.
36 __________________________________________________
The news text does not provide any specific information about an autopilot accident.
37 __________________________________________________
Google has been testing self-driving cars for six years with a fleet of over 20 vehicles, which have self-driven almost a million miles over that period. According to Google’s Chris Urmson, these autonomous vehicles have been involved in 11 minor accidents, but none of these were caused by the self-driving cars. Urmson says Google’s driverless cars have been hit from behind seven times, side-swiped a couple of times, and hit by a car rolling through a stop sign. The majority of the accidents occurred on city streets rather than freeways.
38 __________________________________________________
Tesla is recalling all 363,000 US vehicles with its so-called “Full Self Driving” driver assist software due to safety risks. The National Highway Traffic Safety Administration said that Tesla’s FSD feature “led to an unreasonable risk to motor vehicle safety based on insufficient adherence to traffic safety laws.” The FSD Beta system may allow the vehicle to act unsafe around intersections, such as traveling straight through an intersection while in a turn-only lane, entering a stop sign-controlled intersection without coming to a complete stop, or proceeding into an intersection during a steady yellow traffic signal without due caution. Tesla has identified 18 reports of incidents received between May 8, 2019, and September 12, 2022, that may be related to the conditions described above. NHTSA itself has identified at least 273 crashes that involved one of Tesla’s driver assist systems. There have been high-profile accidents of Tesla cars using FSD or its more rudimentary predecessor known as “Autopilot.” Some of those accidents included fatalities. NHTSA is also investigating that predecessor, Autopilot.
39 __________________________________________________
The text does not provide information related to an autopilot accident.
40 __________________________________________________
On Tuesday, January 30, a modified Hyundai Genesis equipped with technology created by autonomous systems startup Phantom AI was involved in an accident during a demonstration drive. The vehicle was demonstrating its Autopilot-like SAE Level 2 partial autonomous system, designed to maintain lane position, maintain distance between itself and vehicles ahead, and switch lanes automatically once directed using the turning indicator. While the L2 system was engaged and the car was traveling at 60 MPH, a pickup truck ahead dropped a poorly secured garbage bin from its cargo bed onto the roadway. The car in front, a white Nissan Rogue, applied the brakes to prevent hitting the trash bin, and the driver of the Genesis applied the brake in an attempt to prevent a collision, but the car collided with the Nissan going approximately 20 MPH. The Genesis suffered significant damage to the front end. The Automatic Emergency Braking system, which would’ve normally engaged at that point and prevented the collision, was disabled for the demo because it had been throwing too many false positives and was undergoing tuning.
41 __________________________________________________
For the past six years, Google cars have been cruising the roads and streets of California and Texas with a human driver ready to take over from the autonomous machine in an emergency. They have racked up more than one million miles of autonomous experience, 14 accidents (mainly being rear-ended by distracted drivers), and vast amounts of data about this sort of transport.
42 __________________________________________________
A federal investigation into a fatal crash involving an Uber self-driving car concluded the probable cause was a safety driver distracted by their phone. The National Transportation Safety Board investigation also determined that an inadequate safety culture at Uber contributed to the March 2018 crash in Tempe, Arizona. The investigators found that an alert vehicle operator would have had two to four seconds to detect and avoid pedestrian Elaine Herzberg, who was crossing a street when struck by Uber’s self-driving vehicle. The test driver behind the wheel of Uber’s self-driving car was supposed to intervene if the autonomous driving software failed. But the driver was glancing away from the road during 34% of the fatal trip, including 23 glances in the final three minutes before the crash, according to the investigation. A camera in the car recorded the driver. The NTSB found that Uber had no safety plan for its self-driving operation, or equivalent guiding document at the time of the crash. Uber’s self-driving software wasn’t designed to expect that pedestrians outside crosswalks may be crossing the street. The board also said Uber lacked appropriate oversight for vehicle operators. Uber expressed remorse in a statement and that it would continue to improve the safety of its self-driving program. Uber settled with the victim’s family shortly after the death.
43 __________________________________________________
The news text related to the autopilot accident is: "The commercial, which will be aired in Washington, DC, Austin, Tallahassee, Albany, Atlanta and Sacramento does not paint Tesla in the best light. The ad is part of a multimillion dollar advertising campaign by The Dawn Project. It shows a Tesla Model 3, which allegedly has the Full Self-Driving mode turned on, running over a child-sized dummy on a school crosswalk, and then a fake baby in a stroller, in a series of tests by the Dawn Project. In the ad, the car swerves into oncoming traffic, zooms past stopped school buses, and cruises through “do not enter” signs. The Dawn Project says it wants to make computer-controlled systems safer for humanity, shooting its own videos as tests of Tesla’s alleged design flaws. In August, O’Dowd published a video showing a Tesla plowing into child-sized mannequins. Some Tesla fans posted their own videos in defense, using their own dummies or even their own children – YouTube has taken down several test videos involving actual children, citing safety risks. O’Dowd received a cease and desist letter from Tesla over the video, claiming he and the Dawn Project were “disparaging Tesla’s commercial interests and disseminating defamatory information to the public.” Federal investigators are looking into a Musk tweet about disabling driver alerts on Tesla’s “Full Self Driving” driver assist system, joining several other National Highway Traffic Safety Administration probes. NHTSA first investigated Tesla’s driver-assist technology after reports Autopilot-engaged vehicles were crashing into emergency vehicles stopped at the scene of earlier crashes."
44 __________________________________________________
The driver death resulting from a Tesla Model S Autopilot crash on May 7 has ramifications that extend well past a federal investigation in the United States. Government officials from the Canadian province of Ontario told Reuters that the fatal accident has further scrutinized its own autonomous driving technology and that it may have set back the development of its overall self-driving program as a result. Ontario has had some of the most progressive laws on the books for self-driving testing, but has suffered through a lack of applications — and Tesla’s Autopilot fatality certainly won’t help its cause, as the scope of scrutiny on autonomous technology has only gotten wider since the accident.
45 __________________________________________________
Earlier this year Google reported about the eleven “minor accidents” its driverless cars had been involved in over six years of testing — laying the blame for all 11 incidents at the hands of the other human drivers. Google is now trying to train its cars to drive “a bit more humanistically”. The DMV has just published all the official accident reports involving autonomous vehicles tested on California’s roads, covering the period from last September to date, on its website. This data mostly pertains to Google’s driverless vehicles, with eight of the nine reports involving Mountain View robot cars. The other one is an autonomous vehicle made by Delphi Automatic. The reports appear to support Google’s claims that human error by the drivers of the non-autonomous cars is, on the surface, causing accidents. However the difficulties caused by the co-mingling of human and robot driving styles is also in ample evidence. In one report, from April this year, a low-speed rear-shunt occurred when a robot car — in the midst of attempting to turn right at an intersection — applied the brakes to avoid an oncoming car, after initially creeping forward. The human-driven car behind it, also trying to turn right and presumably encouraged by the Lexus creeping forward, then “failed to brake sufficiently” and so collided with the rear of the Google Lexus. In another report, from June this year, a Google Lexus traveling in autonomous vehicle mode was also shunted from behind at low speed by a human-driven car. In this instance the robot car was obeying a red stop sign that was still showing for the lane it was occupying. The human driver behind was apparently spurred on to drive into the back of the stationary Lexus because of a green light appearing — albeit for a left-turn lane (whereas the two cars were actually occupying the straight ahead lane). A third report, from this July, details how another Google Lexus was crashed into from behind by a human driver — this time after decelerating to a stop in traffic because of stopped traffic ahead of a green lit traffic intersection. Presumably the human driver was paying more attention to the green traffic signal than to the changing road conditions. Most of the accidents detailed in the reports occurred at very low speeds. All the DMV’s Google-related accident reports pertain to this year, with six accident reports covering the first half of the year, including two in June and two in April.
46 __________________________________________________
Autonomous-driving firm Waymo's cars have been causing issues in a "dead-end" street in San Francisco, according to local residents. The self-driving vehicles have been going up and down the cul-de-sac at all hours "for weeks", sometimes even having to queue before making multi-point turns to leave the way they came. Waymo claims the vehicles are just "obeying road rules" designed to limit traffic in certain residential streets. The cars sometimes make a detour because of the presence nearby of one of San Francisco's "slow streets", which aim to limit traffic in certain residential areas. In May, a Waymo taxi blocked a road in Arizona after it became confused by the presence of a traffic cone.
47 __________________________________________________
More bad news for Uber: one of the ride-hailing giant’s self-driving Volvo SUVs has been involved in a crash in Arizona — apparently leaving the vehicle flipped onto its side, and with damage to at least two other human-driven cars in the vicinity. The aftermath of the accident is pictured in photos and a video posted to Twitter by a user of @FrescoNews, a service for selling content to news outlets. According to the company’s tweets, the collision happened in Tempe, Arizona, and no injuries have yet been reported. Uber has also confirmed the accident and the veracity of the photos to Bloomberg. Uber has now provided us with the following statement: “We are continuing to look into this incident and can confirm we had no backseat passengers in the vehicle.” TechCrunch understands Uber’s self-driving fleet in Arizona has been grounded, following the incident, while an investigation is undertaken. The company has confirmed the vehicle involved in the incident was in self-driving mode. We’re told no one was seriously injured. Local newspaper reports suggest another car failed to yield to Uber’s SUV, hitting it and resulting in the autonomous vehicle flipping onto its side. Presumably the Uber driver was unable to take over the controls in time to prevent the accident. The third-gen self-driving test cars were redeployed to Arizona from San Francisco in December, after Uber refused to bend to California regulators’ will and seek a permit for testing autonomous driving in the state. It claimed it does not need the permit as all its self-driving test vehicles have a human driver in them too. Given that the Uber vehicle has flipped onto its side it looks to be a high speed crash, which suggests a pretty serious incident versus the mostly minor accidents detailed by Google’s longer-lived self-driving car unit, Waymo, such as low speed rear-end shunts of the vehicles (by human-powered cars driving behind them). Another concerning incident involving an Uber self-driving car occurred last December in California when one of its vehicles ran a red light — though Uber blamed this on human error, rather than the fault of the self-driving tech. (Although a New York Times report, citing two Uber sources, claimed the opposite.) Leaked internal documents have also suggested Uber’s self-driving technology isn’t making steady improvements.
48 __________________________________________________
A self-driving shuttle bus in Las Vegas was involved in a crash on its first day of service. The vehicle, carrying several passengers, was hit by a lorry driving at slow speed. Nobody was injured in the incident which city officials say was the fault of the human driver of the lorry. The man was subsequently given a ticket by police. The shuttle is the first of its kind to be used on public roads in the US. The Las Vegas shuttle, designed to ferry passengers to the famous strip, uses a system developed by Navya, a French company also testing its technology in London. The shuttle carries up to 15 people and has a maximum speed of 30 mph (48km/h), but typically travels at around 15 mph (24km/h). A spokesman for the City of Las Vegas told the BBC the crash was a “fender bender” - a minor collision - and that the shuttle would likely be back out on the road on Thursday after some routine diagnostics tests. “A delivery truck was coming out of an alley,” public information officer Jace Radke said. "The shuttle did what it was supposed to do and stopped. Unfortunately the human element, the driver of the truck, didn’t stop.”
49 __________________________________________________
According to software engineer Sebastian Thrun, the car has covered 140,000 miles with no accidents, other than a bump at traffic lights from a car behind.
50 __________________________________________________
"Google announced last month that its prototype self-driving cars would take to the public roads this summer around its headquarters in Mountain View, California. It also recently revealed that its cars had been involved in 13 minor accidents over the six years of tests."
51 __________________________________________________
Walter Huang, 38, was killed in a crash while using Tesla's Autopilot function. The confusion between fully autonomous self-driving cars and those that simply offer driver assistance technologies is leading to deaths on the road. A Tesla driver in the UK was recently prosecuted for climbing into the passenger seat of his car while it was moving at around 40mph (64km/h) in motorway traffic. He was using Tesla's Autopilot, a system that does allow the car to accelerate, brake and steer by itself on major roads, but is not designed to completely replace a driver. Tesla's Autopilot in particular has been implicated in a number of high profile crashes, two of them fatal. The company denies claims that the Autopilot name itself encourages drivers to hand over control, and has rejected demands from the German government to stop using the term. Nevertheless, since 2016 Tesla's systems have included more prominent warnings to drivers to keep their hands on the wheel, and have been able to lock them out of Autopilot if they fail to do so. Earlier this year, a woman was killed in Arizona by an Uber test car being driven in autonomous mode. It failed to stop when she moved into its path.
52 __________________________________________________
This problem is illustrated in a recent accident report published by the California DMV which described how a Google AV (autonomous vehicle) and its test driver exhibited "an abundance of caution" at a pedestrian crossing. The car braked and another vehicle went into the back of it. The cars sustained damage and the Google test driver was taken to hospital suffering from "minor back pain".
53 __________________________________________________
A self-driving Toyota vehicle collided with a visually-impaired athlete at the Paralympic Games, causing concerns about the limitations of autonomous driving technology. The vehicle was driving at 1 to 2 kilometers per hour around the Olympic Village in Tokyo when it hit the athlete. Following the incident, Toyota’s CEO apologized and stated that autonomous vehicles are not yet realistic for normal roads. The use of the vehicles at the Paralympics has been halted amid an investigation of the incident by the police and the company. The vehicle that hit the athlete was an e-Palette, which was under manual control at the time of the accident. The operator told police they were aware of the person but thought they would stop crossing the street. The athlete hurt his head and legs and was treated inside the athlete’s village. He will miss the men's judo match, which he was scheduled to compete in. Toyota expressed their sincerest apologies to the individual that was injured and wished them a speedy recovery.
54 __________________________________________________
A second crash involving a Tesla car - which includes a self-driving feature known as Autopilot - is being investigated by the US authorities. The accident in Pennsylvania left the driver and his passenger injured. The carmaker said that there was "no evidence" that Autopilot was responsible. It follows an investigation into a fatal accident in Florida where the focus is on the apparent failure of Tesla's technology. In the incident in Pennsylvania, the Model X car hit a guard rail and veered into the eastbound lane, ending up on its roof. In a statement, Tesla said: "Based on the information we have now, we have no reason to believe that Autopilot had anything to do with this accident." It said that it had received an automated alert from the car indicating airbags had been deployed but never received logs containing details about the state of vehicle controls, which would indicate whether Autopilot was on or off. US car safety regulators are scrutinising the Pennsylvania crash to determine whether the Autopilot system was in use at the time. The National Highway Traffic Safety Administration (NHTSA) published a report last week following the death of 40-year-old Joshua Brown, who was killed while driving a Tesla in Florida. It concluded that both the driver and the Autopilot system failed to detect a large tractor-trailer turning left in front of him.
55 __________________________________________________
Uber has been barred from testing its self-driving cars on public roads in Arizona following the accident last week involving one of its testing vehicles that killed a pedestrian crossing the street in its path. Arizona Governor Doug Ducey released a letter sent to Uber CEO Dara Khosrowshahi in which he described the accident as captured by onboard cameras as “disturbing and alarming.” The governor also directed the Arizona Department of Transportation to “suspend” Uber’s self-driving testing access. Uber had already suspended testing of its autonomous test vehicles not only in Arizona, but in all markets following the crash and pending the results of its investigation.
56 __________________________________________________
Last month, the company disclosed its first accident, according to a report filed with the CA DMV. The low-speed accident occurred August 24. The number of accidents involving autonomous vehicles have become more common as companies put more of these self-driving cars on public roads. The vast majority are minor, low-speed incidents. There was just one accident involving a self-driving vehicle (that one was owned by Delphi) reported to the DMV in 2014. So far this year, there have been more than 40 accidents involving self-driving cars reported to CA DMV.
57 __________________________________________________
The news text does not provide any information related to an autopilot accident.
58 __________________________________________________
Uber has pulled its self-driving cars from the roads after an accident which left one of the vehicles on its side. Pictures posted online showed the car on its right side on an Arizona street, next to another badly damaged vehicle. The car - a Volvo SUV - was in self-driving mode at the time of the crash, on Friday, Uber said. No one was hurt. A spokeswoman for the police in Tempe, Arizona said the accident occurred when another vehicle "failed to yield" to the Uber car at a left turn. "There was a person behind the wheel. It is uncertain at this time if they were controlling the vehicle at the time of the collision," spokeswoman Josie Montenegro said. Uber's self-driving cars always have a human in the driving seat who can take over the controls. The company pulled its self-driving vehicles off the road in Arizona at first, followed by test sites in Pennsylvania and California - all three states where it operated the vehicles.
59 __________________________________________________
The text does not provide specific details about an autopilot accident.
60 __________________________________________________
Electric carmaker Tesla reported that a vehicle involved in a fatal crash in California was in Autopilot mode. The Model X car crashed into a roadside barrier and caught fire on 23 March. Tesla confirmed that Autopilot was engaged at the time of the accident involving the 38-year-old driver, who died soon afterwards. However, they did not specify whether the system had detected the concrete barrier. The driver had received several visual and one audible hands-on warning earlier in the drive, and his hands were not detected on the wheel for six seconds prior to the collision. The vehicle logs show that no action was taken by the driver despite having about five seconds and 150m of unobstructed view of the concrete divider. In 2016, a Tesla driver was killed in Florida when his car failed to spot a lorry crossing its path, leading the company to introduce new safety measures. Federal investigators said last year that Tesla "lacked understanding" of the semi-autonomous Autopilot's limitations.
61 __________________________________________________
A U.S. federal agency is investigating a crash involving a 2022 Tesla Model S that may have been operating in Autopilot during a crash that killed three people. Autopilot is Tesla’s advanced driver-assistance system (ADAS) that performs automated functions such as steering, accelerating and automatic braking. The accident, which happened earlier this month, occurred in Newport Beach, California when the Tesla hit a curb and slammed into construction equipment, leaving the car totaled. This is one of more than 30 crashes being investigated by the National Highway Traffic Safety Administration (NHTSA), all of which potentially have involved Autopilot. Out of the 35 special crash investigations into Tesla since 2016 involving the electric vehicle company’s ADAS, Autopilot has been ruled out only in three. A total of 14 crash deaths have been reported in those investigations. This month’s collision is the 42nd included in NHTSA’s special crash investigation of ADAS systems like Autopilot, a probe that began in 2016 after a fatal accident in Florida involving another Tesla Model S that had Autopilot activated.
62 __________________________________________________
Since news broke of the fatal accident involving a Tesla Model S that was on “Autopilot,” the media and various experts have been locked in high-speed Socratic dialogs on the pros and cons of self-driving cars. For the time being, insurance coverage for cars using self-driving and driver-assist technology (which is what the ill-fated driver of the Tesla was using) will be the same as coverage for traditional vehicles. Thanks to this, premiums are likely to rise in the short term. Why? Because there’s a real possibility that widespread use of “semi-autonomous” systems, which leave open the possibility of human error, will produce more accidents, not fewer.
63 __________________________________________________
One of Google’s self-driving cars recently was involved in one of the most damaging driverless car collisions yet. It wasn’t the autonomous car’s fault when a driver ran a red light and collided with the side of the vehicle, causing the airbags to deploy, but it was a scenario the car did not know how to avoid. The driver took over and applied the brakes, but it was too late to prevent a collision. The traffic light was green for six seconds before the self-driving car pulled through — and it was still hit. As self-driving cars are released to the public, there may be accidents involving that “human element” as the public adapts to the technology. Insurers will have to cover these types of collisions — perhaps, ones similar to Google’s self-driving car accident — in the short term. In the future, there also will be new risks to insure, such as sensor damage, satellite failure and other new technology.
64 __________________________________________________
Federal safety regulators are investigating at least 11 accidents involving Tesla cars using Autopilot or other self-driving features that crashed into emergency vehicles when coming upon the scene of an earlier crash. The National Highway Transportation Safety Administration said seven of these accidents resulted 17 injuries and one death. All of the Teslas in question had the self-driving Autopilot feature or the traffic-aware cruise control engaged as they approached the crashes. The accidents under investigation occurred between January 22, 2018, and July 10, 2021, across nine different states. They took place mostly at night, and the post-accident scenes all included control measures like first responder vehicle lights, flares, an illuminated arrow board and road cones. The safety of Tesla’s Autopilot feature has been questioned before. The National Transportation Safety Board found Autopilot partly to blame in a 2018 fatal crash in Florida that killed a Tesla driver. Police in a Houston suburb said there was no one in the driver’s seat of a Tesla that crashed and killed two people in the car earlier this year, a charge that Tesla has denied. But Lars Moravy, Tesla’s vice president of vehicle engineering, confirmed in April in comments to investors that Tesla’s adaptive cruise control was engaged and accelerated to 30 mph before that car crashed. The safety agency said its investigation will allow it to “better understand the causes of certain Tesla crashes,” including “the technologies and methods used to monitor, assist, and enforce the driver’s engagement with driving while Autopilot is in use.” It will also look into any contributing factors in the crashes. The investigation involves the Tesla Y, X, S and 3 with model years 2014 to 2021.
65 __________________________________________________
Tesla is planning to expand its autonomous driving software despite a fatal Tesla crash in Spring, Texas, last month. Local police initially reported that no one was in the Tesla’s driver’s seat at the time of the crash, which killed two men. However, a preliminary federal investigation revealed that security camera footage showed the Tesla’s owner getting into the driver’s seat at the beginning of the trip. The car traveled about 550 feet before crashing. It’s unclear if the driver had time to climb out of the driver’s seat before the crash, or if the driver climbed into the rear seat following the crash. The initial claim had raised questions about the safety of Tesla’s Autopilot, its suite of driver assistance features. Tesla tells its drivers to remain in the driver’s seat and remain attentive when Autopilot is active. The National Transportation Safety Board, which is conducting the investigation into the fatal crash in Texas, has not concluded if Autopilot was active at the time of the crash. The NTSB tested a Model S P100D, the same Tesla model involved in the wreck, and found that adaptive cruise control, one of Autopilot’s feature, could engage, but Autosteer, the other Autopilot feature, which steers a vehicle, could not. The National Highway Traffic Safety Administration (NHTSA) is also investigating, and has 28 active investigations into Tesla’s advanced driver assistance system. NHTSA is also investigating a May 5 crash in which a Tesla crashed into an overturned semi truck on a state road near Fontana, California, killing the Tesla driver. It’s unclear if Autopilot was active in the crash.
66 __________________________________________________
While Cruise’s autonomous vehicles (AVs) have not been in any fatal accidents while driving in San Francisco, the company has come under fire for malfunctioning vehicles stopping in the middle of traffic, blocking other vehicles, emergency vehicles and public transit. A mix of annoyed residents, San Francisco’s fire chief, the police officers’ association and the SFMTA have all expressed concerns about the safety and efficacy of AVs after many such incidents. In response to Cruise vehicles becoming immobilized while operating on public roads and incidents when the robotaxis may have engaged in inappropriately hard braking, NHTSA last December opened a preliminary investigation into the vehicles.
67 __________________________________________________
Autonomous trucking company TuSimple addressed an April crash during its second-quarter earnings call. The crash occurred when one of the company’s autonomous trucks suddenly veered across the I-10 highway in Tuscon, slamming into a concrete barricade. The accident was first revealed via a YouTube video and later reported on by The Wall Street Journal. TuSimple co-founder and CEO, Xiaodi Hou, explained that the accident occurred when a test driver and safety engineer tried to reenter autonomous driving mode before the system computer was ready, causing the truck to swerve and hit the highway barrier. No one was hurt in the accident. Following the incident, TuSimple grounded its entire fleet and began an independent investigation. The company's internal report revealed that the truck swerved due to an outdated command, which should have been erased from the system but wasn’t. The National Highway Traffic Safety Administration has since joined the Federal Motor Carrier Safety Administration’s investigation into the TuSimple highway crash. Despite the accident, TuSimple said it wouldn't affect its plans to begin driver-out operations for Union Pacific Railroad.
68 __________________________________________________
The news text does not contain any information related to an autopilot accident.
69 __________________________________________________
Uber halted all of its autonomous vehicle operations March 19, the day after one of its vehicles struck and killed pedestrian Elaine Herzberg in the Phoenix suburb of Tempe. Uber was testing its self-driving vehicles on public roads in Tempe, Ariz., where the accident occurred, as well as in Pittsburgh, San Francisco and Toronto. In the days and weeks following the fatal accident, it appeared the company’s self-driving vehicle program might end for good. Arizona Governor Doug Ducey, a proponent of autonomous-vehicle technology who invited Uber to the state, suspended the company from testing its self-driving cars following the accident. Investigators determined that Rafaela Vasquez, who was operating the Uber self-driving vehicle involved in the fatal crash, looked down at a phone that was streaming The Voice 204 times during a 43-minute test drive that ended when Herzberg was struck and killed, according to a 318-page police report released by the Tempe Police Department. Based on the data, police reported that Vasquez could have avoided hitting Herzberg if her eyes were on the road. The case has been submitted to the Maricopa County Attorney’s office for review against Vasquez, who could face charges of vehicular manslaughter. The National Transportation Safety Board is also investigating the accident. A preliminary report by the NTSB found Uber’s modified Volvo XC90’s LiDAR and radar first spotted an object in its path about six seconds before the crash. The self-driving system first classified the pedestrian as an unknown object, then as a vehicle and then as a bicycle. At 1.3 seconds before impact, the self-driving system determined that an emergency braking maneuver was needed to mitigate a collision, according to the NTSB. But to reduce the potential for “erratic behavior,” Uber had disabled Volvo’s emergency braking system so it didn’t work when the vehicle was under computer control.
70 __________________________________________________
Tesla’s Autopilot was involved in a third fatal motorcycle crash this summer, raising questions about the driver-assist system’s ability to operate safely. The National Highway Traffic Safety Administration has already launched investigations into the first two crashes and gathered information on the third crash. The three fatal crashes occurred in a 51-day span this summer and follow a similar line of events: A person driving a Tesla in the early morning hours with Autopilot active strikes a motorcycle. The crashes renew questions about whether users of the systems are kept sufficiently engaged and prepared to fully control the vehicle when needed. Ingrid Eva Noon was riding her motorcycle in Palm Beach County, Florida at 2:11 a.m. on Aug. 26 when an impaired driver using Tesla’s Autopilot impacted the rear of Noon’s motorcycle, throwing her onto the Tesla’s windshield and killing her. Utah resident Landon Embry was killed while riding his Harley-Davidson on July 24 at approximately 1:09 am when a Tesla driver using Autopilot collided with the back of his motorcycle. A Tesla driver using Autopilot struck a motorcycle lying on a road on July 7 at 4:47 a.m in Riverside, California. The motorcyclist, who had already fallen off the bike after hitting a dividing wall, was killed. The recent crashes suggest the Tesla system is insufficient, according to motorcycle advocates. Motorcycle safety advocates say they’re concerned that the software fails to see motorcycles and lulls Tesla drivers into a sense of complacency and inattentiveness. The advocates say that the government’s vehicle safety regulations do not adequately protect motorcycle riders and that steps should be taken to better protect them, including testing driver-assist systems like Autopilot for motorcycle detection.
71 __________________________________________________
Uber CEO Dara Khosrowshahi spoke about Uber's self-driving cars in light of the fatal accident in Tempe, Arizona in March. Khosrowshahi said Uber will bring back its self-driving cars “within the next few months. Uber pulled its self-driving cars off the roads following the March fatal crash. Later that month, Uber decided not to reapply for a self-driving car testing permit in California. Uber’s previous permit expired March 31. If Uber wants to continue its tests in California, it will need to apply for a new permit, as well as “address any follow-up analysis or investigations from the recent crash in Arizona,” DMV Deputy Director/Chief Counsel Brian Soublet wrote in a letter to Uber in March. The Information reported Uber’s software was at fault. Specifically, it was reportedly the fault of the software that determines which objects to ignore and which to attend to. Following the report, Uber said it’s actively cooperating with the NTSB and can’t comment on the specifics of the accident.
72 __________________________________________________
On November 19, the California Department of Motor Vehicles suspended Pony.ai's driverless testing permit following a reported collision in Fremont on October 28. Pony.ai, a Chinese autonomous driving startup, has paused its driverless pilot fleet in California after the incident. According to Pony.ai’s collision report, the incident took place on a clear morning when its driverless vehicle was changing lanes using the autonomous mode. The vehicle experienced a collision with a lane divider and street sign, with no other vehicles involved and no injuries occurred. This incident stands out because the vehicle was in autonomous mode and didn’t involve any other vehicle, putting a question mark on Pony.ai’s autonomous driving capabilities.
73 __________________________________________________
Tesla has provided an update to last week’s fatal crash involving its Autopilot system. According to the company, the driver had Autopilot on with the adaptive cruise control follow-distance set to minimum and ignored the vehicle’s warnings to take back control. “The driver had received several visual and one audible hands-on warning earlier in the drive and the driver’s hands were not detected on the wheel for six seconds prior to the collision,” Tesla wrote in a blog post. The driver had about five seconds and 150 meters of unobstructed view of the concrete divider with the crushed crash attenuator, but the vehicle logs show that no action was taken. The crash was severe because the middle divider on the highway had been damaged in an earlier accident. Tesla cautioned that Autopilot does not prevent all accidents, but it does make them less likely to occur.
74 __________________________________________________
"It comes as safety concerns about self-driving cars persist after testing problems and some accidents. Some vehicle tests have gone awry and computer error was deemed partly to blame in a crash involving a Tesla vehicle last year. On Wednesday, a self-driving car operated by Argo AI, a firm backed by Ford, was involved in a crash in Pittsburgh that led to two people being hospitalised. Early reports suggest the accident was the result of human error. Ford declined to comment on whether the vehicle was in self-driving mode at the time, or whether Argo has stopped testing the cars while it investigates the crash. In an email to tech news site the Verge, it said: "We're aware that an Argo AI test vehicle was involved in an accident. We're gathering all the information. Our initial focus is on making sure that everyone involved is safe."
75 __________________________________________________
The U.S. National Highway Traffic Safety Administration (NHTSA) is investigating a report that a Tesla Model Y was involved in an accident while using the company’s Full Self-Driving (FSD) Beta software. The owner of the 2021 Tesla Model Y reported that on Nov. 3 in Brea, California, the vehicle was in FSD Beta mode and while taking a left turn, the car went into the wrong lane and was hit by another driver. The car gave an alert halfway through the turn and the driver tried to assume control, but the car took control and forced itself into the incorrect lane, resulting in severe damage on the driver side. Earlier this month, Tesla recalled nearly 12,000 U.S. vehicles because of a communication error that could trigger a false collision warning or unexpected automatic emergency brake. The recall was prompted after a software update to vehicles with FSD Beta. Last month, NHTSA raised concerns about how FSD was being used on public roads. In August, NHTSA opened a formal safety probe into Tesla’s Autopilot, a different driver assistance software system, after a dozen crashes involving Tesla models and emergency vehicles.
76 __________________________________________________
While law-abiding driverless cars, with all their cameras, sensors, radars and faster-than-human reaction times, could undoubtedly help reduce accidents (90% of which currently caused by driver error), no-one is foolish enough to believe that they will be flawless. They will occasionally crash and perhaps even kill people. A few of the ones being tested in the US have already been involved in crashes. So if you own the car, are you liable? Or is it the car manufacturer? Or is it the maker of the specific piece of equipment that failed? Or is it the software company? "There are some practical and legal issues that need to be addressed," says Ben Howarth, policy advisor at the Association of British Insurers, "particularly about where ultimate responsibility for an autonomous vehicle will rest. "If car drivers are replaced entirely by passengers who have no way of overriding or controlling the systems in the vehicle, it's possible manufacturers would become liable in the case of accidents." "We simply don't have the answers yet," admits Andreas Gissler, managing director of Accenture's Automotive practice. Stephan Appt, partner at law firm Pinsent Masons says: "The key issue is to find a means to enable car drivers to establish that it was not them, or at least not a technical defect of their car, when a crash occurs. "There will be a need for event data recorders - like aeroplane black boxes - to be built into the cars. This, however, raises privacy concerns. Who shall own the data in the data recorder and who shall have the right to claim access to it?" Sorting all this out will take years and lots of legal wrangling, not to mention new regulations at national and international level.
77 __________________________________________________
A full page advertisement in Sunday’s New York Times took aim at Tesla’s “Full Self-Driving” software, calling it “the worst software ever sold by a Fortune 500 company” and offering $10,000, the same price as the software itself to the first person who could name “another commercial product from a Fortune 500 company that has a critical malfunction every 8 minutes.” The ad was taken out by The Dawn Project, a recently founded organization aiming to ban unsafe software from safety critical systems that can be targeted by military-style hackers, as part of a campaign to remove Tesla Full Self-Driving (FSD) from public roads until it has “1,000 times fewer critical malfunctions.”

Tesla’s FSD beta software, an advanced driver assistance system that Tesla owners can access to handle some driving function on city streets, has come under scrutiny in recent months after a series of YouTube videos that showed flaws in the system went viral. The California Department of Motor Vehicles told Tesla it would be “revisiting” its opinion that the company’s test program, which uses consumers and not professional safety operators, doesn’t fall under the department’s autonomous vehicle regulations. The U.S. National Highway Traffic Safety Administration (NHTSA) is investigating a report from the owner of a Tesla Model Y, who reported his vehicle went into the wrong lane while making a left turn in FSD mode, resulting in the vehicle being struck by another driver.

Even if that was the first FSD crash, Tesla’s Autopilot, the automaker’s ADAS that comes standard on vehicles, has been involved in around a dozen crashes. The Dawn Project published a fact check of its claims, referring to its own FSD safety analysis that studied data from 21 YouTube videos totaling seven hours of drive time. The study found that FSD v10 committed 16 scoring maneuver errors on average in under an hour and a critical driving error about every 8 minutes.

Federal regulators have started to take some action against Tesla and its Autopilot and FSD beta software systems. In October, NHTSA sent two letters to the automaker targeting the its use of non-disclosure agreements for owners who gain early access to FSD beta, as well as the company’s decision to use over-the-air software updates to fix an issue in the standard Autopilot system that should have been a recall.

Reviews of the latest version 10.8 are skewed, with some online commenters saying it’s much smoother, and many others stating that they don’t feel confident in using the tech at all. A thread reviewing the newest FSD version on the Tesla Motors subreddit page shows owners sharing complaints about the software, with one even writing, “Definitely not ready for the general public yet…” Another commenter said it took too long for the car to turn right onto “an entirely empty, straight road…Then it had to turn left and kept hesitating for no reason, blocking the oncoming lane, to then suddenly accelerate once it had made it onto the next street, followed by a just-as-sudden deceleration because it changed its mind about the speed and now thought a 45 mph road was 25 mph.” The driver said it eventually had to disengage entirely because the system completely ignored an upcoming left turn, one that was to occur at a standard intersection “with lights and clear visibility in all directions and no other traffic.”

The Dawn Project’s campaign highlights a warning from Tesla that its FSD “may do the wrong thing at the worst time.” “How can anyone tolerate a safety-critical product on the market which may do the wrong thing at the worst time,” said the advocacy group. “Isn’t that the definition of defective? Full Self-Driving must be removed from our roads immediately.”
78 __________________________________________________
One of the test cars Argo AI operates in its Pittsburgh fleet was involved in an accident on Wednesday, with two people sent to hospital (in stable condition) with injuries as a result of the accident. The incident appears to have been caused by a light box truck driving through a red light, rather than by Argo’s test vehicle, based on an early traffic report. The self-driving car was T-boned by the truck, and two of the Argo car’s four occupants were the ones who ended up in hospital. Ford provided the following statement regarding the accident when contacted by TechCrunch: We’re aware that an Argo AI test vehicle was involved in an accident. We’re gathering all the information. Our initial focus is on making sure that everyone involved is safe.
79 __________________________________________________
NHTSA launched an investigation this summer into Teslas rear-ending emergency vehicles while using Autopilot. Chase said she’s concerned that the Tesla technology may play a role in other types of crashes we aren’t yet aware of. Autonomous driving experts have long cautioned that Tesla’s description of “full self-driving,” and its more rudimentary predecessor, Autopilot, may lead to drivers putting too much trust in the technology. Tesla drivers have already died in high-profile crashes using Autopilot, drawing rebukes from the National Transportation Safety Board. US Senators Ed Markey (D-Mass.) and Richard Blumenthal (D-Conn.), have called on the Federal Trade Commission to investigate Tesla and take enforcement action because Tesla’s marketing overstates its vehicles’ abilities, they say. “When drivers’ expectations exceed their vehicle’s capabilities, serious and fatal accidents can and do result,” they wrote.
80 __________________________________________________
When Joshua Brown’s Tesla Model S collided with a semitrailer truck on a Florida highway in May, it became the first known fatality in an autonomous vehicle. Mr. Brown served in the United States Navy for over a decade, and had a long life ahead of him passing at the young age of 40. The outcome of the government investigation into the Tesla collision has the potential to profoundly impact the future of the autonomous vehicle and self-driving industries. Tesla recently came out with its statement, saying that the accident was a rare circumstance, that neither a human nor the autopilot system could have foreseen, and that Model S vehicles are still safer than human drivers on the whole. The National Highway Traffic Safety Administration (NHTSA) — the federal body responsible for preventing crashes and investigating this accident — will certainly have more to say on this topic. Their stance and mandated regulations will have a strong impact on the future of this important technical innovation.
81 __________________________________________________
A self-driving taxi being trialled in Singapore has had its first accident. nuTonomy, the firm behind the scheme, said the car had clipped a small lorry while driving at about four miles per hour. It played down the incident as a "small prang", saying only minor damage was caused and that neither of the two engineers on board were hurt. The Singapore scheme, which began in August, was the first around the world trialling driverless taxis. The nuTonomy spokesperson said that given this was a trial, small accidents were not unexpected and that the idea of the testing was to learn from what went wrong. The project is using six small Renault and Mitsubishi electric vehicles, equipped with the company's software and cameras. Each has a system of lasers which operate like a radar to monitor the car's surroundings. But while the vehicles are doing the driving themselves in a small area of the city, drivers are there to monitor the performance and as a backup in case something goes wrong. Currently the vehicles are not taking paying passengers.
82 __________________________________________________
The recent crash involving a Google self-driving car and a bus was "not a surprise", the US transport secretary has said. Anthony Foxx told the BBC that accidents were inevitable, but that the emerging technology should not be compared "against perfection". Nobody was hurt in the crash, but it was the first time Google's on-board computer has been blamed for causing a collision. One of Google's self-driving cars crashed into a bus in California earlier this year. On the public perception of self-driving cars following the February Google crash, he said: "It's not a surprise that at some point there would be a crash of any technology that's on the road." One challenge would be to tackle the legal issue of responsibility when crashes occur - and whether the passenger should be liable, or, given that the computer was driving, the companies behind the software that failed.
83 __________________________________________________
A Waymo self-driving vehicle was involved in a serious accident in Chandler, Arizona earlier this afternoon. Local police said there were minor injuries from the incident after a sedan swerved into the Waymo van to avoid another collision. Although Waymo has said it will be testing vehicles without safety drivers in Arizona, this was not one of them. An operator was in the driver’s seat at the time of the crash, though the car was in autonomous mode, police said. The sedan’s front crumple zone is wrecked and glass is broken; the van is in better shape, though its front right tire is crushed in. Both vehicles have since been towed. Reportedly the sedan was traveling eastbound and swerved to avoid another car at an intersection, straying into the westbound lanes and hitting the Waymo van. What actions if any the latter took to avoid the collision are unknown at this time. The police provided me with the following statement, which doesn’t add much but confirms the above: We are currently investigating a minor injury collision involving two vehicles, one of which is a Waymo autonomous vehicle. This afternoon around noon a vehicle (Honda sedan) traveling eastbound on Chandler Blvd. had to swerve to avoid striking a vehicle traveling northbound on Los Feliz Dr. As the Honda swerved, the vehicle continued eastbound into the westbound lanes of Chandler Blvd. & struck the Waymo vehicle, which was traveling at a slow speed and in autonomous mode. There was an occupant in the Waymo vehicle sitting in the driver’s seat, who sustained minor injuries. Both the Waymo vehicle & the Honda were towed from the scene. This incident is still under investigation. Waymo has also provided the following statement and video of the accident from the van’s point of view: Today while testing our self-driving vehicle in Chandler, Arizona, another car traveling in an oncoming lane swerved across the median and struck our minivan. Our team’s mission is to make our roads safer – it is at the core of everything we do and motivates every member of our team. We are concerned about the well-being and safety of our test driver and wish her a full recovery.
84 __________________________________________________
One of Google's self-driving cars crashed into a bus in California earlier this month. There were no injuries. It is not the first time one of Google's famed self-driving cars has been involved in a crash, but it may be the first time it has caused one. Google is to meet with California's Department of Motor Vehicles (DMV) to discuss the incident, and determine where the blame lies. On 14 February the car, travelling at 2mph (3km/h), pulled out in front of a public bus going 15mph (24km/h). The human in the Google vehicle reported that he assumed the bus would slow down to let the car out, and so he did not override the car's self-driving computer. The crash happened in Mountain View, near Google's headquarters. In a statement, Google said: "We clearly bear some responsibility, because if our car hadn't moved, there wouldn't have been a collision. That said, our test driver believed the bus was going to slow or stop to allow us to merge into the traffic, and that there would be sufficient space to do that." The company's self-driving cars have clocked up well over a million miles across various states in the US, and until now have only reported minor "fender benders" - the American slang for a small collision. In all of those cases, other road users were to blame. Google releases a monthly report detailing the testing of its self-driving technology. Ahead of the February report's publication, due Tuesday, a traffic incident filing was made public by the DMV. "The Google AV [autonomous vehicle] test driver saw the bus approaching in the left side mirror but believed the bus would stop or slow to allow the Google AV to continue," the report read. "Approximately three seconds later, as the Google AV was re-entering the centre of the lane it made contact with the side of the bus. The Google AV was operating in autonomous mode and travelling at less than two mph, and the bus was travelling at about 15mph at the time of contact." The car's movements were made more complex, the report said, by the presence of sandbags on the road. Google said it had now refined its self-driving algorithm. "From now on, our cars will more deeply understand that buses (and other large vehicles) are less likely to yield to us than other types of vehicles, and we hope to handle situations like this more gracefully in the future." If the DMV considers the Google car to be at fault for the collision, it could be seen as a setback for the company's ambitious autonomous vehicle plans. The bus crash came just four days after a legal breakthrough for the self-driving project - the US National Highway Traffic Safety Administration told Google it would likely give the self-driving computer the same legal treatment as a human driver. That decision would pave the way for self-driving cars without any typical controls, such as a steering wheel or pedals.
85 __________________________________________________
The National Transportation Safety Board is opening an investigation into the fatal accident involving one of Uber’s self-driving cars in Tempe, Arizona. Uber’s self-driving car accident that resulted in a woman’s death raises a number of questions about insurance and liability. Although the car was in self-driving mode, there was a safety driver behind the wheel who theoretically should have been able to intervene. Uber has since halted its self-driving car tests in Arizona, Pittsburgh and California. Last year, the NTSB looked into a 2016 accident involving Tesla’s Autopilot system in Florida. The NTSB partially faulted Tesla for the fatal crash, saying the system operated as intended but that the driver’s inattentiveness, due to over-reliance on the Autopilot system, resulted in the accident. In an earlier statement to TechCrunch, an Uber spokesperson said, “Our hearts go out to the victim’s family. We are fully cooperating with local authorities in their investigation of this incident.”
86 __________________________________________________
While Lyft works to add autonomous vehicles to its network, many of the company’s drivers today are potentially augmented by Level 2 advanced driver assistance systems, known as ADAS, including Tesla’s Autopilot and possibly its Full Self Driving (FSD) software. These systems automate certain driving functions, but drivers still need to keep their hands on the wheel and eyes on the road while they’re engaged. Tesla has come under fire from regulatory bodies due to issues with its ADAS, which has been linked to several accidents. Drivers have also sued Tesla, claiming it falsely advertises the autonomous capabilities of its software. When Korosec asked Zimmer asked whether Lyft had considered prohibiting the use of Level 2 ADAS like Autopilot or FSD, he said that Lyft “think[s] that the regulatory bodies are our best regulators when it comes to that level of safety.” When pressed, Zimmer said that Lyft would “continue to assess” its policy regarding driver use of Level 2 autonomous assistants. “Obviously, driver and rider safety is our top priority. And so to your point, it’s something that will continue to be looked at.”
87 __________________________________________________
The Tesla Model S that braked sharply and triggered an eight-car crash in San Francisco in November had the automaker’s controversial driver-assist software engaged within 30 seconds of the crash, according to data the federal government released Tuesday. The Tesla Model S slowed to 7 mph on the highway at the time of the crash, according to the data. Publicly released video also showed the car moving into the far-left lane and braking abruptly. The Tesla’s driver told authorities that the vehicle’s “full self-driving” software braked unexpectedly and triggered the pileup on Thanksgiving day. The National Highway Traffic Safety Administration then announced that it was sending a special crash investigation team to examine the incident. The pileup took place hours after Tesla CEO Elon Musk announced that its “full self-driving” driver-assist system was available to anyone in North America who requested it and had paid for the option. Tesla’s driver-assist technologies, Autopilot and “full self-driving” are already being investigated by the National Highway Traffic Safety Administration following reports of unexpected braking that occurs “without warning, at random, and often repeatedly in a single drive,” the agency has said in a statement. The agency has received hundreds of complaints from Tesla users. Some have described near crashes and concerns for their safety. Bryan Reimer, an autonomous vehicle researcher with the Massachusetts Institute of Technology’s AgeLab, told CNN Business the revelation that driver-assist technology was engaged raises questions about when NHTSA will act on its investigation, and what the future holds for Tesla’s driver-assist features. “How many more crashes will there be before NHTSA releases findings?” Reimer said. Reimer said it remains to be seen if there’s a recall of any Tesla driver-assist features, and what it means for the automaker’s future. Musk has said before the company would be “worth basically zero” if it doesn’t provide “full self-driving.” This story has been updated to reflect that a driver-assist system was active within 30 seconds of the crash.
88 __________________________________________________
Google’s autonomous vehicles have now logged more than one million miles on roads dominated by human-driven cars. Subjected to the same real-world road conditions as us mere mortals, self-driving cars have been through rain, sleet and snow. Autonomous vehicles have driven the equivalent of circumnavigating the globe 40 times — without incident. In fact, self-driving machines have been hit by human drivers 11 times, but never been the cause of a single accident. According to the data, human driver error is responsible for 94 percent of all crashes across the planet. And regardless of the amount of education or training, human behavior behind the wheel is not improving. Alcohol is now responsible for more than a third of all traffic-related fatalities worldwide. In the United States, one out of four accidents is caused by texting and driving (which is six times more likely to cause an accident than driving drunk). The more technology we put in human hands, the worse our driving habits become. And unlike robots, humans need rest. According to the National Sleep Foundation, 69 percent of adult drivers report driving while drowsy at least once a month. Autonomous vehicles don’t drive drunk, don’t drive distracted and don’t fall asleep at the wheel. Self-driving cars are wired with cameras, infrared sensors, networked maps and a host of other software, which empowers them to accurately avoid dangers in ways humans can’t. They can brake faster, swerve quicker and anticipate changes in road conditions that are imperceptible to the human eye (such as obstacles beyond the visible range of headlights). Robots also communicate with each other more efficiently and effectively than human-navigated vehicles.
89 __________________________________________________
The recent announcement between VW Group, Intel’s computer vision subsidiary Mobileye and Champion Motors was treated no differently. The Volkswagen Group, Intel’s Mobileye and Champion Motors said they plan to deploy Israel’s first self-driving ride-hailing service in 2019 through a joint venture called New Mobility in Israel. The New Mobility in the Israel group’s proposal was formally accepted by the Israeli government during a private ceremony at the Smart Mobility Summit 2018 in Tel Aviv last Monday. The group will begin testing next year in Tel Aviv and roll out the service in phases until reaching full commercialization in 2022. (Intel and Mobileye began testing self-driving cars in Jerusalem in May 2018.) The project will begin with dozens of self-driving vehicles — each one with safety drivers behind the wheel. An early rider program, which would give vetted members of the public access to the service, will likely launch in 2021, one unnamed source familiar with the deal told TechCrunch. Each of the three companies in the joint venture is providing a piece of this self-driving vehicle business puzzle: Champion Motors will run the fleet operations and control center; VW is going to supply the electric vehicles; and Mobileye is handling the self-driving system. All three companies will add the mobility platform and services to be able to deploy a commercial service. The Israeli government is also contributing, with plans to provide legal and regulatory support, share the required infrastructure and traffic data and provide access to infrastructure as needed.
90 __________________________________________________
Douglas Rushkoff: Man died in self-driving Tesla crash, a reminder that humans and autonomous vehicles are incompatible. The latest permutation on this theme occurred in May, when a self-driving Tesla-S failed to register the side of a white tractor-trailer truck against a pale sky. In its statement on the accident, Tesla is quick to remind us that the 40-year-old man killed in the crash was a technology consultant and autonomous vehicle enthusiast – as if a martyr for the greater cause of civic transportation. If anything, the cause of the crash can be chalked up to the incompatibility between humans and autonomous vehicles. Had the tractor-trailer also been driven by computer, it could have been on the same network as the Tesla. Like an air traffic control system, the network could have orchestrated the safe passage of both vehicles. The problems emerge when computerized vehicles don’t have such networking at their disposal. Instead, we’re asking the poor Tesla to drive using the same senses mere humans use - which is why the car missed the fact that its entire field of vision was occupied not by sky, but by truck. As autonomous vehicle proponents like to point out, these problems would be solved if robotic cars weren’t required to share the road with humans. We people are the problem.
91 __________________________________________________
Figures released earlier this week show that four out of the 48 self-driving cars tested on public roads in California have been involved in accidents in the last eight months. The car makers involved, Google and car parts maker Delphi, said the bumps were the fault of humans in other cars. The Google cars involved in these earlier tests are modified Lexus SUVs rather than the purpose-built robot cars.
92 __________________________________________________
The news text does not contain any information related to an autopilot accident.
93 __________________________________________________
General Motors has settled a legal action brought by a motorcyclist knocked over by one of the carmaker's self-driving vehicles. Oscar Nilsson sued soon after the collision with the GM Cruise car in December 2017. Mr Nilsson sustained neck and shoulder injuries that needed lengthy treatment and caused him to take time off work. GM said the settlement had resulted from a mutual desire by both parties to resolve the legal action. Lawyers for both sides told Reuters that final details of the settlement were still being worked out but the case should be concluded by the end of June. The car moved to change lanes in heavy traffic on a San Francisco motorway last year, according to GM's report, but then aborted its manoeuvre when traffic in the lane it was leaving started moving. It then collided with the motorbike, which was moving into the space the car had been moving out of. GM alleged the motorcyclist was partly at fault for the crash. And a police report did cite him for trying to pass the Cruise car. However, he was not given a ticket. About 40 crashes involving self-driving cars have taken place in California since January 2017. Records show 33 of the cars involved were GM Cruise vehicles, but none has been found to have been responsible for any of the crashes.
94 __________________________________________________
Earlier today, news broke of a fatal crash involving one of Uber’s self-driving cars in Tempe, Arizona. In response, Uber halted its self-driving car programs where it currently operates, including in Pittsburgh, Toronto, San Francisco and Phoenix. The U.S. Department of Transportation National Highway Traffic Safety Administration, Tempe Mayor Mark Mitchell and others have since released statements about the crash. In a statement to TechCrunch, the NHTSA said it has sent over its “Special Crash Investigation” team to Temple. “NHTSA is also in contact with Uber, Volvo, Federal, State and local authorities regarding the incident,” the spokesperson said. “The agency will review the information and proceed as warranted.” Over in Tempe, Mitchell called the accident “tragic,” saying the city grieves for Elaine Herzberg, the woman who lost her life. Moving forward, the city of Tempe and its police department will look into the accident to try to figure out what happened, Mitchell said. In the meantime, Mitchell said he supports the step Uber has taken to temporarily suspend its self-driving tests. Over in California, where Uber has also suspended its self-driving car tests, the DMV says it “takes the safe operation of our autonomous vehicle permit holders very seriously,” a DMV spokesperson said in a statement to TechCrunch. Assemblymember Jim Frazier (D-Discovery Bay), who is also chairman of the Assembly Transportation Committee, also chimed in, saying his “heart goes out to the family of the victim.” Earlier today, the National Safety Transportation Board announced it would conduct its own field investigation. That came after Uber’s statement, in which the company expressed its condolences and said the company is working with local authorities in their investigation.
95 __________________________________________________
A Tesla car has crashed into a parked police car in California. The driver suffered minor injuries and told police she was using the car's driver-assisting Autopilot mode. The crash has similarities to other incidents, including a fatal crash in Florida where the driver's "over-reliance on vehicle automation" was determined as a probable cause. Tesla has said customers are reminded they must "maintain control of the vehicle at all times". In a statement, it added: "When using Autopilot, drivers are continuously reminded of their responsibility to keep their hands on the wheel." As yet, it has still to be confirmed that the Autopilot mode was indeed engaged. The California crash appears to be the latest example of semi-autonomous vehicles struggling to detect stationary objects. A Tesla driving in Autopilot hit a stationary fire engine in Utah in May. According to a police report obtained by the Associated Press, the Tesla accelerated before it hit the vehicle. It has also emerged that a Tesla Model 3 driver has blamed Autopilot for a crash in Greece last Friday, in which the car suddenly veered right "without warning". The motorist, You You Xue, voiced his concerns about Autopilot on Facebook. "The vigilance required to use the software, such as keeping both hands on the wheel and constantly monitoring the system for malfunctions or abnormal behaviour, arguably requires significantly more attention than just driving the vehicle normally," he wrote. It is not the first time the Autopilot feature has been linked to dangerous behaviour. In England, a driver was banned from driving after putting his Tesla in Autopilot on the M1 and sitting in the passenger seat. The news comes after two US rights groups urged the Federal Trade Commission to investigate Tesla over its marketing of the assisted driving software. The Center for Auto Safety and Consumer Watchdog said it should be "reasonable" for Tesla owners to believe that their car should be able to drive itself on Autopilot. It called the naming of the Autopilot "deceptive and misleading". The chief executive of Tesla, Elon Musk, has previously complained abut media attention on Tesla crashes. He tweeted: "It's super messed up that a Tesla crash resulting in a broken ankle is front page news and the ~40,000 people who died in US auto accidents alone in past year get almost no coverage." His comments received support from prominent academic and psychologist Steven Pinker, who has in the past voiced concerns about Tesla's Autopilot.
96 __________________________________________________
In the only car, it was determined humans were at fault, not machines — and yet, therein lies the problem. Accidents happen, and a computer must be programmed to react in those situations, sometimes when death is inevitable. In those instances, it’s succinct to say that we’ll have to program computers to kill. Let’s take a look at another scenario. There is a thought experiment called the that asks you to imagine a runaway trolley headed for a group of five people tied up in its path. You’re standing near a lever, however, that will send the trolley to a different set of tracks if you flip it — the only problem is that there is a person tied up on those tracks, as well. You have two options: Do nothing, letting the trolley kill all five people on the main track, or flip the switch and send the trolley to the side track where it will kill one person. In , facilitated by researchers at Michigan State University, 147 subjects were given 3D headsets so they could actually experience this dilemma in an environment as close to reality as possible. Ninety percent of the participants flipped the switch, saving five people to kill one. This isn’t that surprising, as most people would say that five lives saved over one is ethically the right choice — but what happens when we switch the problem up a little bit? Let’s say there is no side track the trolley will divert to if you flip the switch; instead, you’re standing next to a person large enough to stop the vehicle. The only caveat is that you must push him onto the track. The second variation of the problem produces different results, because there is a perceived difference between somebody and . The trend you come across is that not as many people would choose to kill the large man, even if it meant saving more lives overall, because they don’t want to be held personally responsible for his death. Here’s a third scenario: What if were the large person that could stop the trolley via self-sacrifice? Even better, what if your self-driving car turns a corner only to see a crowd of five people standing in the road? Your car either can hit them, sparing your own life, or the onboard AI can run your car off of the road, killing you and saving five lives.,If you answered that flipping the switch in the first iteration of the trolley problem was the right choice, because one death is better than five, then logically you would agree that your self-sacrifice is necessary to save the lives of the five people in the road ahead of you, right? Interestingly, if you defy the framework of logic and would rather choose self-preservation in this scenario, you’re actually in the majority. Jean-Francois Bonnefon and the Toulouse School of Economics in France that these types of logical fallacies run rampant. As such, they believe it will be interesting to watch public opinion inevitably play a role in deciding how the ethics of AI works. Says Bonnefon and company: “[Participants of our study] actually wished others to cruise in utilitarian autonomous vehicles, more than they wanted to buy utilitarian autonomous vehicles themselves.” Essentially, the problem is that people actually driverless cars to sacrifice the occupant in favor of saving a higher number of lives — but only if they don’t have to drive one themselves. Unfortunately, the biggest catch-22 is that people won’t buy autonomous vehicles if they’re designed to kill their passengers, meaning that the status quo allowing split-second human decisions will continue to define accidents and reactions around the world. If we never legalize self-driving cars, our own human driving will continue to contribute to more than a million deaths globally. Employing a fourth scenario, trend toward self-preservation: You’re driving through a tunnel and a child appears at the opening and trips, blocking your exit. You can’t stop, so you’re left with the choice of swerving into a wall to save the child, or running over the child to save yourself. Of 110 people polled, 64 percent said they would continue straight and kill the child. When asked which entities should determine how an autonomous car responds to the tunnel problem, 44 percent of respondents thought it should be the passenger of the vehicle, while 33 percent thought it should be lawmakers. Twelve percent thought the manufacturers or designers should be burdened with that choice; 11 percent responded “other.” Determining who will control these “ethical settings” that guide no-win responses is a huge problem that self-driving cars are going to have to face in terms of liability. Because if a car will have to be programmed to choose between two lives, that means whoever decides how the algorithm is going to function is also possibly condemning to death either bystanders or passengers.,This type of “predetermined” action, an algorithm that chooses to spare children over adults, for example, would almost vicariously put the programmer in the driver’s seat, lending truth to the that a self-driven car will always have a “determinable human operator.” Insurance companies are going to have to wrestle with that one, because in any instance, will be liable if an autonomous vehicle gets into a wreck. If get to decide on your car’s ethics settings and decide to continue straight and kill the child in the tunnel situation, does that make you liable for that child’s death? If it’s left up to the auto company, will they be liable? The repercussions of these decisions extend much further into the future than anybody is able to foresee. As artificial intelligence advances, it may very well use the programmable ethics settings found in self-driving cars as a platform to build upon.
