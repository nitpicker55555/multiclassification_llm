{"num":4,"Time":"11 May 2015","Title":"Self-driving car accidents revealed in California","Content":"Self-driving car accidents revealed in California\nPublished\n11 May 2015\nShare\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nDriverless cars are allowed on the roads in California subject to permits.\nFour out of the 48 self-driving cars on public roads in California have been involved in accidents in the last eight months, according to the state's Department of Motor Vehicles.\nThe agency began issuing permits for the testing of autonomous vehicles in September 2014.\nThree of the four cars belonged to Google, the fourth to parts supplier Delphi.\nBoth firms denied their vehicles had been at fault.\nUnder Californian law, details of car accidents remain confidential.\nHowever, Google said its driverless cars had never been the cause of an accident, and that the majority of \"minor fender-benders\" had been in the form of rear-end collisions from other drivers.\n\"Safety is our highest priority. Since the start of our programme six years ago, we've driven nearly a million miles autonomously, on both freeways and city streets, without causing a single accident,\" said a spokesperson.\nDelphi told the BBC its vehicle was hit while stationary at a crossroads and was in manual driving mode at the time.\n\"A police report indicates the fault of the accident is with the second vehicle, not Delphi. No-one was hurt in the incident,\" said a spokesperson.\nAn anonymous source told the Associated Press that two of the accidents occurred while the vehicles were occupied by human drivers, and all four vehicles were going very slowly at the time of the collisions.\nChris Urmson, director of Google's self driving car programme, wrote in a blog post that there have been 11 accidents involving Google cars since the project began six years ago but not one has been caused by one of its vehicles.\n\"Rear-end crashes are the most frequent accidents in America, and often there's little the driver in front can do to avoid getting hit,\" he said.\nMore on this story\nSelf-driving truck allowed on roads\nPublished\n8 May 2015\nCould driverless cars own themselves?\nPublished\n16 February 2015\nUK kickstarts driverless car changes\nPublished\n11 February 2015\nCar hackers 'drive' car with laptop\nPublished\n26 July 2013","Relevant":true,"Overview":"Here is the analysis of the news you provided in JSON format:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Self-driving car accidents revealed in California\",\n        \"Date_published\": \"11 May 2015\",\n        \"Summary\": \"Four out of the 48 self-driving cars on public roads in California have been involved in accidents in the last eight months, according to the state's Department of Motor Vehicles. Three of the four cars belonged to Google, the fourth to parts supplier Delphi. Both firms denied their vehicles had been at fault. An anonymous source told the Associated Press that two of the accidents occurred while the vehicles were occupied by human drivers, and all four vehicles were going very slowly at the time of the collisions.\",\n        \"Google's Statement\": \"Google said its driverless cars had never been the cause of an accident, and that the majority of 'minor fender-benders' had been in the form of rear-end collisions from other drivers.\",\n        \"Delphi's Statement\": \"Delphi told the BBC its vehicle was hit while stationary at a crossroads and was in manual driving mode at the time. 'A police report indicates the fault of the accident is with the second vehicle, not Delphi. No-one was hurt in the incident,' said a spokesperson.\",\n        \"Chris Urmson's Statement\": \"Chris Urmson, director of Google's self-driving car programme, wrote in a blog post that there have been 11 accidents involving Google cars since the project began six years ago but not one has been caused by one of its vehicles.\"\n    }\n}\nThe news is relevant to autonomous driving accidents, and the specific information about the case includes details about the accidents involving self-driving cars in California, statements from Google and Delphi denying fault, and comments from Chris Urmson regarding the accidents involving Google cars.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-32691887","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":7,"Time":"OCTOBER 4, 2017","Title":"GM's self-driving cars involved in six accidents in September","Content":"By David Shepardson\nWASHINGTON, Oct 4 (Reuters) - General Motors Co’s self-driving unit, Cruise Automation, told California regulators its vehicles were involved in six crashes in the state in September, but said none of automated vehicles were responsible.\nThe accidents did not result in any injuries or serious damage, according to the GM reports, but did demonstrate the challenges for developers of self-driving cars confronted by crowded urban streets.\nA U.S. Senate panel on Wednesday approved legislation that would allow auto makers to greatly expand testing of self-driving cars. Some safety groups have objected to the proposal, saying it gives too much latitude to automakers.\nThe incidents also demonstrate the difficulties of self-driving cars that obey all traffic laws coexisting with less careful human drivers. In total, GM Cruise vehicles have been involved in 13 crashes reported to California regulators in 2017, while Alphabet Inc’s Waymo vehicles have been involved in three crashes.\nCalifornia state law requires that all crashes involving self-driving vehicles be reported, regardless of severity.\nMost of the crashes involved drivers of other vehicles striking the GM cars that were slowing for stop signs, pedestrian or other issues. In one crash, a driver of a Ford Ranger was on his cell phone when he rear-ended a Chevrolet Bolt that was stopped at a red light.\nIn another instance, the driver of a Chevrolet Bolt noticed an intoxicated cyclist in San Francisco going the wrong direction toward the Bolt. The human driver stopped the Bolt and the cyclist hit the bumper and fell over. The bicyclist pulled on a sensor attached to the vehicle causing minor damage.\nIn another incident on Sept. 15 in San Francisco, a Dodge Charger in the left-turn lane attempted to illegally pass a Bolt in driverless mode. The GM Cruise employee took control of the vehicle and the Dodge scraped the front sensor and fled the scene without stopping.\n“All our incidents this year were caused by the other vehicle,” said a Rebecca Mark, spokeswoman for GM Cruise.\nOn Tuesday, Kyle Vogt, the head of Cruise Automation, highlighted his operation’s decision to test vehicles in San Francisco to speed development of the software that will pilot fully self-driving vehicles in the future.\nIn an August Waymo crash in Palo Alto, a driver took control of the autonomous vehicle after a vehicle ahead of it swerved to avoid an object on the road. The Waymo vehicle’s rear bumper scraped the left front bumper of a van in the adjacent lane. (Reporting by David Shepardson; Editing by Sandra Maler)\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n\"General Motors Co’s self-driving unit, Cruise Automation, reported six crashes in California in September.\",\n\"None of the automated vehicles were responsible for the accidents.\",\n\"The accidents did not result in any injuries or serious damage, but highlighted challenges for self-driving cars on crowded urban streets.\",\n\"GM Cruise vehicles have been involved in a total of 13 crashes reported to California regulators in 2017.\",\n\"Most of the crashes involved drivers of other vehicles striking the GM cars that were slowing for stop signs, pedestrians, or other issues.\",\n\"In one incident, a Ford Ranger driver rear-ended a Chevrolet Bolt stopped at a red light.\",\n\"In another instance, a cyclist collided with a Chevrolet Bolt in San Francisco, causing minor damage.\",\n\"On September 15 in San Francisco, a Dodge Charger attempted to illegally pass a Bolt in driverless mode, scraping the front sensor and fleeing the scene.\",\n\"All incidents this year were caused by the other vehicles, according to a GM Cruise spokeswoman.\"\n]\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/autos-selfdriving-crashes\/gms-self-driving-cars-involved-in-six-accidents-in-september-idUSL2N1MF1RO","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":1,"Time":"Thu 22 Dec 2022 15.06 CET","Title":"Tesla behind eight-vehicle crash was in ‘full self-driving’ mode, says driver","Content":"The driver of a 2021 Tesla Model S told California authorities the vehicle was in “full self-driving mode” when the technology malfunctioned, causing an eight-vehicle crash on the San Francisco Bay bridge last month.\nThe crash on Thanksgiving Day resulted in two juveniles being transported to hospital and led to lengthy delays on the bridge. The incident was made public in a police report on Wednesday.\n‘Rude drivers will swerve in my lane’: are Tesla owners paying the price for Musk hate?\nRead more\nIt is the latest in a series of accidents blamed on Tesla technology. The electric automaker’s chief executive, Elon Musk, has heavily promoted “Full Self-Driving” (FSD) software, sold as $15,000 add-on to Tesla vehicles, but it faces legal, regulatory and public scrutiny.\nAfter the San Francisco accident, the driver told police the FSD software malfunctioned.\nThe police report said the vehicle was traveling at 55mph when it shifted lane but braked abruptly, slowing the car to about 20mph. That led to another vehicle hitting the Tesla and a chain reaction of crashes, according to Reuters.\nHowever, police were unable to determine if the software was in operation or that the driver’s account was accurate. The report was made public after a records request.\nThe crash occurred hours after Musk said Tesla would make FSD software available to anyone in North America who requested it. It previously offered the system only to drivers with high safety scores.\nThe police report said that if FSD malfunctioned, the driver should have manually taken control. Tesla has repeatedly said its advanced self-driving technology requires “active driver supervision” and its vehicles “are not autonomous”.\nDrivers are also warned when they install FSD that it “may do the wrong thing at the worst time”.\nThe National Highway Traffic Safety Administration (NHTSA), which is investigating Tesla after reports of braking “without warning, at random, and often repeatedly in a single drive”, did not immediately comment on the San Francisco crash.\nLast summer, NHTSA upgraded the investigation to what it calls an engineering analysis. The chair of the National Transportation Safety Board, Jennifer Homendy, has questioned if “full self-driving” is an accurate description of the technology – and said Tesla must do more to prevent misuse.","Relevant":true,"Overview":"Here is the JSON format response based on the news you provided:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Title\": \"Tesla behind eight-vehicle crash was in ‘full self-driving’ mode, says driver\",\n    \"Date\": \"Last month\",\n    \"Location\": \"San Francisco Bay bridge\",\n    \"Incident_description\": \"The driver of a 2021 Tesla Model S told California authorities the vehicle was in “full self-driving mode” when the technology malfunctioned, causing an eight-vehicle crash on the San Francisco Bay bridge last month. The crash on Thanksgiving Day resulted in two juveniles being transported to hospital and led to lengthy delays on the bridge. The incident was made public in a police report on Wednesday.\",\n    \"Details\": [\n      {\n        \"Cause\": \"The driver reported that the Full Self-Driving (FSD) software malfunctioned, causing the Tesla to shift lanes abruptly and brake abruptly, slowing the car and leading to a chain reaction of crashes.\"\n      },\n      {\n        \"Investigation\": \"Police were unable to determine if the software was in operation or if the driver's account was accurate. The police report was made public after a records request.\"\n      },\n      {\n        \"Tesla's stance\": \"Tesla has repeatedly stated that its advanced self-driving technology requires 'active driver supervision' and that its vehicles 'are not autonomous.' Drivers are warned that the FSD 'may do the wrong thing at the worst time.'\"\n      },\n      {\n        \"Regulatory involvement\": \"The National Highway Traffic Safety Administration (NHTSA) is investigating Tesla after reports of braking 'without warning, at random, and often repeatedly in a single drive.' Last summer, NHTSA upgraded the investigation to what it calls an engineering analysis. The chair of the National Transportation Safety Board, Jennifer Homendy, has questioned if 'full self-driving' is an accurate description of the technology and said Tesla must do more to prevent misuse.\"\n      }\n    ]\n  }\n}\nBased on the news, it is relevant to autonomous driving accidents, and the specific information about the case is provided in the JSON response, including the incident details, the driver's report of the Full Self-Driving (FSD) software malfunction, Tesla's stance on driver supervision, and regulatory involvement in the matter.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2022\/dec\/22\/tesla-crash-full-self-driving-mode-san-francisco","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":9,"Time":"MARCH 29, 2018","Title":"Uber avoids legal battle with family of autonomous vehicle victim","Content":"By Bernie Woodall\nTEMPE, Ariz. (Reuters) - The family of a woman killed by an Uber Technologies Inc self-driving vehicle in Arizona has reached a settlement with the ride services company, ending a potential legal battle over the first fatality caused by an autonomous vehicle.\nUber settles with family of victim in self-driving car crash\n01:04\nCristina Perez Hesano, an attorney with the firm of Bellah Perez in Glendale, Arizona, said “the matter has been resolved” between Uber and the daughter and husband of Elaine Herzberg, 49, who died after being hit by an Uber self-driving SUV while walking across a street in the Phoenix suburb of Tempe earlier this month.\nTerms of the settlement were not given. The law firm representing Herzberg’s daughter and husband, whose names were not disclosed, said they would have no further comment on the matter as they considered it resolved.\nAn Uber spokeswoman declined to comment.\nThe fallout from the accident could stall the development and testing of self-driving vehicles, designed to eventually perform far better than human drivers and to sharply reduce the number of motor vehicle fatalities that occur each year.\nUber has suspended its testing in the wake of the incident. Toyota Motor Corp 7203.T and chipmaker Nvidia Corp NVDA.O have also suspended self-driving testing on public roads, as they and others await the results of investigations into the Tempe accident, believed to be the first death of a pedestrian struck by a self-driving vehicle.\nNvidia’s chief executive, Jensen Huang, said Uber does not use the chipmaker’s self-driving platform architecture.\nSlideshow ( 3 images )\nToyota North America Chief Executive Jim Lentz said the company expects to “soon” resume testing of self-driving vehicles, while warning that the ongoing risks will affect the industry’s progress.\n“There will be mistakes from vehicles, from systems, and a hundred or 500 or a thousand people could lose their lives in accidents like we’ve seen in Arizona,” Lentz said Thursday at a Reuters Newsmakers event connected with the New York auto show.\n“The big question for government is: How much risk are they willing to take? If you can save net 34,000 lives, are you willing to potentially have 10 or 100 or 500 or 1,000 people die?” he said. “And I think the answer to that today is they are not willing to take that risk - and that’s going to really slow down the adoption of autonomous driving.”\nThe March 18 fatality near downtown Tempe also presents an unprecedented liability challenge because self-driving vehicles, which are still in the development stage, involve a complex system of hardware and software often made by outside suppliers.\nHerzberg was pushing a bicycle while walking across a four-lane road outside a crosswalk when she was struck. Video footage from a dash-mounted camera inside the vehicle, released by Tempe police, showed the SUV traveling along a dark street when the headlights suddenly illuminated Herzberg in front of the SUV.\nOther footage showed that in the seconds before the accident, the human safety driver behind the wheel was mostly looking down, not at the road.\nReporting by Bernie Woodall; Additional reporting by David Shepardson in New York; Writing by Peter Henderson and Bernie Woodall; Editing by Bernadette Baum and Leslie Adler\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a settlement between Uber Technologies Inc and the family of a woman, Elaine Herzberg, who was killed by an Uber self-driving vehicle in Arizona. The settlement ends a potential legal battle over the first fatality caused by an autonomous vehicle. The terms of the settlement were not disclosed. Uber has suspended its testing of self-driving vehicles following the incident, and other companies like Toyota and Nvidia have also suspended their self-driving testing on public roads pending investigations into the accident. The incident has raised concerns about the safety and liability of self-driving vehicles, as well as the potential impact on the development and adoption of autonomous driving technology.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-uber-settlement-idUSKBN1H5092","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":6,"Time":"Mon 19 Mar 2018 23.48 CET","Title":"Self-driving Uber kills Arizona woman in first fatal crash involving pedestrian","Content":"An autonomous Uber car killed a woman in the street in Arizona, police said, in what appears to be the first reported fatal crash involving a self-driving vehicle and a pedestrian in the US.\nTempe police said the self-driving car was in autonomous mode at the time of the crash and that the vehicle hit a woman, who was walking outside of the crosswalk and later died at a hospital. There was a vehicle operator inside the car at the time of the crash.\nUber said in a statement on Twitter: “Our hearts go out to the victim’s family. We are fully cooperating with local authorities in their investigation of this incident.” A spokesman declined to comment further on the crash.\nWho's driving? Autonomous cars may be entering the most dangerous phase\nRead more\nThe company said it was pausing its self-driving car operations in Phoenix, Pittsburgh, San Francisco and Toronto. Dara Khosrowshahi, Uber’s CEO, tweeted: “Some incredibly sad news out of Arizona. We’re thinking of the victim’s family as we work with local law enforcement to understand what happened.”\nUber has been testing its self-driving cars in numerous states and temporarily suspended its vehicles in Arizona last year after a crash involving one of its vehicles, a Volvo SUV. When the company first began testing its self-driving cars in California in 2016, the vehicles were caught running red lights, leading to a high-profile dispute between state regulators and the San Francisco-based corporation.\nPolice identified the victim as 49-year-old Elaine Herzberg and said she was walking outside of the crosswalk with a bicycle when she was hit at around 10pm on Sunday. Images from the scene showed a damaged bike. The 2017 Volvo SUV was traveling at roughly 40 miles an hour, and it did not appear that the car slowed down as it approached the woman, said Tempe sergeant Ronald Elcock.\nElcock said he had watched footage of the collision, which has not been released to the public. Police identified the operator of the car as Rafaela Vasquez, 44, and said she was cooperative and there were no signs of impairment.\nA still image taken from video provided by ABC-15 at the scene, where a pedestrian with a bicycle was hit. Photograph: AP\nThe self-driving technology is supposed to detect pedestrians, cyclists and others and prevent crashes.\nJohn M Simpson, privacy and technology project director with Consumer Watchdog, said the collision highlighted the need for tighter regulations of the nascent technology.\n“The robot cars cannot accurately predict human behavior, and the real problem comes in the interaction between humans and the robot vehicles,” said Simpson, whose advocacy group called for a national moratorium on autonomous car testing in the wake of the deadly collision.\nSimpson said he was unaware of any previous fatal crashes involving an autonomous vehicle and a pedestrian.\nTesla Motors was the first to disclose a death involving a self-driving car in 2016 when the sensors of a Model S driving in autopilot mode failed to detect a large white 18-wheel truck and trailer crossing the highway. The car drove full speed under the trailer, causing the collision that killed the 40-year-old behind the wheel in the Tesla.\nEarlier this year, California regulators approved the testing of self-driving cars on public roads without human drivers monitoring inside.\n“The technology is not ready for it yet, and this just sadly proves it,” said Simpson.\nIn one recent incident, California police officers found a Tesla that was stopped in the middle of a five-lane highway and found a driver asleep behind the wheel. The man said the vehicle was in “autopilot”, which is Tesla’s semi-autonomous driver assist system, and he was arrested on suspicion of drunk driving.\nAn Uber self-driving car travels in Pittsburgh, Pennsylvania. Photograph: Angelo Merendino\/AFP\/Getty Images\nIn another recent case, a Tesla car rear-ended a fire truck on a freeway, with the driver again telling the authorities the car was in autopilot mode at the time of the collision.\nMichael G Bennett, an Arizona State University associate research professor who studies autonomous cars, said the self-driving vehicles have become ubiquitous around campus and on the streets in Tempe. Often they have operators behind the wheels, but sometimes they are fully autonomous with no human inside.\nThe fatal collision could spark significant calls for reform and reflections within the industry, he said.\n“It may be problematic for the industry, because one of their central arguments for the value of the technology is that it is superior to human drivers,” said Bennett, adding that autonomous cars should be able to detect pedestrians and avoid hitting them, even if they aren’t in crosswalks: “Every day, pedestrians in cities around the world step outside of the crosswalk.”\nRage against the machine: self-driving cars attacked by angry Californians\nRead more\nThe governor of Arizona, Doug Ducey, has been a strong proponent of allowing corporations to test the technology in his state, publicly slamming other governments for “over regulation” and in 2016 urging Uber to “ditch California” and launch in his region. In March, he issued new rules and said that more than 600 automated vehicles have driven on public roads in the state.\n“Our prayers are with the victim, and our hearts go out to her family,” Patrick Ptak, Ducey’s spokesman, said in an email to the Guardian, adding, “Public safety is our top priority.”\nLinda Bailey, the executive director of the National Association of City Transportation Officials (Nacto), said in an interview that there has not been enough regulatory oversight of testing and that some governments are overwhelmed trying to understand autonomous technology and its limitations.\n“There’s an essential role for the public sector in regulating the safety of these vehicles, which has been largely left to private companies,” she said, adding that Nacto supports third-party testing of the vehicles.\nTempe’s mayor, Mark Mitchell, defended the city’s ongoing support of autonomous vehicles in a statement Monday, saying: “All indications we had in the past show that traffic laws are being obeyed by the companies testing here.”\nThis article was amended on 22 March to correct the name of the Uber safety driver, who police had initially misidentified.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Self-driving Uber kills Arizona woman in first fatal crash involving pedestrian\",\n\"Summary\": \"An autonomous Uber car killed a woman in the street in Arizona, police said, in what appears to be the first reported fatal crash involving a self-driving vehicle and a pedestrian in the US. Tempe police said the self-driving car was in autonomous mode at the time of the crash and that the vehicle hit a woman, who was walking outside of the crosswalk and later died at a hospital. There was a vehicle operator inside the car at the time of the crash.\"\n}\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/mar\/19\/uber-self-driving-car-kills-woman-arizona-tempe","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":11,"Time":"22 May 2018","Title":"Who is to blame for 'self-driving car' deaths?","Content":"Who is to blame for 'self-driving car' deaths?\nPublished\n22 May 2018\nShare\nIMAGE SOURCE,\nREUTERS\nImage caption,\nWalter Huang, 38, was killed in a crash while using Tesla's Autopilot function\nBy Theo Leggett\nBusiness correspondent, BBC News\nThe confusion between fully autonomous self-driving cars and those that simply offer driver assistance technologies is leading to deaths on the road. Who is to blame and what should be done about it?\nSelf-driving cars already exist and there will be more of them in future, but the chances are that you won't be driven by one any time soon.\nYou may, however, already be using a car that can steer, brake or park by itself.\nThe fear is that the hype around driverless cars has led some drivers to test the limits of existing technology in ways that are downright reckless.\nA Tesla driver in the UK, for example, was recently prosecuted for climbing into the passenger seat of his car while it was moving at around 40mph (64km\/h) in motorway traffic.\nAutopilot seat-switch driver banned\nHe was using Tesla's Autopilot, a system that does allow the car to accelerate, brake and steer by itself on major roads, but is not designed to completely replace a driver.\nMedia caption,\nBhavesh Patel was filmed by a passenger in another car\nOther manufacturers such as Volvo, Mercedes-Benz and Cadillac have similar mechanisms.\nBut none of them is designed to be fully autonomous.\nHands on the wheel\nUsing standard criteria established by the US engineering organisation SAE International, cars can be placed into six broad categories depending on the level of automation they contain.\nThey range from Level Zero, where the vehicle is not automated at all, to Level 5, which means it can drive itself on all roads and in all conditions, making the human behind the wheel - and the steering wheel itself - redundant.\nCurrent \"driver assistance\" systems are Level 2 in the jargon, and the driver is meant to keep his or her hands firmly on the wheel.\nBut getting that message across has clearly been a challenge.\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nDrivers who rely too much on driver assistance tech risk killing themselves and others\nTesla's Autopilot in particular has been implicated in a number of high profile crashes, two of them fatal.\nTesla 'removed' from car crash probe\nThe company denies claims that the Autopilot name itself encourages drivers to hand over control, and has rejected demands from the German government to stop using the term.\nIt says feedback from its customers has shown that \"they have a very clear understanding of what Autopilot is, how to properly use it and what features it consists of\".\nNevertheless, since 2016 Tesla's systems have included more prominent warnings to drivers to keep their hands on the wheel, and have been able to lock them out of Autopilot if they fail to do so.\nThat same year, Mercedes faced criticism over advertising that suggested its E-Class was a \"self-driving car\".\nIt later withdrew the adverts, in response, it said, to claims that customers could find them confusing.\nEliminating humans\nAlthough ride-sharing firms like Lyft and Uber have been working hard on developing fully autonomous technology - as have many mainstream manufacturers - Level 5 cars are still some way off.\nWaymo appears to be closer than most.\nLater this year Google's sister company is planning to introduce a driverless taxi service in Phoenix, Arizona. Unlike several other autonomous taxi services being trialled around the world, this one will not need a safety driver in the car.\nIMAGE SOURCE,\nREUTERS\nImage caption,\nChina's Tencent has is now licensed to test its self-driving car on public roads in Shenzhen\nBut the service will only operate in a relatively small \"geo-fenced\" area of Phoenix that the company has intensively mapped. It is still, in effect, a test-bed.\nThere is a big step between this kind of limited service and something that can safely negotiate a densely populated mega-city in all weathers.\nTest drive\n\"Testing and development is different from bringing onto the market,\" explains Prof Walter Brenner of the University of St Gallen in Switzerland and co-author of Autonomous driving - how the driverless revolution will change the world.\n\"They are completely different worlds. The tests are useful because they show both the strengths and the limits of this technology, but they are just tests.\"\nMedia caption,\nUber dashcam footage shows moment before fatal impact\nEarlier this year, a woman was killed in Arizona by an Uber test car being driven in autonomous mode. It failed to stop when she moved into its path.\nUber halts self-driving tests after death\nWill we ever be able to trust self-driving cars?\nClearly, despite all the research being carried out and the money being spent, there is still a lot of work to do before full autonomy becomes a safe, everyday reality.\nResponsibility\nSafety experts believe car companies need to take more responsibility for ensuring consumers don't make mistakes.\n\"Calling this kind of technology Autopilot… that's very misleading for consumers,\" says Matthew Avery of Thatcham Research - a group that tests vehicles on behalf of the UK insurance industry.\n\"They might think 'I just need to push this button and I can let the car drive'.\"\nMore Technology of Business\nIMAGE SOURCE,\nMAGNUM PHOTOS\nThe problem of sending cash back home\nHow cheap dockless hire bikes are flooding the world\nWould you pay for an ad-free internet?\nThe battle of the gas-sucking mega giants is set to begin\nIs Messi's blockchain interest a sign football is changing?\nHe also thinks manufacturers should take further steps to ensure the technology isn't abused, such as having cameras monitoring the driver.\nBut he remains convinced that automation itself has vital safety benefits.\nThere is already evidence that automatic emergency braking and pedestrian detection systems are reducing the number of accidents. But more sophisticated systems can take that process a step further.\nMedia caption,\nTheo Leggett: 'The car brought us to a controlled halt'\n\"What the best systems are doing is integrating lane control, stopping people veering out of their lane, with braking control and distance control.\n\"That can really help keep people out of trouble,\" he says.\n'Harsh punishments'\nWalter Brenner believes there's a need for drivers - and people selling cars - to be better educated about what semi-automated systems can do.\nThere is a risk, he concedes, that even with that knowledge some people might deliberately choose to let the technology do more than it should - to experiment with it, or even to show off.\nIn those cases, he thinks, punishments should be harsh.\n\"There's a big difference between trying out a new feature on an iPhone and playing with technology in a car when you're travelling at 100km\/h (62mph) on a public road,\" he says.\n\"Those people have to be punished because they're risking other people's lives.\"\nFollow Technology of Business editor Matthew Wall on Twitter and Facebook\nClick here for more Technology of Business features","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news discusses the confusion between fully autonomous self-driving cars and those that offer driver assistance technologies. It mentions a fatal accident involving a Tesla driver, Walter Huang, who was using Tesla's Autopilot function. The article also highlights concerns about drivers misusing driver assistance technology, including climbing into the passenger seat while the car is in motion. It mentions that Tesla's Autopilot has been implicated in high-profile crashes, some of which were fatal. Safety experts believe car companies should take more responsibility for ensuring consumers understand the limitations of such technology. The article also discusses the need for better education and potentially harsh punishments for those who misuse semi-automated systems.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/business-44159581","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":14,"Time":"JUNE 5, 2020","Title":"Self-driving sector contends its cars can prevent many more crashes than insurance study says","Content":"By Tina Bellon\n(Reuters) - Companies working on self-driving vehicles have criticized an insurance industry study suggesting that only a third of all U.S. road crashes could be prevented by driverless cars, arguing that the study has underestimated the technology’s capabilities.\nThe study by the Insurance Institute for Highway Safety (IIHS), released on Thursday analyzed 5,000 U.S. crashes and concluded that likely only those caused by driver perception errors and incapacitation could be prevented by self-driving cars.\nThe autonomous vehicle industry quickly responded that its cars were programmed to prevent a vastly higher number of potential crash causes, including more complex errors caused by drivers making inadequate or incorrect evasive maneuvers.\nTaking those design choices into account, autonomous vehicles could avoid some 72% of crashes, said Partners for Automated Vehicle Education, a consortium of self-driving technology companies.\nThe group in a blog post (here here) on Thursday said it was \"fundamentally speculative\" to determine crash avoidance rates.\nNevertheless, companies developing self-driving cars for years have touted their ability to vastly reduce crashes, with some, like General Motors Co, calling them a key part in achieving “zero crashes.”\nThe Alliance for Automotive Innovation, an auto industry group, in a statement said that even reducing traffic fatalities by a third would be something to be proud of, but that its members aim to do more.\nJack Weast, vice president of autonomous vehicle standards at Intel Corp’s Mobileye, in an interview on Friday said the auto industry was assembling a vast list of likely road scenarios and human behavior that every driverless car should be able to navigate safely. Government agencies and insurance companies are part of that process, Weast said.\n“Crashes will never be zero until we have no more human drivers on the road,” he said. “But (self-driving cars) can combine physical laws with behavioral studies and do much more than a human driver.”\nReporting by Tina Bellon in New York; Editing by David Gregorio\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news article is about a study by the Insurance Institute for Highway Safety (IIHS) that analyzed 5,000 U.S. crashes and concluded that only crashes caused by driver perception errors and incapacitation could be prevented by self-driving cars. The autonomous vehicle industry criticized this study, arguing that their self-driving cars are programmed to prevent a vastly higher number of potential crash causes, including more complex errors caused by drivers making inadequate or incorrect evasive maneuvers. According to Partners for Automated Vehicle Education, a consortium of self-driving technology companies, autonomous vehicles could avoid some 72% of crashes. The Alliance for Automotive Innovation, an auto industry group, also commented on the study, stating that even reducing traffic fatalities by a third would be an achievement. Jack Weast, vice president of autonomous vehicle standards at Intel Corp’s Mobileye, mentioned that the auto industry is working on a comprehensive list of likely road scenarios and human behavior that driverless cars should be able to navigate safely.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-idUSKBN23C2T7","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":12,"Time":null,"Title":"California regulator probes crashes involving GM's Cruise robotaxis","Content":"Companies\nAlphabet Inc\nFollow\nGeneral Motors Co\nFollow\nSAN FRANCISCO, Aug 18 (Reuters) - California's autos regulator said on Friday it is investigating \"recent concerning incidents\" involving autonomous vehicles operated by General Motors (GM.N) unit Cruise in San Francisco and asked the company to take half its robotaxis off the roads.\nThe statement from California Department of Motor Vehicles (DMV) came after a Cruise robotaxi was involved in a crash with an emergency vehicle in San Francisco late on Thursday, the latest accident involving the self-driving cars.\nThe regulator also said it has asked Cruise to immediately reduce its active fleet of vehicles by 50% until the investigation is complete and Cruise takes actions to improve road safety. Cruise has agreed to a 50% reduction, it added.\n\"The DMV reserves the right, following investigation of the facts, to suspend or revoke testing and\/or deployment permits\" if it is determined to be an unreasonable risk to public safety, the regulator said in a statement.\nAdvertisement · Scroll to continue\nCruise said one of its cars \"entered the intersection on a green light and was struck by an emergency vehicle that appeared to be en route to an emergency scene\" after 10 p.m. on Thursday (0500 GMT Friday).\nThe car \"did identify the risk of a collision and initiated a braking maneuver, reducing its speed, but was ultimately unable to avoid the collision,\" the company, which is investigating the incident, said in a statement on Friday.\nAdvertisement · Scroll to continue\nInitial investigation shows the collision occurred when a fire truck was operating in an emergency with its forward facing red lights and siren on, the San Francisco Police Department said in a statement to Reuters.\nThe police said the sole passenger in the autonomous vehicle (AV) was transported to a local hospital with non-life-threatening injuries.\nThe California Public Utilities Commission (CPUC) last week voted to allow robotaxis from Cruise and Alphabet's (GOOGL.O) Waymo to operate at all hours of the day throughout San Francisco and charge passengers for rides despite strong opposition from residents and city agencies.\nAdvertisement · Scroll to continue\nThe two have been running robotaxi tests limited by times and geographic areas within San Francisco.\nCity Attorney David Chiu asked the CPUC on Thursday to halt its decision while the city files for a re-hearing. \"We have seen that this technology is not yet ready, and poor AV performance has interfered with the life-saving operations of first responders. San Francisco will suffer serious harms from this unfettered expansion,\" he said in a statement.\nReporting by Abhirup Roy and Hyunjoo Jin in San Francisco; editing by Diane Craft and Sonali Paul\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Based on the provided news article, here's the information in JSON format:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"California regulator probes crashes involving GM's Cruise robotaxis\",\n        \"Description\": \"California's autos regulator said on Friday it is investigating 'recent concerning incidents' involving autonomous vehicles operated by General Motors (GM.N) unit Cruise in San Francisco and asked the company to take half its robotaxis off the roads. The statement from California Department of Motor Vehicles (DMV) came after a Cruise robotaxi was involved in a crash with an emergency vehicle in San Francisco late on Thursday, the latest accident involving the self-driving cars. The regulator also said it has asked Cruise to immediately reduce its active fleet of vehicles by 50% until the investigation is complete and Cruise takes actions to improve road safety. Cruise has agreed to a 50% reduction, it added. 'The DMV reserves the right, following investigation of the facts, to suspend or revoke testing and\/or deployment permits' if it is determined to be an unreasonable risk to public safety, the regulator said in a statement.\",\n        \"Incident Details\": {\n            \"Date\": \"August 18\",\n            \"Location\": \"San Francisco\",\n            \"Description\": \"A Cruise robotaxi was involved in a crash with an emergency vehicle. The collision occurred when a fire truck was operating in an emergency with its forward-facing red lights and siren on. The sole passenger in the autonomous vehicle (AV) was transported to a local hospital with non-life-threatening injuries.\",\n            \"Response Actions\": \"The California Department of Motor Vehicles (DMV) asked Cruise to reduce its active fleet by 50% until the investigation is complete. Cruise agreed to the reduction. The DMV also reserved the right to suspend or revoke testing and\/or deployment permits if it is determined to be an unreasonable risk to public safety.\"\n        }\n    }\n}\nThe news is indeed relevant to autonomous vehicles, specifically incidents involving GM's Cruise robotaxis in San Francisco. The specific information about the case includes the date, location, description of the incident, and the response actions taken by the California DMV.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/gms-cruise-robotaxi-collides-with-fire-truck-san-francisco-2023-08-19\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":3,"Time":"10:40 PM GMT+2•June 6, 2023","Title":"A Waymo self-driving car killed a dog in ‘unavoidable’ accident","Content":"cameraImage Credits: Kirsten Korosec\nA Waymo robotaxi operating in autonomous mode struck and killed a small dog last month in San Francisco, according to an incident report filed with the California Department of Motor Vehicles. The incident appears to have been unavoidable, based on information provided in the report.\nAny collision involving an autonomous vehicle — even one in which the company is not at fault — can cause a backlash, particularly in a city like San Francisco, where there is already tension between city officials, AV tech companies and the public. If technological capability and a favorable regulatory environment are two legs of a solid AV commercialization stool, public perception is the crucial third. And a self-driving car killing a sweet pooch has the potential to kick out that third leg.\nIt also comes at a critical time for the industry and, specifically, for Waymo, which is expanding its robotaxi network in Phoenix and gearing up to charge fares for fully autonomous (meaning no human behind the wheel) rides in San Francisco.\nAccording to the report, one of Waymo’s self-driving Jaguar I-Pace cars was traveling on Toland Street, a low-speed street near Waymo’s depot, and the dog ran into the street. The vehicle was operating in autonomous mode, and a human safety operator was in the driver’s seat at the time of the accident.\nThe human operator didn’t see the dog, but the vehicle’s autonomous system did. However, a number of factors, including the speed and trajectory of the dog’s path, made the collision unavoidable, according to Waymo.\nIn response to our questions about the incident, Waymo sent TechCrunch the following statement:\nOn May 21 in San Francisco, a small dog ran in front of one of our vehicles with an autonomous specialist present in the driver’s seat, and, unfortunately, contact was made. The investigation is ongoing, however the initial review confirmed that the system correctly identified the dog which ran out from behind a parked vehicle but was not able to avoid contact. We send our sincere condolences to the dog’s owner. The trust and safety of the communities we are in is the most important thing to us and we’re continuing to look into this on our end.\nNeither the safety operator nor the autonomous system braked to avoid collision, according to Waymo. In both cases, that’s because of the “unusual path” the dog took at “a high rate of speed directly towards the side of the vehicle,” said a Waymo spokesperson.\nOne of the ways Waymo evaluates its autonomous driver’s collision avoidance performance is by comparing it to that of a model for a non-impaired, with eyes always on the conflict (NIEON) human driver. A Waymo spokesperson told TechCrunch that the company reconstructed last month’s event in simulation against the NIEON model, but the analysis showed a collision in this case was unavoidable.\nSagar Behere, VP of safety at AV verification and validation startup Foretellix, told TechCrunch that timing is a key factor in an AV’s ability to avoid collision. (Behere spoke to TechCrunch about AV technology generally, and not about Waymo specifically.)\n“If you saw the object, when did you see it? Did you see it in time to be able to act on it and make a good evasive maneuver?” said Behere. “Or maybe you saw it and predicted it would move in a way that required you to take no action? Or maybe you were about to take action, but then the object changed course.”\nDespite Waymo’s potential for plausible deniability here, the company and the industry at large are still at risk of a downturn in public perception, which could make AV expansion plans more difficult.\nRecent studies show that public perception around autonomous vehicles is improving, albeit slowly and mainly in regard to lower levels of automated driving like advanced driver assistance systems. People trust advanced driver assistance systems in today’s new vehicles (systems that require a human to be ready to take control) over fully autonomous vehicles when it comes to crash prevention, according to a study from the AAA’s Foundation for Traffic Safety, which collected responses from 2018 to 2020\nAnother study that collected nearly 6,000 responses between February and June 2022 found that trust in AVs decreases after a crash, while levels of concern increase. However, those with prior knowledge of AVs are more optimistic toward the technology, even after a crash, because they generally accept that AVs will not always make the right decisions at early stages of adoption.\nPublic perception aside, Waymo could face investigations from regulatory bodies like the National Highway Traffic Association. NHTSA requires manufacturers and operators of high-level autonomous vehicles to submit incident reports for crashes if the autonomous driving system was in use at any time within 30 seconds of the crash and the crash resulted in property damage or injury. The agency told TechCrunch it had reached out to Waymo for more information, but has no open investigations into the company at this time.\nIn 2018, when an autonomous vehicle from Uber’s now-shuttered AV unit hit and killed a pedestrian, the National Transportation Safety Board (NTSB) launched an investigation. Usually, the NTSB launches a highway investigation when there’s been a significant crash that highlights a potential national safety issue. A spokesperson from the agency told TechCrunch she doesn’t believe NTSB has any current investigations involving Waymo.\nThis article has been updated with NHTSA’s statement.\nMore TechCrunch\nSecurity researcher warns of chilling effect after feds search phone at airport\nBeing is an app that wants to help users map out and address mental health concerns\nOnlyFans CEO Ami Gan steps down\nVR is dead","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"A Waymo self-driving car killed a dog in ‘unavoidable’ accident\",\n\"Description\": \"A Waymo robotaxi operating in autonomous mode struck and killed a small dog last month in San Francisco. The incident was reported to the California Department of Motor Vehicles and appears to have been unavoidable. The vehicle was operating autonomously, and a human safety operator was present in the driver’s seat at the time. Neither the safety operator nor the autonomous system braked to avoid the collision due to the dog's unusual path and high speed directly toward the vehicle. Waymo has expressed condolences to the dog's owner and is conducting an investigation. There is a potential impact on public perception of autonomous vehicles, which could affect Waymo's expansion plans.\",\n\"Date\": \"May 21\",\n\"Location\": \"San Francisco\",\n\"Vehicle Involved\": \"Waymo self-driving Jaguar I-Pace car\",\n\"Human Operator Present\": \"Yes\",\n\"Braking Attempted\": \"No, due to the dog's unusual path and high speed\",\n\"Investigation Status\": \"Ongoing\",\n\"Potential Impact\": \"Public perception of autonomous vehicles may be affected, and regulatory bodies like NHTSA may conduct investigations.\"\n}\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2023\/06\/06\/a-waymo-self-driving-car-killed-a-dog-in-unavoidable-accident\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":8,"Time":"Published 5:41 PM EST, Wed December 21, 2022","Title":"Tesla ‘full self-driving’ triggered an eight-car crash, a driver tells police","Content":"Washington\nCNN\n — \nA driver told authorities that their Tesla’s “full-self-driving” software braked unexpectedly and triggered an eight-car pileup in the San Francisco Bay Area last month that led to nine people being treated for minor injuries including one juvenile who was hospitalized, according to a California Highway Patrol traffic crash report.\nCNN Business obtained the report detailing the crash through a public records request Wednesday. California Highway Patrol reviewed videos that show the Tesla vehicle changing lanes and slowing to a stop.\nCalifornia Highway Patrol said in the Dec. 7 report that it could not confirm if “full self-driving” was active at the time of the crash. A highway patrol spokesperson told CNN Business on Wednesday that it would not determine if “full self-driving” was active, and Tesla would have that information.\nThe crash occurred about lunchtime on Thanksgiving, snarling traffic on Interstate 80 east of the Bay Bridge as two lanes of traffic were closed for about 90 minutes as many people traveled to holiday events. Four ambulances were called to the scene.\nThe pileup took place just hours after Tesla CEO Elon Musk had announced that Tesla’s driver-assist software “full self-driving” was available to anyone in North America who requested it. Tesla had previously restricted access to drivers with high safety scores on its rating system.\n“Full self-driving” is designed to keep up with traffic, steer in the lane and abide by traffic signals. It requires an attentive human driver prepared to take full control of the car at any moment. It’s delighted some drivers but also alarmed others with its limitations. Drivers are warned by Tesla when they install “full self-driving” that it “may do the wrong thing at the worst time.”\nThe report states that the Tesla Model S was traveling at about 55 mph and shifted into the far left-hand lane, but then braked abruptly, slowing the car to about 20 mph. That led to a chain reaction that ultimately involved eight vehicles to crash, all of which had been traveling at typical highway speeds.\nTesla’s driver-assist technologies, Autopilot and “full self-driving” are already being investigated by the National Highway Traffic Safety Administration following reports of unexpected braking that occurs “without warning, at random, and often repeatedly in a single drive.”\nThe agency has received hundreds of complaints from Tesla drivers. Some have described near crashes and concerns about their safety. This summer NHTSA upgraded the investigation to what it calls an engineering analysis, an indication that it’s seriously considering a recall.\nNHTSA told CNN Business a few days after the Thanksgiving Day crash that it was gathering addition information from Tesla and law enforcement about the crash.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Tesla ‘full self-driving’ triggered an eight-car crash, a driver tells police\",\n\"Location\": \"San Francisco Bay Area\",\n\"Date\": \"December 7\",\n\"Injuries\": {\n\"Total\": 9,\n\"Minor\": 8,\n\"Hospitalized\": 1 (juvenile)\"\n},\n\"Description\": \"A driver reported that their Tesla with 'full self-driving' software braked unexpectedly, causing an eight-car pileup on Interstate 80 east of the Bay Bridge during lunchtime on Thanksgiving. The crash resulted in nine people being treated for injuries. California Highway Patrol reviewed videos of the Tesla changing lanes and slowing to a stop but could not confirm if 'full self-driving' was active at the time. The incident occurred shortly after Elon Musk announced wider availability of 'full self-driving' software. 'Full self-driving' is designed to assist drivers but requires human readiness to take control. The Tesla Model S involved in the crash was traveling at about 55 mph and abruptly slowed to about 20 mph, causing a chain reaction involving eight vehicles. The incident comes amid an investigation by the National Highway Traffic Safety Administration into Tesla's driver-assist technologies, including 'full self-driving,' due to reports of unexpected braking incidents.\"\n}\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2022\/12\/21\/business\/tesla-fsd-8-car-crash\/index.html","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":10,"Time":"5:51 AM GMT+2•August 22, 2022","Title":"Human drivers in UK won’t be liable for accidents when vehicle is self-driving","Content":"cameraImage Credits: Alexander Spatari \/ Getty Images\nManufacturers operating self-driving vehicles in the U.K. will be liable for a vehicle’s actions when in autonomous mode, according to the country’s new roadmap to achieve a widespread rollout of self-driving vehicles by 2025.\nThe British government unveiled the roadmap over the weekend, announcing $119 million in funding for AV projects and an additional $41 million for research to support safety and new legislation.\nThe roadmap stated that new legislation will build on existing laws and state that manufacturers are responsible for the actions of self-driving vehicles, “meaning a human driver would not be liable for incidents related to driving while the vehicle is in control of driving.”\nThis distinction could set a precedent globally, where liability in the case of autonomous vehicle accidents can still be somewhat of a gray area. When an autonomous Uber test vehicle killed a pedestrian in 2017, the human safety operator in the vehicle was charged with negligent homicide with a dangerous instrument. Uber faced no charges. And after a deadly crash in 2019 involving a Tesla vehicle operating in Autopilot, the driver, not Tesla, was charged with two felony counts of vehicular manslaughter with gross negligence.\nAs the country prepares to write new laws in regard to AVs, Britain has opened a consultation period on a “safety ambition” for self-driving vehicles to be as safe as a human driver. The results from the consultation would help form the standards for operating autonomous vehicles on public roads, as well as potential sanctions if those standards are not met.\n“It is still quite a big leap from assisted driving, where the driver is still in control, to self-driving, where the car takes control. It is important that the government does study how these vehicles would interact with other road users on different roads and changing weather conditions,” AA president, Edmund King, said in a statement. “However the ultimate prize, in terms of saving thousands of lives and improving the mobility of the elderly and the less mobile, is well worth pursuing.”\nAside from drafting legislation to ensure the safety of autonomous technology, the U.K. wants to take advantage of the emerging market for AVs, which it values at around $50 billion and estimates could create 38,000 new jobs. Of the total investment amount, about $24 million will be used to help kick-start commercial self-driving services, such as autonomous grocery delivery or shuttle pods in airports. Another $7 million will be put toward further market research and to support commercialization.\n“We want the UK to be at the forefront of developing and using this fantastic technology, and that is why we are investing millions in vital research into safety and setting the legislation to ensure we gain the full benefits that this technology promises,” said transport secretary Grant Shapps in a statement.\nThe government said vehicles that can drive themselves on motorways could be available to buy within the next year for drivers with valid driver’s licenses. While Teslas are some of the best-selling vehicles in the U.K., current legislation does not allow for British drivers to try out Tesla’s full self-driving beta, its advanced driver assistance system (ADAS) that comes with a variety of automated driving functions. New legislation could open the door not only for ride-hail, delivery and public transportation, but also for cars equipped with higher levels of ADAS.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Manufacturers operating self-driving vehicles in the U.K. will be liable for a vehicle’s actions when in autonomous mode, according to the country’s new roadmap to achieve a widespread rollout of self-driving vehicles by 2025. The roadmap stated that new legislation will build on existing laws and state that manufacturers are responsible for the actions of self-driving vehicles, 'meaning a human driver would not be liable for incidents related to driving while the vehicle is in control of driving.' The article also discusses the consultation period on a 'safety ambition' for self-driving vehicles, the potential opening of the market for AVs in the U.K., and the government's plans to allow vehicles that can drive themselves on motorways within the next year.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/08\/21\/human-drivers-in-uk-wont-be-liable-for-accidents-when-vehicle-is-self-driving\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":19,"Time":"22 March 2018","Title":"Uber self-driving crash: Footage shows moment before impact","Content":"Uber self-driving crash: Footage shows moment before impact\nPublished\n22 March 2018\nShare\nMedia caption,\nUber dashcam footage shows moment before fatal impact\nPolice have released two videos showing the moments leading up to a fatal crash involving a self-driving Uber car in Tempe, Arizona, on Sunday.\nIn the 14-second video, the autonomous vehicle is seen failing to slow down before hitting Elaine Herzberg, 49, who is walking her bike across the road.\nOne video shows dashcam footage of the impact. The other, an Uber operator monitoring the car's controls.\nUber has suspended self-driving tests in North America following the crash.\nIn footage released on Wednesday by the Tempe police department, the human Uber operator sitting inside the Volvo appears to be looking down at something while the vehicle is travelling in autonomous mode.\nMoments later, the woman appears visibly shocked as she looks up to see Ms Herzberg crossing the highway in their path seconds before impact.\nIMAGE SOURCE,\nREUTERS\nImage caption,\nUber has been testing autonomous Volvo vehicles\n\"The video is disturbing and heartbreaking to watch, and our thoughts continue to be with Elaine's loved ones,\" Uber said in a statement.\n\"Our cars remain grounded, and we're assisting local, state and federal authorities in any way we can,\" the statement added.\nPolice said the accident happened on Sunday night, adding that Ms Herzberg had not been using a pedestrian crossing.\nMs Herzberg was taken to a local hospital following the collision but died of her injuries.\nThe US National Highway Traffic Safety Administration and the National Transportation Safety Board earlier said they would be investigating the incident in Tempe.\nWhile self-driving cars have been involved in multiple accidents, it is thought to be the first time an autonomous car has been involved in a fatal collision with a pedestrian.\nMore on this story\nUber halts self-driving tests after death\nPublished\n20 March 2018\nToyota suspends US driverless car tests\nPublished\n20 March 2018","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Uber self-driving crash: Footage shows moment before impact\",\n\"Published_date\": \"22 March 2018\",\n\"Location\": \"Tempe, Arizona\",\n\"Victim\": \"Elaine Herzberg, 49\",\n\"Description\": \"The news is about a fatal crash involving a self-driving Uber car in Tempe, Arizona. In the 14-second video footage, the autonomous vehicle failed to slow down before hitting Elaine Herzberg, who was walking her bike across the road. Uber suspended self-driving tests in North America following the crash. The video shows the human Uber operator inside the Volvo appearing to be looking down at something while the vehicle was in autonomous mode. Moments later, the operator appears visibly shocked as she looks up to see Ms. Herzberg crossing the highway seconds before impact. Elaine Herzberg was taken to a local hospital following the collision but died of her injuries. This incident marked the first time an autonomous car was involved in a fatal collision with a pedestrian.\"\n}\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/world-us-canada-43497364","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":20,"Time":"FEBRUARY 29, 2016","Title":"Google says it bears 'some responsibility' after self-driving car hit bus","Content":"By David Shepardson\n(Reuters) - Alphabet Inc's GOOGL.O Google said on Monday it bears \"some responsibility\" after one of its self-driving cars struck a municipal bus in a minor crash earlier this month.\nA prototype of Google's own self-driving vehicle is seen during a media preview of Google's current autonomous vehicles in Mountain View, California September 29, 2015. REUTERS\/Elijah Nouvelage\nThe crash may be the first case of one of its autonomous cars hitting another vehicle and the fault of the self-driving car. The Mountain View, California-based Internet search leader said it made changes to its software after the crash to avoid future incidents.\nIn a Feb. 23 report filed with California regulators, Google said the crash took place in Mountain View on Feb. 14 when a self-driving Lexus RX450h sought to get around some sandbags in a wide lane.\nGoogle said in the filing the autonomous vehicle was traveling at less than 2 miles per hour, while the bus was moving at about 15 miles per hour.\nThe vehicle and the test driver “believed the bus would slow or allow the Google (autonomous vehicle) to continue,” it said.\nBut three seconds later, as the Google car in autonomous mode re-entered the center of the lane, it struck the side of the bus, causing damage to the left front fender, front wheel and a driver side sensor. No one was injured in the car or on the bus.\nGoogle said in a statement on Monday that “we clearly bear some responsibility, because if our car hadn’t moved, there wouldn’t have been a collision. That said, our test driver believed the bus was going to slow or stop to allow us to merge into the traffic, and that there would be sufficient space to do that.”\nThe Santa Clara Valley Transportation Authority will investigate the circumstances of the accident, Stacey Hendler Ross, spokeswoman for the Santa Clara Valley Transportation Authority, said on Monday.\nShe said the Google car caused minor damage to the bus, striking the “pivoting joint,” or flexible area in the middle of the articulated bus. After the crash, 15 passengers on the bus were transferred to another bus.\nAn investigation to determine liability is pending, she said.\nJohn M. Simpson, privacy project director for advocacy group Consumer Watchdog, said the crash “is more proof that robot car technology is not ready for auto pilot.”\nRELATED COVERAGE\nUnlocking iPhone would leave millions exposed, Apple to tell Congress\nYahoo says may write down goodwill value of Tumblr\nSee more stories\nA spokesman for the California Department of Motor Vehicles said on Monday it will speak to Google to gather additional information, but added “the DMV is not responsible for determining fault.”\nSOFTWARE REFINED\nGoogle said it has reviewed this incident “and thousands of variations on it in our simulator in detail and made refinements to our software. From now on, our cars will more deeply understand that buses (and other large vehicles) are less likely to yield to us than other types of vehicles, and we hope to handle situations like this more gracefully in the future.”\nThere has been no official determination of fault in the crash. Google has previously said that its autonomous vehicles have never been at fault in any crashes.\nThe Mountain View Police Department said no police report was filed in the incident.\nA spokesman for the U.S. National Highway Traffic Safety Administration declined to comment.\nThe crash comes as Google has been making the case that it should be able to test vehicles without steering wheels and other controls.\nIn December, Google criticized California for proposing regulations that would require autonomous cars to have a steering wheel, throttle and brake pedals when operating on public roads. A licensed driver would need to be ready to take over if something went wrong.\nGoogle said in November that in six years of its self-driving project, it has been involved in 17 minor accidents during more than two million miles of autonomous and manual driving combined.\n“Not once was the self-driving car the cause of the accident,” Google said at the time.\nReporting by David Shepardson, additional reporting by Bernie Woodall; editing by Chris Reese, G Crosse\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"A self-driving car from Alphabet Inc's Google struck a municipal bus in a minor crash in Mountain View, California, on February 14. The self-driving car was traveling at less than 2 miles per hour, while the bus was moving at about 15 miles per hour. The incident occurred when the self-driving car sought to get around some sandbags in a wide lane. Both the vehicle and the test driver believed the bus would slow or allow the autonomous vehicle to continue, but the self-driving car struck the side of the bus, causing damage to the left front fender, front wheel, and a driver-side sensor. No injuries were reported in the car or on the bus. Google acknowledged some responsibility for the incident and made software refinements to better handle situations involving buses and other large vehicles.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-google-selfdrivingcar-idUSKCN0W22DG","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":17,"Time":"Updated 8:01 PM EDT, Tue August 22, 2023","Title":"GM’s Cruise slashed fleet of robotaxis by 50% in San Francisco after collisions","Content":"CNN\n — \nCalifornia authorities have asked General Motors to “immediately” take some of its Cruse robotaxis off the road after autonomous vehicles were involved in two collisions – including one with an active fire truck – last week in San Francisco.\nCalifornia’s Department of Motor Vehicles confirmed to CNN that it is investigating “recent concerning incidents involving Cruise vehicles in San Francisco.”\n“The DMV is in contact with Cruise and law enforcement officials to determine the facts and requested Cruise to immediately reduce its active fleet of operating vehicles by 50% until the investigation is complete and Cruise takes appropriate corrective actions to improve road safety,” the department said in a statement.\nThat means Cruise, which is the self-driving subsidiary of General Motors, can have no more than 50 driverless cars in operation during the day, and 150 in operation at night, according to the department.\nThe California DMV said that Cruise has agreed to the request, and a spokesperson from Cruise told CNN that the company is investigating the firetruck crash as well.\nAccidents come just after Cruise’s big win in San Francisco\nThe accidents come less than two weeks after California regulators officially gave the green light for Cruise and competitor Waymo to charge money for robotaxi trips around San Francisco at any time of day. Prior to the approval, Cruise was only authorized to offer fared passenger service from driverless cars overnight from 10 pm to 6 am, when there are fewer pedestrians or traffic that could confuse the autonomous vehicle’s software.\nThe collisions, which both occurred on Thursday, reveal potential risks of driverless technology.\nIn a blog post, Cruise’s general manager for San Francisco said the firetruck crash occurred when an emergency vehicle that appeared to be en route to an emergency scene moved into an oncoming lane of traffic to bypass a red light. Cruise’s driverless car identified the risk, the blog post said, but it “was ultimately unable to avoid the collision.”\nThat crash resulted in one passenger being taken to the hospital via ambulance for seemingly minor injuries, according to the company.\nCruise told CNN the other crash on Thursday took place when another car ran a red light “at a high rate of speed.”\n“The AV detected the vehicle and braked but the other vehicle made contact with our AV. There were no passengers in our AV and the driver of the other vehicle was treated and released at the scene,” Hannah Lindow, a Cruise spokesperson, told CNN.\nIt is unclear whether the two accidents would have been avoided had there been a human driver rather than an autonomous vehicle (AV) involved – but the crashes were not the only two incidents involving Cruise’s driverless cars in San Francisco last week.\nOn Tuesday, Cruise confirmed on X, formerly known as Twitter, that one of its driverless taxis drove into a construction area and stopped in wet concrete.\n“This vehicle has already been recovered and we’re in communication with the city about this,” the company said.\nRoadblocks for autonomous vehicles\nThe recent events underscore the challenges of creating safe, fully driverless passenger vehicles.\nGeneral Motors acquired Cruise Automation in 2016 for $1 billion, solidifying its place in the autonomous vehicles race, but many companies have since scaled back, or abandoned their driverless car ambitions. The endeavor has proven costly, and mastering all situations that humans might face behind the wheel is difficult and time-consuming.\nRidesharing giants Uber and Lyft have both sold autonomous vehicle units in recent years. Even Tesla CEO Elon Musk, who has been optimistic about autonomous vehicle technology, has yet to fully deliver on his promise.\nTesla vehicles now come with the option to add a “full self-driving” feature in beta-testing for $15,000, but drivers must agree to “stay alert, keep your hands on the steering wheel at all times and maintain control of your car.”","Relevant":true,"Overview":"Here is the analysis of the news you provided in JSON format:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"GM’s Cruise slashed fleet of robotaxis by 50% in San Francisco after collisions\",\n        \"Summary\": \"California authorities have asked General Motors to “immediately” take some of its Cruise robotaxis off the road after autonomous vehicles were involved in two collisions – including one with an active fire truck – last week in San Francisco. The California Department of Motor Vehicles is investigating these incidents. Cruise has agreed to reduce its active fleet of operating vehicles by 50% during the investigation. One collision involved a firetruck, resulting in minor injuries to a passenger, while the other involved a car running a red light at a high speed, with no passengers injured. There was also an incident where a Cruise driverless taxi drove into a construction area and stopped in wet concrete. These events highlight the challenges and safety concerns in the development of fully autonomous vehicles.\"\n    }\n}\nThe news is indeed relevant to autonomous driving accidents, as it discusses two collisions involving General Motors' Cruise robotaxis in San Francisco. The specific information about the cases is included in the JSON response, providing details about the accidents and their consequences.","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2023\/08\/21\/tech\/gm-cruise-to-cut-robotaxi-fleet-in-san-francisco-after-collisions\/index.html","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":15,"Time":"Sun 28 Jun 2015 16.58 CEST","Title":"Google's self-driving cars are accident-prone – but it may not be their fault","Content":"Google’s self-driving cars are having a rough time on the streets of Mountain View, California. But a look at the evidence suggests it’s human error and not robots that are to blame.\nIn recent months, Google’s fleet of experimental self-driving cars have suffered five minor accidents while driving 200,000 miles around this sleepy Silicon Valley suburb. That is nearly ten times the national average for ‘property only’ fender benders, according to the National Highway Traffic Safety Administration.\nUsing a public records act request, the Guardian has obtained a report of the most recent incident, filed by Google in early June with the California Department of Motor Vehicles. In the report, Google notes that a self-driving Lexus was struck from behind 17 seconds after stopping at a traffic light. The other vehicle, a Honda Accord, simply drove into the back of it.\nSince April, Google’s Lexus SUVs have also been rear-ended by a BMW S3, a Toyota Camry and a Ford Expedition. In each case, the Google vehicle was either stationary or travelling at less than five miles per hour, giving its robotic driver no chance to avoid an impact. In fact, when robot cars meet in unfavourable circumstances, it seems they don’t collide.\nLast week it was reported that a self-driving Audi owned by Delphi Automotive took “appropriate action” to avoid one of Google’s self-driving Lexus cars after the Google car cut the Audi off on a Californian road.\nBut if Google’s self-driving algorithms are not to blame, why are its cars experiencing so many accidents?\nOne explanation could be the spinning laser scanners on their roofs, says Raj Rajkumar, designer of several autonomous cars at Carnegie Mellon University, including the winner of a 2007 self-driving vehicle competition run by Darpa, a US military research agency. “It is a distraction, and when people get distracted, I can imagine behaviours changing,” he says.\n“Another reason could be that Google cars have the Google logo splashed on them, saying they are self-driving cars. People looking at that could be distracted from their normal mode of operations,” he adds. Rajkumar is now CEO of Ottomatika, a company that helped develop technology for the first vehicle to complete a transcontinental self-driving road trip, from San Francisco to New York, in March. He noticed that passing drivers would often whip out a phone to take photos or videos of his car.\nOf course, the promise of self-driving cars is that they will reduce – or even eliminate – road traffic fatalities. “About 33,000 people die on America’s roads every year. That’s why so much of the enthusiasm for self-driving cars has focused on their potential to reduce accident rates,” says Chris Urmson, director of Google’s self-driving car program. He also points out that minor fender-benders like the ones in Mountain View often go unreported.\nThe few dozen experimental self-driving cars currently operating on public streets are packed with laser, radar, sonar and video sensors. This gives them a 360-degree view of the road ahead (and behind) that a human driver could never match. After travelling over 1.8 million miles in California, they have managed to avoid any serious accidents – and may have even prevented some from happening.\nHowever, there has been virtually no research on how human motorists respond to robotic vehicles, says Anuj Pradhan, a behavioural scientist at the University of Michigan Transportation Research Institute (UMTRI). “We do not fully understand the human reaction where self-driving cars are involved,” he says. “It’s an important question that we haven’t started looking at yet.”\nTwo of his colleagues at UMTRI, Michael Sivak and Brandon Schoettle, believe that driving is far more of a human interaction than you might expect. They found that in several types of car crashes, male-to-male accidents are underrepresented and female-to-female crashes are overrepresented, suggesting that our perceptions of fellow motorists are critical. “Furthermore, in many situations, drivers make eye contact and proceed according to the feedback received from other drivers,” they say. “Such feedback would be absent in interactions with self-driving vehicles.”\nWhen self-driving cars do become available to buy, they will be sharing the road with humans for decades to come. “Self-driving cars may have a ‘better’ driving style but it may not be a human driving style,” says Pradhan, “And that could affect how we predict or react to them.” He says that many self-driving car companies are now actively trying to humanise their algorithms to match the way people drive, slowing right down for curves, for instance, or hesitating at traffic lights.\nOne phenomenon that may help to reduce accidents in the short term: the distinctive and potentially distracting lidar sensors on top of vehicles are disappearing. In Google’s latest generation of self-driving cars, which received their permits to operate on California’s roads last week, the laser scanner has shrunk to a barely noticeable dome. Many other autonomous vehicles, including a Cadillac SRX built by Raj Rajkumar, hide them altogether. “We specifically made sure there was nothing on the car that makes it stand out,” he says.\nAnuj Pradhan thinks a better approach might be to identify autonomous vehicles so that motorists can give them leeway. “Should self-driving cars have a special marking so we can react accordingly?” he wonders. “If I see a learner driver, I give it a little more following distance. Perhaps that’s how regular drivers would react to a self-driving car.”\nUltimately, say Michael Sivak and Brandon Schoettle, we should be realistic about just how safe self-driving cars will make our highways. “It is not a foregone conclusion that a self-driving vehicle would ever perform more safely than an experienced, middle-aged driver,” they say. “And during the transition period when conventional and self-driving vehicles share the road, safety might actually worsen, at least for conventional vehicles.”\nGoogle’s rash of rear-ends might just be a coincidence, then, but we shouldn’t expect people to stop driving into robots anytime soon.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about Google's self-driving cars experiencing several minor accidents in Mountain View, California. In recent months, Google's fleet of experimental self-driving cars has suffered five minor accidents while driving 200,000 miles around the Silicon Valley suburb. These accidents have occurred when the Google vehicles were either stationary or traveling at less than five miles per hour, and it appears that human error may be to blame. The article also discusses potential reasons for these accidents, including the distraction caused by spinning laser scanners on the cars' roofs and the visibility of the Google logo on the vehicles, which might attract attention and distractions from other drivers. While there has been virtually no research on how human motorists respond to robotic vehicles, some experts suggest that the interactions between self-driving cars and human drivers could be a complex factor in accidents. Ultimately, the article raises questions about the safety of self-driving cars and their coexistence with human-driven vehicles on the road.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2015\/jun\/28\/google-self-driving-cars-accidents","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":16,"Time":"August 28, 2021","Title":"Toyota halts all self-driving e-Palette vehicles after Olympic village accident","Content":"Aug 27 (Reuters) - Toyota Motor (7203.T) said on Friday it had suspended all self-driving e-Palette transportation pods at the Tokyo Paralympic Games village, a day after one of the vehicles collided with and injured a visually impaired pedestrian.\nIn a YouTube video, Toyota Chief Executive Akio Toyoda apologized for the incident and said he offered to meet the person but was unable to do so.\n“A vehicle is stronger than a person, so I was obviously worried about how they were,” he said, answering questions about the incident.\nAdvertisement · Scroll to continue\nToyoda said the accident showed the difficulty for the self-driving vehicle to operate in the special circumstances of the village during the Paralympics with people there who are visually impaired or have other disabilities.\n“It shows that autonomous vehicles are not yet realistic for normal roads,” he said.\nThe vehicle had stopped at a T junction and was about to turn under manual control of the operator, who was using the vehicle's joystick control, when the vehicle hit the athlete going at around 1 or 2 kilometres an hour, Toyoda said. He said Paralympic officials had told him that the athlete, who remained conscious, was taken to the athlete village medical centre for treatment and was able to walk back to their residence.\nAdvertisement · Scroll to continue\nToyota Motor Corporation President Akio Toyoda, arrives in their e-Palette autonomous concept vehicle to give his presentation at the Tokyo Motor Show, in Tokyo, Japan October 23, 2019.... Acquire Licensing Rights Read more\nThe athlete is due to compete on Saturday at 1030 local time, Toyoda said.\nThe e-Palette, a fully autonomous battery-electric vehicle, was adapted specifically for use during the Tokyo Olympic and Paralympic Games, with large doors and electric ramps to allow groups of athletes to board quickly.\nThe company said it was cooperating with a local police probe to determine the cause of the accident, adding that it would also conduct its own investigation.\nAdvertisement · Scroll to continue\nIn its statement, the Japanese automaker said it would continue to coordinate closely with the Tokyo Organising Committee of the Olympic and Paralympic Games to prevent any further incidents.\nParalympic Games organizers were not immediately available for comment.\nThe 2020 Paralympic Games began on Tuesday in Tokyo, even as Japan struggles with its worst COVID-19 outbreak, with record daily cases and an overwhelmed medical system. read more\nReporting by Sanjana Shivdas in Bengaluru and Tim Kelly in Tokyo; Editing by Ramakrishnan M., Joseph White and Steve Orlofsky\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here's the analysis of the news article in JSON format:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Title\": \"Toyota halts all self-driving e-Palette vehicles after Olympic village accident\",\n    \"Date\": \"Aug 27 (Reuters)\",\n    \"Description\": \"Toyota Motor (7203.T) said on Friday it had suspended all self-driving e-Palette transportation pods at the Tokyo Paralympic Games village, a day after one of the vehicles collided with and injured a visually impaired pedestrian.\",\n    \"Additional_information\": {\n      \"Incident_details\": \"The vehicle had stopped at a T junction and was about to turn under manual control of the operator, who was using the vehicle's joystick control when the vehicle hit the athlete going at around 1 or 2 kilometers an hour.\",\n      \"Injuries\": \"The athlete, who remained conscious, was taken to the athlete village medical center for treatment and was able to walk back to their residence.\",\n      \"Response_from_Toyota\": \"Toyota Chief Executive Akio Toyoda apologized for the incident and said he offered to meet the person but was unable to do so. He also stated that autonomous vehicles are not yet realistic for normal roads.\",\n      \"Vehicle_description\": \"The e-Palette is a fully autonomous battery-electric vehicle, adapted specifically for use during the Tokyo Olympic and Paralympic Games, with large doors and electric ramps to allow groups of athletes to board quickly.\",\n      \"Investigation\": \"The company said it was cooperating with a local police probe to determine the cause of the accident, adding that it would also conduct its own investigation.\"\n    }\n  }\n}\nThis news is relevant to autonomous driving accidents, as it reports an incident involving a self-driving e-Palette vehicle colliding with a visually impaired pedestrian at the Tokyo Paralympic Games village. The incident details, injuries, Toyota's response, vehicle description, and ongoing investigations have been extracted as specific information.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/toyota-halts-all-self-driving-e-pallete-vehicles-after-olympic-village-accident-2021-08-27\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":23,"Time":"31 August 2021","Title":"Tokyo 2020: Toyota restarts driverless vehicles after accident","Content":"Tokyo 2020: Toyota restarts driverless vehicles after accident\nPublished\n31 August 2021\nShare\nIMAGE SOURCE,\nTOYOTA\nToyota is resuming operations of its autonomous vehicles at the Paralympic Games village in Tokyo following an accident.\nServices of the e-Palette pods were halted after a vehicle hit a visually impaired athlete last week.\nThe athlete was not seriously injured, but he had to pull out of an event because of cuts and bruises.\nThe vehicles will now have more operator control and extra staff to ensure they do not hit any more people.\nAramitsu Kitazono, a member of Japan's judo team, was hit as he was walking across a pedestrian crossing on Thursday.\nMr Kitazono was unable to compete in his 81kg category because of the accident.\nIn a statement late on Monday, Toyota said: \"The vehicle's sensor detected the pedestrian crossing and activated the automatic brake, and the operator also activated the emergency brake. The vehicle and pedestrians, however, came into contact before it came to a complete halt.\"\nThe company said that operators would now be given control over how fast the vehicles travel, with two members of safety staff on board, rather than one, to help look out for pedestrians.\nNew safety features will also include louder warning sounds, while pedestrian guides at busy crossings in the Paralympic village will be increased to 20 from six.\nToyota also said that it would continue to make safety improvements \"on a daily basis\" until the village closes.\nThe company also said it was co-operating with a local police investigation to determine the cause of the accident.\nChief executive's apology\nOn Friday, Toyota chief executive Akio Toyoda made a public apology after the incident.\n\"A vehicle is stronger than a person, so I was obviously worried about how they were,\" he said in a YouTube video.\nMr Toyoda said the accident illustrated just how difficult it was to operate self-driving vehicles in the special circumstances of the village during the Paralympics, with people there who are visually impaired or have other disabilities.\n\"It shows that autonomous vehicles are not yet realistic for normal roads,\" he added.\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nToyota's e-Palette driverless vehicle was shown in Shanghai last year\nAutonomous ambitions\nThe company's e-Palette pod, a fully autonomous electric vehicle, was adapted specifically for use during the Tokyo Olympic and Paralympic Games, with large doors and electric ramps to allow groups of athletes to board quickly.\nThe world's biggest carmaker, like many of its motor industry rivals, is trying to develop autonomous vehicles to operate safely on public roads.\nThe e-Palette was unveiled at the CES technology show in Las Vegas in 2018, with the company touting it as \"a symbol of mobility that goes beyond cars to provide customers services and new values.\"\nAt that time, Mr Toyoda declared that Toyota was going to transform itself from a car company to a \"mobility company\".\nYou may also be interested in:\nMedia caption,\nWhat happens if you fall asleep in a self-driving car?\nRelated Topics\nToyota\nDriverless cars\nJapan\nCar industry\nMore on this story\nToyota to cut production by 40% amid chip crisis\nPublished\n19 August 2021\nToyota halts donations to US election objectors\nPublished\n9 July 2021\n'Self-driving' cars to be allowed on roads this year\nPublished\n28 April 2021","Relevant":true,"Overview":"Here is the analysis of the news you provided in JSON format:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"Toyota has restarted its driverless vehicles at the Paralympic Games village in Tokyo after an accident. The e-Palette pods were halted due to a vehicle hitting a visually impaired athlete. The athlete sustained cuts and bruises but was not seriously injured. Toyota has implemented additional safety measures, including more operator control, extra staff to prevent further accidents, and louder warning sounds. They are also cooperating with a local police investigation to determine the accident's cause. Toyota's CEO, Akio Toyoda, publicly apologized for the incident and expressed the challenges of operating self-driving vehicles in the unique circumstances of the Paralympic village. Despite the incident, Toyota continues to work on autonomous vehicles for public roads.\"\n}\nIn summary, the news is relevant to autonomous driving accidents, as it discusses an accident involving Toyota's autonomous vehicles at the Paralympic Games village in Tokyo. The specific information about the case includes details about the accident, the safety measures implemented by Toyota, and the CEO's apology and comments on the challenges of autonomous vehicle operation in unique circumstances.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/business-58390290","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":18,"Time":"Tue 8 May 2018 12.00 CEST","Title":"Uber’s self-driving car saw the pedestrian but didn’t swerve – report","Content":"An Uber self-driving test car which killed a woman crossing the street detected her but decided not to react immediately, a report has said.\nThe car was travelling at 40mph (64km\/h) in self-driving mode when it collided with 49-year-old Elaine Herzberg at about 10pm on 18 March. Herzberg was pushing a bicycle across the road outside of a crossing. She later died from her injuries.\nAlthough the car’s sensors detected Herzberg, its software which decides how it should react was tuned too far in favour of ignoring objects in its path which might be “false positives” (such as plastic bags), according to a report from the Information. This meant the modified Volvo XC90 did not react fast enough.\nThe report also said the human safety driver was not paying close enough attention to intervene before the vehicle struck the pedestrian.\nArizona suspended Uber’s self-driving vehicle testing after the incident. The company later settled with Herzberg’s family.\nUber and the US National Transportation Safety Board (NTSB) are investigating the incident. Uber has already reached its preliminary conclusion, according to the report. A comprehensive NTSB report is expected later.\n“We’re actively cooperating with the NTSB in their investigation. Out of respect for that process and the trust we’ve built with NTSB, we can’t comment on the specifics of the incident,” an Uber spokesperson said in a statement. “In the meantime, we have initiated a top-to-bottom safety review of our self-driving vehicles programme, and we have brought on former NTSB chair Christopher Hart to advise us on our overall safety culture. Our review is looking at everything from the safety of our system to our training processes for vehicle operators, and we hope to have more to say soon.”\nThe collision marked the first fatality attributed to a self-driving car, the development of which has frequently been labelled as the only way to eliminate road deaths for those inside and outside the car.\nThe incident was not the first controversy to involve Uber’s self-driving efforts, which the company sees as key to its survival as a ride-sharing or taxi firm. The company has been involved in a long-running battle with former Google self-driving car outfit Waymo over theft of technology around Anthony Levandowski.\nUber’s self-driving technology was also called 5,000 times worse than Waymo’s in an independent analysis in 2017, while it has had legal tussles with various US states where it has tried to test vehicles.\nTesla car that crashed and killed driver was running on Autopilot, firm says\nGoogle sibling Waymo launches fully autonomous ride-hailing service","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Uber’s self-driving car saw the pedestrian but didn’t swerve – report\",\n\"Description\": \"An Uber self-driving test car which killed a woman crossing the street detected her but decided not to react immediately, a report has said. The car was travelling at 40mph (64km\/h) in self-driving mode when it collided with 49-year-old Elaine Herzberg at about 10pm on 18 March. Herzberg was pushing a bicycle across the road outside of a crossing. She later died from her injuries. Although the car’s sensors detected Herzberg, its software which decides how it should react was tuned too far in favour of ignoring objects in its path which might be 'false positives' (such as plastic bags), according to a report from the Information. This meant the modified Volvo XC90 did not react fast enough. The report also said the human safety driver was not paying close enough attention to intervene before the vehicle struck the pedestrian. Arizona suspended Uber’s self-driving vehicle testing after the incident. The company later settled with Herzberg’s family. Uber and the US National Transportation Safety Board (NTSB) are investigating the incident. Uber has already reached its preliminary conclusion, according to the report. A comprehensive NTSB report is expected later.\"\n}\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/may\/08\/ubers-self-driving-car-saw-the-pedestrian-but-didnt-swerve-report","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":22,"Time":"December 16, 2022","Title":"U.S. opens safety probe into autonomous driving system in GM's Cruise vehicles","Content":"WASHINGTON, Dec 16 (Reuters) - U.S. auto safety regulators said on Friday they have opened a formal safety probe into the autonomous driving system in vehicles produced by General Motors Co's (GM.N) robotaxi unit Cruise LLC after reports of two injuries in rear-end crashes.\nThe National Highway Traffic Safety Administration (NHTSA) said it has received notices of incidents in which self-driving Cruise vehicles \"may engage in inappropriately hard braking or become immobilized.\"\nThe agency said while both issues \"appear to be distinct, they each result in the Cruise vehicles becoming unexpected roadway obstacles.\" The safety agency's preliminary evaluation covers 242 Cruise autonomous vehicles and is the first step before it could seek a recall.\nThe investigation follows reports of three crashes in which Cruise vehicles were struck from behind by other vehicles after the autonomous vehicles braked quickly.\nAdvertisement · Scroll to continue\nCruise is offering limited service in San Francisco with a small fleet of Chevrolet Bolt EVs. Cruise on Thursday announced it had secured the first of the two California permits it needs to charge riders night and day across all of San Francisco, where it currently sells trips in a small part of the city.\nCruise said it has \"driven nearly 700,000 fully autonomous miles in an extremely complex urban environment with zero life-threatening injuries or fatalities. ... There’s always a balance between healthy regulatory scrutiny and the innovation we desperately need to save lives, which is why we’ll continue to fully cooperate with NHTSA or any regulator in achieving that shared goal.\"\nAdvertisement · Scroll to continue\nNHTSA said it plans to fully assess the potential safety-related issues posed by these two types of incidents and will review \"the commonality and safety logic of the hard braking incidents\" and the \"frequency, duration and safety consequences associated with the vehicle immobilization incidents.\"\nGM shares were down 3.5% Friday at $36.31 in midday trading.\nSan Francisco officials did not immediately comment Friday on the NHTSA investigation, which could be a setback to the company's deployment plans.\nAdvertisement · Scroll to continue\nLast month, Cruise Chief Operating Officer Gil West told Reuters the company plans to enter a \"large number of markets\" and scale operations up to \"thousands of vehicles\" in 2023.\nNHTSA said the issues \"may introduce multiple potential hazards such as a collision with a Cruise vehicle, risk to a stranded passenger exiting an immobilized Cruise vehicle, or obstruction of other traffic including emergency vehicles.\"\nAdvertisement · Scroll to continue\nThe agency said it has three reports of Cruise vehicles' automated driving system \"initiating a hard braking maneuver in response to another road user that was quickly approaching from the rear. In each case, the other road user subsequently struck the rear of the ADS-equipped vehicle.\"\nCruise said police did not ticket their vehicles in any of the three crashes.\nNHTSA said it has been notified of multiple reports involving Cruise vehicles operating without onboard human supervision becoming immobilized, which \"may strand vehicle passengers in unsafe locations, such as lanes of travel or intersections, and become an unexpected obstacle to other road users.\"\nCruise in September recalled and updated software in 80 self-driving vehicles after a June crash in San Francisco that left two people injured. NHTSA said the recalled software could “incorrectly predict” an oncoming vehicle’s path. Cruise said it had determined this unusual scenario would not recur after the update.\nGM and Cruise in February disclosed they petitioned the NHTSA for permission to deploy some self-driving vehicles without steering wheels, mirrors, turn signals or windshield wipers. That petition is still pending.\nReporting by David Shepardson; editing by Emelia Sithole-Matarise, Chizu Nomiyama and Jonathan Oatis\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"U.S. auto safety regulators have opened a formal safety probe into the autonomous driving system in vehicles produced by General Motors Co's robotaxi unit Cruise LLC after reports of two injuries in rear-end crashes. The National Highway Traffic Safety Administration (NHTSA) is investigating incidents in which self-driving Cruise vehicles may engage in inappropriately hard braking or become immobilized, resulting in the Cruise vehicles becoming unexpected roadway obstacles. The investigation follows reports of three crashes in which Cruise vehicles were struck from behind by other vehicles after the autonomous vehicles braked quickly. NHTSA plans to fully assess the potential safety-related issues posed by these incidents.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/technology\/us-opens-safety-probe-into-autonomous-driving-system-gms-cruise-vehicles-2022-12-16\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":21,"Time":"August 28, 2023","Title":"Focus: Tesla braces for its first trial involving Autopilot fatality","Content":"SAN FRANCISCO, Aug 28 (Reuters) - Tesla Inc (TSLA.O) is set to defend itself for the first time at trial against allegations that failure of its Autopilot driver assistant feature led to death, in what will likely be a major test of Chief Executive Elon Musk's assertions about the technology.\nSelf-driving capability is central to Tesla’s financial future, according to Musk, whose own reputation as an engineering leader is being challenged with allegations by plaintiffs in one of two lawsuits that he personally leads the group behind technology that failed. Wins by Tesla could raise confidence and sales for the software, which costs up to $15,000 per vehicle.\nTesla faces two trials in quick succession, with more to follow.\nThe first, scheduled for mid-September in a California state court, is a civil lawsuit containing allegations that the Autopilot system caused owner Micah Lee’s Model 3 to suddenly veer off a highway east of Los Angeles at 65 miles per hour, strike a palm tree and burst into flames, all in the span of seconds.\nThe 2019 crash, which has not been previously reported, killed Lee and seriously injured his two passengers, including a then-8-year old boy who was disemboweled. The lawsuit, filed against Tesla by the passengers and Lee's estate, accuses Tesla of knowing that Autopilot and other safety systems were defective when it sold the car.\nAdvertisement · Scroll to continue\nMUSK 'DE FACTO LEADER' OF AUTOPILOT TEAM\nThe second trial, set for early October in a Florida state court, arose out of a 2019 crash north of Miami where owner Stephen Banner’s Model 3 drove under the trailer of an 18-wheeler big rig truck that had pulled into the road, shearing off the Tesla's roof and killing Banner. Autopilot failed to brake, steer or do anything to avoid the collision, according to the lawsuit filed by Banner's wife.\nAdvertisement · Scroll to continue\nTesla denied liability for both accidents, blamed driver error and said Autopilot is safe when monitored by humans. Tesla said in court documents that drivers must pay attention to the road and keep their hands on the steering wheel.\n\"There are no self-driving cars on the road today,\" the company said.\nThe civil proceedings will likely reveal new evidence about what Musk and other company officials knew about Autopilot’s capabilities – and any possible deficiencies. Banner’s attorneys, for instance, argue in a pretrial court filing that internal emails show Musk is the Autopilot team's \"de facto leader\".\nAdvertisement · Scroll to continue\nTesla and Musk did not respond to Reuters’ emailed questions for this article, but Musk has made no secret of his involvement in self-driving software engineering, often tweeting about his test-driving of a Tesla equipped with \"Full Self-Driving\" software. He has for years promised that Tesla would achieve self-driving capability only to miss his own targets.\nTesla won a bellwether trial in Los Angeles in April with a strategy of saying that it tells drivers that its technology requires human monitoring, despite the \"Autopilot\" and \"Full Self-Driving\" names. The case was about an accident where a Model S swerved into the curb and injured its driver, and jurors told Reuters after the verdict that they believed Tesla warned drivers about its system and driver distraction was to blame.\nAdvertisement · Scroll to continue\nSTAKES HIGHER FOR TESLA\nThe stakes for Tesla are much higher in the September and October trials, the first of a series related to Autopilot this year and next, because people died.\n\"If Tesla backs up a lot of wins in these cases, I think they're going to get more favorable settlements in other cases,\" said Matthew Wansley, a former General Counsel of nuTonomy, an automated driving startup and Associate Professor of Law at Cardozo School of Law.\nOn the other hand, \"a big loss for Tesla - especially with a big damages award\" could \"dramatically shape the narrative going forward,\" said Bryant Walker Smith, a law professor at the University of South Carolina.\nIn court filings, the company has argued that Lee consumed alcohol before getting behind the wheel and that it is not clear whether Autopilot was on at the time of crash.\nJonathan Michaels, an attorney for the plaintiffs, declined to comment on Tesla’s specific arguments, but said \"we're fully aware of Tesla's false claims including their shameful attempts to blame the victims for their known defective autopilot system.\"\nIn the Florida case, Banner's attorneys also filed a motion arguing punitive damages were warranted. The attorneys have deposed several Tesla executives and received internal documents from the company that they said show Musk and engineers were aware of, and did not fix, shortcomings.\nIn one deposition, former executive Christopher Moore testified there are limitations to Autopilot, saying it \"is not designed to detect every possible hazard or every possible obstacle or vehicle that could be on the road,\" according to a transcript reviewed by Reuters.\nIn 2016, a few months after a fatal accident where a Tesla crashed into a semi-trailer truck, Musk told reporters that the automaker was updating Autopilot with improved radar sensors that likely would have prevented the fatality.\nBut Adam (Nicklas) Gustafsson, a Tesla Autopilot systems engineer who investigated both accidents in Florida, said that in the almost three years between that 2016 crash and Banner’s accident, no changes were made to Autopilot’s systems to account for cross-traffic, according to court documents submitted by plaintiff lawyers.\nThe lawyers tried to blame the lack of change on Musk. \"Elon Musk has acknowledged problems with the Tesla autopilot system not working properly,\" according to plaintiffs' documents. Former Autopilot engineer Richard Baverstock, who was also deposed, stated that \"almost everything\" he did at Tesla was done at the request of \"Elon,\" according to the documents.\nTesla filed an emergency motion in court late on Wednesday seeking to keep deposition transcripts of its employees and other documents secret. Banner’s attorney, Lake \"Trey\" Lytal III, said he would oppose the motion.\n\"The great thing about our judicial system is Billion Dollar Corporations can only keep secrets for so long,\" he wrote in a text message.\nReporting by Dan Levine and Hyunjoo Jin in San Francisco, Editing by Peter Henderson and Grant McCool\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n{\n\"Title\": \"Tesla braces for its first trial involving Autopilot fatality\",\n\"Trial Date\": \"Mid-September in a California state court (Micah Lee case)\",\n\"Accident Description\": \"Micah Lee’s Model 3 veered off a highway at 65 miles per hour, struck a palm tree, and burst into flames, resulting in death and serious injuries to passengers, including an 8-year-old boy who was disemboweled.\",\n\"Allegations\": \"Plaintiffs accuse Tesla of knowing that Autopilot and other safety systems were defective when it sold the car.\",\n\"Tesla's Defense\": \"Tesla denies liability, blames driver error, and emphasizes the need for human monitoring while using Autopilot.\"\n},\n{\n\"Trial Date\": \"Early October in a Florida state court (Stephen Banner case)\",\n\"Accident Description\": \"Stephen Banner’s Model 3 drove under the trailer of an 18-wheeler big rig truck, shearing off the Tesla's roof and killing Banner. Autopilot failed to brake, steer, or avoid the collision.\",\n\"Allegations\": \"Banner's wife filed a lawsuit claiming that Autopilot failed to take preventive actions.\",\n\"Punitive Damages\": \"Banner's attorneys argued for punitive damages, and internal documents suggest Musk and engineers were aware of Autopilot shortcomings.\",\n\"Tesla's Defense\": \"Tesla argues that Lee consumed alcohol before the crash and that it's not clear if Autopilot was active during the incident.\"\n},\n{\n\"Stakes\": \"The outcomes of these trials have significant implications for Tesla, as they involve fatalities. Winning these cases could impact future settlements, while a loss with substantial damages could shape the narrative against Tesla.\",\n\"Engineering Leadership\": \"Elon Musk's role as a leader in self-driving software engineering is being questioned, with allegations suggesting his involvement in Autopilot's development.\"\n}\n]\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/tesla-braces-its-first-trial-involving-autopilot-fatality-2023-08-28\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":27,"Time":"Updated 3:14 PM EDT, Wed June 15, 2022","Title":"Teslas using driver-assist systems were involved in 273 crashes over the past 9 months, according to NHTSA","Content":"Washington, DC\nCNN Business\n — \nThe National Highway Traffic Safety Administration released on Wednesday nine months of crash data from vehicles using driver-assist technologies like Tesla Autopilot as well as fully autonomous vehicles like Waymo’s robotaxis.\nNHTSA broke crash data into two categories based on the level of the autonomous systems: driver-assist systems – which offer speed and steering input – and fully autonomous technologies, which are intended to one day safely function without human intervention. NHTSA found that there have been 367 crashes in the last nine months involving vehicles that were using these driver-assist technologies. 273 of the incidents involved a Tesla system, either its “full self-driving” software or its precursor, Tesla Autopilot.\nThere were 130 crashes involving fully automated driving systems, 62 of which were Waymo crashes. Transdev, a shuttle operator, reported 34 crashes, and Cruise, which offers robotaxis for General Motors in San Francisco, reported 23.\nThe data lacks critical context like fleet size or the number of miles traveled, making it impossible to fairly compare the safety of the different technologies. Not all relevant crashes may be included in the data set, NHTSA said, because crash data recording may vary widely among manufacturers.\nA Tesla dashboard is shown as the Autopilot software disengages during a drive in Brooklyn, New York.\nChristopher Goodney\/Bloomberg\/Getty Images\n“I would advise caution before attempting to draw conclusions based only on the data we’re releasing. In fact, the data alone may raise more questions than they answer,” NHTSA administrator Steven Cliff told reporters in a briefing Tuesday.\nTwo of the technologies with the most reported crashes are also two of the most commonly used systems. Tesla Autopilot, for example, comes standard on all of its vehicles, unlike competing driver-assist systems from other automakers. Drivers describe using Autopilot regularly because they say it can make them feel less fatigued after long drives. Waymo, the other company with the most total crashes, operates the most extensive robotaxi service in the country, with operations in much of metropolitan Phoenix, Arizona and San Francisco.\nFor the first time, automakers and robotaxi operators have had to report to NHTSA data about crashes involving these vehicles. NHTSA says it will use the data to identify safety issues and intervene as necessary. Pony.ai, which is testing robotaxis in California, recalled three of its vehicles this year following data NHTSA gathered from this process.\nOf the 497 crashes total, 43% occurred in California. The state is home to Silicon Valley, making it a hotspot for testing new technologies.\nNHTSA found that of the 367 driver-assist crashes reported, there were six fatalities and five serious injuries.\nFederal investigators expand probe into Tesla Autopilot crashes\nThe safety risks of these new technologies have drawn the attention of safety advocates for years. There are not specific regulations for driver-assist systems, leaving automakers to market and describe the systems as they so choose.\nTesla’s Autopilot and “full self-driving” software have been especially controversial. NHTSA’s investigation into Teslas rear-ending first responders’ vehicles was expanded last week and could lead to a recall.\nThe National Transportation Safety Board has investigated fatal crashes involving Autopilot and called for the automaker to make changes, such as developing technology to more effectively sense the driver’s level of engagement and alert them when their engagement is lacking.\nTesla has released data since 2018 claiming that Autopilot has a lower crash rate per mile than typical driving. But safety experts caution that Tesla’s analysis compares apples to oranges, as most Autopilot driving takes place on highways, where crash rates per mile are much lower than all driving.\nTesla states that drivers using Autopilot must remain alert and be prepared to take full control of the vehicle at a moment’s notice. However, drivers using technologies like Autopilot risk becoming distracted, experts say.\nWaymo says it sees value in nationally standardized and uniform crash reporting as autonomous driving is developed.\nJustin Sullivan\/Getty Images\nA 2021 MIT study found that Tesla drivers looked away from the road more frequently while using Autopilot than when driving without the driver-assist system.\nNHTSA said that its investigation into Teslas rear-ending emergency vehicles while using Autopilot found that in 37 of 43 crashes with detailed car log data available, drivers had their hands on the wheel in the last second prior to the collision.\nFor years, Tesla detected torque on the wheel to determine if a driver was engaged. It’s begun to use an in-car camera for detecting distraction, which many safety experts say is a superior method, as cameras can track eye movement.\n“We see value in having nationally standardized and uniform crash reporting during this early stage of the development and deployment of autonomous driving technology, and there’s public benefit in NHTSA sharing its findings,” Waymo said in response to the data. Tesla did not respond to a request for comment.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Teslas using driver-assist systems were involved in 273 crashes over the past 9 months, according to NHTSA\",\n\"Summary\": \"The news is about accidents involving vehicles using driver-assist systems, including Tesla Autopilot. The National Highway Traffic Safety Administration (NHTSA) released nine months of crash data, revealing that there have been 273 crashes involving Tesla systems. The data also includes crashes related to fully autonomous vehicles like Waymo's robotaxis.\",\n\"Key_Facts\": {\n\"Total_Crashes\": 273,\n\"Crashes_with_Fatalities\": 6,\n\"Crashes_with_Serious_Injuries\": 5,\n\"Companies_Involved\": {\n\"Tesla\": 273,\n\"Waymo\": 62,\n\"Transdev\": 34,\n\"Cruise\": 23\n},\n\"Geographic_Distribution\": {\n\"California_Crashes\": 43% of the total\n},\n\"Data_Quality_Caveats\": \"The data lacks critical context like fleet size or the number of miles traveled, making it impossible to fairly compare the safety of the different technologies. Not all relevant crashes may be included in the data set due to variations in crash data recording among manufacturers.\"\n},\n\"Regulatory_Status\": \"There are no specific regulations for driver-assist systems, leaving automakers to market and describe the systems as they choose. Safety advocates have raised concerns about the safety of these technologies.\",\n\"NHTSA's Response\": \"NHTSA plans to use the data to identify safety issues and intervene as necessary. The investigation into Tesla's Autopilot crashes has been expanded, and it could lead to a recall.\",\n\"Waymo's Response\": \"Waymo sees value in nationally standardized and uniform crash reporting as autonomous driving technology develops.\"\n}\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2022\/06\/15\/cars\/nhtsa-autopilot-robotaxi-crash-data\/index.html","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":28,"Time":"JUNE 4, 2020","Title":"Self-driving cars could only prevent a third of U.S. crashes: study","Content":"By Tina Bellon\n(Reuters) - Self-driving cars, long touted by developers as a way to eliminate road deaths, could likely only prevent a third of all U.S. road crashes, according to a study released on Thursday.\nThe Insurance Institute for Highway Safety (IIHS), a research group financed by U.S. insurers, found the remaining crashes were caused by mistakes that self-driving systems are not equipped to handle any better than human drivers.\nPartners for Automated Vehicle Education, a consortium of self-driving companies and researchers, said in a statement on Thursday the study wrongly assumed that automated cars could only prevent crashes caused by perception errors and incapacitation.\nSome 72% of crashes were avoidable, based on the study’s calculations, if accidents caused by speeding and violation of traffic laws were included, the consortium said.\nTraffic experts say roughly nine in 10 crashes result from human error and more than 36,000 people are estimated to have died in U.S. car crashes last year.\nSelf-driving vehicle developers, including traditional automakers and technology companies, have repeatedly positioned fully automated driving as a tool to drastically reduce road deaths.\nBut not all human mistakes can be eliminated by camera, radar and other sensor-based technology, according to the IIHS analysis of more than 5,000 representative police-reported crashes nationwide.\nOne-third of all crashes were the exclusive result of sensing and perception errors, or driver incapacitation, the study found.\nMost crashes were due to more complex errors, such as making wrong assumptions about other road users’ actions, driving too fast or too slow for road conditions, or making incorrect evasive maneuvers. Many crashes resulted from multiple mistakes.\n“Our goal was to show that if you don’t deal with those issues, self-driving cars won’t deliver massive safety benefits,” said Jessica Cicchino, IIHS vice president for research and a coauthor of the study.\nReporting by Tina Bellon; Editing by Richard Chang\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the analysis of the provided news article in JSON format:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Title\": \"Self-driving cars could only prevent a third of U.S. crashes: study\",\n    \"Source\": \"Reuters\",\n    \"Summary\": \"Self-driving cars, long touted by developers as a way to eliminate road deaths, could likely only prevent a third of all U.S. road crashes, according to a study released on Thursday. The Insurance Institute for Highway Safety (IIHS), a research group financed by U.S. insurers, found the remaining crashes were caused by mistakes that self-driving systems are not equipped to handle any better than human drivers.\",\n    \"Key_Findings\": [\n      \"Self-driving cars may only prevent one-third of U.S. road crashes.\",\n      \"The Insurance Institute for Highway Safety (IIHS) conducted the study.\",\n      \"Remaining crashes are caused by mistakes not handled better by self-driving systems.\",\n      \"Some 72% of crashes could be avoided, including those caused by speeding and traffic violations.\",\n      \"Approximately nine in 10 crashes result from human error.\",\n      \"One-third of crashes are due to sensing and perception errors or driver incapacitation.\",\n      \"Most crashes result from complex errors, like making wrong assumptions about other road users' actions, driving too fast or too slow, or making incorrect evasive maneuvers.\"\n    ]\n  }\n}\nThe news article discusses a study conducted by the Insurance Institute for Highway Safety (IIHS), which indicates that self-driving cars may only prevent one-third of U.S. road crashes. The study also highlights that the remaining crashes are caused by mistakes that self-driving systems are not better equipped to handle than human drivers. Additionally, the article mentions that some 72% of crashes could potentially be avoided, including those caused by speeding and traffic violations. It is important to note that most crashes are attributed to human error, and not all of these errors can be eliminated by camera, radar, and sensor-based technology used in self-driving vehicles.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-self-driving-idUSKBN23B294","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":24,"Time":"Thu 9 Nov 2017 11.52 CET","Title":"Self-driving bus involved in crash less than two hours after Las Vegas launch","Content":"It took less than two hours for Las Vegas’s brand new self-driving shuttle to end up in a crash on Wednesday – thanks to a human.\nThe autonomous bus made its debut on public roads around the so called Innovation District in downtown Las Vegas in front of cameras and celebrities, dubbed America’s first self-driving shuttle pilot project geared toward the public. But within two hours it had already been involved in a minor crash with a lorry. No injuries were reported.\nJenny Wong, a passenger on the shuttle at the time of the crash, told local news station KSNV: “The shuttle just stayed still. And we were like, it’s going to hit us, it’s going to hit us. And then it hit us.\n“The shuttle didn’t have the ability to move back. The shuttle just stayed still.”\nLas Vegas police officer Aden Ocampo-Gomez said the truck’s driver was at fault for the crash and was cited for illegal backing.\n“The shuttle did what it was supposed to do, in that its sensors registered the truck and the shuttle stopped to avoid the accident,” the city said in a statement. “Unfortunately the delivery truck did not stop and grazed the front fender of the shuttle. Had the truck had the same sensing equipment that the shuttle has, the accident would have been avoided.”\nThe oval-shaped shuttle can seat up to eight people and has an attendant and computer monitor, but no steering wheel or brake pedals. Developed by French company Navya, it uses GPS, electronic kerb sensors and other technology to find its way at no more than 15mph.\nBefore it crashed, dozens of people had lined up to get a free trip on a 0.6-mile loop around Fremont East, Las Vegas, including Nascar driver Danica Patrick and magic duo Penn and Teller. City spokesman Jace Radke said the shuttle took two more loops after the crash.\nThe year-long pilot project, sponsored by AAA Northern California, Nevada and Utah, is expected to carry 250,000 people. The AAA said human error was responsible for more than 90% of the 30,000 deaths on US roads in 2016, and that robotic cars could help reduce the number of incidents.\nGoogle sibling Waymo announced on Tuesday that it is launching a fully autonomous Uber-like ride-hailing service with no human driver behind the wheel in Phoenix, Arizona in the next few months, making it the first such service accessible to the public with no one to take control in an emergency.\nLiving my anxiety dream: taking a ride in a Google self-driving car","Relevant":true,"Overview":"Here's the JSON format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Self-driving bus involved in crash less than two hours after Las Vegas launch\",\n        \"Description\": \"The autonomous bus made its debut on public roads in Las Vegas and was involved in a minor crash with a lorry within two hours of its launch. No injuries were reported. The crash was attributed to the truck's driver who was cited for illegal backing. The shuttle's sensors registered the truck and stopped to avoid the accident, but the truck did not stop and grazed the front fender of the shuttle. The shuttle is an oval-shaped vehicle developed by French company Navya, with seating for up to eight people, an attendant, and a computer monitor. It uses GPS and electronic sensors for navigation and had no steering wheel or brake pedals. The pilot project, sponsored by AAA, aims to carry 250,000 people and addresses the potential for reducing human errors on the roads with autonomous vehicles.\"\n    }\n}\nThe news is indeed about an autonomous driving-related incident involving a self-driving bus in Las Vegas, which was involved in a crash within two hours of its launch.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2017\/nov\/09\/self-driving-bus-crashes-two-hours-after-las-vegas-launch-truck-autonomous-vehicle","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":25,"Time":"Thu 16 Feb 2023 20.41 CET","Title":"Tesla recalls 362,000 vehicles over self-driving software flaws that risk crashes","Content":"Tesla said it would recall 362,000 US vehicles to update its Full Self-Driving (FSD) Beta software after regulators said on Thursday the driver assistance system did not adequately adhere to traffic safety laws and could cause crashes.\nThe National Highway Traffic Safety Administration (NHTSA) said the Tesla software allows a vehicle to “exceed speed limits or travel through intersections in an unlawful or unpredictable manner increases the risk of a crash”.\nTesla will release an over-the-air (OTA) software update free of charge, and the electric vehicle maker said is not aware of any injuries or deaths that may be related to the recall issue. The automaker said it had 18 warranty claims.\nTesla shares were down 1.6% at $210.76 on Thursday afternoon.\nThe recall covers 2016-2023 Model S, Model X, 2017-2023 Model 3, and 2020-2023 Model Y vehicles equipped with FSD Beta software or pending installation.\nNHTSA asked Tesla to recall the vehicles, but the company said despite the recall it did not concur with NHTSA’s analysis. The move is a rare intervention by federal regulators in a real-world testing program that the company sees as crucial to the development of cars that can drive themselves. FSD Beta is used by hundreds of thousands of Tesla customers.\nThe setback for Tesla’s automated driving effort comes about two weeks before the company’s 1 March investor day, during which its chief executive Elon Musk is expected to promote the EV maker’s artificial intelligence capability and plans to expand its vehicle lineup.\nTesla could not immediately be reached for comment.\nNHTSA has an ongoing investigation it opened in 2021 into 830,000 Tesla vehicles with driver assistance system Autopilot over a string of crashes with parked emergency vehicles. NHTSA is reviewing whether Tesla vehicles adequately ensure drivers are paying attention. NHTSA said on Thursday despite the FSD recall its “investigation into Tesla’s Autopilot and associated vehicle systems remains open and active”.\nTesla said in “certain rare circumstances … the feature could potentially infringe upon local traffic laws or customs while executing certain driving maneuvers”.\nPossible situations where the problem could occur include traveling or turning through certain intersections during a yellow traffic light and making a lane change out of certain turn-only lanes to continue traveling straight, NHTSA said.\nNHTSA said “the system may respond insufficiently to changes in posted speed limits or not adequately account for the driver’s adjustment of the vehicle’s speed to exceed posted speed limits”.\nLast year, Tesla recalled nearly 54,000 US vehicles with FSD Beta software that may allow some models to conduct “rolling stops” and not come to a complete stop at some intersections, posing a safety risk, NHTSA said.\nTesla and NHTSA say FSD’s advanced driving features do not make the cars autonomous and require drivers to pay attention.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Tesla is recalling 362,000 US vehicles to update its Full Self-Driving (FSD) Beta software due to concerns raised by the National Highway Traffic Safety Administration (NHTSA). The NHTSA found that the FSD Beta software did not adhere adequately to traffic safety laws and could potentially lead to crashes. The recall covers Model S, Model X, Model 3, and Model Y vehicles from 2016 to 2023 equipped with FSD Beta software or pending installation. Tesla plans to release a free over-the-air software update to address the issue. The company has not reported any injuries or deaths related to the recall. Additionally, Tesla is facing ongoing investigations related to its Autopilot system's safety and driver attention monitoring by the NHTSA.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2023\/feb\/16\/tesla-recall-full-self-driving-cars","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":26,"Time":"Tue 27 Mar 2018 04.33 CEST","Title":"Arizona suspends Uber’s self-driving car testing after fatality","Content":"Arizona governor Doug Ducey suspended Uber’s self-driving vehicle testing on Monday following a pedestrian fatality in a Phoenix suburb last week.\nDucey told Uber’s chief executive Dara Khosrowshahi that video footage of the crash raised concerns about the company’s ability to safely test its technology in Arizona.\nHe said he expects public safety to be the top priority for those who operate self-driving cars. “The incident that took place on 18 March is an unquestionable failure to comply with this expectation,” Ducey said.\nSelf-driving Uber kills Arizona woman in first fatal crash involving pedestrian\nRead more\nThe move by the Republican governor marks a major step back from his embrace of self-driving vehicles. He previously welcomed Uber and other autonomous vehicle companies to use Arizona as a place for testing under few, if any, regulations.\nIn early March, he authorized self-driving vehicle companies to run tests without a person in the car to act as a safety operator.\nPolice in Tempe released a 22-second video showing a woman walking from a darkened area onto a street just before an Uber SUV strikes her. The Volvo was in self-driving mode with a human backup driver at the wheel when it struck 49-year-old Elaine Herzberg, police said.\nUber’s human backup driver appears on the video to be looking down before crash and appears startled about the time of the impact.\nExperts who viewed the video said the SUV’s sensors should have seen the woman pushing a bicycle and braked before the impact.\nThe fatal crash in Tempe was the first fatality involving a self-driving vehicle in the US. Uber immediately suspended its self-driving vehicle testing in Arizona, Pittsburgh, San Francisco and Toronto.\nOn Friday, The New York Times reported the company’s own documents showed the testing program was rife with issues. They included trouble driving through construction zones and requiring far more human intervention than competing companies.\nIn Arizona, companies such as Uber only need to carry minimum liability insurance to operate self-driving cars. They are not required to track crashes or report any information to the state.\nUber did not immediately respond to a request for comment.","Relevant":true,"Overview":"Here is the JSON format response based on the news you provided:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Title\": \"Arizona suspends Uber’s self-driving car testing after fatality\",\n    \"Summary\": \"Arizona governor Doug Ducey suspended Uber’s self-driving vehicle testing on Monday following a pedestrian fatality in a Phoenix suburb last week. The incident involved a self-driving Uber vehicle that struck and killed a woman named Elaine Herzberg. The vehicle was in self-driving mode with a human backup driver at the wheel during the incident. This marks the first fatal crash involving a pedestrian and a self-driving vehicle in the US. The governor expressed concerns about the safety of Uber's technology and emphasized the importance of public safety when operating self-driving cars.\"\n  }\n}\nThe news is indeed relevant to autonomous driving accidents, as it discusses a fatal accident involving a self-driving Uber vehicle and provides specific information about the incident, including the victim's name, the circumstances of the accident, and the suspension of Uber's self-driving testing in Arizona.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/mar\/27\/arizona-suspends-ubers-self-driving-car-testing-after-fatality","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":33,"Time":"MARCH 20, 2018","Title":"Fatal U.S. self-driving auto accident raises novel legal questions","Content":"By Tina Bellon\nNEW YORK (Reuters) - The death of a pedestrian hit by a self-driving Uber vehicle in Arizona this week could offer a test of who can be held legally responsible for accidents when a human is no longer at the wheel, legal experts said.\nTraffic passes an intersection just north of the location where a woman pedestrian was struck and killed by an Uber self-driving sport utility vehicle in Tempe, Arizona, U.S., March 19, 2018. REUTERS\/Rick Scuteri\nAny litigation that arises from the accident, the first fatality involving a fully autonomous vehicle, could pit the ride-hailing service against technology suppliers and the vehicle’s manufacturer, legal experts said. It could also provide a window into confidential indemnification agreements that companies developing self-driving car systems may have reached to shield themselves legally.\nIn Sunday’s accident, Elaine Herzberg, 49, was walking her bicycle outside the crosswalk on a four-lane road in the Phoenix suburb of Tempe when she was hit by the Volvo XC90 SUV, which was traveling in autonomous mode at about 40 miles per hour, police said.\nTempe Police Chief Sylvia Moir, who saw video of the crash, told the San Francisco Chronicle late Monday that it “would have been difficult to avoid this collision in any kind of mode (autonomous or human-driven) based on how she came from the shadows right into the roadway.”\nThe Tempe Police Department said in a statement later that the department does not determine fault in vehicle collisions. “Ultimately the investigation will be submitted to the Maricopa County Attorney’s Office for review and any potential charges,” it said.\nIt was not clear whether a lawsuit would be filed on behalf of Herzberg’s estate.\nBryant Walker Smith, a law professor at the University of South Carolina with expertise in self-driving cars, said most states, including Arizona, require drivers to exercise due care to avoid hitting pedestrians on a roadway, even if the pedestrians use an unmarked crosswalk.\nSergei Lemberg, a California-based lawyer who has brought several lawsuits against self-driving car manufacturers, said that Uber, Volvo, which manufactured the sport utility vehicle, and any companies that supplied self-driving technology could potentially be held liable if litigation ensued from the Tempe accident. The “safety” driver who was behind the wheel but who appears not to have operated the vehicle could also be named as a defendant, Lemberg said.\n“Suing all these parties would be my top thought right now,” he said.\nRELATED COVERAGE\nFactbox: How United States, others regulate autonomous vehicle testing\nUber said in a statement it was fully cooperating with local authorities investigating the accident.\nVolvo, the Swedish car brand owned by China’s Geely, confirmed its vehicle was involved in the crash but said the software controlling the SUV was not its own. Volvo said in 2015 it would accept full liability when one of its vehicles equipped with its own self-driving system was in autonomous mode. The company said in 2017 it had agreed to supply Uber with 24,000 self-driving vehicles.\nUntil now, most litigation over non-fatal accidents involving self-driving vehicles has been confidentially settled, according to legal experts. A case in which a motorcyclist sued General Motors GM.N after a collision last year with one of the company's autonomous test cars in San Francisco is pending.\nGM has said in a statement regarding the lawsuit that in its collision report the San Francisco Police Department determined that the autonomous vehicle was not at fault in the accident.\nLegal experts said liability in the Tempe case would depend on the facts of the accident and the results of U.S. National Highway Safety Administration and National Transportation Safety Board investigations.\nWITH NO DRIVER, SPOTLIGHT ON DESIGN\nCar accident litigation usually turns on whether a driver acted negligently, or failed to exercise a reasonable level of care.\nBy contrast, a lawsuit involving an autonomous vehicle could revolve around whether the self-driving system had a design defect, legal experts said.\nDesign defect claims do not require a finding of fault or negligence. To prevail, a plaintiff must show only that a product had an inherent design defect that would render it unsafe.\nAutomakers and software writers could counter with detailed data gathered by on-board sensors on how cars behaved during a collision, experts said, to show that it was impossible for the vehicle to avoid a collision and that all the systems functioned properly.\nOne question that would likely arise in litigation is whether Uber, Volvo or any companies that supplied self-driving technology entered into indemnification agreements, in which an entity assumes another company’s liability costs.\nPaul Rafferty, a California-based lawyer who represents carmakers, said such agreements in the autonomous vehicle world are confidential and disputes over indemnification are generally arbitrated or mediated behind closed doors.\nVolvo declined to comment on whether it had entered into indemnification agreements with Uber.\nJennifer Dukarski, a Detroit-based lawyer representing suppliers of autonomous-vehicle technology, said the lawsuit filed by the San Francisco motorcyclist against GM raised only negligence claims.\nSo if any litigation in the Arizona accident alleges design defect claims, it could set legal precedent and be “very impactful” on the nascent autonomous vehicle industry, she said.\nRegardless of who is at fault, the University of South Carolina’s Smith said he would expect Uber, which has faced past scrutiny over its business practices, to settle any litigation quickly to avoid the public spotlight.\n“Only if Uber believes that it was wholly without fault could I see this case going to trial,” he said.\nReporting by Tina Bellon in New York; Additional reporting by Paul Lienert in Detroit; Editing by Noeleen Walder and Frances Kerry\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a fatal self-driving auto accident involving a self-driving Uber vehicle in Arizona. The accident resulted in the death of a pedestrian named Elaine Herzberg, who was hit by the Volvo XC90 SUV traveling in autonomous mode at about 40 miles per hour. The article discusses the potential legal responsibility for the accident, including Uber, Volvo, and companies supplying self-driving technology. It also mentions the involvement of a 'safety' driver who may be named as a defendant. The liability in the case depends on the facts of the accident and the results of investigations by U.S. National Highway Safety Administration and National Transportation Safety Board. Furthermore, the article highlights the possibility of design defect claims and the impact they could have on the autonomous vehicle industry. Uber is mentioned as fully cooperating with local authorities investigating the accident.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-uber-liability-anal-idUSKBN1GW2SP","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":34,"Time":"27 April 2016","Title":"Volvo to test self-drive cars in UK","Content":"Volvo to test self-drive cars in UK\nPublished\n27 April 2016\nShare\nIMAGE SOURCE,\nVOLVO\nImage caption,\nAdapted versions of Volvo's XC90 vehicle will be used in the trials\nSwedish carmaker Volvo plans to run driverless car trials on public roads around London from next year.\nInitially the cars will be semi-autonomous, with real families travelling in them.\nSimilar trials will run in the Swedish city Gothenburg.\nDrive Me London, as the trial is dubbed, will record data from these everyday journeys to help develop the technology and adapt it to real-world conditions.\nDetails of which roads the cars will be tested on are not yet known. The trial is due to expand in 2018 from a handful of autonomous cars to 100, making it the largest of its kind in the UK.\n\"Autonomous driving (AD) represents a leap forward in car safety,\" said Håkan Samuelsson, president of Volvo Cars.\n\"The sooner AD cars are on the roads, the sooner lives will start being saved.\"\nUp to 90% of car accidents are presently caused by driver error and research suggests that autonomous driving could reduce this by at least 30%.\nMultiple benefits\nIMAGE SOURCE,\nGOOGLE\nImage caption,\nGoogle wants self-drive cars with no steering wheels or pedals\nBritain wants to be at the forefront of autonomous driving and, this summer, cars will take to the pavements around the O2 arena in Greenwich in London, as well as in Milton Keynes. Other driverless car trials are running in Coventry and Bristol.\nThis week in the US, Google - which is also at the forefront of the driverless car revolution - joined forces with Volvo and Ford, as well as taxi-hailing firms Uber and Lyft, to form a coalition to promote the benefits of self-driving cars and help push through the necessary legislation to make fully autonomous vehicles a reality.\nSome experts predict that cars without a person in the driving seat could hit the roads as early as 2021 although, for that to happen, there need to be big changes in legislation around the world.\nMr Samuelsson said that there were \"multiple benefits\" to autonomous driving including the reduction of congestion and pollution, as well as safety.\n\"That is why governments globally need to put in place the legislation and infrastructure to allow AD cars onto the streets as soon as possible. The car industry cannot do it all by itself. We need governmental help.\"\nMore on this story\nGoogle partners Uber on driverless cars\nPublished\n26 April 2016\nVolvo plans China driverless car tests\nPublished\n7 April 2016\nRelated Internet Links\nVolvo\nThe BBC is not responsible for the content of external sites.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Volvo plans to run driverless car trials on public roads around London from next year. Initially, the cars will be semi-autonomous, with real families traveling in them. The trial, called 'Drive Me London,' will record data from everyday journeys to help develop the technology and adapt it to real-world conditions. Details of which roads the cars will be tested on are not yet known. The trial is due to expand in 2018 from a handful of autonomous cars to 100, making it the largest of its kind in the UK. Håkan Samuelsson, president of Volvo Cars, mentioned that autonomous driving could reduce car accidents caused by driver error by at least 30%. Britain is actively involved in autonomous driving trials and aims to be at the forefront of this technology.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-36149043","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":30,"Time":"Fri 1 Jul 2016 19.43 CEST","Title":"Tesla driver killed while using autopilot was watching Harry Potter, witness says","Content":"The Tesla driver killed in the first known fatal crash involving a self-driving car may have been watching a Harry Potter movie at the time of the collision in Florida, according to a truck driver involved in the crash.\nThe truck driver, Frank Baressi, 62, told the Associated Press that the Tesla driver Joshua Brown, 40, was “playing Harry Potter on the TV screen” during the collision and was driving so fast that “he went so fast through my trailer I didn’t see him”.\nTesla driver dies in first fatal crash while using autopilot mode\nRead more\nThe disclosure raises further questions about the 7 May crash in Williston, Florida, which occurred after Brown put his Model S into Tesla’s autopilot mode, which is able to control a car while it’s driving on the highway.\nThe fatal crash, which federal highway safety regulators are now investigating, is a significant setback and a public relations disaster for the growing autonomous vehicle industry. Tesla Motors Inc’s shares, however, were down less than 1% on Friday in early trading.\nBaressi, who did not immediately respond to requests for comment, said the Harry Potter movie “was still playing when he died and snapped a telephone pole a quarter mile down the road”. He told the AP, however, that he heard the movie but didn’t see it.\nThe Florida highway patrol told Reuters that there was a portable DVD player in the vehicle.\nAccording to Tesla’s account of the crash, the car’s sensor system, against a bright spring sky, failed to distinguish a large white 18-wheel truck and trailer crossing the highway. In a blogpost, Tesla said the self-driving car attempted to drive full speed under the trailer “with the bottom of the trailer impacting the windshield of the Model S”.\nThe disclosure raises further questions about the May crash, which occurred after Joshua Brown put his Model S into Tesla’s autopilot mode, which is able to control a car while it’s driving on the highway. Photograph: Facebook\nThe top of the vehicle was “torn off by the force of the collision”, according to a police report in the local Levy County Journal.\nBaressi was uninjured.\nElon Musk, the CEO of Tesla, tweeted his condolences regarding the “tragic loss”, but the company’s statement deflected blame for the crash. His 537-word statement noted that this was Tesla’s first known autopilot death in roughly 130m miles driven by customers.\n“Among all vehicles in the US, there is a fatality every 94 million miles,” the statement said.\nIt goes on to say that the car’s autonomous software is designed to nudge consumers to keep their hands on the wheels to make sure they’re paying attention. “Autopilot is getting better all the time, but it is not perfect and still requires the driver to remain alert,” the company said.\nNews of the death came just as BMW announced that it is joining forces with US computer chip firm Intel and Mobileye, an Israeli tech company, to develop its own self-driving cars.\nGoogle, Uber and numerous automakers have also tested self-driving technology. BMW said the goal of the new collaboration was to develop cars that would eventually allow them to take their eyes off the road.\nBrown, who owned a technology company called Nexu Innovation, was a Tesla enthusiast who posted videos of his car on autopilot on YouTube. One of them showed his vehicle avoiding a crash on the highway. The footage racked up 1m views after Musk tweeted it.\nOne of his first videos appeared to show Brown temporarily driving with no hands in slow-moving traffic. The Associated Press also reported that records show he received eight speeding tickets in six years.\nSince Tesla introduced the autopilot mode last October, Model S drivers have recorded videos of themselves online pushing the technology to its limits.\nTesla drivers post viral, self-driving 'stunts' using autopilot technology\nRead more\nOn YouTube, there are multiple videos showing drivers with their hands off the wheel. Musk has advised against this. However, one of the videos was uploaded by actress Talulah Riley, Musk’s second wife.\nAt the end of Tesla’s blogpost announcing Brown’s death, the company described the victim, who they did not name, as someone with “a loving family and we are beyond saddened by their loss”.\n“He was a friend to Tesla and the broader [electric vehicle] community, a person who spent his life focused on innovation and the promise of technology and who believed strongly in Tesla’s mission.”\nThe Associated Press contributed reporting","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a Tesla driver who was killed while using autopilot. The driver, Joshua Brown, may have been watching a Harry Potter movie at the time of the collision in Florida. The collision occurred when the Tesla, operating in autopilot mode, failed to distinguish a large white 18-wheel truck and trailer crossing the highway. The top of the vehicle was torn off by the force of the collision, resulting in Joshua Brown's death. The truck driver, Frank Baressi, mentioned that the Harry Potter movie was still playing when Brown died, and there was a portable DVD player in the vehicle. The incident has raised questions about the safety of Tesla's autopilot system and is being investigated by federal highway safety regulators.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/jul\/01\/tesla-driver-killed-autopilot-self-driving-car-harry-potter","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":32,"Time":"Fri 1 Jul 2016 01.14 CEST","Title":"Tesla driver dies in first fatal crash while using autopilot mode","Content":"The first known death caused by a self-driving car was disclosed by Tesla Motors on Thursday, a development that is sure to cause consumers to second-guess the trust they put in the booming autonomous vehicle industry.\nThe 7 May accident occurred in Williston, Florida, after the driver, Joshua Brown, 40, of Ohio put his Model S into Tesla’s autopilot mode, which is able to control the car during highway driving.\nWhat happens when Tesla’s AutoPilot goes wrong: owners post swerving videos\nRead more\nAgainst a bright spring sky, the car’s sensors system failed to distinguish a large white 18-wheel truck and trailer crossing the highway, Tesla said. The car attempted to drive full speed under the trailer, “with the bottom of the trailer impacting the windshield of the Model S”, Tesla said in a blogpost.\nA police report in the Levy County Journal said the top of the vehicle “was torn off by the force of the collision”. The truck driver, Frank Baressi, 62, of Tampa, Florida, was uninjured, the Journal reported.\nAmerica’s National Highway Traffic Safety Administration (NHTSA) has opened an inquiry into the accident.\nBrown owned a technology company called Nexu Innovations and was a Tesla enthusiast who posted videos of his car on autopilot on YouTube. One video showed his car avoiding a collision on a highway, racking up 1m views after it was tweeted by Tesla CEO Elon Musk.\nOwner video of Autopilot steering to avoid collision with a truckhttps:\/\/t.co\/FZUAXSjlR7\n— Elon Musk (@elonmusk) April 17, 2016\nIn its 537-word statement on the incident, the electric vehicle company repeatedly went out of its way to shift blame for the accident. The first paragraph notes that this was Tesla’s first known autopilot death in some 130 million miles driven by its customers. “Among all vehicles in the US, there is a fatality every 94 million miles,” the company then notes.\nIt goes on to say that the car’s autonomous software is designed to nudge consumers to keep their hands on the wheels to make sure they’re paying attention. “Autopilot is getting better all the time, but it is not perfect and still requires the driver to remain alert,” the company said.\nAt the end of its blogpost, Tesla stated: “The customer who died in this crash had a loving family and we are beyond saddened by their loss,” the company said. “He was a friend to Tesla and the broader [electric vehicle] community, a person who spent his life focused on innovation and the promise of technology and who believed strongly in Tesla’s mission.”\nOur condolences for the tragic loss https:\/\/t.co\/zI2100zEGL\n— Elon Musk (@elonmusk) June 30, 2016\n“Preliminary reports indicate the vehicle crash occurred when a tractor-trailer made a left turn in front of the Tesla at an intersection on a non-controlled access highway. The driver of the Tesla died due to injuries sustained in the crash,” the NHTSA said.\nThe car continued to travel after passing under the truck’s trailer, veered off the road, and then crashed through two fences and into a power pole, the local police report said. Brown died at the scene.\nThe accident comes at a time when Americans have just started to become more comfortable with letting machines take the wheel. Tesla has generated enormous fanfare with its autopilot mode and inspired consumers – despite the company’s warnings – to see just how much they can do while letting the car drive.\nA popular video posted online appeared to show a man taking a nap as his car navigated busy traffic. One member of a Tesla owner message board recently quipped that he uses his free hands to play air guitar to Billy Idol songs. Another claimed that during a road trip, he noticed Tesla’s autopilot camera struggling to pick out lines during bright sunlight in the morning or at dusk.\nOther carmakers have said they will be releasing autonomous vehicles in the near future. Within a year, General Motors plans to test self-driving taxis with the ride-hailing app Lyft. One startup backed by Google is testing completely autonomous trucks in Nevada that will let drivers rest as they complete cross-country shipping routes.\nMusk recently hinted that the company plans to include its autopilot feature in its upcoming Model 3, which at $35,000 is being marketed to middle-class drivers.\nStatistically, self-driving cars are about to kill someone. What happens next?\nRead more\nThe appeal of the technology is obvious. And Tesla isn’t wrong when it says that statistically, vehicles driving themselves have a much better safety record than ones driven by humans.\nBut the Tesla accident offers the first test of how consumers will react when reminded that they are putting their lives in the hands of computer code when they turn over control of the wheel.\nBrown, for his part, appeared to believe that he was safe when letting go of the wheel. In one of the YouTube videos he posted of his Model S driving itself in slow-moving traffic, he said, “It takes all the stress out of it.”\n“You get to your destination slightly slower but at least now you don’t have to worry about anything. Just let it go.”","Relevant":true,"Overview":"Here is the analysis of the news you provided in JSON format:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Title\": \"Tesla driver dies in first fatal crash while using autopilot mode\",\n    \"Date\": \"7 May\",\n    \"Location\": \"Williston, Florida\",\n    \"Driver\": \"Joshua Brown, 40, of Ohio\",\n    \"Description\": \"The news is about an autonomous driving accident involving a Tesla vehicle. The accident occurred on 7 May in Williston, Florida, when the driver, Joshua Brown, put his Tesla Model S into autopilot mode. The vehicle's sensors failed to distinguish a large white 18-wheel truck and trailer, resulting in a collision that led to the first known death caused by a self-driving car. The car attempted to drive under the trailer, impacting the windshield and tearing off the top of the vehicle. The truck driver was uninjured. The National Highway Traffic Safety Administration (NHTSA) initiated an inquiry into the accident. The news also mentions Tesla's statements about the incident and the debate around autonomous driving technology.\"\n  }\n}\nThe news is relevant to autonomous driving accidents, and the specific information about the case has been extracted in detail.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/jun\/30\/tesla-autopilot-death-self-driving-car-elon-musk","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":31,"Time":"Mon 14 Nov 2022 12.00 CET","Title":"Landmark trial involving Tesla autopilot weighs if ‘man or machine’ at fault","Content":"Tesla will play a major role in a manslaughter trial this week over a fatal crash caused by a vehicle operating on autopilot, in what could be a defining case for the self-driving car industry.\nAt the trial’s heart is the question of who is legally responsible for a vehicle that can drive – or partially drive – itself.\nTesla under US criminal investigation over self-driving claims, sources say\nRead more\nKevin George Aziz Riad is on trial for his role in a 2019 crash. Police say Riad exited a freeway in southern California in a Tesla Model S, ran a red light and crashed into a Honda Civic, killing Gilberto Lopez and Maria Guadalupe Nieves-Lopez. Tesla’s autopilot system, which can control speed, braking and steering, was engaged at the time of the crash that killed the couple, who were on their first date.\nTesla does not face charges in the case, but trial could shape public perceptions of the company and act as a test case for whether the technology has advanced faster than legal standards, experts say.\n“Who’s at fault, man or machine?” Edward Walters, an adjunct professor at the Georgetown University law school who specializes in the law governing self-driving cars. “The state will have a hard time proving the guilt of the human driver because some parts of the task are being handled by Tesla.”\nRiad’s lawyer has said that his client should not have been charged with a crime while prosecutors have argued Riad’s speeding and failure to brake were reckless.\nThe trial comes as the electric carmaker faces growing scrutiny and criticism that its autopilot has made drivers inattentive and contributed to accidents and deaths. Elon Musk, the company cofounder, has said that Tesla is significantly more safe when used with its autopilot system, and has touted it as a step to fully autonomous driving.\nIn September, Musk said he believed the company had a “moral obligation” to roll out what he describes as “full self-driving” software, even if it was not perfect and Tesla faced lawsuits, because doing so could save lives.\nBut Tesla’s system has faced ongoing scrutiny and has been implicated in numerous collisions, some of them fatal. US federal regulators are currently investigating more than a dozen Tesla crashes into parked first responder vehicles over a period of four years, resulting in multiple injuries and one death.\nThe US justice department is investigating whether Tesla itself should face criminal charges over its self-driving claims, Reuters reported, which experts have said could pose a challenge to prosecutors in the California trial.\n“The DoJ probe helps [Riad] because his claim is going to be ‘I relied on their advertising. Therefore, I was not aware of the risk there,’” said Robert Blecker, a criminal law professor at New York Law School.\nIn addition to the criminal trial related to the crash, the family of Gilberto Lopez is suing Tesla in a trial scheduled for July.\nTesla’s self-driving technology fails to detect children in the road, group claims\nRead more\n“I can’t say that the driver was not at fault, but the Tesla system, autopilot and Tesla spokespeople encourage drivers to be less attentive,” Donald Slavik, an attorney whose firm is representing Lopez’s family in a lawsuit against Tesla, told Reuters.\nTesla understood the risks of its system but failed to manage those, Slavik said. “Tesla knows people are going to use autopilot and use it in dangerous situations,” he said.\nThe ongoing legal and regulatory scrutiny of Tesla could shape perception of the company, which poses a risk as it looks to defend itself in coming lawsuits, said Bryant Walker Smith, a law professor at the University of South Carolina, who is also an adviser on new transportation technology.\n“The narrative of Tesla potentially shifts from this innovative tech company doing cool things to this company just mired in legal trouble. That is the risk, and narrative is very important in civil litigation because both sides tell a jury a story,” he said.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous driving accidents. The trial involves a fatal crash in 2019 where a Tesla Model S operating on autopilot ran a red light and crashed into a Honda Civic, resulting in the deaths of Gilberto Lopez and Maria Guadalupe Nieves-Lopez. The question in the trial is who is legally responsible for the vehicle's actions when it can partially drive itself. Tesla's autopilot system was engaged at the time of the crash. The trial could shape public perceptions of Tesla and self-driving technology, as well as whether legal standards have kept pace with technological advancements in autonomous driving. There are also ongoing investigations by the US justice department into Tesla's self-driving claims, and the family of Gilberto Lopez is suing Tesla in a separate trial.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2022\/nov\/14\/tesla-autopilot-landmark-case-man-v-machine","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":37,"Time":"13 December 2018","Title":"Uber told self-drive cars unsafe days before accident","Content":"Uber told self-drive cars unsafe days before accident\nPublished\n13 December 2018\nShare\nIMAGE SOURCE,\nEPA\nA leaked email from an employee warned Uber's bosses that its self-drive cars were involved in multiple crashes, only days before one killed a pedestrian.\nRobbie Miller, operations manager for Uber's self-driving trucks, wrote to the firm's top executives saying the cars were \"routinely in accidents\".\nThis was partly down to faults with the technology and partly because of the \"poor behaviour\" of operators, he said.\nUber has yet to respond to the revelations.\nHowever, it told news and information website The Information in a statement: \"Right now the entire team is focused on safely and responsibly returning to the road in self-driving mode.\n\"We have every confidence in the work that the team is doing to get us there.\n\"Our team remains committed to implementing key safety improvements, and we intend to resume on-the-road self-driving testing only when these improvements have been implemented and we have received authorisation from the Pennsylvania Department of Transportation.\"\nWhy you have (probably) already bought your last car\nUber ends Arizona driverless car programme\nToyota suspends US driverless car tests after fatal Uber accident\nMr Miller's email was sent to the head of Uber's autonomous vehicle unit, Eric Meyhofer, six other executives and lawyers.\nSeveral of the drivers were not \"properly vetted or trained\", he wrote.\nIt was sent on 13 March, only days before a fatal collision in Tempe, Arizona in which an autonomous Uber car hit and killed pedestrian Elaine Herzberg.\nUber suspended all its tests following the accident, which is still under investigation by the National Transportation Safety Board.\nIn June it was revealed that the safety operator of the car was watching TV just before the accident.\nIn his email, Mr Miller wrote: \"The cars are routinely in accidents resulting in damage. This is usually the result of poor behaviour of the operator or the AV (autonomous vehicle) technology.\n\"A car was damaged nearly every other day in February. We shouldn't be hitting things every 15,000 miles.\"\nIMAGE SOURCE,\nREUTERS\nImage caption,\nNational Transportation Safety Board investigators have examined the vehicle involved in the crash\nHe went on to describe three incidents, including one where a car \"drove on the sidewalk for several metres\". In another, an accident was avoided only because a vehicle in the adjacent lane swerved to miss the Uber prototype, according to his email.\nMr Miller complained that such incidents were \"ignored\", with the logs left unreviewed by senior managers for days or even weeks.\n\"This is not how we should be operating,\" he said.\nHe recommended that Uber reduce its fleet size by up to 85%, stop tests after all accidents and review any incident, even minor ones \"immediately\".\nMr. Miller is currently working at a self-driving truck start-up led by Anthony Levandowski, who was accused of stealing secrets from Google's self-driving division Waymo.\nMr Levandowski was fired by Uber after he declined to testify in the lawsuit filed by Waymo against the firm. The case was settled in February with Uber giving a stake of its company to Waymo and agreeing not to use its technology in its self-driving cars.\nMr. Miller also worked at Mr Levandowski's prior company Otto, which was acquired by Uber, and worked for him at Waymo.\nMore on this story\nUber crash driver was 'watching TV'\nPublished\n22 June 2018\nUber car 'failed to identify' pedestrian\nPublished\n24 May 2018","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about Uber's self-driving cars being involved in multiple accidents, including one fatal accident in Tempe, Arizona, where an autonomous Uber car hit and killed a pedestrian named Elaine Herzberg. The accidents were attributed to faults with the technology and poor behavior of operators. Robbie Miller, the operations manager for Uber's self-driving trucks, sent an email to Uber's top executives warning them about the routine accidents and described specific incidents, including a car driving on the sidewalk and near-miss accidents. Miller recommended reducing the fleet size, stopping tests after all accidents, and reviewing incidents immediately. The email was sent on 13 March 2018, just days before the fatal collision. Uber suspended all its self-driving tests following the accident, which was under investigation by the National Transportation Safety Board.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-46552604","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":38,"Time":"20 November 2019","Title":"Uber self-driving crash 'mostly caused by human error'","Content":"Uber self-driving crash 'mostly caused by human error'\nPublished\n20 November 2019\nShare\nMedia caption,\nUber dashcam footage shows moment before fatal impact\nBy Dave Lee\nNorth America technology reporter\nA distracted safety operator in an Uber self-driving car was primarily to blame for a fatal crash in 2018, a US regulator has ruled.\nThe National Transportation Safety Board (NTSB) said an “inadequate safety culture” at Uber was also a major contributing factor.\nAs too were poor, or in some cases non-existent, rules governing firms testing self-driving technology.\nThe findings follow a near two-year investigation.\nIn a hearing to discuss the results, the NTSB urged national regulators to create a more thorough review process before the technology can be used on public roads.\nElaine Herzberg, 49, was killed when an Uber-owned self-driving car - operating in autonomous mode - struck her as she crossed a road in Tempe, Arizona, on 18 March 2018.\nIn the car was safety driver Rafaela Vasquez who, according to investigators, had been streaming a TV show on her mobile phone while behind the wheel.\nDashcam footage showed Ms Vasquez spent 36% of the journey that evening looking at the device.\nIn its experiments with driverless cars, Uber has mandated that a human operator pays attention at all times so they can take over in difficult situations or when the vehicle encounters a situation it does not know how to handle.\nUber crash driver was 'watching TV'\nSelf-driving Uber in fatal crash had 'safety flaws'\n\"Had the vehicle operator been attentive, she would likely have had sufficient time to detect and react to the crossing pedestrian to avoid the crash or mitigate the impact,\" the NTSB ruled.\nUber’s computers detected Ms Herzberg 5.6 seconds before impact, the NTSB said, but did not correctly identify her as a person.\nIMAGE SOURCE,\nNTSB\nImage caption,\nThe NTSB mapped the incident - yellow rings show metres ahead of the vehicle, orange lines centre of mapped travel and purple area the car's path\nThe report said Ms Herzberg was acting unsafely in attempting to cross the road where she did - investigators said toxicology reports suggested she had taken drugs that may have impaired her judgement.\nNevertheless, the NTSB said Uber had an \"inadequate safety culture, exhibited by a lack of risk assessment mechanisms, of oversight of vehicle operators, and of personnel with backgrounds in safety management”.\nIt acknowledged on Tuesday that the company had made significant changes since the accident.\nUber said it welcomed the recommendations.\n\"We deeply regret the March 2018 crash that resulted in the loss of Elaine Herzberg’s life, and we remain committed to improving the safety of our self-driving program,” said Nat Beuse, head of safety at Uber’s advanced technologies group.\n“Over the last 20 months, we have provided the NTSB with complete access to information about our technology and the developments we have made since the crash. While we are proud of our progress, we will never lose sight of what brought us here or our responsibility to continue raising the bar on safety.”\n_____\nFollow Dave Lee on Twitter @DaveLeeBBC\nDo you have more information about this or any other technology story? You can reach Dave directly and securely through encrypted messaging app Signal on: +1 (628) 400-7370\nMore on this story\nUber crash driver was 'watching TV'\nPublished\n22 June 2018\nSelf-driving Uber in fatal crash had 'safety flaws'\nPublished\n6 November 2019","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Uber self-driving crash 'mostly caused by human error'\",\n\"Published_date\": \"20 November 2019\",\n\"Summary\": \"A distracted safety operator in an Uber self-driving car was primarily to blame for a fatal crash in 2018, according to a ruling by the National Transportation Safety Board (NTSB). The NTSB also cited an 'inadequate safety culture' at Uber and poor rules governing self-driving technology testing as contributing factors. The crash occurred on 18 March 2018 when an Uber-owned self-driving car struck and killed Elaine Herzberg, who was crossing a road in Tempe, Arizona. The safety driver in the car had been distracted, streaming a TV show on her mobile phone while behind the wheel. The NTSB found that if the operator had been attentive, the crash might have been avoided. Uber's computers detected Ms. Herzberg but did not correctly identify her as a person. The NTSB also noted that Ms. Herzberg was acting unsafely by attempting to cross the road where she did, possibly impaired by drugs. Uber acknowledged its 'inadequate safety culture' and welcomed the NTSB's recommendations.\"\n}\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-50484172","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":39,"Time":"September 1, 2022","Title":"GM startup Cruise recalls and revises self-driving software after crash","Content":"Companies\nLaw Firms\nGeneral Motors Co\nFollow\nCruise, LLC\nFollow\nWASHINGTON, Sept 1 (Reuters) - General Motors startup Cruise LLC (GM.N) said Thursday it had recalled and updated software in 80 self-driving vehicles after a June crash in San Francisco that left two people injured.\nFederal regulators said the recalled software could \"incorrectly predict\" an oncoming vehicle's path. Cruise said it had determined this unusual scenario would not recur after the update.\nThe National Highway Traffic Safety Administration (NHTSA) has stepped up its scrutiny of advanced driver assistance systems and autonomous vehicle systems in recent months. Last year, it directed all automakers and tech companies to promptly report crashes involving self-driving vehicles.\nNHTSA said Thursday that Cruise's recall filing \"to address a safety defect in its automated driving systems software\" was required by law.\nNHTSA added it \"expects all manufacturers, including those developing automated driving systems, to continuously ensure that they are meeting their requirements to initiate a recall for any safety issue that poses an unreasonable risk to safety.\"\nNHTSA said the recalled Cruise software could \"in certain circumstances when making an unprotected left, cause the (autonomous driving system) to incorrectly predict another vehicle’s path or be insufficiently reactive to the sudden path change of a road user.\"\nAdvertisement · Scroll to continue\nCruise disclosed Thursday that after the June 3 crash in San Francisco, it temporarily prevented its vehicles from making unprotected left turns and reduced the area in which its vehicles could operate.\nAfter the software update on July 6, Cruise said it had gradually reintroduced unprotected left turns, which refers to turning left at an intersection with a solid green light that directs all traffic, rather than a designated green arrow just for turning vehicles.\nAdvertisement · Scroll to continue\nCruise emphasized in a statement Thursday all vehicles had software updates and the recall \"does not impact or change our current on-road operations.\"\nThe company added \"Cruise AVs are even better equipped to prevent this singular, exceptional event.\"\nNHTSA said \"an improper (Automated Driving Systems) response can increase the risk of a crash.\"\nThe agency said last month it had opened a special investigation into the Cruise crash.\nAdvertisement · Scroll to continue\nIn rare circumstances, Cruise said the software caused the autonomous vehicle to hard brake while performing an unprotected left turn that it deemed necessary to avoid a severe front-end collision.\nThe self-driving vehicle \"had to decide between two different risk scenarios and chose the one with the least potential for a serious collision at the time, before the oncoming vehicle’s sudden change of direction,\" Cruise said.\nAdvertisement · Scroll to continue\nCruise also noted a police report found the party most at fault for the June crash was the other vehicle, which was traveling at 40 miles per hour in a 25-mile zone.\nIn March, startup technology firm Pony.ai agreed to recall some versions of its autonomous driving system software, after an October crash in California, that had been used in three vehicles.\nGM has lost nearly $5 billion since 2018 trying to build a robotaxi business in San Francisco and disclosed in July it lost $500 million on Cruise during the second quarter as it began charging for rides in a limited area of San Francisco.\nGM and Cruise in February disclosed they petitioned NHTSA for permission to deploy self-driving vehicles without steering wheels, mirrors, turn signals or windshield wipers.\nLast month, NHTSA said it would extend a public comment period on the request.\nReporting by David Shepardson; Editing by Jason Neely, Bernadette Baum and Alexander Smith\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here is the analysis of the news in JSON format:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"GM startup Cruise recalls and revises self-driving software after crash\",\n        \"Date\": \"September 1\",\n        \"Location\": \"San Francisco\",\n        \"Injuries\": \"Two people injured\",\n        \"Recalled vehicles\": \"80 self-driving vehicles\",\n        \"Issue\": \"Recalled software could incorrectly predict an oncoming vehicle's path\",\n        \"Actions taken\": [\n            \"Recalled and updated software\",\n            \"Temporarily prevented unprotected left turns\",\n            \"Reduced the operational area\",\n            \"Gradually reintroduced unprotected left turns after a software update\",\n            \"Highlighted that all vehicles had software updates\"\n        ],\n        \"NHTSA involvement\": [\n            \"NHTSA has increased scrutiny of advanced driver assistance systems and autonomous vehicle systems\",\n            \"NHTSA required Cruise to address the safety defect in its automated driving systems software\",\n            \"NHTSA expects all manufacturers, including those developing automated driving systems, to ensure safety and initiate recalls when necessary\",\n            \"NHTSA opened a special investigation into the Cruise crash\",\n            \"NHTSA emphasized that an improper Automated Driving Systems response can increase the risk of a crash\"\n        ],\n        \"Cruise's explanation\": [\n            \"Software caused the autonomous vehicle to hard brake in rare circumstances during an unprotected left turn\",\n            \"The vehicle had to choose between two risk scenarios to avoid a severe front-end collision\",\n            \"Police report found the other vehicle was most at fault for the crash\"\n        ],\n        \"Financial information\": [\n            \"GM has lost nearly $5 billion since 2018 in building a robotaxi business in San Francisco\",\n            \"GM disclosed a $500 million loss on Cruise during the second quarter\",\n            \"GM and Cruise petitioned NHTSA for permission to deploy self-driving vehicles without traditional controls\"\n        ]\n    }\n}\nBased on the provided news, it is relevant to autonomous driving because it discusses a recall and software update by GM's startup Cruise for self-driving vehicles following a crash in San Francisco. The specific information, as detailed as possible, includes the date, location, injuries, the number of recalled vehicles, the issue with the recalled software, actions taken by Cruise, NHTSA's involvement, Cruise's explanation of the software issue, and some financial information related to GM and Cruise.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/gms-cruise-recalls-revises-self-driving-software-after-crash-2022-09-01\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":42,"Time":"20 March 2018","Title":"Toyota suspends US driverless car tests after fatal Uber accident","Content":"Toyota suspends US driverless car tests after fatal Uber accident\nPublished\n20 March 2018\nShare\nIMAGE SOURCE,\nAFP\/GETTY\nImage caption,\nToyota has been researching driverless car technology. It said it would halt tests in the US, but continue in other countries.\nToyota has suspended US tests of driverless cars on public roads following a fatal accident in Arizona involving one of Uber Technologies' self-driving vehicles.\nToyota said it was concerned about the \"emotional effect\" the incident might have on its test drivers.\nThe carmaker said it did not have a timeline for re-starting the trials.\nThe Arizona accident has revived debate about whether autonomous vehicles are being put into use prematurely.\nAnalysts say the technology has the potential to reduce accidents and expand transportation options for the disabled and elderly, but some have warned that the technology is not ready and urged regulators to introduce more stringent safety tests.\n\"Hopefully Congress will take note and stop rushing to deploy this immature technology,\" Missy Cummings, a professor of engineering at Duke University wrote on Twitter after the accident in Arizona.\nA survey last year by Pew Research Center found that more than half of Americans would not want to ride in a driverless car if given the option and expressed some level of worry about such vehicles.\nThe accident in Tempe, Arizona on Sunday is believed to be the first fatality involving a fully autonomous vehicle.\nPolice and federal officials are investigating the details of the incident, in which a 49-year-old woman was killed by an Uber car operating in autonomous mode. A human monitor was also behind the wheel.\nOn Tuesday, Tempe police said they had reviewed video of the crash and repeated that fault had not been determined.\nUber said after the accident that it would temporarily halt driverless car tests.\nToyota has also been performing trials of its Chauffeur mode on public roads in Michigan and California. The firm previously said it expected some of its cars to be equipped with automated driving technology by 2020.\n\"Because we feel the incident may have an emotional effect on our test drivers, we have decided to temporarily pause our Chauffeur mode testing on public roads,\" a spokesman said.\nHe said the firm is continuing its tests of driverless cars in other countries.\nMore on this story\nUber halts self-driving tests after death\nPublished\n20 March 2018\nToyota hails a ride with Uber\nPublished\n25 May 2016\nUber strikes deal to buy self-drive Volvos\nPublished\n20 November 2017","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Toyota suspends US driverless car tests after fatal Uber accident\",\n\"Date_published\": \"20 March 2018\",\n\"Summary\": \"Toyota has suspended US tests of driverless cars on public roads following a fatal accident in Arizona involving one of Uber Technologies' self-driving vehicles. The accident in Tempe, Arizona, is believed to be the first fatality involving a fully autonomous vehicle. A 49-year-old woman was killed by an Uber car operating in autonomous mode, with a human monitor also behind the wheel. Toyota cited concerns about the emotional impact on its test drivers and did not provide a timeline for resuming the trials.\"\n}\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/business-43478158","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":35,"Time":"Thu 17 Dec 2015 19.47 CET","Title":"Self-driving cars are coming, and the technology promises to save lives","Content":"C\nompanies like Google and Tesla have held up autonomous vehicles as a panacea for road accidents, which claim over 32,000 American lives each year. This month, Obama put stock in that idea, signing a $305bn transportation bill that includes grants for self driving vehicles. That backs up 10 years of US Department of Transportation work on technical studies and policies surrounding autonomous cars. But will all this investment and research actually result in a future free of car accidents?\nMany researchers say forecasting the impact of self-driving cars on reducing accidents is difficult because there just isn’t enough real-life road testing and data to make a reliable forecast.\nThe National Highway Traffic Safety Administration, however, found that over 90% of car crashes involve human error. In the remaining cases, factors such as weather and failed car components were primary causes. Robots can potentially be programmed to make faster and better decisions while avoiding human pitfalls such as driving while drunk or texting.\nA paper from global consulting firm McKinsey & Company estimates that a 90% crash reduction rate could have saved the US economy $190bn in 2012 alone.\nIf you have something react faster than humans, you could still save lots of lives\nRichard Wallace, Center for Automotive Research\nDemonstrating that autonomous cars are safe and able to reduce accidents will be crucial for convincing the public that ceding driving to computers is a good idea. Skepticism, and even fear of losing control of the activities that play a big role in our lives, already abound about allowing robots to manage our lives. Questions have been raised about whether they could make ethical and safe decisions when confronted with complex situations that involve not only cars, but also pedestrians and other obstacles. Should they be programmed to protect their occupants at all cost, or to make a move that could seriously injure the riders but lead to fewer deaths overall in a collision?\n“We certainly think that [autonomous cars are] going to significantly lower the rate of fatalities. We just don’t have a full sense of it,” said Richard Wallace, director of transportation systems analysis at the nonprofit Center for Automotive Research in Michigan. “I think what you can say for certain is that as technology progresses, the artificial intelligence that will be doing the driving will become more and more perfected.”\nThe highway to progress\nTechnological advances, such as seat belts and antilock brakes, have helped to lower the number of overall car crashes from 6.7m in 1988 to 5.7m in 2013, according to the National Highway Traffic Safety Administration. Fatal crashes declined from 42,130 in 1988 to 30,057 in 2013.\nResearchers believe that computer-driven cars will lessen the severity of injuries caused in car accidents. “If you have something react faster than humans, even if it can’t completely avoid a crash, you could still save lots of lives,” Wallace said. A robot car can still get rear-ended by a human-driven car, for example, but its sensors could detect the pending collision and maneuver the car quickly to lengthen the distance before impact.\nCar manufacturers such as Audi, Toyota and Tesla have announced plans to launch autonomous cars, or cars that can autopilot in certain conditions, within five years. But predictions for when a large number of consumers will choose them over conventional vehicles vary and depend on assumptions such as affordability and changes in insurance and other regulations, according to Kara M Kockelman, a professor at the University of Texas at Austin who has studied the issue. IHS Automotive, a market research firm, expects a global sales of nearly 12m self-driving cars, or 10% of the global total, in 2035. But it cautions that the forecast might be low if research and development efforts in creating self-driving cars continue to increase.\nCarmakers have been gradually automating functions to help humans navigate and avoid accidents for over 10 years. Many cars now come with sensors and video cameras to display blind spots and beep when sliding into a tight parking spot. Newer technology uses sound or a vibrating steering wheel to alert drivers when the cars are swerving into another lane without the turn signal on.\nEmbedding more sensors and cameras to capture high-resolution images and measure distance all around a car would enable it to take over the driving task. Wireless communication technology that allows cars to talk to each other could improve traffic flow, increase the number of cars that a highway can accommodate and reduce fuel consumption.\n“If we have connected, automated vehicles, it becomes possible to drive them much closer together and to have them follow each other smoothly, in contrast to today’s stop and go traffic,” said Steve Shladover, a program manager at the UC Berkeley Institute of Transportation Studies. “This has the potential to double the effective capacity of a highway lane.”\nThe numbers game\nResearchers are increasingly trying to quantify the benefits of specific crash-avoidance technology. A publicly funded 2011 study by General Motors and Virginia Tech Transportation Institute looked at how several technologies, such as visual and audio warning or automatic braking, could prevent crashes in which cars backed into another object. Simulations indicated that those technologies could reduce backing crashes by 32%, or nearly 65,000, annually nationwide. But the paper also cautioned that serious limitations in the amount and types of data available made it difficult to predict the real-world impact of these technologies.\nAt the Massachusetts Institute of Technology, Jason Gao is studying ways to reduce congestion if cars can talk to each other wirelessly. Cars that enter a congested zone would request digital tokens from those leaving it. If no tokens are available, then drivers would get a turn-by-turn audio instruction for traveling around the traffic jam. The digital tokens serve as permits to travel through an area. Gao is looking at whether pricing those permits could affect the number of cars that pass through and help to control the traffic flow. In a simulation that compared his car-to-car communication design with one in place in Singapore – where a toll system scans for dashboard-mounted transponders in cars and charges drivers for entering a congested zone – Gao said his system could increase the car speed by an average of 8%. Cars could travel faster because they could coordinate the traffic flow through the congestion zone among themselves in advance.\nDown the road\nGoogle has garnered lots of publicity for loosing its driverless cars in Silicon Valley and Austin – last month, police stopped one of its peapod-shaped cars for driving too slowly. Ford is testing a self-driving Fusion hybrid in a new 32-acretest track at the University of Michigan campus, designed to simulate an urban driving environment. Audi has done its own test runs, including a trial on a section of a Florida highway that was set aside for autonomous car research.\nResearch is also emerging to show how safe autonomous cars might be when they run alongside conventional cars on the road. In an analysis of car accidents involving self-driving cars from 2012 to 2015, published in October by the University of Michigan’s Transportation Research Institute, autonomous cars attained a higher crash rate per million miles traveled than conventional cars – but robot cars were not at fault in any of the crashes. Injuries from those crashes were also milder than those that involved only conventional vehicles, the report said. The analysis looked at publicly available crash data from 1.2m miles of test drives by Google, Delphi and Audi and found just 11 accidents.\nWhether self-driving cars will replace conventional vehicles will depend not only on technical improvements, but also policies that will need to be put in place. Those rules have only started to emerge. Just this week, California proposed regulations for autonomous cars, and it could become the first state to allow consumers to ride in them. The proposal, if adopted, would require humans to take control of the cars when necessary and take responsibility for tickets and accidents. That means at the end, the blame for crashes will still go to humans, not robots.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news discusses the potential of self-driving cars to reduce accidents and the challenges associated with their adoption. It mentions that companies like Google and Tesla see autonomous vehicles as a solution to road accidents, and it highlights the signing of a $305 billion transportation bill by Obama to support self-driving vehicles. The article also mentions that over 90% of car crashes involve human error, and autonomous cars could potentially make faster and better decisions. It includes quotes from experts discussing the potential safety benefits of autonomous cars and the need for further research. Additionally, it discusses technological advancements in autonomous driving and mentions specific studies on crash-avoidance technology and congestion reduction through car-to-car communication. Finally, it mentions that self-driving cars have been involved in accidents but were not at fault, and it emphasizes the importance of policy development for autonomous vehicle adoption.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2015\/dec\/17\/self-driving-cars-safety-future-interactive","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":45,"Time":"Updated 11:09 AM EDT, Fri September 18, 2020","Title":"Uber self-driving car operator charged in pedestrian death","Content":"Washington, DC\nCNN\n — \nThe Uber test driver who was responsible for monitoring one of the company’s self-driving cars that hit and killed a pedestrian in 2018 was charged with negligent homicide this week.\nRafaela Vasquez was watching television on her smartphone when the Uber self-driving vehicle struck Elaine Herzberg, who was crossing a road in Tempe, Arizona, according to a National Transportation Safety Board investigation. It was the first fatality involving a fully autonomous vehicle.\nVasquez pleaded not guilty to the charge, according to the Maricopa County Attorney’s Office, and the court ordered her released with ankle monitoring. Vasquez’s attorney did not immediately respond to a request for comment.\nThe NTSB investigation released in November 2019 concluded that the crash was caused because Vasquez was distracted by her phone, and that Uber’s inadequate safety culture contributed to the crash. Furthermore, the NTSB found that Uber’s system could not correctly classify and predict the path of a pedestrian crossing midblock, which occurred in the incident involving Vasquez.\nUber had also deactivated a forward collision warning and automatic emergency braking system on the Volvo, according to the NTSB. Uber made revisions to its program following guidance from the NTSB.\n“Over the last 20 months, we have provided the NTSB with complete access to information about our technology and the developments we have made since the crash,” Uber’s Nat Beuse, who leads self-driving safety efforts, said in a statement at the time. “While we are proud of our progress, we will never lose sight of what brought us here or our responsibility to continue raising the bar on safety.”\nA grand jury in Maricopa County, Arizona charged Vasquez with the felony long after Uber had been cleared of any potential charges. In March 2019, the Yavapai County Attorney’s Office concluded there was no basis for criminal liability against Uber. It declined to detail how its decision was made, or answer any questions on the case. Uber also declined to comment.\nLiability questions surrounding self-driving cars continue to be up in the air, but experts say this case is unlikely to set a significant precedent for the future of self-driving laws, given the circumstances.\nVasquez was an Uber employee who was supposed to be monitoring the vehicle’s performance, and taking control if necessary for safety’s sake. If self-driving vehicles go mainstream, there won’t be test drivers behind the wheel, and experts expect companies to be liable for crashes. Some companies are developing vehicles without human controls, making it impossible for passengers to control a vehicle.\nEd Walters, who teaches a class on autonomous vehicles at Georgetown Law, said it’s unclear who would be charged if Vasquez hadn’t allegedly been watching TV on her phone.\n“The fact she was watching TV makes her an easy and maybe convenient person to accept responsibility,” Walters said. “Remove that fact and it could easily be Uber.”\nBryant Walker Smith, a professor at the University of South Carolina School of Law who studies autonomous vehicles, expressed concern about the signal it sends to prosecute only the driver.\n“It will come across to many that that’s where all the fault is, and the companies behind these don’t have responsibility,” Smith said. “Uber’s testing program created circumstances where this was foreseeable and almost inevitable. Someone spends their whole day seeing a vehicle drive well and starts assuming that it will drive well.”\nOther experts in autonomous vehicle law felt differently about the charge.\n“Uber trains and requires its safety drivers to remain alert.  If one of them does not do so, it is difficult to say Uber committed a crime,” said Todd Benoff, a lawyer at Alston & Bird. “If the driver had been drinking that would also be a crime, but not one that Uber should be charged for.”\nUber did face the risk of a civil lawsuit, and settled with Herzberg’s family fewer than two weeks after the crash. Details of the agreement weren’t released.","Relevant":true,"Overview":"Here is the JSON format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Title\": \"Uber self-driving car operator charged in pedestrian death\",\n    \"Summary\": \"The news is about an incident involving an Uber self-driving car that hit and killed a pedestrian in 2018. The Uber test driver, Rafaela Vasquez, who was responsible for monitoring the self-driving car, was charged with negligent homicide. The incident occurred in Tempe, Arizona, and was the first fatality involving a fully autonomous vehicle. The National Transportation Safety Board (NTSB) investigation concluded that Vasquez was distracted by her smartphone at the time of the accident. The NTSB also found that Uber's safety culture was inadequate, and its system could not correctly predict the path of a pedestrian. Additionally, Uber had deactivated certain safety features on the vehicle. Vasquez pleaded not guilty to the charge. Uber settled with the victim's family in a civil lawsuit.\"\n  }\n}\nThe news is indeed about an autonomous driving-related incident, specifically involving an Uber self-driving car that struck and killed a pedestrian. The specific information includes details about the incident, the charges against the Uber test driver, the NTSB investigation findings, and the settlement with the victim's family.","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2020\/09\/18\/cars\/uber-vasquez-charged\/index.html","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":44,"Time":"8:47 PM GMT+1•February 29, 2016","Title":"Google’s Self-Driving Car Gets Into A Minor Accident While The AI Was Driving","Content":"It was bound to happen eventually: one of Google’s self-driving cars has gotten into a minor accident – and, for once, it was while the AI was in control.\nGoogle’s cars have actually gotten into a dozen-plus fender benders over the years.. but in the previous cases, the government-mandated human driver was actually the one controlling the vehicle when the accidents occured.\nIn this case (DMV report here), Google says it outright: yep, the vehicle was in autonomous mode.\nHere’s what happened, straight from the DMV report:\nA Google Lexus-model autonomous vehicle (“Google AV”) was traveling in autonomous mode eastbound on El Camino Real in Mountain View in the far right-hand land approaching the Castro St. intersection. As the Google AV approached the intersection, it signaled its intent to make a right turn on red onto Castro St. The Google AV then moved to the right-hand side of the lane to pass traffic in the same lane that was stopped at the intersection and proceeding straight. However, the Google AV had to come to a stop and go around sandbags positioned around a storm drain that were blocking its path. When the light turned green, traffic in the lane continued past the Google AV. After a few cars had passed, the Google AV began to proceed back into the center lane to pass the sand bags. A public transit bus was approaching from behind. The Google AV test driver saw the bus approaching in the left side mirror but believed the bus would stop or slow to allow the Google AV to continue. Approximately three seconds later, as the Google AV was reentering the center of the lane it made contact with the side of the bus. The Google AV was operating in autonomous mode and traveling at less than 2 mph, and the bus was travelling at about 15 mph at the time of contact.\nThe Google AV sustained bus damage to the left front fender, the left front wheel and one of its driver’s-side sensors. There were no injuries reported at the scene.\ntl;dr: Google Car tried to zip around some stopped cars, had to stop because of some sandbags in the road, tried to merge back into the other lane to get around them, and it hit the side of a passing bus which it assumed would slow to let it merge back in to the lane.. No injuries.\nIt’s a weird fringe case, but it’s exactly these fringe cases that makes building self-driving cars so damned hard. Hell, the accident was one any human could’ve gotten into — that is unless you’ve been driving in the Bay Area for a few years and have lost all faith that the buses here will slow down for you.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Google’s Self-Driving Car Gets Into A Minor Accident While The AI Was Driving\",\n\"Summary\": \"A Google self-driving car was involved in a minor accident while the AI was in control. The incident occurred when the car attempted to make a right turn on red but had to stop for sandbags blocking its path. When it tried to merge back into the lane, it collided with a passing bus. No injuries were reported, and the Google AV sustained damage to its left front fender, left front wheel, and a driver's-side sensor.\"\n}\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2016\/02\/29\/googles-self-driving-car-gets-into-a-minor-accident-while-the-ai-was-driving\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":48,"Time":"3:01 AM GMT+2•September 1, 2018","Title":"Apple is late to a self driving milestone — its first test car accident","Content":"cameraImage Credits: Emmanuel Dunand \/ AFP \/ Getty Images\nApple’s secretive self-driving vehicle program has disclosed its first accident, according to a report filed with the California Department of Motor Vehicles.\nThe low speed accident, which occurred August 24, is a milestone of sorts for the company, albeit not one that is being celebrated. These days, as more companies head out onto public streets to test their autonomous vehicle systems, accidents have become more common. The vast majority are minor, low-speed incidents.\nThere was just one accident involving a self-driving vehicle (that one was owned by Delphi) reported to the DMV in 2014. So far this year, there have been more than 40 accidents involving self-driving cars reported to CA DMV.\nThe first fatal autonomous vehicle accident, which involved an Uber self-driving vehicle striking a pedestrian, occurred in March in Arizona.\nThe Apple test car was attempting to merge onto an expressway near its headquarters in Cupertino, California, and traveling about 1 mile per hour, when it was rear-ended by a Nissan Leaf, according to the report. There were no injuries reported. Both parties reported moderate damage to their vehicles.\nApple doesn’t talk about its self-driving vehicle program. The tech company’s permit with the California Department of Motor Vehicles, the agency responsible for monitoring AVs in the state, is the only official acknowledgment that it even has a program. Apple’s self-driving program has been considered an open secret in Silicon Valley. And more recently, CEO Tim Cook has made references to the company’s interest in autonomous systems. In an interview with Bloomberg, he called it the mother of all AI projects. But the company doesn’t talk about its program or its ultimate product plans.\nThe accident report doesn’t reveal much, beyond the make and model of Apple’s test vehicle. The self-driving test vehicle involved in the accident was a 2016i Lexus RX450H. This the same make and model that Google used to test its self-driving system.","Relevant":true,"Overview":"Here is the JSON format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Accident_date\": \"August 24\",\n        \"Location\": \"Near Apple's headquarters in Cupertino, California\",\n        \"Description\": \"Apple's self-driving test car, a 2016 Lexus RX450H, was rear-ended by a Nissan Leaf while attempting to merge onto an expressway at a speed of about 1 mile per hour. The accident resulted in moderate damage to both vehicles, but no injuries were reported.\",\n        \"Context\": \"This is Apple's first disclosed accident related to its secretive self-driving vehicle program. While accidents involving autonomous vehicles have become more common, this one is considered a milestone for the company.\"\n    }\n}\nThe news is indeed about an autonomous driving accident involving Apple's self-driving test car. The specific information about the accident has been extracted and presented in the JSON format as requested.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2018\/08\/31\/apple-is-late-to-a-self-driving-milestone-its-first-test-car-accident\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":40,"Time":"Updated 11:23 PM GMT+2, July 28, 2023","Title":"The backup driver in the 1st death by a fully autonomous car pleads guilty to endangerment","Content":"PHOENIX (AP) — The backup Uber driver for a self-driving vehicle that killed a pedestrian in suburban Phoenix in 2018 pleaded guilty Friday to endangerment in the first fatal collision involving a fully autonomous car.\nMaricopa County Superior Court Judge David Garbarino, who accepted the plea agreement, sentenced Rafaela Vasquez, 49, to three years of supervised probation for the crash that killed 49-year-old Elaine Herzberg. Vasquez told police that Herzberg “came out of nowhere” and that she didn’t see Herzberg before the March 18, 2018, collision on a darkened Tempe street.\nVasquez had been charged with negligent homicide, a felony. She pleaded guilty to an undesignated felony, meaning it could be reclassified as a misdemeanor if she completes probation.\nAuthorities say Vasquez was streaming the television show “The Voice” on a phone and looking down in the moments before Uber’s Volvo XC-90 SUV struck Herzberg, who was crossing with her bicycle.\nRELATED COVERAGE\nCongress urged to revive long-stalled debate about regulating self-driving vehicles\nTesla shouldn’t call driving system Autopilot because humans are still in control, Buttigieg says\nCruise wants to test self-driving cars all over California\nVasquez’s attorneys said she was was looking at a messaging program used by Uber employees on a work cellphone that was on her right knee. They said the TV show was playing on her personal cellphone, which was on the passenger seat.\nDefense attorney Albert Jaynes Morrison told Garbarino that Uber should share some blame for the collision as he asked the judge to sentence Vasquez to six months of unsupervised probation.\n“There were steps that Uber failed to take,” he said. By putting Vasquez in the vehicle without a second employee, he said. “It was not a question of if but when it was going to happen.”\nProsecutors previously declined to file criminal charges against Uber, as a corporation. The National Transportation Safety Board concluded Vasquez’s failure to monitor the road was the main cause of the crash.\n“The defendant had one job and one job only,” prosecutor Tiffany Brady told the judge. “And that was to keep her eyes in the road.”\nMaricopa County Attorney Rachel Mitchell said in a statement after the hearing that her office believes the sentence was appropriate “based on the mitigating and aggravating factors.”\nThe contributing factors cited by the NTSB included Uber’s inadequate safety procedures and ineffective oversight of its drivers, Herzberg’s decision to cross the street outside of a crosswalk and the Arizona Department of Transportation’s insufficient oversight of autonomous vehicle testing.\nThe board also concluded Uber’s deactivation of its automatic emergency braking system increased the risks associated with testing automated vehicles on public roads. Instead of the system, Uber relied on the human backup driver to intervene.\nIt was not the first crash involving an Uber autonomous test vehicle. In March 2017, an Uber SUV flipped onto its side, also in Tempe when it collided with another vehicle. No serious injuries were reported, and the driver of the other car was cited for a violation.\nHerzberg’s death was the first involving an autonomous test vehicle but not the first in a car with some self-driving features. The driver of a Tesla Model S was killed in 2016 when his car, operating on its Autopilot system, crashed into a semitrailer in Florida.\nNine months after Herzberg’s death, in December 2019, two people were killed in California when a Tesla on Autopilot ran a red light, slammed into another car. That driver was charged in 2022 with vehicular manslaughter in what was believed to be the first felony case against a motorist who was using a partially automated driving system.\nIn Arizona, the Uber system detected Herzberg 5.6 seconds before the crash. But it failed to determine whether she was a bicyclist, pedestrian or unknown object, or that she was headed into the vehicle’s path, the board said.\nThe backup driver was there to take over the vehicle if systems failed.\nThe death reverberated throughout the auto industry and Silicon Valley and forced other companies to slow what had been a fast march toward autonomous ride-hailing services. Uber pulled its self-driving cars out of Arizona, and then-Gov. Doug Ducey prohibited the company from continuing its tests of self-driving cars.\nVasquez had previously spent more than four years in prison for two felony convictions — making false statements when obtaining unemployment benefits and attempted armed robbery — before starting work as an Uber driver, according to court records.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"The backup driver in the 1st death by a fully autonomous car pleads guilty to endangerment\",\n\"Location\": \"Phoenix, Arizona\",\n\"Date\": \"March 18, 2018\",\n\"Victim\": \"Elaine Herzberg\",\n\"Driver\": \"Rafaela Vasquez\",\n\"Charges\": \"Guilty plea to endangerment (originally charged with negligent homicide)\",\n\"Sentencing\": \"Three years of supervised probation (with the possibility of reclassification to misdemeanor)\",\n\"Circumstances\": \"Rafaela Vasquez, the backup Uber driver for a self-driving vehicle, pleaded guilty to endangerment in the first fatal collision involving a fully autonomous car. The collision occurred on a darkened Tempe street when Vasquez's Uber SUV struck and killed Elaine Herzberg, who was crossing with her bicycle. Vasquez stated that Herzberg 'came out of nowhere,' and she did not see her before the collision.\",\n\"Additional_Details\": \"Vasquez was reportedly distracted by streaming the TV show 'The Voice' on a phone and looking down moments before the collision. Her attorneys argued that Uber should share some blame for the collision due to inadequate safety procedures and failing to provide a second employee in the vehicle. The National Transportation Safety Board (NTSB) concluded that Vasquez's failure to monitor the road was the primary cause of the crash. The NTSB also cited contributing factors, including Uber's safety procedures, Herzberg's decision to cross outside of a crosswalk, and the Arizona Department of Transportation's oversight of autonomous vehicle testing.\",\n\"Relevance_to_Autonomous_Driving\": true\n}\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/autonomous-vehicle-death-uber-charge-backup-driver-1c711426a9cf020d3662c47c0dd64e35","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":47,"Time":"Sun 13 Mar 2016 22.38 CET","Title":"Your next car will be hacked. Will autonomous vehicles be worth it?","Content":"You’re about to drive to work. You turn on the ignition – and a message on the dash lights up. “We’ve hacked your car! Pay 10 bitcoin to get it back.”\nHacking into software and then demanding a ransom to release it – what’s known as ransomware – is not new. Finnish security expert Mikko Hypponen fully expects it to become a reality as self-driving or “autonomous” cars start to become more commonplace.\nAlready, one hacker claims to have taken control of some systems on board a passenger plane he was on, getting as far as issuing a “climb command” that he accessed through the entertainment system. Another pair of hackers caused a Jeep to crash in July 2015 by accessing some of the car’s software through another poorly protected entertainment system. At the Defcon hacking conference, as far back as 2011, hackers were asking if they could write a virus that would be transmitted car to car.\nDo not try this at home: hacking your brain with electronics\nRead more\nHypponen, chief research officer at the Finnish security firm F-Secure, told an audience at SXSW that in the 25 years he had worked in cybersecurity, he had seen a big shift in the type of people who do the hacking, as well their motivations. “When I entered this field, the hackers had no real motive – they were doing it because they could.”\nHe says there are now generally five types of hackers:\nGood “white hat” hackers, who break security so that a weakness can be found, fixed and ultimately improved\nActivist hackers, like Anonymous, who are politically but not perennially motivated\nNation-states and foreign intelligence agencies, a growing issue over the past 10 years\nSupporters of extremism of which Isis is the only really credible threat thus far\nCriminals, who Hypponen says now make as much as 95% of all malware, using hacking to make millions of dollars\nIt is the criminals motivated by money that present the biggest threat and are likely to increasingly target self-driving cars; the multiple components in cars and lack of rigour by carmakers has made this a pressing issue. “Legacy manufacturers who build cars have a long history of safety but not of security, and that’s why they are starting to learn the hard way. Now they take it seriously – and last year was a wake-up call,” he said of the Jeep hack.\nRobert Hartwig, president of the Insurance Information Institute (III), says the US market for cyber insurance is growing massively, from $2bn in 2015 to a predicted $7.5bn in 2020. “This is America, and if you have a breach of personal data, you are absolutely positively going to be sued. The legal fees and settlement costs will be more than the cost of the attack.”\nThe III estimates that by 2030, 25% of all cars sold will be autonomous, marking a slightly slower pace than Google et al might have you believe. Hartwig also said that there will be an estimated 80% fewer traffic accidents because of the increased safety of autonomous cars. Data will be critical to this, allowing policies to be based on precise driving habits, safety and how many miles people actually drive – not just what they say they do.\nJonathan Matus, CEO of the company behind the Zendrive app, explained how it uses the built-in motion and positioning sensors in smartphones to monitor driving, including rapid acceleration, sharp turns, stop signal compliance – and phone use, which is a major factor in the number of global road deaths each year. Despite car ownership peaking, the number of deaths is actually increasing, he said.\nNew cars already have complex electrical diagnostic systems that include various monitoring systems – so don’t tell a cop you haven’t been speeding, because your car won’t back you up, said Hartwig.\nHe pointed out that the road might actually be the last place to be overhauled for autonomous vehicles; Norway is already exploring an autonomous ferry, while planes are already so automated, even for takeoff and landing, that “the skills of pilots are atrophying”, Hartwig said.\nHuman-controlled cars will eventually be forbidden to drive on the road, Hypponen said, except for on racetracks. Matus said the same was certainly true of horses, suggesting yet another future threat to electronically controlled cars that could be harder to detect. “If you wanted to slow US GDP, all you would have to do is increase the commute time in every urban environment by 15 minutes. Just tweak a few cars, or get one to put on the brake … even if these things happen a few times, it will affect the confidence of consumers.”\nEven though he sees bad things happen all the time, Hypponen remains positive about self-driving cars, he said. “The internet has brought us more good than bad. Overall, technology improves our lives and business, even with the risks. And I’ll be able to watch cat videos on YouTube while I’m ‘driving’.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news discusses concerns about the security of autonomous vehicles and the potential for hacking incidents. It mentions instances of hackers gaining control over onboard systems of a passenger plane and causing a Jeep to crash in 2015. The article highlights that criminal hackers motivated by money pose a significant threat to self-driving cars, citing the lack of security in the car manufacturing industry. It also discusses the growing market for cyber insurance and predicts that by 2030, 25% of all cars sold will be autonomous.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/mar\/13\/autonomous-cars-self-driving-hack-mikko-hypponen-sxsw","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":51,"Time":"MARCH 20, 2018","Title":"Hyundai Motor cautious about self-driving cars after Uber accident","Content":"By Reuters Staff\nA Hyundai logo is seen on media day at the Paris auto show, in Paris, France, September 29, 2016. REUTERS\/Benoit Tessier\nSEOUL (Reuters) - Hyundai Motor 005380.KS said on Tuesday it is cautious about developing autonomous vehicles because of safety concerns after an Uber [UBER.UL] self-driving car hit and killed a woman in the United States.\nThe accident involving the U.S. technology firm’s car in Arizona marked the first fatality linked to an autonomous vehicle and a potential blow to the technology expected to transform transportation.\nYoon Sung-hoon, a director at Hyundai Motor, said safety concerns are a big factor in the development of autonomous cars, and as a result the South Korean automaker is “cautious about mass producing self-driving cars.”\n“When we evaluated other companies vehicles, they had more relaxed safety standards,” he told reporters, adding that Hyundai is taking more time than rivals to develop autonomous technology to guarantee safety.\n”No one knows under what situation accidents will occur.”\nHyundai, which has been slow in rolling out self-driving cars, has said it plans commercialise level 4 autonomous vehicles - which can operate without human input or oversight under select conditions - by 2021.\nReporting by Hyunjoo Jin; Editing by Shri Navaratnam\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Hyundai Motor expressed caution about developing autonomous vehicles due to safety concerns after an Uber self-driving car hit and killed a woman in the United States. This accident in Arizona marked the first fatality linked to an autonomous vehicle. Yoon Sung-hoon, a director at Hyundai Motor, cited safety concerns as a significant factor in their approach to autonomous car development. Hyundai is taking more time than rivals to ensure safety in autonomous technology. They plan to commercialize level 4 autonomous vehicles, which can operate without human input or oversight under select conditions, by 2021.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-hyundai-motor-autonomous-idUSKBN1GW17H","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":53,"Time":"MARCH 21, 2018","Title":"BMW to double self-driving car testing fleet despite U.S. fatality","Content":"By Reuters Staff\nMUNICH (Reuters) - BMW will not change its strategy on autonomous vehicle testing despite the death of a pedestrian struck by a self-driving car during tests by ride-hailing firm Uber [UBER.UL], senior executives said on Wednesday.\nFILE PHOTO: A logo of the German luxury carmaker BMW is seen during the company's annual news conference in Munich, Germany, March 21, 2018. REUTERS\/Michael Dalder\/File Photo\nThe German carmaker added it would double the size of its autonomous vehicle testing fleet to around 80 this year.\n“Our estimation about autonomous driving technology remains unchanged even though this appears to be an extremely regrettable accident,” Klaus Froehlich, BMW’s board member responsible for research and development, said of the fatality. [nL1N1R1168]\n“The path to autonomous driving is a long one. I have spoken about a mission to Mars,” he said, adding BMW was conducting its own tests under a high level of security.\nFroehlich said BMW’s self-driving cars would undergo a test regime equivalent to 250 million driven kilometers (155 million miles).\nOf this, 20 million km will be on real roads, while a giant supercomputer will simulate traffic scenarios in a virtual test regime equivalent to 230 million kms, Froehlich explained.\nSelf-driving cars will appear sooner if cities dedicate special lanes for autonomous cars in ring-fenced areas.\n“In a dedicated space for only autonomous vehicles, it is easier to anticipate what other vehicles and traffic will do,” Froehlich said. “This makes it easier to program vehicle reflexes and may even allow a car to have fewer sensors and less processing power than a vehicle which needs to navigate normal traffic with things like bicycle couriers.”\nBMW plans to launch an autonomous vehicle in 2021. Introducing a vehicle earlier than this is not plausible, since chipmakers and software designers have not yet developed a computer capable of processing the sheer volume of data generated by a self-driving car, Froehlich said.\nBMW is preparing for a new era of on-demand mobility where customers locate and hail vehicles using smartphones. Ride-hailing and car-sharing could be replaced by fleets of autonomous cars, once self-driving cars are roadworthy, Froehlich said.\nReporting by Edward Taylor; Editing by Mark Potter\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"BMW will not change its strategy on autonomous vehicle testing despite the death of a pedestrian struck by a self-driving car during tests by ride-hailing firm Uber. The German carmaker added it would double the size of its autonomous vehicle testing fleet to around 80 this year. Klaus Froehlich, BMW’s board member responsible for research and development, mentioned that their estimation about autonomous driving technology remains unchanged even though this appears to be an extremely regrettable accident. BMW’s self-driving cars will undergo a test regime equivalent to 250 million driven kilometers, with 20 million km on real roads and a virtual test regime equivalent to 230 million kms simulated by a giant supercomputer. BMW plans to launch an autonomous vehicle in 2021, preparing for a new era of on-demand mobility.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-bmw-selfdriving-idUSKBN1GX2E4","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":49,"Time":"Wed 26 Oct 2022 22.07 CEST","Title":"Tesla under US criminal investigation over self-driving claims, sources say","Content":"Tesla is under criminal investigation in the United States over claims that the company’s electric vehicles can drive themselves, three people familiar with the matter said.\nThe US Department of Justice (DoJ) launched the previously undisclosed investigation last year following more than a dozen crashes, some of them fatal, involving Tesla’s driver assistance system known as Autopilot, which was activated during the accidents, the people said.\nAs early as 2016, Tesla’s marketing materials have touted Autopilot’s capabilities. On a conference call that year, Elon Musk, the Tesla chief executive, described it as “probably better” than a human driver.\nLast week, Musk said on another call Tesla would soon release an upgraded version of “full self-driving” software, allowing customers to travel “to your work, your friend’s house, to the grocery store without you touching the wheel”.\nA video currently on the company’s website says: “The person in the driver’s seat is only there for legal reasons. He is not doing anything. The car is driving itself.”\nHowever, the company also has explicitly warned drivers that they must keep their hands on the wheel and maintain control of their vehicles while using Autopilot.\nThe Tesla technology is designed to assist with steering, braking, speed and lane changes but its features “do not make the vehicle autonomous”, the company says on its website.\nSuch warnings could complicate any case the justice department might wish to bring, the sources said.\nTesla, which disbanded its media relations department in 2020, did not respond to written questions from Reuters on Wednesday. Musk also did not respond to written questions seeking comment. A justice department spokesperson declined to comment.\nMusk said in an interview with Automotive News in 2020 that Autopilot problems stem from customers using the system in ways contrary to Tesla’s instructions.\nFederal and California safety regulators are already scrutinizing whether claims about Autopilot’s capabilities and the system’s design imbue customers with a false sense of security, inducing them to treat Teslas as truly driverless cars and become complacent behind the wheel with potentially deadly consequences.\nThe justice department investigation potentially represents a more serious level of scrutiny because of the possibility of criminal charges against the company or individual executives, the people familiar with the inquiry said.\nSign up to Business Today\nFree daily newsletter\nGet set for the working day – we'll point you to all the business news and analysis you need every morning\nPrivacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.\nAs part of the latest investigation, prosecutors in Washington and San Francisco are examining whether Tesla misled consumers, investors and regulators by making unsupported claims about its driver assistance technology’s capabilities, the sources said.\nOfficials conducting their inquiry could ultimately pursue criminal charges, seek civil sanctions or close the investigation without taking any action, they said.\nThe DoJ’s Autopilot investigation is far from recommending any action partly because it is competing with two other DoJ investigations involving Tesla, one of the sources said. Investigators still have much work to do and no decision on charges is imminent, this source said.\nThe justice department may also face challenges in building its case, said the sources, because of Tesla’s warnings about over-reliance on Autopilot.\nBarbara McQuade, a former US attorney in Detroit who prosecuted automotive companies and employees in fraud cases and is not involved in the current investigation, said investigators likely would need to uncover evidence such as emails or other internal communications showing that Tesla and Musk made misleading statements about Autopilot’s capabilities on purpose.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Tesla is currently under criminal investigation by the US Department of Justice (DoJ) regarding claims that their electric vehicles can drive themselves. The investigation was initiated following more than a dozen crashes, some of which were fatal, involving Tesla's driver assistance system known as Autopilot. The Autopilot system was activated during these accidents. The news also mentions that Tesla has promoted Autopilot's capabilities in their marketing materials since 2016, with Elon Musk describing it as 'probably better' than a human driver. Tesla recently announced an upgraded version of 'full self-driving' software. The company's website contains a video stating that the person in the driver's seat is only there for legal reasons, as the car is driving itself. However, Tesla has also explicitly warned drivers to keep their hands on the wheel and maintain control of their vehicles while using Autopilot. The investigation is examining whether Tesla misled consumers, investors, and regulators with unsupported claims about its driver assistance technology's capabilities. The outcome of the investigation is still pending, and it may involve criminal charges or civil sanctions against the company or individual executives.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2022\/oct\/26\/tesla-criminal-investigation-self-driving-claims-sources","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":54,"Time":"Tue 12 Jan 2016 23.00 CET","Title":"Google reports self-driving car mistakes: 272 failures and 13 near misses","Content":"Google’s self-driving cars might not yet have caused a single accident on public roads, but it’s not for want of trying.\nBetween September 2014 and November 2015, Google’s autonomous vehicles in California experienced 272 failures and would have crashed at least 13 times if their human test drivers had not intervened, according to a document filed by Google with the California Department of Motor Vehicles (DMV).\nWhen California started handing out permits for the testing of self-driving cars on public roads, it had just a few conditions. One was that manufacturers record and report every “disengagement”: incidents when a human safety driver had to take control of a vehicle for safety reasons.\nGoogle lobbied hard against the rule. Ron Medford, director of safety for the company’s self-driving car project, wrote at the time: “This data does not provide an effective barometer of vehicle safety. During testing most disengages occur for benign reasons, not to avoid an accident.”\nCrash involving self-driving Google car injures three employees\nRead more\nThe first annual reports were due on 1 January, and Google is the first company to share its data publicly. The figures show that during the 14-month period, 49 Google self-driving cars racked up over 424,000 autonomous miles and suffered 341 disengagements, when either the cars unexpectedly handed control back to their test drivers, or the drivers intervened of their own accord. The reports include both Google’s own prototype “Koala” cars and its fleet of modified Lexus RX450h.\nA Lexus RX450h modified with Google’s autonomous car software and sensors. Photograph: Tony Avelar\/AP\nIn 272 of those disengagements, the car detected a technology failure such as a communications breakdown, a strange sensor reading or a problem in a safety-critical system such as steering or braking.\nGoogle calls these “immediate manual control” disengagements. As the name suggests, the test driver is given audio and visual signals to alert them that they should take over driving without delay. Google test drivers typically responded to these warnings in 0.8 seconds.\nDrivers took over ‘many thousands of times’\nIn the remaining 69 disengagements, the human driver took control of the car on their own initiative, simply by grabbing the steering wheel or pressing the accelerator or brake pedal. The car automatically cedes control when this happens. Drivers do this fairly regularly if they suspect the car is doing (or is about to do) something hazardous or in response to other road users.\nHowever, Google admits that its drivers actually took over from their vehicles “many thousands of times” during the period. The company is reporting only 69 incidents because Google thinks California’s regulations require it only to report disengagements where drivers were justified in taking over, and not those where the car would have coped on its own.\nThe company decides this by replaying each disengagement in its online simulator over and over again. Google says that its powerful software, which now drives over 3m virtual miles each day, can accurately predict the behaviour of other drivers, pedestrians and cyclists and can thus determine whether the test driver’s intervention was required for safety.\nThis isn’t representative of where the technology will be when we’re ready to release it\nChris Urmson, director of Google’s self-driving car program\nBryant Walker Smith, assistant professor in the School of Law at the University of South Carolina, says the DMV could reasonably ask for more information. “Google could be clearer on how it draws the line between those driver-initiated disengagements that it reports and those that it does not,” he says. “The DMV is entitled to interpret its own rule, and it may have questions on this point.”\nConsumer Watchdog, a California-based campaign group, said the report shows that self-driving cars still need a human driver behind the wheel. Privacy project director John M Simpson said: “It’s unfathomable that Google is pushing back against those sensible safety protecting regulations. How can Google propose a car with no steering wheel or brakes when its own tests show that in 15 months the robot technology failed and handed control to the driver 272 times and a driver decided to intervene 69 times?\n“Release of the disengagement report was a positive step, but Google should also release any video it has of the disengagement incidents, as well as any technical data it collected.”\nIn 56 of the 69 driver disengagements reported to the DMV, Google calculated that its car would probably not have come into contact with another object. But, admits Google in its report, “we identified some aspect of the [car]’s behavior that could be a potential cause of contacts in other environments or situations if not addressed. This includes proper perception of traffic lights, yielding properly to pedestrians and cyclists, and violations of traffic laws.”\nGoogle classified the final 13 disengagements as “simulated contacts”: situations that would have resulted in a crash had the human driver not taken over. “In these cases, we believe a human driver could have taken a reasonable action to avoid the contact but the simulation indicated the [car] would not have taken that action,” the company says.\nAll self-driving car firms have filed disengagement reports\nThe report could be seen as a blow to Google’s insistence that self-driving cars should be fully autonomous. It latest prototypes are designed to operate without any driving controls for their human occupants to take over in an emergency (although those currently on public roads do have backup controls fitted).\n“It demonstrates that it is valuable to have a safety driver in the vehicle while testing, which is something we’ve always believed,” said Chris Urmson, director of Google’s self-driving car program. “But if you look at [regular] drivers, they’re effectively untrained in America. Expecting them to vigilantly monitor a system that operates as well as this does is a really a very challenging problem.”\nThe latest software for the Tesla Model S gives it autopilot capability. Photograph: The Washington Post\/Getty Images\nGoogle is not the only company to have filed a disengagement report with the DMV. Volkswagen\/Audi, Mercedes-Benz, Delphi, Tesla, Bosch and Nissan have all filed reports, which are currently under review by the department to confirm that they contain all the required information. The DMV told the Guardian that it does not currently have an expected date to complete its analysis of the data or draw conclusions from it.\nWhile Google has been testing its self-driving cars since 2008, the company will not be releasing disengagement data from before 2014. “This is the period we’re required to share with the DMV. Any data we would have from before that is just outdated,” Urmson says.\nGoogle notes that disengagements have been getting less common over the period of the report. However, Urmson cautions against expecting disengagements to drop regularly, year on year. “We’re continually adding capabilities to our vehicles, pushing them into more challenging situations,” he says. “Over the long view, we’d expect disengagements to be improving, but as we test in more challenging weather or driving situations, you could expect locally this to not look as good. And it really isn’t representative of where the technology will be when we’re ready to release it.”\nGoogle’s parent company, Alphabet, is reported to be planning to spin out its self-driving car technology into its own business later this year.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Google reports self-driving car mistakes: Between September 2014 and November 2015, Google’s autonomous vehicles in California experienced 272 failures and would have crashed at least 13 times if their human test drivers had not intervened. The disengagements include technology failures, human driver interventions, and instances where the car would have crashed if not for human intervention. Google's report also discusses the classification of these disengagements and mentions that self-driving car firms, including Google, have filed disengagement reports with the DMV.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/jan\/12\/google-self-driving-cars-mistakes-data-reports","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":56,"Time":"Fri 17 Jul 2015 16.56 CEST","Title":"Crash involving self-driving Google car injures three employees","Content":"Three Google employees have been injured in a crash involving one of the company’s self-driving cars.\nGoogle revealed the accident happened on 1 July when its car was rear-ended while stationary on a public road in Mountain View, California.\nIt is the first accident involving one of Google’s fleet of self-driving vehicles to have resulted in injury.\nThe modified Lexus sports utility vehicle, 20 of which are operating on public roads, was hit by another driver. Three Google employees within the self-driving car were taken to hospital complaining of whiplash and later released. The driver of the other car also complained of neck and back pain.\nThe Google car was stationary in a queue of traffic when it was hit by the vehicle travelling at 17mph, according to an accident report filed by Google.\nAccident prone\nThis is the 14th accident in six years and about 1.9 million miles of testing, according to the company. In 11 of the 14 accidents, Google said its car was rear-ended.\nTwo self-driving cars avoid each other on Californian roads\nRead more\nGoogle has also said that its cars have not caused any of the collisions while driving themselves. In 2011, a Google employee took one of the company’s cars to run an errand and rear-ended another vehicle, but the car was under human control at the time.\nChris Urmson, head of Google’s self-driving car programme, wrote that his automated cars “are being hit surprisingly often” by distracted drivers.\n“The clear theme is human error and inattention. We’ll take all this as a signal that we’re starting to compare favourably with human drivers.”\nUrmson said his team was exploring whether its cars could do something to alert distracted drivers before a collision. Honking would be one possibility, but Urmson said the noise could annoy residents of Mountain View where the cars are tested.\nGoogle acknowledges its self-driving cars had 11 minor accidents","Relevant":true,"Overview":"Here's the analysis of the news in JSON format:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Crash involving self-driving Google car injures three employees\",\n        \"Description\": \"Three Google employees have been injured in a crash involving one of the company’s self-driving cars. Google revealed the accident happened on 1 July when its car was rear-ended while stationary on a public road in Mountain View, California. It is the first accident involving one of Google’s fleet of self-driving vehicles to have resulted in injury. The modified Lexus sports utility vehicle, 20 of which are operating on public roads, was hit by another driver. Three Google employees within the self-driving car were taken to the hospital complaining of whiplash and later released. The driver of the other car also complained of neck and back pain. The Google car was stationary in a queue of traffic when it was hit by the vehicle traveling at 17mph, according to an accident report filed by Google.\"\n    }\n}\nThe news is relevant to autonomous driving accidents, and it provides specific information about a recent accident involving a Google self-driving car, including the date, location, and injuries to Google employees.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2015\/jul\/17\/crash-self-driving-google-car-injures-three","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":58,"Time":"Published 12:39 AM GMT+2, June 6, 2015","Title":"Google releases more details on self-driving car accidents","Content":"NEW YORK (AP) — Google is disclosing more details about the 12 accidents involving its self-driving cars so far as part of a commitment to provide monthly updates about the safety and performance of the vehicles.\nThe summary released Friday described all of the collisions as minor, saying no injuries were reported. As it has been doing for several weeks, Google said that the self-driving technology was not to blame for any of the accidents. In one case, however, an employee used the self-driving car to run an errand and rear-ended another car that was stopped in traffic. Google had previously disclosed that accident, which happened in August 2011.\nGoogle’s breakdown of the accidents came just two days after company co-founder Sergey Brin told shareholders that the company had already disclosed most of the pertinent information about the crashes.\nConsumer Watchdog, a group that has been a longtime Google critic, has been pushing the Mountain View, California, company to release all of the accident reports filed with the California Department of Motor Vehicles and other law enforcement agencies. Dissatisfied with Google’s accounting, Consumer Watchdog on Friday renewed its call for the company to release the official accident reports.\nThe Associated Press has asked Google and the California Department of Motor Vehicles for the reports. Both have refused, citing privacy concerns.\nWhile the latest disclosures fell short of providing the official accident reports, they did give previously unreleased information on the locations and dates and circumstances of the 12 accidents.\nGoogle Inc. started testing the cars in 2009, and the first accident was in May 2010.\nThe company says six of the accidents happened while the car was in autonomous driving mode. The other six happened while staffers were driving, including one incident where the car was hit by another driver who rolled through a stop sign. Google says the self-driving car automatically applied the brakes when it detected the other vehicle, and Google’s driver took manual control once the brakes were applied. The Google vehicle sustained some damage.\nAll but two of the accidents happened in Google’s hometown of Mountain View, where the company plans to begin testing its latest self-driving car — a pod-like vehicle — this summer.\nWhile several of the accidents happened at low speeds or while the car was stopped, in one case a Google vehicle was driving 63 miles per hour on a highway in San Jose, California, when another vehicle veered into its side.\nGoogle’s cars have been involved in four accidents so far this year, according to the company. It says the cars travel about 10,000 miles a week on public streets. The vehicles have driven about one million miles in autonomous mode and Google’s drivers have been in control for 800,000 additional miles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Google has disclosed more details about the 12 accidents involving its self-driving cars. All of the collisions were described as minor, with no reported injuries. Google emphasized that its self-driving technology was not to blame for any of the accidents. One accident occurred when an employee used a self-driving car to run an errand and rear-ended another car in traffic. Six of the accidents happened while the car was in autonomous driving mode, and the other six occurred while staff members were driving. Google's cars have been involved in a total of four accidents this year, with the vehicles traveling approximately 10,000 miles a week on public streets. The company has driven about one million miles in autonomous mode, with drivers in control for an additional 800,000 miles.\"\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/b69c5ae3cdcc48168d2ae6e3c7199faf","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":55,"Time":"Published 11:47 PM GMT+2, June 15, 2022","Title":"US report: Nearly 400 crashes of automated tech vehicles","Content":"DETROIT (AP) — Automakers reported nearly 400 crashes over a 10-month period involving vehicles with partially automated driver-assist systems, including 273 with Teslas, according to statistics released Wednesday by U.S. safety regulators.\nThe National Highway Traffic Safety Administration cautioned against using the numbers to compare automakers, saying it didn’t weight them by the number of vehicles from each manufacturer that use the systems, or how many miles those vehicles traveled.\nAutomakers reported crashes from July of last year through May 15 under an order from the agency, which is examining such crashes broadly for the first time.\n“As we gather more data, NHTSA will be able to better identify any emerging risks or trends and learn more about how these technologies are performing in the real world,” said Steven Cliff, the agency’s administrator.\nTesla’s crashes happened while vehicles were using Autopilot, “Full Self-Driving,” Traffic Aware Cruise Control, or other driver-assist systems that have some control over speed and steering. The company has about 830,000 vehicles with the systems on the road.\nThe next closest of a dozen automakers that reported crashes was Honda, with 90. Honda says it has about six million vehicles on U.S. roads with such systems. Subaru was next with 10, and all other automakers reported five or fewer.\nIn a June 2021 order, NHTSA told more than 100 automakers and automated vehicle tech companies to report serious crashes within one day of learning about them and to disclose less-serious crashes by the 15th day of the following month. The agency is assessing how the systems perform and whether new regulations may be needed.\nSix people were killed in the crashes involving driver-assist systems, and five were seriously hurt, NHTSA said. Of the deaths, five occurred in Teslas and one was reported by Ford. Three of the serious injuries were in Teslas, while Honda and Ford each reported one.\nTesla’s crash number may appear elevated somewhat because it uses telematics to monitor its vehicles and get real-time crash reports. Other automakers don’t have such capability, so their reports may come slower or crashes may not be reported at all, NHTSA said. A message was left seeking comment from Tesla.\nTesla’s crashes accounted for nearly 70% of the 392 reported by the dozen automakers. Although the Austin, Texas, automaker calls its systems Autopilot and “Full Self-Driving,” it says the vehicles cannot drive themselves and the drivers must be ready to intervene at all times.\nAuto safety advocates said driver-assist and self-driving systems have potential to save lives, but not until NHTSA sets minimum performance standards and requires safety improvements to protect all road users.\n“It’s clear that U.S. road users are unwitting participants in beta testing of automated driving technology,” said Cathy Chase, president of Advocates for Highway and Auto Safety.\nSen. Edward Markey, D-Mass., said that although NHTSA’s data has limitations, it’s not isolated evidence that Tesla has ignored regulations, putting the public in danger. There has been “a never ending parade of reports” of Teslas on automated systems rolling through stop signs or braking for no reason, he said. NHTSA also is investigating Teslas that crash into parked emergency vehicles.\n“As today’s data suggests, this contempt for auto safety laws has real-world consequences,” Markey said while urging NHTSA to take enforcement action.\nBut many Tesla owners love the automation. Craig Coombs of Alameda, California, said he uses the systems in stop-and-go traffic and on frequent highway trips. “They really reduce driver fatigue overall,” he said.\nHe gives himself a “moderate” grade for paying attention while using the system but says he never takes his mind off the road entirely. He knows the technology isn’t perfect, and said he has had to take over driving at times.\nManufacturers were not required to report how many vehicles they have on the road that have the systems, nor did they have to report how far those vehicles traveled, or when the systems are in use, NHTSA said. At present, those numbers aren’t quantifiable, an agency official said.\nHowever, NHTSA has used the data to seek a recall, open investigations and provide information for existing inquiries, officials said.\n“This will help our investigators quickly identify potential defect trends that can emerge,” Cliff said. “These data will also help us identify crashes that we want to investigate and provide more information about how people in other vehicles interact with the vehicles.”\nHonda said it has packaged the systems to sell more of them, which could influence its numbers. “The population of vehicles that theoretically could be involved in a reportable event is much greater than the population of vehicles built by automakers with a less-aggressive deployment strategy,” the company said.\nAlso, reports to NHTSA are based on unverified customer statements about whether automated systems were running at the time of a crash. Those crashes may not qualify for reporting to NHTSA after more data is gathered, Honda said.\nNHTSA’s order also covered companies that are running fully autonomous vehicles, and 25 reported a total of 130 crashes. Google spinoff Waymo led with 62, followed by Transdev Alternative Services with 34 and General Motors-controlled Cruise LLC with 23.\nWaymo, the autonomous vehicle unit of Alphabet Inc., said it has more than 700 autonomous vehicles in its fleet. The company is running a fully autonomous ride-hailing service in Arizona and testing one in California. The company said all the crashes happened at low speeds, with air bags inflating in only two of them.\nIn 108 of the crashes involving fully autonomous vehicles, no injuries were reported, and there was only one serious injury. In most of the crashes, vehicles were struck from the rear.\n____\nAP Data Journalist Larry Fenn in New York and Video Journalist Terry Chea in Alameda, California, contributed. This story has been corrected to show that six deaths and five serious injuries were reported in the crashes.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about nearly 400 crashes of vehicles with partially automated driver-assist systems, including 273 with Teslas, reported over a 10-month period. Six people were killed in these crashes, and five were seriously hurt. Tesla's crashes happened while vehicles were using Autopilot, 'Full Self-Driving,' Traffic Aware Cruise Control, or other driver-assist systems. Tesla's crashes accounted for nearly 70% of the 392 reported by the dozen automakers. Auto safety advocates are calling for minimum performance standards and safety improvements for driver-assist and self-driving systems.\"\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/self-driving-car-crash-data-ae87cadec79966a9ba56e99b4110b8d6","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":57,"Time":"7:32 PM GMT+1•March 20, 2018","Title":"Toyota pauses automated driving testing on U.S. roads following Uber accident","Content":"Automaker Toyota has temporarily ceases its public road testing of its fully autonomous ‘Chauffeur’ system in the U.S. after an accident earlier this week saw an Uber self-driving test vehicle strike a pedestrian, which ultimately resulted in her death.\nPolice have stated that initial findings suggest the accident would’ve been extremely difficult to avoid regardless of whether a human or an AV system was in control at the time, because of how quickly the victim crossed in front of the moving vehicle (outside of a crosswalk), but Toyota has indicated to Bloomberg that it’s stopping testing for now due to the potential “emotional effect on [its] test drivers.”\nToyota spokesperson Brian Lyons noted that the automaker couldn’t speculate on the cause of the crash or its implications for the future of the self-driving industry, which is a fairly standard line I’ve heard across automakers and others involved in the industry thus far, and which suggests a fair reluctance to make any lasting material decisions before all information is available regarding the Uber incident.\nToyota has been working on both its ‘Chauffeur’ fully automated driving system, as well as ‘Guardian,’ an advanced-driver assist system that is designed to institute fail-safes for intervening to prevent accidents when a human driver’s behavior puts themselves or others in danger.","Relevant":true,"Overview":"Here is the analysis of the news in JSON format:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"Toyota has temporarily paused its public road testing of its fully autonomous ‘Chauffeur’ system in the U.S. following an accident involving an Uber self-driving test vehicle that struck a pedestrian, resulting in her death. Police have stated that initial findings suggest the accident would’ve been extremely difficult to avoid regardless of whether a human or an AV system was in control at the time. Toyota is stopping testing due to the potential 'emotional effect on its test drivers.' Toyota spokesperson Brian Lyons noted that the automaker couldn’t speculate on the cause of the crash or its implications for the future of the self-driving industry.\"\n}\nThe news is indeed about an autonomous driving accident involving Toyota's autonomous testing, and the specific information about the case has been extracted as detailed as possible.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2018\/03\/20\/toyota-pauses-automated-driving-testing-on-u-s-roads-following-uber-accident\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":60,"Time":"11:00 PM GMT+2•May 28, 2019","Title":"Is your autonomous vehicle Sally the sports car or blood-thirsty Christine?","Content":"Lucas Dahlin\nContributor\nLucas Dahlin is an associate in Goodwin’s Intellectual Property group. He focuses his practice on complex intellectual property matters, with a particular experience in patent and trade-secret litigation.\nMore posts by this contributor\nAutonomous vehicle IP protection — when HAL is driving\nAutonomous cars are driving the reinvention of IP protection\nJulius Jefferson\nContributor\nJulius Jefferson is an associate in Goodwin’s IP Litigation group. Prior to joining Goodwin, he clerked for judges of the District of Delaware and of the Eastern District of Texas. Before attending law school, Julius was a Research Fellow at Wyeth Pharmaceuticals (now Pfizer), where he worked on finding cures for Alzheimer’s disease.\nMore posts by this contributor\nAutonomous vehicle IP protection — when HAL is driving\nAutonomous cars are driving the reinvention of IP protection\nDarryl M. Woo\nContributor\nDarryl Woo is a partner in Goodwin's IP Litigation group. He is a veteran trial lawyer, concentrating his practice on patent litigation and other complex technology litigation.\nMore posts by this contributor\nAutonomous vehicle IP protection — when HAL is driving\nAutonomous cars are driving the reinvention of IP protection\nWhen automobiles first started to appear alongside horse-drawn buggies, horses were the initial victims of the technology. They would not be struck by the slow-moving vehicles so much as be frightened into runaways. Sometimes the horses themselves suffered injury; other times it was property damage and pedestrian injury as the terrified steeds trampled everything in their paths.\nAs cars got faster and more numerous, pedestrians began to fall direct victim to moving vehicles, and it wasn’t long before rules of the road, and product and tort liability laws, imposed order to avoid carnage. Still, even today we have an ever-growing number of distracted and inept drivers turning our crowded highways into a dystopic version of real-life Frogger.  \nEnter the autonomous vehicle. All the benefits of driving, without having to drive! Proponents of driverless cars believe that autonomous technology will make cars safer and lead to a 90% reduction in accident frequency by 2050, as more than 90% of car crashes are caused by driver error.\nThere is certainly no shortage of news stories about injuries and fatalities resulting from drunk or distracted driving, and other accidents caused by drivers’ behavior. Text your friends or binge watch Black Mirror; with an autonomous vehicle, it’s OK. Or is it? All goes well until your AV decides that the pedestrian in front of you isn’t really there, or mistakes the debris trailing a garbage truck for lane guidance, and steers you into a concrete barrier. \nSome companies are closer than others to completely driverless vehicles, but the edge case driving situations are still a challenge, as we sadly found not long ago when an AV hit a pedestrian walking her bicycle across a dark highway in Arizona. Although there was a driver present who could have taken the controls, she didn’t. One can hardly blame her inattention, for the whole point of autonomous driving technology is to allow for, if not encourage, drivers to disengage from the task.\nThe “autonomous vehicle paradox” of inducing drivers to disconnect because they are not needed most of the time is confounding. At least in the interim, until autonomous systems can reliably achieve better than a 98% safety rate (roughly the rate of human drivers), autonomous systems will need to be supplemented by a human driver for emergencies and other unexpected situations.\nWhat happens and who is at fault when an accident occurs during or even after this transition period? Before the advent of autonomous vehicle technology, car accidents would typically invoke one of two legal theories:  driver negligence and manufacturers’ products liability. The legal theory of negligence seeks to hold people accountable for their actions and leads to financial compensation from drivers, or more commonly their insurance companies, for the drivers’ conduct behind the wheel. Products liability legal theories, on the other hand, are directed at companies that make and sell the injury-causing products, such as defective air bags, ignition switches, tires, or the cars themselves. Applying current legal theories to autonomous vehicle accident situations presents many challenges.   \nSuppose artificial intelligence (AI), or whatever makes a car autonomous, fails to detect or correct for a slippery curve. Perhaps a coolant leak from some car ahead covers the road with antifreeze, which can seen by the human behind the wheel, yet is all but invisible to the AI system. If the AV has manual override controls and an accident occurs, is the driver at fault for not taking the controls to avoid the crash? Is the car manufacturer at fault for not sensing the road condition or correcting for it? If both, how should fault be apportioned? \nIf a conventional vehicle was involved, the case against the driver may depend on proof that their behavior fell below an applicable standard of care. Not having one’s hands on the steering wheel would most likely be considered negligent behavior with such a car, and likely, so would being distracted by texting on a smartphone. But the self-driving feature of an autonomous vehicle by its very nature encourages driver inattention and lack of engagement with the controls. So would we be willing to find the driver at fault in the above instance for not taking over?\nCar manufacturers have expressed different views on liability.\nAs to the manufacturer of a conventional vehicle, liability might depend on whether a system or part was defective. A conventional vehicle in good condition, with no suspension, brake or steering defects, would likely allow the manufacturer to escape the brunt of liability in the above scenario. The manufacturer of an autonomous vehicle with human override controls, however, might try to shift at least some portion of fault to the driver, but would or should society allow that? The driver might argue he or she reasonably relied upon the AV, but should the manufacturer instead be held responsible where the hazard was visible and driver intervention could have avoided the accident?\nThe outcome might differ if the vehicle was completely autonomous and no human possibly could have intervened, but that vehicle may be years away.\nWhen such an AV comes to market, would, or should it be considered “defective” if it fails to detect or correct for the unexpectedly slippery surface? And if so, would it be considered defective merely because the failures occurred, or would proof also require some showing of errors in the AI software? Given that AI algorithms can evolve on their own and be dependent on millions of miles or hours of training data, how would one prove a “defect” in the software? Would it be fair to hold the programmer or software supplier accountable if the algorithm at the time of the accident differed substantially from the original, and the changes were effected by the AI algorithm having “taught” itself?\nAnother issue is the “hive mind.” One way AI could learn is by processing the collective experiences of other connected AVs, a process at one time used by Tesla. But if a significant proportion of other AVs upload erroneous data that is acted upon, what then?\nIn light of these issues, and as technology moves toward complete control of the vehicle with increasingly less human intervention, we may see the law evolve to place more emphasis on products liability theories and perhaps strict liability rather than negligence. It is not far-fetched that the price tag of an AV of the future not only include the R&D and component costs, but an “insurance” component to cover the costs of accidents. Such an evolution would be consistent with the decreasing role of the human driver, though it is somewhat inconsistent with a car manufacturer’s ability to exert full control over an AI system’s learning process, not to mention the driving environment.\nIn the present interim period when at least some human intervention is required, car manufacturers have expressed different views on liability. Some, like Volvo, have publicly stated that they will accept full responsibility whenever one of their vehicles is involved in an accident while in autonomous mode. But others, like Tesla, are attempting to shift liability to drivers when accidents happen by requiring some modicum of driver engagement, even in autonomous mode.\nFor example, to activate the capability to pass other cars in autonomous mode, drivers of Teslas once had to trigger the turn signal (Tesla recently announced a new version that would dispense with this requirement). Having drivers perform this seemingly insignificant but deliberate action could help auto manufacturers shift legal liability to the driver. Performing that simple action not only tells the car to pass, but suggests the driver has made a decision that the maneuver is safe and therefore is willing to, or should, accept responsibility for the consequences if it is not.  \nWhat about the ethics of the AI programming or training?\nThe underlying technology itself presents further complications to ascertain who is at fault. As alluded to above, one aspect of AI, better characterized as “machine learning,” is that its behavior is more or less a “black box” developed from millions of a variety of inputs and cannot be well-understood as might a strictly math-based algorithm.\nPut another way, we might be incapable of knowing exactly how the machine decided to act as it did. In such an instance, if the AI box was negligently trained, or “trained” on a simulator rather than based on real-world driving, could the author of the simulator instead be held accountable for the box’s failure to handle the edge case scenario that resulted in the accident? \nWhat about the ethics of the AI programming or training? A recent study found that current AI systems are perhaps 20% less likely to identify pedestrians if they are people of color. Was that due to the AI training on an insufficiently diverse subject base, or is there some other explanation? A recent survey conducted by MIT concluded that people ascribe a hierarchy to whose lives might be spared in edge cases where a crash is unavoidable and the question is not whether, but which, lives will perish. According to survey participants, human lives should be spared over those of animals; the lives of many should be spared over those of a few; and the young should be spared at the expense of the aged.\nInterestingly, people also thought there should be a preference for someone pushing a stroller and observing traffic laws, the bottom line being that if an AV is programmed according to such ethics, your odds of being hit by an AV might increase significantly if you are a lone person jay-walking across a busy highway. In the moral hierarchy of the study, being a cat, dog or criminal is at the lowest level of protection, though one must wonder how a vehicle will be able to distinguish a human criminal from a non-criminal — real-time connection to prison records? And what happens if, say, animal activist hackers alter the programming to prefer saving animals over people?\nIf the MIT survey is to be believed, such a hierarchy and variability exist today, only tucked away in the subconsciousness of human drivers rather than in machines. Think about that the next time you cross the street.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news discusses the challenges and legal implications of autonomous vehicles, including accidents and liability issues. It mentions a specific case in Arizona where an autonomous vehicle hit a pedestrian walking her bicycle across a dark highway. It also discusses the potential shift of liability from car manufacturers to drivers, the role of AI in autonomous vehicles, and ethical considerations in AI programming and training for self-driving cars.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2019\/05\/28\/is-your-autonomous-vehicle-sally-the-sports-car-or-blood-thirsty-christine\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":59,"Time":"Tue 1 Mar 2016 08.28 CET","Title":"Google self-driving car collides with bus in California, accident report says","Content":"One of Google’s self-driving cars has collided with a public bus in Mountain View, an accident report has revealed, in what appears to be the first example of one of the company’s prototype cars causing an accident.\nThe incident happened on 14 February and was reported to California’s department of motor vehicles in an accident report that the agency posted on 29 February.\nThe car was rolling at 2mph (3kph) and the bus at 15mph. No one was injured.\nThe report does not address fault. However, Google wrote that its car was trying to get around some sandbags on a street when its left front struck the right side of the bus.\nThe car’s test driver, who under state law must be in the front seat to grab the wheel when needed, thought the bus would yield and did not have control when the collision happened, according to Google’s report.\nIf it is determined the Google vehicle was at fault, it would be the first time one of its SUVs caused an accident while in autonomous mode.\nJessica Gonzalez, a spokeswoman from the DMV, said the agency hoped to speak with Google on Monday about what went wrong.\nIn a detailed statement from its monthly report on its self-driving car project, Google said that the incident happened on El Camino Real, a busy six-lane boulevard with hundreds on intersections. The car was following a recent change to its programming, following “the spirit of the road” as well as the traffic code by hugging the far side of the right-turn lane to allow other cars to pass on the left.\n“It then detected sandbags near a storm drain blocking its path, so it needed to come to a stop. After waiting for some other vehicles to pass, our vehicle, still in autonomous mode, began angling back toward the center of the lane at around 2mph – and made contact with the side of a passing bus traveling at 15mph. Our car had detected the approaching bus, but predicted that it would yield to us because we were ahead of it,” Google said.\n“Our test driver, who had been watching the bus in the mirror, also expected the bus to slow or stop. And we can imagine the bus driver assumed we were going to stay put. Unfortunately, all these assumptions led us to the same spot in the lane at the same time. This type of misunderstanding happens between human drivers on the road every day.”\nThis type of misunderstanding happens between human drivers on the road every day.\nGoogle's monthly self-driving car report\nGoogle says it has refined its software following the incident, acknowledging that buses and other large vehicles are less likely to yield. “In this case we clearly bear some responsibility because if our car hadn’t moved there would’t have been a collision.”\n“We hope to handle situations like this more gracefully in the future.”\nHilary Rowen, a partner at the insurance regulation practice Sedgwick LLP and an expert in the issue of self-driving cars and legal responsibility, said the case is a good example of a conundrum that will soon be common.\n“Here, the software didn’t avoid the accident, but the human could have taken over,” she said. “Who’s at fault – the driver, the bus driver, or the software?\nRowen said in real world situations, both the driver and injured party will actually be incentivized to blame the software which, if found to be guilty, will leave the driver’s record clear and likely have a higher payout for the injured party.\n“Everybody’s going to be blaming the software all the time,” Rowen said. “All the time.” Rowen still thinks autonomous car insurance will be cheaper than human-driven car insurance because humans aren’t very good drivers.\n“At a very visceral level, people will accept a higher chance of being maimed or killed by a human being than they will by being maimed or killed by software,” she said. “The self-driving car will likely be able to make better risk calculations.”\nGoogle has been testing two dozen Lexus SUVs outfitted with sensors and cameras near the tech firm’s Silicon Valley headquarters.\nGoogle cars have been involved in more than a dozen collisions. In most cases, Google’s cars were rear-ended. No one has been seriously injured.\nGoogle reports self-driving car mistakes: 272 failures and 13 near misses\nRead more","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Google self-driving car collides with bus in California, accident report says\",\n\"Date\": \"14 February\",\n\"Location\": \"Mountain View, California\",\n\"Description\": \"One of Google’s self-driving cars has collided with a public bus in Mountain View, an accident report has revealed, in what appears to be the first example of one of the company’s prototype cars causing an accident. The car was rolling at 2mph (3kph) and the bus at 15mph. No one was injured. The report does not address fault. However, Google wrote that its car was trying to get around some sandbags on a street when its left front struck the right side of the bus. The car’s test driver, who under state law must be in the front seat to grab the wheel when needed, thought the bus would yield and did not have control when the collision happened, according to Google’s report.\",\n\"Additional_Information\": [\n\"Google refined its software following the incident, acknowledging that buses and other large vehicles are less likely to yield.\",\n\"Hilary Rowen, an expert in self-driving cars and legal responsibility, commented on the conundrum of determining fault in such situations, suggesting that both the driver and injured party may blame the software.\"\n]\n}\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/feb\/29\/google-self-driving-car-accident-california","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":63,"Time":"24 January 2018","Title":"Tesla and GM self-drive cars involved in road collisions","Content":"Tesla and GM self-drive cars involved in road collisions\nPublished\n24 January 2018\nShare\nIMAGE SOURCE,\nCULVER CITY FIREFIGHTERS\nImage caption,\nThe front of the Model S crumpled upon impact\nTwo vehicles reportedly engaged in self-drive modes - a Tesla Model S and a General Motors Chevy Bolt - have been involved in separate road accidents in California.\nCulver City's fire service said the Tesla had \"ploughed into the rear\" of one of its fire engines parked at the scene of an accident on Monday.\nThe car's owner subsequently claimed it had been in Autopilot mode at the time.\nThe GM incident resulted in a collision with a motorbike in San Francisco.\nThe rider says the car - which was using GM's Cruise Automation technology - caused him serious injury and is now suing GM, according to local newspaper The Mercury News.\nGM has alleged the motorcyclist was at fault. The event dates back to December, but has come to light only now.\nCar-makers suggest self-drive technologies should make the roads safer, but at present California requires a driver to remain behind the wheel so they can retake control at short notice.\nHowever, the state's Department of Motor Vehicles is currently considering new regulations that would allow tests on public roads without a human sitting in the driver's seat.\nThe US National Transportation Board (NTSB) has said it will investigate the Tesla crash.\nAccording to a tweet by the Culver City Firefighters, the Model S was travelling at 65mph (105km\/h) when the impact occurred.\nIMAGE SOURCE,\nCULVER CITY FIREFIGHTERS\nImage caption,\nThe fire engine was parked at the time of the accident, the fire service says\n\"Amazingly, there were no injuries,\" an official said.\nTesla has the ability to analyse data gathered by its vehicles' on-board computers to determine the cause of crashes, and has shared information with the press about previous high-profile accidents.\nHowever, for now the car company has limited itself to saying that \"Autopilot is intended for use only with a fully attentive driver\" and that it has instructed drivers to keep their hands on the steering wheel while employing it.\nWill we ever be able to trust driverless cars?\nWill you need a driving licence in the age of self-driving cars?\nDegree in 'flying car' engineering offered online\nThe NTSB previously investigated Tesla after a Model S crash in 2016 in which the driver died.\nIt held the company partly accountable saying the Autopilot system had given the victim \"leeway... to divert his attention to something other than driving\".\nSince the accident, Tesla has introduced an update that brings its cars to a halt if they detect a driver's hands are not on the wheel.\n'Wobbled and fell'\nThe Chevy Bolt accident was on 7 December during the morning commute.\nThe injured motorcyclist, Oscar Nilsson, is a commercial photographer who had previously worked with GM and other car brands on commercial projects.\nAccording to an accident report filed by GM, its vehicle had been changing lanes when the gap ahead closed.\nIMAGE SOURCE,\nGENERAL MOTORS\nImage caption,\nGM has said that it believes its self-driving technology represents \"a giant leap forward\" for road safety\nWhile the Chevy Bolt had been re-centring itself in the original lane, GM said, a Honda motorcycle travelling at a faster speed had moved into the car's way, glanced off its side, wobbled and fallen over.\n\"Safety is our primary focus when it comes to developing and testing our self-driving technology,\" a GM spokesman told the BBC.\n\"In this matter, the SFPD [San Francisco Police Department] collision report stated that the motorcyclist merged into our lane before it was safe to do so.\"\nHowever, Mr Nilsson disputes this account and alleges the car \"suddenly veered\" into his lane.\nHis lawyer has further alleged the Chevy's back-up driver had tried to grab the wheel to prevent the accident but was too late to do so.\n\"Why don't these folks take some responsibility?\" the lawyer told the Mercury News.\nMr Nilsson claims he has suffered neck and shoulder injuries, adding he has had to take sick leave, and is claiming damages.\nDetails of the legal dispute have emerged a week after GM published its annual self-driving safety report.\nIt also announced that it hoped to deploy test vehicles with no steering wheel or pedals as soon as next year.\nMore on this story\nSelf-drive shuttle bus in crash on first day\nPublished\n8 November 2017\nUber strikes deal to buy self-drive Volvos\nPublished\n20 November 2017\nTesla Autopilot 'partly to blame' for crash\nPublished\n12 September 2017\nRelated Internet Links\nMercury News\nThe BBC is not responsible for the content of external sites.","Relevant":true,"Overview":"Here is the analysis of the news you provided in JSON format:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": [\n        {\n            \"Date\": \"24 January 2018\",\n            \"Location\": \"California\",\n            \"Incident_1\": {\n                \"Vehicle\": \"Tesla Model S\",\n                \"Description\": \"The Tesla Model S ploughed into the rear of a fire engine parked at the scene of an accident. The car's owner claimed it had been in Autopilot mode at the time.\"\n            },\n            \"Incident_2\": {\n                \"Vehicle\": \"General Motors Chevy Bolt\",\n                \"Description\": \"The GM incident resulted in a collision with a motorbike in San Francisco. The rider alleges that the car, using GM's Cruise Automation technology, caused him serious injury and is now suing GM.\"\n            },\n            \"Additional_Information\": {\n                \"Regulations\": \"California requires a driver to remain behind the wheel for self-drive technologies, but new regulations are being considered to allow tests on public roads without a human driver.\",\n                \"Investigation\": \"The US National Transportation Board (NTSB) has said it will investigate the Tesla crash.\"\n            }\n        },\n        {\n            \"Date\": \"7 December\",\n            \"Location\": \"San Francisco\",\n            \"Incident_3\": {\n                \"Vehicle\": \"General Motors Chevy Bolt\",\n                \"Description\": \"During the morning commute, the Chevy Bolt had an accident involving a Honda motorcycle. The motorcyclist, Oscar Nilsson, alleges that the Chevy veered into his lane, causing him injury.\"\n            },\n            \"GM's Response\": \"GM stated that its vehicle had been changing lanes when the gap ahead closed, and the motorcyclist merged into their lane before it was safe to do so. GM emphasizes safety in developing and testing its self-driving technology.\"\n        }\n    ]\n}\nThe news is indeed about autonomous driving accidents. It includes information about two separate incidents involving autonomous vehicles, one with a Tesla Model S and another with a General Motors Chevy Bolt. Both incidents resulted in collisions, and there are details about the circumstances and legal disputes surrounding these accidents.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-42801772","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":64,"Time":"MARCH 20, 2018","Title":"Arizona says not time to rein in self-driving cars after Uber fatality","Content":"By Heather Somerville\nTEMPE, Ariz.\/DETROIT (Reuters) - Arizona officials said on Tuesday they saw no immediate need to tighten rules on the testing of self-driving cars in the state in reaction to a fatal accident involving an Uber autonomous vehicle that has focused attention on the safety of the new technology.\nMeanwhile, Toyota Motor Corp 7203.T said it will pause autonomous vehicle testing following Sunday's accident in which an Uber Technologies Inc self-driving SUV struck and killed a woman in Tempe, Arizona.\nAutomakers and tech companies are evaluating whether or not to suspend their autonomous vehicle programs in the wake of the first fatality involving a self-driving vehicle. Uber said on Monday it was suspending its own program.\nToyota said it was temporarily pausing its testing on U.S. public roads to help its test drivers, who could be experiencing “an emotional effect” from the incident.\n“This ‘timeout’ is meant to give them time to come to a sense of balance about the inherent risks of their jobs,” the automaker said in a statement.\nUber, along with other technology companies and automakers, has been testing in Arizona, which regulates autonomous vehicles with a lighter touch than neighboring states such as California.\nThe state has a long history of allowing automakers to test new vehicles on its wide, open roads, and a 2015 executive order by Governor Doug Ducey paved the way for companies to test autonomous technology without interference by the legislature. More than 600 self-driving vehicles are now testing on Arizona roads, according to the governor’s office.\nOn Tuesday, Arizona’s director for policy and communications at the state’s department of transportation, Kevin Biesty, said existing regulations were sufficient and that the state had no immediate plans to issue new rules.\n“We believe we have enough in our laws right now to regulate automobiles,” Biesty told Reuters. “There will be issues that the legislature will have to address in the future as these become more widespread.”\nRELATED COVERAGE\nFactbox: How United States, others regulate autonomous vehicle testing\nDuring the early phase of self-driving vehicle testing, Arizona refrained from adding new restrictions on companies testing on state roads, Biesty said, adding they did not believe any new regulations would add to safety.\nBiesty said his agency was waiting for federal safety regulators to conclude an investigation before drawing any conclusions. Arizona’s self-driving vehicle oversight committee has not planned any meetings or actions, he said.\nAlso on Tuesday, Tempe Mayor Mark Mitchell issued a statement saying he supported Uber’s decision to suspend testing until the end of the investigation. His office said the mayor had not asked other autonomous vehicle companies to suspend testing in the city.\nFull details are still forthcoming surrounding the death on Sunday night of pedestrian Elaine Herzberg after she was struck by Uber’s test vehicle, a Volvo SC 90 sport utility vehicle, operating in autonomous mode..\nHerzberg, who was homeless, was crossing a four-lane road with her bicycle outside of the crosswalk when she was struck.\nThe Maricopa County Attorney’s Office in Phoenix said it was awaiting the results of an investigation by Tempe police before reviewing whether any charges should be filed. Officials with the National Transportation Safety Board and National Highway Traffic Safety Administration also are investigating.\nThe Tempe police department said in a brief statement it “would like to reaffirm that fault has not been determined in this case.”\nThe NTSB said investigators had viewed video captured by a camera in the Uber vehicle, and were gathering data from the vehicle and Uber. A photo posted on Twitter by the NTSB showed the front right corner of the vehicle’s hood seriously dented. Investigators will be in Tempe for the rest of the week and will not release findings until reviews of information from the scene and analysis of vehicle data are finished, the agency said.\nNational Transportation Safety Board (NTSB) investigators examine a self-driving Uber vehicle involved in a fatal accident in Tempe, Arizona, U.S., March 20, 2018. National Transportation Safety Board\/Handout via REUTERS\nUber’s Advanced Technologies Group office in Tempe appeared empty on Tuesday except for a security guard who said only a small security crew was working at the site since the accident. The autonomous vehicles normally filling the parking lot outside were nowhere to be seen.\nThe fatality is drawing fresh attention to the safety of autonomous vehicles, and the challenges of testing them on public streets. Self-driving cars have been involved in minor accidents, but nearly all have been blamed on human motorists hitting the autonomous vehicle.\nThe outcome of the investigations in Arizona will be pivotal for companies racing to profit from robo-taxi services and automated delivery vehicles. Among them are General Motors Co GM.N, Alphabet Inc's GOOGL.O Waymo unit, ride services company Lyft, Ford Motor Co F.N and others.\nFor a graphic showing one measure of autonomous vehicle performance, click here: (tmsnrt.rs\/2DFUPgA)\nWaymo earlier this month said it began operating self-driving vehicles in Arizona without human minders, offering rides to select customers. GM, through its Cruise Automation unit, has said it plans to launch a robo-taxi service next year, and said on Tuesday it stood by that timeline.\nSelf-driving startup nuTonomy, owned by Aptiv Plc APTV.N, said it was temporarily halting its testing on public roads in Boston, following the city's request.\nAnalysts and experts said the fatality involving Uber could slow progress toward deployment in the sector.\n“What this incident indicates is that the state of autonomous driving (and especially Uber) is very far from where it needs to be to become market-ready,” Richard Windsor, technology analyst for London-based Edison Investment Research, said in a blog post on Tuesday.\nWriting by Alexandria Sage, Additional reporting by Nick Carey, Joseph White and Paul Lienert in Detroit, Sydney Maki in Tempe, Ariz., David Schwartz in Phoenix, and David Shepardson in Washington; Editing by Jonathan Oatis, Matthew Lewis and Cynthia Osterman\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Arizona says not time to rein in self-driving cars after Uber fatality\",\n\"Location\": \"Tempe, Arizona\",\n\"Date\": \"March 20, 2018\",\n\"Summary\": \"The news is about a fatal accident involving an Uber autonomous vehicle in Tempe, Arizona. A self-driving Uber SUV struck and killed a woman named Elaine Herzberg, who was crossing a four-lane road with her bicycle outside of the crosswalk. The accident has raised concerns about the safety of self-driving cars. Various companies, including Toyota and Uber, have suspended their autonomous vehicle testing. Investigations by the National Transportation Safety Board (NTSB) and other authorities are ongoing to determine the cause of the accident and potential charges. The incident has drawn attention to the challenges of testing autonomous vehicles on public streets and could impact the development of robo-taxi services and automated delivery vehicles by companies like General Motors, Waymo, Lyft, and Ford.\"\n}\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-uber-toyota-idUSKBN1GW2GG","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":62,"Time":"8:43 PM GMT+2•July 1, 2022","Title":"Behind the scenes of Waymo’s worst automated truck crash","Content":"cameraImage Credits: Waymo\nThe most serious crash to date involving a self-driving truck might have resulted in only moderate injuries, but it exposed how unprepared local government and law enforcement are to deal with the new technology.\nOn May 5, a Class 8 Waymo Via truck operating in autonomous mode with a human safety operator behind the wheel was hauling a trailer northbound on Interstate 45 toward Dallas, Texas. At 3:11 p.m., just outside Ennis, the modified Peterbilt was traveling in the far right lane when a passing truck and trailer combo entered its lane.\nThe driver of the Waymo Via truck told police that the other semi truck continued to maneuver into the lane, forcing Waymo’s truck and trailer off the roadway. She was later taken to a hospital for injuries that Waymo described in its report to the National Highway Traffic Safety Administration as “moderate.” The other truck drove off without stopping.\nWhile Waymo’s autonomous semi truck was not at fault in the hit and run, the incident highlights gaps in reporting mechanisms, and raises questions about how ready the public and law enforcement are to cope with heavy, fast-moving vehicles that have no human driver.\nThe stakes for the autonomous trucking industry, which is still in its infancy, couldn’t be any higher. One crash, even if the company is not at fault, could tarnish the public’s image of the technology. \nWaymo’s trucking origins\nWaymo started testing its driverless technology with semi trucks in 2017, beginning in California and Arizona. At the time, it was in the middle of an epic legal battle with Uber over technology allegedly taken from Waymo by engineer Anthony Levandowski, and subsequently purchased by Uber as part of self-driving truck startup Otto.\nWaymo’s self-driving trucks, which are part of a delivery and logistics division the company calls Waymo Via, rely on similar technologies to its robotaxis: a suite of sensors, including cameras, radars and lidars, and powerful on-board computers. All have qualified truck drivers — known as autonomous specialists — in the driver’s seat.  \nIn 2018, Waymo began hauling freight in Georgia, and it branded its delivery business Waymo Via in 2020. It then expanded into New Mexico and Texas, and inked deals with logistics companies like J.B. Hunt, UPS and C.H. Robinson. Earlier this month, it committed to a long-term strategic partnership with Uber and announced a pilot delivery program with home goods e-tailer Wayfair.\nThat pilot is due to start in July on the same stretch of I-45 highway where the May crash occurred. \nInside the crash\nUsing reports from local police and the Department of Transportation, and data supplied by Waymo to NHTSA, TechCrunch has attempted to reconstruct the worst self-driving truck crash on U.S. roads to date.\nAccording to Waymo, the Peterbilt 579 truck was not carrying freight for any customers or partners; it was conducting “standard” testing with a weighted load.\nBehind the wheel was a 40-year-old autonomous specialist with a decade of truck driving experience; there was also a software operator on board. Like many workers in Waymo vehicles, both were actually employed by Transdev, a multinational transit and mobility company.\nAlthough the ultimate aim of automated trucks is to eliminate, or at least greatly reduce, staffing costs, self-driving truck startups today operate with a safety driver and an engineer or technician on board.\nWaymo reported that its truck was driving in autonomous mode at 62 miles per hour, slightly below the speed limit, when the other truck entered its lane and forced it off the road. \nA Waymo Via self-driving truck was hit by another semi truck in May 2022. Image Credits: Ennis Police Department\nWaymo told TechCrunch that the safety operator did not take control of the truck from its autonomous system. \n“The technology was not a factor, as this collision was caused by a human driver of another vehicle when they crossed the lane line and collided with the cab of Waymo’s vehicle and continued driving,” spokesperson Katherine Barna wrote in an email.\nEnnis PD photos, obtained under public records laws, show the Waymo truck and trailer by the side of the highway. They appear to have been prevented from sliding onto a parallel suburban road by a crash barrier. An Ennis police officer noted the truck itself sustained only minor damage: one picture shows damage to the truck’s lidar laser-ranging sensor.\nPhoto of Waymo Via’s lidar sensor, which was damaged in a crash in Texas. Image Credits: Ennis Police Department\nThe driver, however, was taken to a nearby hospital with unspecified, moderate injuries. The attending officer classified the incident as a hit and run. Waymo told TechCrunch that it understands the driver is doing well, following their injury. The driver did not respond to a request from TechCrunch for comment.\nBecause the system was active during at least some of the 30 seconds preceding the collision, Waymo was required to report it to NHTSA, to comply with the agency’s Standing General Order on Crash Reporting for automated vehicles.\nGaps in the system\nThere are no checkboxes on a Texas Department of Transportation crash report to record whether the vehicles involved are operating with full or partial automation, and that information was not recorded in the narrative section of the Waymo crash report.\nEnnis PD Detective Paul Asby, who later investigated the incident, told TechCrunch that he did not know the truck was operating autonomously at the time of the collision.\nAt the hospital, the Waymo driver told police the hit-and-run vehicle belonged to Helwig Trucking, a local carrier with about 15 trucks. (Waymo also confirmed that the truck’s cameras captured enough details to identify the other vehicle.) Helwig did not respond to a request for comment.\nThe driver left her phone number with the police and was released from the hospital, and the Waymo truck was towed away. Waymo also provided a contact number to the police. Detective Asby was assigned to the case, and quickly established that the crash was the fault of the Helwig driver. He contacted the company to get its side of the story, and its insurance details. But when it came to Waymo, Asby met a wall of silence.\n“I was going to speak to the driver because she was taken to the hospital but I’ve tried to contact her cell phone and it says it’s not a valid number,” he said. “The same thing for the passenger who was in there with her.”\nSubsequent calls to Waymo itself went unanswered. “They never did return my calls. I inactivated the case, but the insurance information is in there if they want it,” he says. “Maybe they’re so rich they don’t care.” \nWaymo told TechCrunch that it is not aware of any attempt by Ennis PD to contact it for information, and that it did not have any need to contact the department itself. \nHow it’s going\nThe Ennis crash is not the only one to have involved a Waymo semi truck. In February, a similar Waymo Peterbilt 579 traveling southbound on Interstate 10 near Sacaton, Arizona, was struck by a box truck traveling in the adjacent lane, and which had just also hit a motor coach. The Waymo vehicle was traveling 50 mph in a 75 mph limit zone. TechCrunch was not immediately able to source a police report detailing the crash; there were no reported injuries.\nIf Waymo had not been required to report the crashes to NHTSA, there is a chance they might never have come to light. The official crash reports gathered by Texas, which has welcomed multiple self-driving truck operations to its highways, appear insufficient to fully record incidents involving driverless vehicles. Local law enforcement has historically been similarly ill-equipped to deal with driving systems instead of driving humans. \nWaymo is trying to close those gaps, says Barna. “Waymo has built the Waymo Driver to interact with First Responders; and has worked closely with public safety officials to ensure the safe introduction of our technology in every market that we operate in,” she told TechCrunch.  “We have a team with decades of law enforcement experience that has provided training to hundreds of officers and firefighters in California, Arizona and Texas detailing best practices for safe interactions with Waymo vehicles.”\n“We’ve got a mountain of work to do integrating these things into society,” said Steve Viscelli, a sociologist at the University of Pennsylvania who studies trucking, and acts as an advisor to Aurora’s self-driving truck effort. “We need to talk a lot more about what they mean for supply chains, for workers and for the highway. There are a lot of people who are going to do stupid and aggressive stuff around them because they don’t like self-driving vehicles.”\nWaymo has told the U.S. Department of Transportation that it has 47 trucks, which have driven more than 1.6 million miles. It would not disclose to TechCrunch how many of those miles were driven under some level of automated control.\nAutomated trucking companies have “got the basic driving stuff down,” says Viscelli. “It’s what happens with the family on vacation and the tire’s off, or when there’s construction that changes the shape of the road, or debris on the highway. It’s when you have confidence in those issues that’s going to determine when they’re on the road. But I would not be surprised to see trucks without drivers on lanes next year.”\nUpdated: TechCrunch updated the article to reflect that Waymo also provided a contact number to the police.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Behind the scenes of Waymo’s worst automated truck crash\",\n\"Date\": \"May 5, 2022\",\n\"Location\": \"Interstate 45, Ennis, Texas\",\n\"Description\": \"On May 5, a Class 8 Waymo Via truck operating in autonomous mode with a human safety operator behind the wheel was involved in a crash just outside Ennis, Texas. The Waymo truck was forced off the roadway by another semi-truck, resulting in 'moderate' injuries to the Waymo driver. The other truck fled the scene. Waymo reported that its truck was driving in autonomous mode at 62 miles per hour at the time of the collision. The safety operator did not take control of the truck, and Waymo stated that the collision was caused by the human driver of the other vehicle. Gaps in reporting mechanisms and law enforcement readiness to handle autonomous vehicle incidents were highlighted by this crash.\"\n}\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/07\/01\/behind-the-scenes-of-waymos-worst-automated-truck-crash\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":65,"Time":"MARCH 28, 2018","Title":"China's Geely calls for caution in self-driving tech after fatal collision","Content":"By Norihiko Shirouzu\nBEIJING (Reuters) - The Chinese owner of Swedish car brand Volvo said on Wednesday that the autonomous driving industry should move forward prudently in the wake of a deadly accident in the United States this month that has rekindled a debate about safety.\nA Volvo XC 90 car is seen during an interview with CEO Hakan Samuelsson at the Volvo Cars Showroom in Stockholm, Sweden July 5, 2017. TT News Agency\/Jonas Ekstromer\/via REUTERS\/File Photo\nLi Shufu, the head of Zhejiang Geely Holding Group Co Ltd said at an event in Beijing it was important for automakers and governments to look at regulations and what sort of standards were needed to keep people safe.\n“One accident can kill the whole industry... So we must move with caution,” Li, who took a $9 billion stake in German carmaker Daimler AG last month, said at a Volvo-sponsored event on connected and automated cars.\nHe said excessive haste would backfire, and that a single accident could delay progress by as much as a year.\n“The accident reminds us that no matter how fast we develop, safety is the number one priority,” he said. “If safety cannot be guaranteed, no automated cars (should be allowed).”\nThe comments come as the global industry reels from a fatal accident in the Tempe, Arizona, involving a self-driving car from Uber Technologies Inc, the first death attributed to a self-driving car operating in autonomous mode.\nUber has suspended North American tests of its autonomous vehicles after the fatal collision on March 18. Its partner, chipmaker Nvidia Corp, said this week it has also suspended self-driving tests across the globe.\nThe accident has ramped up pressure on the industry to prove its software and sensors are safe.\nVolvo Cars Chief Executive Hakan Samuelsson said accidents were unfortunate and the industry had to move carefully, but that “saying no to technology would be the wrong answer” given human error was also a big factor in many collisions.\n“Technology is still the most promising answer to reducing (accidents),” he said at the event in Beijing.\nIn China, official support for driverless technology remains strong. The government gave its consent to internet firm Baidu Inc to test self-driving cars on city streets just days after the fatal accident in the United States.\nBaidu, which has a self-driving project dubbed Apollo, is leading China’s push in driverless technology, with the government keen to keep up with global rivals such as Tesla Inc and Waymo, the self-driving arm of Google parent Alphabet Inc.\nReporting by Norihiko Shirouzu; Writing by Adam Jourdan; Editing by Christopher Cushing\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the analysis of the provided news article in JSON format:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news article discusses China's Geely owner expressing caution in the self-driving technology industry after a deadly accident in the United States. Although it mentions autonomous driving, it primarily focuses on the need for caution, industry safety regulations, and the impact of accidents on the industry's progress. There is no specific information about the details of the fatal accident itself.\"\n}\nBased on the analysis, the news is relevant to autonomous driving but does not contain specific information about the details of a particular autonomous driving accident. Therefore, the \"Is_relevant\" key is set to true, and the \"Specific_information\" key provides a summary of the article's content.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/autos-selfdriving-volvo-idINKBN1H416C","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":61,"Time":"Wed 15 Jun 2022 22.27 CEST","Title":"Teslas account for 273 of nearly 400 US crashes involving driver-assist systems","Content":"US automakers reported nearly 400 crashes in 10 months involving vehicles with partially automated driver-assist systems, including 273 with Teslas, according to statistics released by federal safety regulators.\nTesla investigation deepens after more than a dozen US ‘Autopilot’ crashes\nRead more\nThe National Highway Traffic Safety Administration (NHTSA) cautioned against using the numbers to compare automakers, saying it did not weight them by the number of vehicles from each manufacturer or how many miles those vehicles traveled.\nAutomakers reported crashes from July 2021 through 15 May 2022 under an order from the agency.\nSteven Cliff, the NHTSA administrator, said: “As we gather more data, NHTSA will be able to better identify any emerging risks or trends and learn more about how these technologies are performing in the real world.”\nTesla crashes happened while vehicles were using Autopilot, Full Self-Driving, Traffic Aware Cruise Control or other driver-assist systems that have some control over speed and steering. The company has about 830,000 such vehicles on the road.\nThe next closest of a dozen automakers that reported crashes was Honda, with 90. Honda says it has about 6m vehicles on US roads with such systems. Subaru was next with 10. All other automakers reported five or fewer crashes.\nIn a June 2021 order, NHTSA told more than 100 automakers and tech companies to report serious crashes within one day of learning about them and to disclose less-serious crashes by the 15th day the following month.\nSix people were killed in crashes involving driver-assist systems and five were seriously hurt, NHTSA said. Of the deaths, five were in Teslas and one was reported by Ford. Three serious injuries were in Teslas. Honda and Ford each reported one.\nTesla’s crash number may appear elevated because it gets real-time crash reports. Other automakers do not, so their reports may come slower or crashes may not be reported at all, NHTSA said.\nTesla’s crashes accounted for nearly 70% of 392 reported. Although Tesla calls its systems Autopilot and Full Self-Driving, it says the vehicles cannot drive themselves and the drivers must be ready to intervene at all times.\nA message was left seeking comment from Tesla.\nAuto safety advocates said driver-assist and self-driving systems have potential to save lives, but not until NHTSA sets minimum performance standards and requires safety improvements.\n“It’s clear that US road users are unwitting participants in beta testing of automated driving technology,” said Cathy Chase, president of Advocates for Highway and Auto Safety.\nEd Markey, a US senator from Massachusetts, said although the NHTSA data had limitations, it was not isolated evidence that Tesla has ignored regulations and putting the public in danger.\nThere have been “a never ending parade of reports” of Teslas on automated systems rolling through stop signs or braking for no reason, he said. NHTSA also is investigating Teslas that crash into parked emergency vehicles.\n“As today’s data suggests, this contempt for auto safety laws has real-world consequences,” Markey said, urging NHTSA to take enforcement action.\nNHTSA has used the data to seek a recall, open investigations and provide information for existing inquiries, officials said.\n“This will help our investigators quickly identify potential defect trends that can emerge,” Cliff said. “These data will also help us identify crashes that we want to investigate and provide more information about how people in other vehicles interact with the vehicles.”\nNHTSA’s order also covered companies running fully autonomous vehicles: 25 reported a total of 130 crashes. Google spinoff Waymo led with 62, followed by Transdev Alternative Services with 34 and General Motors-controlled Cruise with 23.\nWaymo said it had more than 700 autonomous vehicles in its fleet. The company is running a ride-hailing service in Arizona and testing one in California. The company said all crashes happened at low speeds, with air bags inflating in two.\nIn 108 crashes involving fully autonomous vehicles, no fatalities were reported and there was only one serious injury. In most crashes, vehicles were struck from the rear.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Teslas account for 273 of nearly 400 US crashes involving driver-assist systems\",\n\"Summary\": \"US automakers reported nearly 400 crashes in 10 months involving vehicles with partially automated driver-assist systems, including 273 with Teslas. Six people were killed in crashes involving driver-assist systems, and five were seriously hurt. Tesla’s crashes accounted for nearly 70% of the 392 reported incidents. The NHTSA has used the data to seek a recall, open investigations, and provide information for existing inquiries.\"\n}\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/us-news\/2022\/jun\/15\/tesla-us-car-carashes-driver-assist-systems","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":68,"Time":"May 18, 2021","Title":"California regulator reviews Tesla's self-driving claims","Content":"BERKELEY, Calif., May 17 (Reuters) - A California regulator said on Monday it is reviewing whether Tesla Inc (TSLA.O) violated regulations by falsely promoting its advanced driver-assistance systems as being \"full self-driving.\"\nTesla's driver assistant features, which it describes as \"Autopilot\" or \"full self-driving\" are under growing scrutiny following a series of accidents and online videos of Tesla cars driving with no one in the driver's seat.\nTesla charges $10,000 for its semi-automated features such as lane changing and parking assistance under its full self-driving (FSD) software.\nTesla says on its website that the features do not make the vehicle autonomous, but the term FSD is widely used by CEO Elon Musk who has over 50 million Twitter followers.\nConsumer Groups and experts have criticised Tesla for misleading consumers into believing that the car can drive by itself without providing an adequate driver monitoring system to prevent the system's misuse, contributing to fatal accidents.\nAdvertisement · Scroll to continue\nA Tesla car driver killed in a recent accident in California had praised the automaker's \"full self-driving\" features and posted videos of himself apparently driving with his hands off the wheel.\nA 25-year-old man was also arrested earlier this month after driving a Tesla while in the backseat in the San Francisco Bay Area.\nThe California Department of Motor Vehicles (DMV) said in a statement it has \"the matter under review,\" referring to the regulation that \"prohibits a company from advertising vehicles for sale or lease as autonomous...\"\nAdvertisement · Scroll to continue\nA violation of the rule could lead to the suspension or the revocation of a manufacturer's occupational license, among other administrative actions, it added, without elaborating.\nTesla was not immediately available for comment.\n\"We call on Tesla to stop using hyperbolic terminology to sell its already impressive technology,\" Jason Levine, executive Director at Center For Auto Safety, told Reuters.\nReporting by Hyunjoo Jin; editing by Richard Pullin\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here is the JSON format response based on the news you provided:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"A California regulator is reviewing whether Tesla Inc violated regulations by falsely promoting its advanced driver-assistance systems as being 'full self-driving.' Tesla's driver assistant features, described as 'Autopilot' or 'full self-driving,' are under scrutiny due to a series of accidents and videos of Tesla cars driving with no one in the driver's seat. Tesla charges $10,000 for its semi-automated features under its full self-driving (FSD) software. While Tesla claims on its website that the features do not make the vehicle autonomous, CEO Elon Musk often uses the term FSD. Consumer groups and experts have criticized Tesla for misleading consumers and contributing to fatal accidents. The California Department of Motor Vehicles (DMV) is reviewing the matter, and a violation of the rule could lead to sanctions against Tesla.\"\n}\nThe news is relevant to autonomous driving, specifically regarding Tesla's promotion of its advanced driver-assistance systems as \"full self-driving,\" which is under review by a California regulator due to concerns and scrutiny surrounding accidents and misleading claims.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/technology\/california-regulator-reviews-teslas-self-driving-claims-2021-05-17\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":70,"Time":"MARCH 27, 2018","Title":"Nvidia halts self-driving tests in wake of Uber accident","Content":"By Alexandria Sage, Sonam Rai\n(Reuters) - Chipmaker Nvidia Corp said on Tuesday it has suspended self-driving tests across the globe, a week after an Uber Technologies Inc autonomous vehicle struck and killed a woman crossing a street in Arizona.\nA NVIDIA logo is shown at SIGGRAPH 2017 in Los Angeles, California, U.S. July 31, 2017. REUTERS\/Mike Blake\nUber is one of Nvidia’s partners and uses some of its self-driving technology. Nvidia’s shares closed down 7.8 percent at $225.52, wiping out more than $11 billion in market value.\nNvidia shares have more than doubled in value in the past 12 months on bets that the company will become a leader in chips for driverless cars, data centers and artificial intelligence.\nUber suspended North American tests of its autonomous vehicles after the fatal collision on March 18 in Tempe, Arizona. And Arizona on Monday suspended permission for Uber to test self-driving vehicles on public streets.\nThe California Department of Motor Vehicles said on Tuesday that Uber indicated it would not renew its current permit to test autonomous vehicles in California.\nUnder current terms, Uber is allowed to test autonomous vehicles on California public roads until March 31.\nUber should be given a chance to understand what went wrong, Nvidia Chief Executive Officer Jensen Huang said at the company’s annual GPU Conference in San Jose, California. “We don’t know what happened.”\nIn addition, Huang said he expected investments in self-driving technology would rise, not fall, in the aftermath of the Uber fatality.\nNvidia leads the autonomous industry with its artificial intelligence platform and has partnered with major global automakers such as Volkswagen AG VOWG_p.DE, Tesla Inc and Audi AG.\nRELATED COVERAGE\nNvidia using cloud to simulate self-driving vehicles covering billions of miles\nNvidia has been conducting testing of self-driving cars around the globe, including in Santa Clara, California; New Jersey, Japan and Germany,\n“Our global fleet of manually driven data collection vehicles continue to operate,” a company spokesman said.\nReuters earlier on Tuesday reported about the test suspension, citing a source.\n\"Nvidia has no choice but to take steps in the context of the fear, uncertainty and outrage likely to be stimulated by a robot car killing a human being,\" Roger Lanctot, an automotive technology analyst with Strategy Analytics, wrote in a blog post bit.ly\/2GaIBCN on Tuesday.\nThe Nvidia DRIVE platform is used by over 370 companies developing self-driving technology, including automakers and robotaxi companies and makers of self-driving hardware, such as sensors, Huang said on Tuesday.\nAnalyst Kinngai Chan from Summit Insight Group said he does not expect the suspension to hurt Nvidia’s earnings.\nThe question is whether it would be one or two years before fully autonomous vehicles are ready, Chan said, adding if it is pushed out further then the multiple for stock needs to come down as the valuation is very high.\nNvidia trades at 37.60 times forward 12-month earnings, compared with 18.92 for its peer group.\nNvidia, which generates the bulk of its revenue from chips used in gaming and data centers, reported automotive revenue of $132 million in its fiscal fourth quarter ended Jan. 28. That mainly consists of revenue from infotainment and development agreements for self-driving cars.\nReporting by Alexandria Sage in San Fransisco and Sonam Rai in Bengaluru, Additional reporting by Supantha Mukherjee and Rama Venkat Raman in Bengaluru; Editing by Shounak Dasgupta and Leslie Adler\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the analysis of the news in JSON format:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"Nvidia has suspended self-driving tests worldwide following an incident in which an Uber autonomous vehicle struck and killed a woman in Arizona. Uber, one of Nvidia's partners, utilizes some of Nvidia's self-driving technology. The incident led to a significant drop in Nvidia's shares, losing more than $11 billion in market value. Uber also suspended its North American autonomous vehicle tests, and Arizona revoked Uber's permission to test self-driving vehicles on public streets. The California Department of Motor Vehicles stated that Uber would not renew its current permit to test autonomous vehicles in California. Despite the incident, Nvidia's CEO, Jensen Huang, expressed the belief that investments in self-driving technology would increase. Nvidia is a leading player in the autonomous industry, with partnerships with major automakers such as Volkswagen, Tesla, and Audi.\"\n}\nThe news is indeed about autonomous driving accidents, specifically regarding Nvidia's decision to suspend self-driving tests after an Uber autonomous vehicle struck and killed a woman in Arizona.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-nvidia-idUSKBN1H32E0","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":69,"Time":"6:05 AM GMT+1•December 11, 2018","Title":"Report: A manager at Uber’s self-driving unit warned executives about safety issues just days before fatal crash","Content":"cameraImage Credits: Justin Sullivan\n(opens in a new window)\n\/ Getty Images\nLess than a week before an Uber self-driving SUV prototype struck and killed a pedestrian in Tempe, Arizona last March, a manager sent executives an email cautioning that its autonomous vehicle unit needed to “work on establishing a culture rooted in safety,” reports The Information.\nRobbie Miller, then a manager in the unit’s testing operations, was preparing to leave the company when he warned that “cars are routinely in accidents resulting in damage” and backup drivers, who sit behind the wheel in self-driving cars for safety reasons, weren’t properly trained or fired even if they made repeated mistakes.\nIn his March 13 email, Miller wrote that “the cars are routinely in accidents resulting in damage. This is usually the result of poor behavior of the operator or the AV technology. A car was damaged nearly every other day in February. We shouldn’t be hitting things every 15,000 miles. Repeated infractions for poor driving rarely results in termination. Several of the drivers appear to not have been properly vetted or trained.” Miller, who previously held positions in self-driving programs at Google and Otto, also gave a comprehensive list of suggestions he believed would reduce the likelihood of accidents.\nFive days later, on March 18, the Tempe collision occurred, resulting in the death of 49-year-old Elaine Herzberg, who was crossing a street when she was hit by the SUV. Uber immediately halted testing of its autonomous cars on public roads and reached a settlement with Herzberg’s family two weeks later. It was later revealed in a police report that the car’s backup safety driver was watching videos on her phone when the crash occurred. The incident also raised serious questions about the safety of Uber’s self-driving technology system.\nUber resumed testing of its self-driving cars in July, but in manual mode and with new safety standards in place. Before the crash, Uber had planned to launch a limited self-driving taxi service in the suburbs of Phoenix that would let users hail autonomous cars without backup drivers. Alphabet’s Waymo, one of Uber’s main rivals in the autonomous vehicle space, launched a self-driving taxi service there last week, but with backup drivers.\nThe email, obtained and published by The Information, was sent to Eric Meyhofer, head of Uber’s autonomous vehicle unit and John Thomason, vice president of software, as well as five other executives and lawyers.\nIt details several safety incidents that had occurred in the months before the fatal collision in Arizona, including one in which a prototype swerved off the road and drove on the sidewalk for several meters. Miller described delays in reviewing incident logs and wrote, “This is not how we should be operating.”\nDuring that period, Uber continued to test hundreds of self-driving vehicles on the streets of San Francisco, Pittsburgh and suburbs of Phoenix, Arizona, before the program was brought to a temporary halt by the fatal accident in Tempe.\nMiller’s suggestions included reducing the size of Uber’s fleet of autonomous vehicles by 85 percent so the software quality team could better review safety incidents, have more than one backup driver in each prototype (Uber had previously used two backup drivers per vehicle) and allow individual employees, including engineers or operation managers, to shut down testing if they believed there were potential safety issues.\n“The cars are collecting more than enough data for the engineers to be aware of and solve the issues they encounter. Reducing the fleet size dramatically would not change that nor would it slow down the current rate of progress,” Miller wrote. “It would significantly reduce ATG’s likelihood of being involved in an accident. This sentiment is echoed widely throughout ATG.”\nThe Information report says that Miller’s assertions were supported by interviews with five current and 15 former Uber employees. While Meyhofer and the other executives who were sent the email did not respond to it, Miller was told by his supervisor that the team would discuss it. The Information’s sources also said his suggestions were discussed during the internal safety review that followed the Tempe accident.\nTechCrunch has contacted Uber for comment.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Report: A manager at Uber’s self-driving unit warned executives about safety issues just days before fatal crash\",\n\"Date\": \"March 18\",\n\"Location\": \"Tempe, Arizona\",\n\"Victim\": \"Elaine Herzberg (49-year-old)\",\n\"Description\": \"Less than a week before an Uber self-driving SUV prototype struck and killed a pedestrian in Tempe, Arizona last March, a manager sent executives an email cautioning that its autonomous vehicle unit needed to “work on establishing a culture rooted in safety.” The email warned about the poor behavior of operators, accidents resulting in damage, and backup drivers not being properly trained or fired even if they made repeated mistakes.\",\n\"Incidents Mentioned\": [\n{\n\"Date\": \"Before the fatal collision in Arizona\",\n\"Description\": \"Several safety incidents occurred, including a prototype swerving off the road and driving on the sidewalk for several meters.\"\n}\n],\n\"Response\": \"Uber immediately halted testing of its autonomous cars on public roads, reached a settlement with Herzberg’s family two weeks later, and resumed testing of self-driving cars in manual mode with new safety standards. Uber had planned to launch a limited self-driving taxi service before the crash but had to delay the plan.\"\n}\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2018\/12\/10\/report-a-manager-at-ubers-self-driving-unit-warned-executives-about-safety-issues-just-days-before-fatal-crash\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":67,"Time":"Tue 5 Jul 2016 23.44 CEST","Title":"Why self-driving cars aren't safe yet: rain, roadworks and other obstacles","Content":"Last week’s fatal crash involving a Tesla Model S offers a startling reminder that driverless technology is still a work in progress.\nAs Tesla’s own blogpost on the “tragic loss” points out, the autopilot technology that was controlling Joshua Brown’s car when it ploughed into a truck is in a “public beta phase”. That means the software has been released into the wild to be stress-tested by members of the public so that bugs can be flushed out. It’s the kind of approach we are used to seeing when we gain early access to new email applications or virtual reality headsets. As Apple co-founder Steve Wozniak told the New York Times: “Beta products shouldn’t have such life-and-death consequences”.\nUntil the investigation into the tragic incident concludes, we won’t know whether it was caused by a software glitch or human error – particularly with reports suggesting the driver may have been watching a Harry Potter DVD. All we know is that “neither autopilot nor the driver” noticed the white side of the tractor trailer against the brightly lit sky “so the brake was not applied”.\nTesla’s autopilot uses both cameras and radar to detect and avoid obstacles, so in this case we know there must have been a double failure. The cameras struggled with the glare from the sun, while the radar – according to Musk – “tunes out what looks like an overhead road sign to avoid false braking events”.\nElon Musk may have taken to aggressively dismissing coverage of the crash on his Twitter account, but there are still significant every day flaws that presents obstacles to wider adoption of self-driving car technology.\nSensor fusion\nWhen you have multiple sensors giving conflicting information, which one do you defer to? This seemed to be an issue at play in the fatal Tesla crash, where the one sensor that did spot the truck misinterpreted it as a road sign overhead.\n“The big question for driverless car makers is: how does the intelligence of the machine know that the radar sensor is the one to believe? That’s the secret sauce,” says Sridhar Lakshmanan, a self-driving car specialist and engineering professor at the University of Michigan-Dearborn.\nRoadworks\nWhen Delphi sent an autonomous car 3,400 miles across the US in April 2015, engineers had to take control of the car only for a 50-mile stretch. The reason? Unpredictable urban conditions with unmarked lanes and heavy roadworks. In other words, an average city commute.\nSandbags (and assumptions)\nOne of Google’s self-driving cars collided with a public bus in Mountain View in February as it tried to navigate some sandbags on the street. In attempting to move around the sandbags, the car’s left front struck the side of the bus that was trying to overtake. The car had detected the bus but predicted it would yield, and the test driver behind the wheel also made that assumption.\n“Unfortunately, all these assumptions led us to the same spot in the lane at the same time. This type of misunderstanding happens between human drivers on the road every day,” said Google of the incident.\nWeather\nAdverse weather conditions create visibility problems for both people and the sensors that power driverless technology. Rain can reduce the range and accuracy of laser-based Lidar sensors, obscure the vision of on-board cameras and create confusing reflections and glare. In a bid to improve the performance of driverless technology in soggy conditions, Google has started testing its cars on public roads near Seattle, where regular rain is guaranteed.\nHacking\nAs cars become more hi-tech they become more vulnerable to hacking. With driverless vehicles, the extra computers, internet connectivity and sensors increase the possible vulnerabilities. In a proof-of-concept attack, security researcher Jonathan Petit showed that lidar can be easily fooled into detecting a non-existent obstacle using a handheld laser pointer, which can force the car to slow down, stop or swerve.\nHumans\nJust as humans are at fault in more than 90% of car accidents, so too can they be the weakest link in semi-autonomous vehicles – particularly when a functionality labelled as “autopilot” encourages users to place their trust in the machine. “Maybe these intermediate levels [of automation] are not a viable consumer product,” says Richard Wallace, the director of the Transportation Systems Analysis group within the Center for Automotive Research. “They go a little too far in encouraging drivers to check out and yet they aren’t ready to take control.”\nAnd other humans\nIt’s not just the humans inside cars with self-driving technology, but those in other vehicles that need to be vigilant. Accident rates involving driverless cars are twice as high as for regular cars, according to a study by the University of Michigan’s Transportation Research Institute which looked at data from Google, Delphi and Audi.\nHowever the driverless cars weren’t at fault – they are typically hit from behind by inattentive or aggressive humans unaccustomed to self-driving motorists being such sticklers for the road rules. Google has started to programme its cars differently to behave in more familiar, human ways, such as inching forward at a four-way stop to indicate they will be moving next.\nBut it’s this collision where the biggest challenges for technology firms lie, encouraging adoption of rapidly developing new technology for a population that is quirky, unpredictable and, in turn, both sceptical and overtrusting.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Why self-driving cars aren't safe yet: rain, roadworks and other obstacles\",\n\"Summary\": \"The news discusses the challenges and issues surrounding self-driving cars, including a fatal crash involving a Tesla Model S. It highlights various obstacles to the widespread adoption of self-driving car technology, such as sensor fusion problems, roadworks, sandbags, adverse weather conditions, hacking vulnerabilities, and human factors. The article also mentions higher accident rates involving driverless cars due to interactions with other human drivers.\"\n}\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/jul\/05\/tesla-crash-self-driving-car-software-flaws","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":75,"Time":"20 March 2018","Title":"Uber halts self-driving car tests after death","Content":"Uber halts self-driving car tests after death\nPublished\n20 March 2018\nShare\nIMAGE SOURCE,\nREUTERS\nUber said it is suspending self-driving car tests in all North American cities after a fatal accident.\nA 49-year-old woman was hit by a car and killed as she crossed the street in Tempe, Arizona.\nWhile self-driving cars have been involved in multiple accidents, it is thought to be the first time an autonomous car has been involved in a fatal collision.\nUber said that its \"hearts go out to the victim's family\".\n\"We're fully cooperating with @TempePolice and local authorities as they investigate this incident\", the company said in a statement on Twitter.\nSkip twitter post by Uber Comms\nAllow Twitter content?\nThis article contains content provided by Twitter. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Twitter’s cookie policy\n, external\nand privacy policy\n, external\nbefore accepting. To view this content choose ‘accept and continue’.\nAccept and continue\nThe BBC is not responsible for the content of external sites.\nEnd of twitter post by Uber Comms\nPolice said the accident happened Sunday night while the car was in autonomous mode. A human monitor was also behind the wheel.\nPolice said the woman, Elaine Herzberg, had not been using a pedestrian crossing. Herzberg was taken to a local hospital where she died.\nThe US National Highway Traffic Safety Administration and the National Transportation Safety Board said they were sending teams to Tempe.\n'Wake up call'\nCompanies including Ford, General Motors, Tesla and Waymo are investing heavily in research to develop self-driving cars, which are often characterised as the future of the industry and hailed as a way to reduce traffic accidents.\nDriverless cars on UK roads by 2021 - really?\nUK plans 200-mile 'country roads' driverless trial\nMany states across America have welcomed the tests in the hope of keeping themselves at the forefront of new technology.\nHowever, there have been warnings that the technology is being deployed before it is ready.\nAnthony Foxx, who served as US Secretary of Transportation under former President Barack Obama, called the accident a \"wake up call to the entire [autonomous vehicle] industry and government to put a high priority on safety.\"\nMore than a dozen states in the US allow autonomous vehicles on the roads to some degree. Officials typically require a person to be on hand either in the car or remotely in case something goes wrong, according to the Center for Automotive Research.\nThe US is working on national safety guidelines for such vehicles.\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nA self-driving car at the Google headquarters in 2015\nConsumer Watchdog, a lobby group that has warned of the risks of autonomous cars, on Monday called for a moratorium of such vehicles on public roads, describing the accident as a \"tragedy we have been fighting years to prevent\".\n\"We hope our calls for real regulation of driverless cars will be taken seriously going forward by Silicon Valley and the Trump Administration,\" the group wrote on Twitter.\nUber started testing driverless cars in Pittsburgh in 2016. The ride-hailing firm has also been testing driverless cars in San Francisco, Pittsburgh, Toronto and the Phoenix area, which includes Tempe.\nThe death comes a year after Uber took its self-driving cars off the road following an accident that left a Volvo SUV on its side in Arizona. The programme was later reinstated.\nSelf-driving shuttle bus in crash on first day\nAre driverless pods the future for world cities?\nCarla Bailo, president and chief executive of the Center for Automotive Research, said more information about how the crash occurred is necessary before officials can say what went wrong and how the self-driving system should be improved.\nShe also said the fatality should be considered in the context of all accidents.\nMore than 37,000 people, including almost 6,000 pedestrians, died in traffic accidents in the US in 2016, according to the US Department of Transportation.\n\"We need to be fair and look at all the data,\" she said. \"But I don't think anybody is taking this lightly. By far safety is the first concern.\nTempe Mayor Mark Mitchell said he supports autonomous car tests because of the technology's potential. He also praised Uber's decision to suspend the programme as \"responsible\".\n\"Our city leadership and Tempe Police will pursue any and all answers to what happened in order to ensure safety moving forward,\" he said.","Relevant":true,"Overview":"Here is the analysis of the provided news in JSON format:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Uber halts self-driving car tests after death\",\n        \"Published_date\": \"20 March 2018\",\n        \"Location\": \"Tempe, Arizona\",\n        \"Description\": \"Uber suspended self-driving car tests in all North American cities after a fatal accident. A 49-year-old woman, Elaine Herzberg, was hit and killed by an autonomous car while crossing the street. This incident marked the first fatal collision involving an autonomous vehicle. Police reported that the car was in autonomous mode with a human monitor behind the wheel when the accident occurred. Uber expressed condolences to the victim's family and pledged cooperation with the investigation. The US National Highway Traffic Safety Administration and the National Transportation Safety Board sent teams to Tempe to investigate. This accident raised concerns about the deployment of self-driving technology before it was fully ready, prompting calls for safety regulations and a moratorium on public road testing of autonomous cars. The incident highlighted the challenges of ensuring the safety of autonomous vehicles and the need for ongoing research and regulation.\"\n    }\n}\nThe news is indeed about an autonomous driving accident involving Uber's self-driving car in Tempe, Arizona. The specific information includes the title, published date, location, a description of the incident, and its implications, such as investigations and calls for safety regulations.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/business-43459156","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":76,"Time":"Published 2:27 PM EDT, Sat July 29, 2023","Title":"Uber self-driving car test driver pleads guilty to endangerment in pedestrian death case","Content":"CNN\n — \nThe Uber test driver behind the wheel of one of the company’s self-driving cars, when it hit and killed a pedestrian in 2018, pleaded guilty to endangerment and was sentenced to three years of supervised probation Friday, according to officials.\nRafaela Vasquez was watching television on her smartphone in March 2018 when the Uber self-driving vehicle fatally struck Elaine Herzberg, 49, who was crossing a road in Tempe, Arizona, according to a National Transportation Safety Board investigation. Herzberg’s death was the first known fatality involving a fully autonomous vehicle.\nUber self-driving car operator charged in pedestrian death\nA judge in the Superior Court of Maricopa sentenced Rafaela Vasquez to three years of supervised probation and determined that the charge would only be designated a misdemeanor “upon successful completion of her sentence,” according to a news release.\nThe Uber-employed safety driver behind the wheel of the car was meant to monitor the car’s performance and intervene if the autonomous driving software failed, as previously reported by CNN.\nBut the National Transportation Safety Board’s 2019 investigation found that Vasquez was looking away from the road for over a third of the trip. The board concluded that the crash was “avoidable” if the safety driver had been alert and also found that an inadequate safety culture at Uber contributed to the crash. The company’s self-driving software wasn’t designed to expect that pedestrians outside crosswalks may be crossing the street, according to the investigation.\nSelf-driving cars were supposed to take over the road. What happened?\n“The defendant in this matter was responsible for the operation of a vehicle on our city streets that ended with a woman being killed,” Maricopa County Attorney Rachel Mitchell said in the court’s news release. “Determining an appropriate plea in this case involved considering a multitude of factors. We believe the Judge ordered an appropriate sentence based on the mitigating and aggravating factors.”\n“Getting behind the wheel of a car is a serious responsibility. Regardless of whatever technology might be available to drivers, safety for everyone on the street and in the vehicle must always be a driver’s first priority,” Mitchell went on.\nUber death leaves questions about self-driving car liability unanswered\nThe plea agreement also stipulates that Vasquez pay restitution “to all victims,” including insurance companies involved.\nVasquez was initially charged with negligent homicide in 2020. She pleaded not guilty to the charge.\nUber reached a settlement with the victim’s family less than two weeks after her death. The company did not face criminal charges. In December, the ride-share giant said it plans to launch a fully driverless service in collaboration with Motional in 2023.\nCNN has reached out to Vasquez’s attorney, Albert Jaynes Morrison, for comment.\nCNN’s Matt McFarland contributed to this report.","Relevant":true,"Overview":"Here is the JSON format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is about an autonomous driving-related incident involving an Uber self-driving car. In March 2018, a pedestrian named Elaine Herzberg was fatally struck and killed by an Uber self-driving vehicle in Tempe, Arizona. The Uber test driver, Rafaela Vasquez, who was behind the wheel during the accident, has pleaded guilty to endangerment and was sentenced to three years of supervised probation. The incident marked the first known fatality involving a fully autonomous vehicle. The National Transportation Safety Board's investigation found that Vasquez had been looking away from the road for over a third of the trip, and they concluded that the crash could have been avoided if the safety driver had been alert. Additionally, an inadequate safety culture at Uber was cited as a contributing factor to the accident. Vasquez was initially charged with negligent homicide but pleaded not guilty. Uber reached a settlement with the victim's family, and the company did not face criminal charges. The news also mentions Uber's plan to launch a fully driverless service in collaboration with Motional in 2023.\"\n}\nThe news is indeed about an autonomous driving accident involving an Uber self-driving car, and the specific information about the case has been extracted as detailed as possible.","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2023\/07\/29\/business\/uber-self-driving-car-death-guilty\/index.html","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":77,"Time":"MARCH 22, 2018","Title":"Self-driving car industry confronts trust issues after Uber crash","Content":"By Alexandria Sage, Tina Bellon, Nick Carey\n(Reuters) - The fatal accident involving an Uber self-driving car cranks up pressure on the self-driving vehicle industry to prove its software and sensors are safe in the absence of strong government standards, experts in the field said.\nFILE PHOTO: U.S. National Transportation Safety Board (NTSB) investigators examine a self-driving Uber vehicle involved in a fatal accident in Tempe, Arizona, U.S., March 20, 2018. National Transportation Safety Board\/Handout via REUTERS\nAutomakers including General Motors Co GM.N, technology companies such as Alphabet Inc GOOGL.O and ride services providers like Uber Technologies Inc [UBER.UL] have all urged policy makers at the federal and state level not to put a heavy regulatory hand on an industry still in development. They have said their extensive testing demonstrates commitment to safety.\nUber is currently looking for a head of global safety operations who would “drive key strategic programs including Safety Experience and Building Trust,” according to a job posting on the company’s website. The search was posted before the Arizona fatality.\nThe Uber accident in Tempe, Arizona this week was the first death attributed to a self-driving car operating in autonomous mode. It has given ammunition to critics of the industry concerned that the lack of clear standards allows manufacturers to test faulty or partially developed technology on public streets.\nWell before Sunday’s fatal accident, industry executives had begun to confront questions about whether self-driving cars can be trusted. They have opened up about their testing methods without revealing secrets of system designs.\nPublic disclosure of self-driving car testing data is inconsistent and varies by state. California requires manufacturers to report instances when an autonomous vehicle system disengages. Arizona does not.\n“There is no question whatsoever that regulations are coming,” said Doug Mehl, a partner at A.T. Kearney’s automotive practice, based in Detroit. “But right now (automakers), software developers and service providers have an opportunity to shape what those regulations are going to look like.”\nAlphabet’s Waymo self-driving car unit has underscored in a report that its autonomous vehicles have now logged 5 million miles in real-world testing, and billions more in computer simulations. GM’s Cruise Automation unit has highlighted its decision to teach its driving system to navigate San Francisco’s congested streets.\nStill, Amnon Shashua, head of Intel Corp's INTC.O Mobileye vision systems unit, said the industry must do more. He has called for the self-driving vehicle industry to develop \"provable safety assurances\".\n“We need to prove that these vehicles are much, much safer than humans,” Shashua told Reuters. “How do you go and guarantee that you have a technology that the probability of a fatality per one hour of driving is 1,000 times better than a human? Nobody talks about that because nobody knows what to do.”\nNO FEDERAL STANDARDS\nMost self-driving vehicles are equipped with radar sensors and lidar sensors, which use lasers to detect obstacles around the vehicle. There are no federal standards yet specifying how such systems should work. Congress and federal regulators are still debating how tightly to regulate such systems.\n“There should be vision tests for the sensors they are using, both static and dynamic to see how well they work,” said Missy Cummings, a Duke University mechanical engineering professor.\nThe short video recorded by cameras in the Uber vehicle that struck pedestrian Elaine Herzberg while crossing a street in Tempe, Arizona late Sunday raises questions about whether the Uber system responded better than a human driver, experts said on Wednesday.\nUber has hired human operators to sit in driver’s seats of its autonomous vehicles to intervene if necessary. The video released by Tempe police shows a human operator behind the wheel of the Uber vehicle before the impact.\nThe operator is seen looking down, away from the street, in the seconds before the vehicle struck Herzberg. She was pushing a bicycle across the street from left lane into the right lane where the Uber vehicle was driving.\n“It seems it should have detected her,” Daniel Sperling, director of the Institute for Transportation Studies at University of California Davis told Reuters in an email after viewing the video. “It seems unlikely that a human driver would have done better. We do want AVs to do better than us and the potential exists.”\nAmericans were wary of autonomous vehicle technology even before Sunday’s fatality.\nAccording to a Reuters\/Ipsos opinion poll released in late January, two-thirds of Americans are uncomfortable about the idea of riding in self-driving cars.\n“The greater risk for the industry is that if people feel it is unsafe, or the testing is unsafe, you’ll see a real backlash against this technology,” said Matthew Johnson-Roberson, co-director of the University of Michigan Ford Center for Autonomous Vehicles.\nReporting by Alexandria Sage in San Francisco, Nick Carey and Paul Lienert in Detroit, Tina Bellon in New York. Editing by Joseph White, Kenneth Maxwell and David Gregorio\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a fatal accident involving an Uber self-driving car in Tempe, Arizona, which has raised concerns about the safety of self-driving vehicles. The accident occurred while a human operator was behind the wheel, and there are questions about whether the Uber system responded better than a human driver. The incident has also highlighted the lack of clear standards and regulations for self-driving car testing and safety. Various companies, including Uber, Alphabet's Waymo, and General Motors, are mentioned in the context of self-driving technology development and testing. Additionally, the news discusses the industry's efforts to shape future regulations and ensure the safety of autonomous vehicles.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-uber-trust-idUSKBN1GY15F","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":72,"Time":"Sat 28 Aug 2021 04.59 CEST","Title":"Toyota pauses Paralympics self-driving buses after one hits visually impaired athlete","Content":"Toyota has apologised for the “overconfidence” of a self-driving bus after it ran over a Paralympic judoka in the athletes’ village and said it would temporarily suspend the service.\nThe Japanese athlete, Aramitsu Kitazono, will be unable to compete in his 81kg category this weekend after being left with cuts and bruises following the impact with the “e-Palette” vehicle. His injuries prompted a personal intervention from the president of Toyota, Akio Toyoda.\n“We are very sorry that the accident has made many people worried,” Toyoda said. “A vehicle is stronger than a person, so I was obviously worried about how they were. It shows that autonomous vehicles are not yet realistic for normal roads.”\nAs part of its sponsorship of Tokyo 2020, Toyota has been showcasing its autonomous vehicles via a shuttle service, which has been running around the clock in the athletes’ village.\nOn Thursday, however, one of the buses pulled away from a T-junction and drove through a pedestrian crossing while Kitazono, a visually impaired athlete, was walking across.\nTokyo police said that vehicle operators had told them they “were aware that a person was there but thought [the person] would [realize that a bus was coming] and stop crossing the [street]”, according to the Asahi Shimbun newspaper.\nUber shuts down self-driving operation in Arizona after fatal crash\nRead more\nOn Friday, Toyoda apologised for the incident and said the use of the e-Palette buses would be temporarily suspended.\nToyoda visited the site of the accident and also reviewed video footage of the incident. He admitted “there had been overconfidence” in the technology of autonomous driving.\nKitozano’s coach, Yoshiyasu Endo, told the Japan Times that the 30-year-old athlete was recovering well. “He wanted to take good care of himself. We feel regret, but I think he is the most disappointed,” Endo said.\nToyota said in a statement: “We will conduct our own thorough investigation into the cause of the accident, and we will continue to coordinate closely with the Tokyo Organising Committee of the Olympic and Paralympic Games to ensure that we prevent any incidents from occurring in the future.”","Relevant":true,"Overview":"Here is the JSON format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Title\": \"Toyota pauses Paralympics self-driving buses after one hits visually impaired athlete\",\n    \"Description\": \"Toyota has apologised for the “overconfidence” of a self-driving bus after it ran over a Paralympic judoka in the athletes’ village and said it would temporarily suspend the service. The Japanese athlete, Aramitsu Kitazono, will be unable to compete in his 81kg category this weekend after being left with cuts and bruises following the impact with the “e-Palette” vehicle. His injuries prompted a personal intervention from the president of Toyota, Akio Toyoda.\",\n    \"Details\": \"On Thursday, one of the autonomous buses drove through a pedestrian crossing while Kitazono, a visually impaired athlete, was walking across. Vehicle operators claimed they were aware that a person was there but thought the person would realize that a bus was coming and stop crossing the street. Toyota's president, Akio Toyoda, apologized for the incident and admitted overconfidence in autonomous driving technology. The use of the e-Palette buses has been temporarily suspended, and Toyota will conduct an investigation into the cause of the accident.\"\n  }\n}\nThe news is indeed about an autonomous driving-related incident involving a self-driving bus hitting a visually impaired Paralympic athlete. The specific information regarding the incident has been extracted and provided in the JSON response.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2021\/aug\/28\/toyota-pauses-paralympics-self-driving-buses-after-one-hits-visually-impaired-athlete","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":74,"Time":"Tue 12 May 2015 03.51 CEST","Title":"Google acknowledges its self-driving cars had 11 minor accidents","Content":"Google Inc revealed Monday that its self-driving cars had been in 11 minor traffic accidents since it began experimenting with the technology six years ago.\nThe company released the number after Associated Press reported that Google had notified the California Department of Motor Vehicles of three collisions involving its self-driving cars since September, when reporting all accidents became a legal requirement of permits for the tests on public roads.\nThe director of Google’s self-driving car project wrote in a web post that all 11 accidents were minor – “light damage, no injuries” – and happened over 1.7m miles (2.8m km) of tests, including nearly one million miles in self-driving mode.\n“Not once was the self-driving car the cause of the accident,” Chris Urmson wrote.\n“Cause” is a key word: like Delphi Automotive, a parts supplier which suffered an accident in October with one of its two test cars, Google said it was not at fault.\nDelphi sent AP an accident report showing its car was hit, but Google has not made public any records, so both enthusiasts and critics of the emerging technology have only the company’s word on what happened. The motor vehicles department said it could not release details from accident reports.\nThe lack of transparency troubles critics who want the public to be able to monitor the rollout of a technology that even its developers acknowledge remains imperfect.\nJohn Simpson, privacy project director of the non-profit Consumer Watchdog, said Google’s ultimate goal was a car without a steering wheel or pedals. This could prevent a person from taking over if a car loses control, making it “even more important that the details of any accidents be made public so people know what the heck’s going on”.\nDelphi’s accident report showed the front of its 2014 Audi SQ5 had been moderately damaged when it was broadsided by another car while waiting to make a left turn. The car was not in self-driving mode, a company spokeswoman, Kristen Kinley, said.\nFive other companies with testing permits told AP they had had no accidents. In all, 48 cars are licensed to test on state roads.\nThat left Google, which has outfitted 23 Lexus SUVs with driverless technology. It acknowledged “a handful of minor fender-benders, light damage, no injuries, so far caused by human error and inattention”.\nOn Monday Urmson posted a more complete accounting online, going back to the program’s origins in 2009.\nThe Google cars have been rear-ended seven times, often when stopped “but also on the freeway”, he wrote. In other collisions, the cars were sideswiped or “hit by a car rolling through a stop sign”. Eight of the 11 collisions were on city streets.\nHe also described instances in which Google’s cars avoided hitting other cars or cyclists as they drove on streets near the company’s Silicon Valley headquarters.\nNevada, Michigan and Florida have passed laws welcoming tests of self-driving cars on to their roads. Their regulators said they were not aware of any reports.\nCalifornia’s regulators provided the total – four accidents since September – but would not comment about their nature or severity, citing a longstanding state law making collision reports confidential.\nSome details, however, were revealed by a person familiar with these reports: two accidents happened while the cars were in self-driving mode. In the other two, the person required to be behind the wheel was in control.\nAll four happened when the test car was moving at speeds of less than 10mph (16km\/h), said the person, who spoke on condition of anonymity for lack of authorization to discuss the reports publicly.\nA chief selling point for self-driving cars is safety.\nTheir cameras, radar and laser sensors provide a far more detailed understanding of their surroundings than humans have. Reaction times should be faster. Cars could be programmed to adjust if they sense a crash coming – move a few feet, tighten seat belts, honk the horn or flash lights at a distracted driver.\nThe priority so far is not avoiding fender-benders, but teaching them to avoid causing a serious accident that could set back acceptance of the technology for years, said Raj Rajkumar, a pioneer of the technology with Carnegie Mellon University.\nThe national rate for reported “property-damage-only crashes” is about 0.3 for every 100,000 miles (160,000km) driven, according to the national highway traffic safety administration.\nGoogle’s 11 accidents over 1.7m miles would work out to 0.6 for each 100,000 miles but, as company officials said, as many as five million minor accidents are not reported to authorities each year so it is hard to gauge how typical this is.\n“Even when our software and sensors can detect a sticky situation and take action earlier and faster than an alert human driver,” Urmson wrote. “Sometimes we won’t be able to overcome the realities of speed and distance. Sometimes we’ll get hit just waiting for a light to change.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Google has acknowledged that its self-driving cars were involved in 11 minor traffic accidents over the past six years. The accidents resulted in light damage with no injuries and occurred during 1.7 million miles of testing, including nearly one million miles in self-driving mode. Google emphasized that the self-driving cars were not the cause of any of these accidents, and they were primarily rear-ended, sideswiped, or hit by cars rolling through stop signs. The company also mentioned that the accidents were often caused by human error and inattention. There is a lack of transparency as Google has not made public any records of these accidents, which has raised concerns among critics who advocate for greater transparency in the development of self-driving technology.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2015\/may\/12\/google-acknowledges-its-self-driving-cars-had-11-minor-accidents","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":79,"Time":"February 17, 2023","Title":"Tesla recalls 362,000 U.S. vehicles over Full Self-Driving software","Content":"Companies\nTesla Inc\nFollow\nWASHINGTON, Feb 16 (Reuters) - Tesla Inc (TSLA.O) will recall more than 362,000 U.S. vehicles to update its Full Self-Driving (FSD) Beta software after U.S. regulators said on Thursday the driver assistance system did not adequately adhere to traffic safety laws and could cause crashes.\nThe National Highway Traffic Safety Administration (NHTSA) said the Tesla software allows a vehicle to \"exceed speed limits or travel through intersections in an unlawful or unpredictable manner increases the risk of a crash.\"\nAdvertisement · Scroll to continue\nTesla said it disagreed with NHTSA's analysis but ultimately acquiesced to the safety agency's January request. The electric vehicle manufacturer has previously clashed with NHTSA, which has a number of pending Tesla probes, over other safety issues and recall demands.\nTesla, whose shares closed down 5.7% at $202.04, will release an over-the-air (OTA) software update free of charge. The EV maker said it is not aware of any injuries or deaths that may be related to the recall issue. The automaker said it had 18 warranty claims.\nAdvertisement · Scroll to continue\nThe recall covers 2016-2023 Model S and Model X vehicles, 2017-2023 Model 3, and 2020-2023 Model Y vehicles equipped with FSD Beta software or pending installation.\nU.S. senators Ed Markey and Richard Blumenthal, both Democrats, said the recall was \"long overdue,\" adding, \"Tesla must finally stop overstating the real capabilities of its vehicles.\"\nThis is a fresh setback for Tesla's driver assistance system. Chief Executive Elon Musk has repeatedly missed his own targets to achieve self-driving capability, which he has touted as a potential cash cow.\nAdvertisement · Scroll to continue\nThe move is a rare intervention by federal regulators in a real-world testing program that the company sees as crucial to the development of cars that can drive themselves.\nThe recall comes less than two weeks before the company's March 1 investor day, during which Musk is expected to promote the EV maker's artificial intelligence capability and plans to expand its vehicle lineup.\nWhile Tesla's Autopilot feature assists with steering, accelerating and braking for other vehicles and pedestrians within its lane, the company says FSD is a more advanced system “designed to provide more active guidance and assisted driving” under active supervision of the driver.\nAdvertisement · Scroll to continue\nTesla reported having $2.9 billion in deferred revenue at the end of 2022 related to \"access to our FSD features, internet connectivity, free Supercharging programs and over-the-air software updates primarily on automotive sales.\"\nTesla could not be reached for comment, but Musk tweeted on Thursday that the word \"recall\" for an over-the-air software update is \"anachronistic and just flat wrong!\"\n[1\/4]A Tesla Model S charges at a Tesla supercharger station in Cabazon, California, U.S. May 18, 2016. REUTERS\/Sam Mircovich Acquire Licensing Rights\nTesla released FSD Beta to nearly all of its 400,000 FSD customers in the United States and Canada in the fourth quarter, when it recognized FSD revenue of $324 million. It said it expects to recognize nearly $1 billion of deferred revenue that remains over time as software updates are delivered.\nONGOING PROBES\nMusk has positioned FSD technology as one of several artificial intelligence initiatives at Tesla.\nLast May, in an interview with members of a Tesla owners club, Musk called full self driving \"essential\" for the company. \"It’s really the difference between Tesla being worth a lot of money or worth basically zero.\"\nNHTSA has an ongoing investigation it opened in 2021 into 830,000 Tesla vehicles with driver assistance system Autopilot over a string of crashes with parked emergency vehicles. NHTSA is reviewing whether Tesla vehicles adequately ensure drivers are paying attention. NHTSA said on Thursday despite the FSD recall its \"investigation into Tesla’s Autopilot and associated vehicle systems remains open and active.\"\nTesla said in \"certain rare circumstances ... the feature could potentially infringe upon local traffic laws or customs while executing certain driving maneuvers.\"\nPossible situations where the problem could occur include traveling or turning through certain intersections during a yellow traffic light and making a lane change out of certain turn-only lanes to continue traveling straight, NHTSA said.\nNHTSA said \"the system may respond insufficiently to changes in posted speed limits or not adequately account for the driver's adjustment of the vehicle's speed to exceed posted speed limits.\"\nLast year, Tesla recalled nearly 54,000 U.S. vehicles with FSD Beta software that may allow some models to conduct \"rolling stops\" and not come to a complete stop at some intersections, posing a safety risk, NHTSA said.\nTesla and NHTSA say FSD's advanced driving features do not make the cars autonomous and require drivers to pay attention.\nIn December, NHTSA opened two new special investigations into crashes involving Tesla vehicles, including an eight-vehicle crash in San Francisco on the Bay Bridge in which a driver reported the FSD feature had malfunctioned.\nSince 2016, NHTSA has opened more than three dozen investigations involving Tesla crashes where advanced driver assistance systems were suspected of use and 19 deaths were reported.\nReporting by David Shepardson in Washington Additional reporting by Joseph White in Detroit and Hyun Joo Jin in San Francisco Editing by Ben Klayman, Peter Henderson and Matthew Lewis\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Based on the provided news article, it is relevant to autonomous driving accidents. Here is the specific information extracted in JSON format:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Title\": \"Tesla recalls 362,000 U.S. vehicles over Full Self-Driving software\",\n    \"Date\": \"Feb 16\",\n    \"Summary\": \"Tesla Inc will recall more than 362,000 U.S. vehicles to update its Full Self-Driving (FSD) Beta software after U.S. regulators said the driver assistance system did not adequately adhere to traffic safety laws and could cause crashes. The National Highway Traffic Safety Administration (NHTSA) said the Tesla software allows a vehicle to 'exceed speed limits or travel through intersections in an unlawful or unpredictable manner increases the risk of a crash.'\",\n    \"Affected_Vehicles\": [\n      \"2016-2023 Model S and Model X vehicles\",\n      \"2017-2023 Model 3\",\n      \"2020-2023 Model Y vehicles equipped with FSD Beta software or pending installation.\"\n    ],\n    \"Reaction\": \"U.S. senators Ed Markey and Richard Blumenthal, both Democrats, said the recall was 'long overdue,' adding, 'Tesla must finally stop overstating the real capabilities of its vehicles.'\",\n    \"Additional_Information\": [\n      \"Tesla's Autopilot feature assists with steering, accelerating and braking for other vehicles and pedestrians within its lane, but FSD is a more advanced system 'designed to provide more active guidance and assisted driving' under active supervision of the driver.\",\n      \"Tesla reported having $2.9 billion in deferred revenue at the end of 2022 related to 'access to our FSD features, internet connectivity, free Supercharging programs and over-the-air software updates primarily on automotive sales.'\",\n      \"NHTSA has ongoing investigations into Tesla vehicles, including one related to Autopilot and another regarding FSD technology. The system was found to potentially infringe upon local traffic laws and customs in certain circumstances.\"\n    ]\n  }\n}\nThis JSON format provides detailed information about the news article, including the title, date, summary, affected vehicles, reactions from U.S. senators, and additional information about Tesla's self-driving technology and ongoing investigations by the National Highway Traffic Safety Administration (NHTSA).","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/tesla-recalls-362000-us-vehicles-over-full-self-driving-software-2023-02-16\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":83,"Time":"Published 4:47 AM GMT+2, March 20, 2018","Title":"Self-driving vehicle strikes and kills pedestrian in Arizona","Content":"A self-driving Uber SUV struck and killed a pedestrian in suburban Phoenix in the first death involving a fully autonomous test vehicle — a crash that could have far-reaching consequences for the new technology.\nThe fatality Sunday night in Tempe was the event many in the auto and technology industries were dreading but knew was inevitable.\nUber immediately suspended all road-testing of such autos in the Phoenix area, Pittsburgh, San Francisco and Toronto. The testing has been going on for months as automakers and technology companies like the ride-hailing service compete to be the first with cars that operate on their own.\nThe Volvo was in self-driving mode with a human backup driver at the wheel when it hit 49-year-old Elaine Herzberg as she was walking a bicycle outside the lines of a crosswalk, police said. She died at a hospital.\nUber CEO Dara Khosrowshahi expressed condolences on his Twitter account and said the company is working with local law enforcement on the investigation.\nThe National Transportation Safety Board, which makes recommendations for preventing crashes, and the National Highway Traffic Safety Administration, which can enact regulations, sent investigators.\nTempe police Sgt. Ronald Elcock said local authorities haven’t drawn any conclusions about who is at fault but urged people to use crosswalks. He told reporters at a news conference Monday the Uber vehicle was traveling around 40 mph when it hit Helzberg immediately as she stepped on to the street.\nNeither she nor the backup driver showed signs of impairment, he said.\nThe public’s image of the vehicles will be defined by stories like the crash in Tempe, said Bryant Walker Smith, a University of South Carolina law professor who studies self-driving vehicles.\nAlthough the Uber vehicle and its human backup could be at fault, it may turn out that there was nothing either could have done to stop the crash, he said.\nEither way, the fatality could hurt the technology’s image and lead to a push for more regulations at the state and federal levels, Smith said.\nAutonomous vehicles with laser, radar and camera sensors and sophisticated computers have been billed as the way to reduce the more than 40,000 traffic deaths a year in the U.S. alone. Ninety-four percent of crashes are caused by human error, the government says.\nAutonomous vehicles don’t drive drunk, don’t get sleepy and aren’t easily distracted. But they do have faults.\n“We should be concerned about automated driving,” Smith said. “We should be terrified about human driving.”\nIn 2016, the latest year available, more than 6,000 U.S. pedestrians were killed by vehicles.\nThe federal government has voluntary guidelines for companies that want to test autonomous vehicles, leaving much of the regulation up to states.\nMany states, including Michigan and Arizona, have taken a largely hands-off approach, hoping to gain jobs from the new technology, while California and others have taken a harder line.\nCalifornia is among states that require manufacturers to report any incidents during the testing phase. As of early March, the state’s motor vehicle agency had received 59 such reports.\nArizona Gov. Doug Ducey used light regulations to entice Uber to the state after the company had a shaky rollout of test cars in San Francisco. Arizona has no reporting requirements.\nHundreds of vehicles with automated driving systems have been on Arizona’s roads.\nDucey’s office expressed sympathy for Herzberg’s family and said safety is the top priority.\nThe crash in Arizona isn’t the first involving an Uber autonomous test vehicle. In March 2017, an Uber SUV flipped onto its side, also in Tempe. No serious injuries were reported, and the driver of the other car was cited for a violation.\nHerzberg’s death is the first involving an autonomous test vehicle but not the first in a car with some self-driving features. The driver of a Tesla Model S was killed in 2016 when his car, operating on its Autopilot system, crashed into a tractor-trailer in Florida.\nThe NTSB said that driver inattention was to blame but that design limitations with the system played a major role in the crash.\nThe U.S. Transportation Department is considering further voluntary guidelines that it says would help foster innovation. Proposals also are pending in Congress, including one that would stop states from regulating autonomous vehicles, Smith said.\nPeter Kurdock, director of regulatory affairs for Advocates for Highway and Auto Safety in Washington, said the group sent a letter Monday to Transportation Secretary Elaine Chao saying it is concerned about a lack of action and oversight by the department as autonomous vehicles are developed. That letter was planned before the crash.\nKurdock said the deadly crash should serve as a “startling reminder” to members of Congress that they need to “think through all the issues to put together the best bill they can to hopefully prevent more of these tragedies from occurring.”\n___\nKrisher reported from Detroit, Fonseca reported from Flagstaff, Arizona. Susan Montoya Bryan in Albuquerque, New Mexico, and Jacques Billeaud in Phoenix contributed to this story.\n___\nThis story has been corrected to show that federal investigators found Tesla’s Autopilot system was a factor in the deadly Florida crash.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Self-driving vehicle strikes and kills pedestrian in Arizona\",\n\"Description\": \"A self-driving Uber SUV struck and killed a pedestrian in suburban Phoenix in the first death involving a fully autonomous test vehicle.\",\n\"Location\": \"Tempe, Arizona\",\n\"Victim\": \"Elaine Herzberg, 49 years old\",\n\"Details\": \"The Volvo was in self-driving mode with a human backup driver at the wheel when it hit Elaine Herzberg as she was walking a bicycle outside the lines of a crosswalk. She died at a hospital. Uber CEO Dara Khosrowshahi expressed condolences and the company is working with local law enforcement on the investigation.\",\n\"Investigation\": \"The National Transportation Safety Board and the National Highway Traffic Safety Administration sent investigators. Local authorities haven’t drawn any conclusions about who is at fault yet but urged people to use crosswalks. Neither the victim nor the backup driver showed signs of impairment.\",\n\"Impact\": \"The fatality could have far-reaching consequences for autonomous vehicle technology, potentially leading to more regulations at the state and federal levels.\",\n\"Statistics\": \"In 2016, over 6,000 U.S. pedestrians were killed by vehicles. The government states that 94 percent of crashes are caused by human error.\",\n\"Regulations\": \"The federal government has voluntary guidelines for autonomous vehicle testing, leaving regulation decisions up to states. Some states like California have stricter reporting requirements, while others like Arizona have lighter regulations.\"\n}\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/fc96c4ec74fc4f68bfda092304bea82e","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":82,"Time":"Mon 26 Sep 2016 11.19 CEST","Title":"Google's self-driving car in broadside collision after other car jumps red light","Content":"One of Google’s self-driving cars was involved in one of the worst autonomous vehicle accident yet, when a driver ran a red light and collided with the passenger side door of the modified Lexus SUV.\nThe driver of the vehicle passed through a red light as one of Google’s fleet of autonomous Lexus SUVs passed through a green light on Friday afternoon. The collision, which occurred at the intersection between El Camino Rea and Phyllis Ave in Mountain View, California, caused the Google car’s airbags to be deployed, and caved in its front and rear right-side doors.\nMountain View police said that no injuries were reported, but the Google car had to be towed away on a flatbed truck.\n@davidnield permission granted. pic.twitter.com\/WyKFCJE2Vz\n— Ron van Zuylen (@grommet) September 25, 2016\nGoogle’s autonomous vehicles are no stranger to accidents, suffering multiple impacts with various vehicles. Mostly they’ve been rear-ended when either driving slowly or stationary at a junction, suffering minor damage. The side impact in this most recent accident is one of only a few that have caused major damage to the expensive test vehicles.\nGoogle said that the car was in self-driving mode with a person sitting at the steering wheel. The Google car hit the brakes automatically on seeing the other car crossing the red light, followed by the human behind the wheel doing the same, but it wasn’t enough to prevent the collision.\nJames Allen, who happened upon the crash, told KBCW: “I’ve never seen one in an accident and I see at least 30 to 40 a day. They’re very good cars, that’s why I was so shocked.”\nIs the mass sharing of driverless cars about to reshape our suburbs?\nRead more\nA Google spokesperson told 9to5Google: “Our light was green for at least six seconds before our car entered the intersection. Thousands of crashes happen everyday on US roads, and red-light running is the leading cause of urban crashes in the US. Human error plays a role in 94% of these crashes, which is why we’re developing fully self-driving technology to make our roads safer.”\nGoogle’s fleet of autonomous cars have covered over 2m miles and has been involved in around two dozen accidents, with only one – a collision with a bus – being the fault of the self-driving car.\nCrash involving self-driving Google car injures three employees","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"One of Google’s self-driving cars was involved in one of the worst autonomous vehicle accidents when a driver ran a red light and collided with the passenger side door of the modified Lexus SUV. The collision occurred at the intersection between El Camino Rea and Phyllis Ave in Mountain View, California. Mountain View police reported no injuries, but the Google car's airbags were deployed, and its front and rear right-side doors were caved in. The Google car was in self-driving mode with a person sitting at the steering wheel. Both the autonomous system and the human driver hit the brakes when they saw the other car crossing the red light, but they couldn't prevent the collision.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/sep\/26\/google-self-driving-car-in-broadside-collision-after-other-car-jumps-red-light-lexus-suv","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":86,"Time":"6 November 2019","Title":"Uber in fatal crash had safety flaws say US investigators","Content":"Uber in fatal crash had safety flaws say US investigators\nPublished\n6 November 2019\nShare\nIMAGE SOURCE,\nREUTERS\nAn Uber self-driving test vehicle that hit and killed a woman in 2018 had software problems, according to US safety investigators.\nElaine Herzberg, 49, was hit by the car as she was crossing a road in Tempe, Arizona.\nThe US National Transportation Safety Board (NTSB) found the car failed to identify her properly as a pedestrian.\nThe detailed findings raised a series of safety issues but did not determine the probable cause of the accident.\nThe safety board is expected to make that finding when it meets on 19 November.\nThe findings, released on Tuesday, may also be used to help shape recommendations for the developing autonomous driving industry. The sector has come under sharp scrutiny in the wake of the accident.\nThe fatal crash occurred in March 2018, and involved a Volvo XC90 that Uber had been using to test its self-driving technology.\nJust before the crash, Ms Herzberg had been walking with a bicycle across a poorly lit stretch of a multi-lane road.\nUber 'not criminally liable' for car death\nUber crash driver was 'watching TV'\nUber warned about self-drive car crashes\nAccording to the NTSB, Uber's test vehicle failed to correctly identify the bicycle as an imminent collision until just before impact.\nBy that time, it was too late for the vehicle to avoid the crash.\n\"The system design did not include a consideration for jaywalking pedestrians,\" the NTSB said.\nMedia caption,\nUber dashcam footage shows moment before fatal impact\nThe report also said there were 37 crashes of Uber vehicles in self-driving mode between September 2016 and March 2018.\nIn a statement, Uber said: \"We deeply value the thoroughness of the NTSB's investigation into the crash and look forward to reviewing their recommendations\".\nEarlier this year, prosecutors ruled that the company is not criminally liable for the death of Ms Herzberg.\nHowever, the car's back-up driver could still face criminal charges.\nDash-cam footage released by police after the incident appeared to show the vehicle's back-up driver, Rafaela Vasquez, taking her eyes off the road moments before the accident.\nFurther records from the streaming service Hulu suggested that Ms Vasquez had been streaming a television talent show on a phone at the time of the crash.\nFollowing the crash, authorities in Arizona suspended Uber's ability to test self-driving cars on the state's public roads.\nThe company subsequently pulled the plug on its autonomous car operation in Arizona, although the company later resumed tests in Pennsylvania\nMore on this story\nUber 'not criminally liable' for car death\nPublished\n6 March 2019\nUber crash driver was 'watching TV'\nPublished\n22 June 2018\nWho is to blame for 'self-driving car' deaths?\nPublished\n22 May 2018","Relevant":true,"Overview":"Here is the JSON format analysis of the news you provided:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Uber in fatal crash had safety flaws say US investigators\",\n        \"Date_published\": \"6 November 2019\",\n        \"Location\": \"Tempe, Arizona\",\n        \"Victim\": \"Elaine Herzberg, 49\",\n        \"Incident_description\": \"An Uber self-driving test vehicle that hit and killed a woman in 2018 had software problems, according to US safety investigators. Elaine Herzberg, 49, was hit by the car as she was crossing a road in Tempe, Arizona.\",\n        \"Investigation findings\": \"The US National Transportation Safety Board (NTSB) found the car failed to identify her properly as a pedestrian. The detailed findings raised a series of safety issues but did not determine the probable cause of the accident.\",\n        \"Expected findings\": \"The safety board is expected to make that finding when it meets on 19 November.\",\n        \"Impact on autonomous driving industry\": \"The findings, released on Tuesday, may also be used to help shape recommendations for the developing autonomous driving industry. The sector has come under sharp scrutiny in the wake of the accident.\",\n        \"Vehicle involved\": \"The fatal crash occurred in March 2018 and involved a Volvo XC90 that Uber had been using to test its self-driving technology.\",\n        \"Contributing factors\": \"According to the NTSB, Uber's test vehicle failed to correctly identify the bicycle as an imminent collision until just before impact. By that time, it was too late for the vehicle to avoid the crash. 'The system design did not include a consideration for jaywalking pedestrians,' the NTSB said.\",\n        \"Previous incidents\": \"The report also said there were 37 crashes of Uber vehicles in self-driving mode between September 2016 and March 2018.\",\n        \"Uber's response\": \"In a statement, Uber said: 'We deeply value the thoroughness of the NTSB's investigation into the crash and look forward to reviewing their recommendations'.\",\n        \"Legal implications\": \"Earlier this year, prosecutors ruled that the company is not criminally liable for the death of Ms. Herzberg. However, the car's back-up driver could still face criminal charges.\",\n        \"Suspension of testing\": \"Following the crash, authorities in Arizona suspended Uber's ability to test self-driving cars on the state's public roads. The company subsequently pulled the plug on its autonomous car operation in Arizona, although the company later resumed tests in Pennsylvania.\"\n    }\n}\nThe news is indeed relevant to autonomous driving accidents, and the specific information about the case has been extracted as detailed as possible in the JSON format.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/business-50312340","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":87,"Time":null,"Title":"China's Geely calls for caution in self-driving tech after fatal collision","Content":"By Norihiko Shirouzu\nBEIJING (Reuters) - The Chinese owner of Swedish car brand Volvo said on Wednesday that the autonomous driving industry should move forward prudently in the wake of a deadly accident in the United States this month that has rekindled a debate about safety.\nLi Shufu, the head of Zhejiang Geely Holding Group Co Ltd [GEELY.UL] said at an event in Beijing it was important for automakers and governments to look at regulations and what sort of standards were needed to keep people safe.\n“One accident can kill the whole industry... So we must move with caution,” Li, who took a $9 billion stake in German carmaker Daimler AG last month, said at a Volvo-sponsored event on connected and automated cars.\nHe said excessive haste would backfire, and that a single accident could delay progress by as much as a year.\n“The accident reminds us that no matter how fast we develop, safety is the number one priority,” he said. “If safety cannot be guaranteed, no automated cars (should be allowed).”\nThe comments come as the global industry reels from a fatal accident in the Tempe, Arizona, involving a self-driving car from Uber Technologies Inc [UBER.UL], the first death attributed to a self-driving car operating in autonomous mode.\nUber has suspended North American tests of its autonomous vehicles after the fatal collision on March 18. Its partner, chipmaker Nvidia Corp, said this week it has also suspended self-driving tests across the globe.\nThe accident has ramped up pressure on the industry to prove its software and sensors are safe.\nVolvo Cars Chief Executive Hakan Samuelsson said accidents were unfortunate and the industry had to move carefully, but that “saying no to technology would be the wrong answer” given human error was also a big factor in many collisions.\n“Technology is still the most promising answer to reducing (accidents),” he said at the event in Beijing.\nIn China, official support for driverless technology remains strong. The government gave its consent to internet firm Baidu Inc to test self-driving cars on city streets just days after the fatal accident in the United States.\nBaidu, which has a self-driving project dubbed Apollo, is leading China’s push in driverless technology, with the government keen to keep up with global rivals such as Tesla Inc and Waymo, the self-driving arm of Google parent Alphabet Inc.\nReporting by Norihiko Shirouzu; Writing by Adam Jourdan; Editing by Christopher Cushing\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"China's Geely, the owner of Swedish car brand Volvo, called for caution in the self-driving tech industry following a fatal collision in the United States. Li Shufu, the head of Zhejiang Geely Holding Group, emphasized the importance of regulations and safety standards. He stated that excessive haste in autonomous driving development could lead to setbacks, and safety must be the top priority. The news discusses a recent fatal accident involving a self-driving car from Uber in Tempe, Arizona, which has put pressure on the industry to ensure the safety of its technology. While Volvo's CEO, Hakan Samuelsson, acknowledged the need for caution, he also stressed that technology remains a promising solution to reducing accidents. The Chinese government's support for driverless technology is mentioned, including its approval for Baidu Inc to test self-driving cars shortly after the fatal accident in the United States.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-volvo-idUSKBN1H4123","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":88,"Time":"Published 9:05 AM EST, Fri March 8, 2019","Title":"Uber death leaves questions about self-driving car liability unanswered","Content":"Washington, DC\nCNN Business\n — \nA year after the first fatality caused by a fully self-driving car, questions about liability in the event of a death involving the cars are still completely up in the air.\nOfficials announced earlier this week that Uber won’t face criminal charges in the death of a pedestrian struck and killed by one of its self-driving cars nearly a year ago in Tempe, Arizona.\nThe Yavapai County Attorney’s Office said it conducted a thorough review of the evidence and determined there was no basis for criminal liability against Uber. It did not detail how the decision was made and has declined to answer any questions on the case.\nThe pedestrian was walking a bicycle across a road at night. Uber’s self-driving software system initially classified the pedestrian as an unknown object, then as a vehicle, then as a bicycle, but never braked.\nHowever, the Uber employee who was behind the wheel of the SUV could still face criminal charges. Companies working on self-driving cars, such as Uber, have test drivers who are supposed to intervene if the car fails to act properly.\nIt’s not clear why the prosecutor’s office made its decision — if it is because Uber bore no responsibility or that there was no law under which the company could be charged. That means a first chance for the industry, legal experts and society to discuss this issue may have been lost. And it’s a discussion that needs to be had, because it’s a hard problem to solve: Experts say regulators will have to juggle holding businesses accountable for wrongdoing and concerns that they will be slowing down a technology expected to save lives.\nTesla's promise of 'full-self-driving' angers autonomous vehicle experts\n“If we start throwing Uber executives in jail and having large criminal penalties, it’ll be harder to test and more lives will be lost because of human errors,” said Ed Walters, who teaches courses on robotics law at Georgetown University and Cornell University. “We should limit criminal liability for the most extraordinary circumstances.”\nWalters said criminal liability should begin when there’s evidence that a company knew people’s lives would be lost, and ignored that information. An example is if a company released a product while knowing its sensors failed to identify people on bicycles or couldn’t see pedestrians in the rain.\nCriminal liability would be different in a case involving a self-driving car that had a test driver behind the wheel, Walters said. The company would have to know that its self-driving technology would fail in a way that threatened lives, and that the test driver would be unable to fix it.\nIt’s generally difficult to hold companies criminally liable, according to Bryant Walker Smith, a professor at the University of South Carolina School of Law who studies autonomous vehicles. The circumstances would have to be egregious and reckless: for example, a company that paid bonuses to test drivers whose vehicles had the most close calls with pedestrians. A company that falsified information on the quality of its cars, misleading the government, would also have a higher chance of being criminally liable.\nUltimately, Walters said, he could foresee fully autonomous vehicles and human driven vehicles being held to different standards. An autonomous vehicle might make mistakes a human would never make, but it also would never make some mistakes that humans routinely do. Self-driving cars could instead be judged against each other. The rationale behind this, according to Walters, is that self-driving cars could be much safer than human drivers generally, even if in a handful of situations they’re worse.\nUber wants to test self-driving cars again after fatality\nThe Uber incident had unusual circumstances, so it’s not a model case for setting law and policy around fully self-driving cars, according to Walters. Video showed the test driver was distracted and not watching the road.\nAnd test drivers aren’t expected to be present when companies eventually deploy self-driving cars on a large scale.\nArizona is a rarity among US states because it does not have a vehicular manslaughter statute. Prosecutors would have to rely on a general manslaughter charge, which in Arizona typically involves DUIs, speeding, aggressive driving or racing, none of which match the circumstances of the Uber fatality.\n“The attorney’s office seems to have concluded either that Uber’s conduct was not bad enough to warrant criminal charges or that there just isn’t a specific crime to fit Uber’s conduct, however bad,” Smith said.\nUber isn’t facing criminal charges but has dealt with adverse effects on its business as a result of the incident. The company temporarily halted its vehicle test program and shut down its self-driving operations in Arizona, laying off 300 workers.\nIn December, Uber resumed testing vehicles in autonomous mode. But the fatality increased public skepticism of self-driving vehicles, and slowed efforts to pass autonomous vehicle legislation on Capitol Hill.\nUber also faced the risk of a civil lawsuit. Edwards said this is why the company moved quickly to settle with the family of Elaine Herzberg, the pedestrian who was killed, shortly after her death. Uber and Herzberg’s family settled fewer than two weeks after the crash. Details of the agreement weren’t revealed.\nUltimately it may take another tragedy before there is any clarity on criminal liability and self-driving cars.\n“The larger issue that not many people seem to be talking about is, should we require self-driving cars to be perfect before they can be sold?” said Todd Benoff, an attorney at the law firm Alston & Bird who is focused on autonomous vehicles. “No one has ever even suggested we do the same thing with human drivers.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Uber death leaves questions about self-driving car liability unanswered\",\n\"Location\": \"Tempe, Arizona\",\n\"Date\": \"A year after the first fatality caused by a fully self-driving car\",\n\"Details\": \"A year after the first fatality caused by a fully self-driving car, questions about liability in the event of a death involving the cars are still completely up in the air. Officials announced earlier this week that Uber won’t face criminal charges in the death of a pedestrian struck and killed by one of its self-driving cars nearly a year ago in Tempe, Arizona. The Yavapai County Attorney’s Office said it conducted a thorough review of the evidence and determined there was no basis for criminal liability against Uber. It did not detail how the decision was made and has declined to answer any questions on the case. The pedestrian was walking a bicycle across a road at night. Uber’s self-driving software system initially classified the pedestrian as an unknown object, then as a vehicle, then as a bicycle, but never braked. However, the Uber employee who was behind the wheel of the SUV could still face criminal charges. Companies working on self-driving cars, such as Uber, have test drivers who are supposed to intervene if the car fails to act properly. It’s not clear why the prosecutor’s office made its decision — if it is because Uber bore no responsibility or that there was no law under which the company could be charged. That means a first chance for the industry, legal experts and society to discuss this issue may have been lost. And it’s a discussion that needs to be had because it’s a hard problem to solve...\",\n\"Conclusion\": \"Ultimately it may take another tragedy before there is any clarity on criminal liability and self-driving cars.\"\n}\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2019\/03\/08\/tech\/uber-arizona-death-criminal\/index.html","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":85,"Time":"Wed 24 Jan 2018 12.55 CET","Title":"GM sued by motorcyclist in first lawsuit to involve autonomous vehicle","Content":"General Motors is facing one of the first lawsuits to involve an autonomous vehicle, after a collision between its Cruise self-driving car and a motorbike in California.\nMotorcyclist Oscar Nilsson is suing GM stating that the Chevrolet Bolt, which was operating in autonomous mode with a backup driver behind the wheel, “suddenly veered back into Nilsson’s lane, striking Nilsson and knocking him to the ground”.\nThe accident happened on 7 December in heavy traffic in the Hayes Valley district of San Francisco, with the GM vehicle reportedly travelling at 12mph and the motorcycle 17mph. The accident report filed with the California disputes Nilsson’s claims.\nThe report states that the autonomous vehicle was driving in the centre of three one-way lanes. It attempted to move into the left lane when the available space was truncated by braking traffic, causing the GM car to abort its move and return to the centre of the middle lane.\n“As the Cruise AV was re-centring itself in the lane, a motorcycle that that had just lane-split between two vehicles in the centre and right lanes moved into the centre lane, glanced the side of the Cruise AV, wobbled and fell over,” says the report. “The motorcyclist was determined to be at fault for attempting to overtake and pass another vehicle on the right under conditions that did not permit that movement in safety.”\nNilsson was able to walk his motorbike to the side of the road before exchanging information with the GM car’s occupants and receiving medical care for shoulder pain.\nNilsson’s lawyer Sergei Lemberg disputed the report. He told Mercury News: “I don’t know what a police officer can tell, after the fact.\n“I don’t know that it’s fair to blame this act on the completely innocent person who’s just driving down the road and gets hit.”\nGM said that it intentionally tests its self-driving vehicles in challenging conditions. A GM spokesperson told Jalopnik: “Safety is our primary focus when it comes to developing and testing our self-driving technology. In this matter, the San Francisco Police Department collision report determined that the motorcyclist merged into our lane before it was safe to do so.”\nThe company has been testing its autonomous vehicles in San Francisco since August, allowing employees to hail the cars with a backup driver behind the wheel. Its cars were involved in six no-fault collisions in September.\nWho is to blame when technology is in the driving seat?\nAs more self-driving cars take to the streets, lawsuits such as Nilsson’s are inevitable, but it highlights the dilemma of autonomous vehicles: who pays in the case of an accident.\nTesting vehicles are clearly operated by the company developing the technology, in this case GM, but once the vehicles are bought and owned by individuals the picture of who is to blame becomes much more murky. Should the driver, who may or may not be in direct control of the vehicle at the time, have responsibility for all operations of the vehicle?\nIt’s likely that the vehicle’s collection of data as part of its operation that could be used to prove fault, but whether the companies operating them will want to set a precedent and give up that data remains to be seen.\nProfessor Bryant Walker Smith, from the University of South Carolina’s School of Law, told Mercury News that he expects companies to settle suits quickly if they believe their technology is at fault, but fight fiercely should human operators of other vehicles be at fault.\nGM is not the only traditional automotive manufacturer racing to challenge the likes of Alphabet’s Waymo and Uber with self-driving technology. A collision involving a car from Ford’s autonomous vehicle firm Argo recently sent two people to hospital.\nWho’s driving? Autonomous cars may be entering the most dangerous phase","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"GM sued by motorcyclist in first lawsuit to involve autonomous vehicle\",\n\"Description\": \"General Motors is facing one of the first lawsuits to involve an autonomous vehicle, after a collision between its Cruise self-driving car and a motorbike in California. Motorcyclist Oscar Nilsson is suing GM stating that the Chevrolet Bolt, which was operating in autonomous mode with a backup driver behind the wheel, 'suddenly veered back into Nilsson’s lane, striking Nilsson and knocking him to the ground.' The accident happened on 7 December in heavy traffic in the Hayes Valley district of San Francisco, with the GM vehicle reportedly traveling at 12mph and the motorcycle 17mph. The accident report filed with the California disputes Nilsson’s claims.\"\n}\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/jan\/24\/general-motors-sued-motorcyclist-first-lawsuit-involve-autonomous-vehicle","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":90,"Time":null,"Title":"Daughter of Uber autonomous vehicle victim retains lawyer","Content":"By David Schwartz\nPHOENIX, Ariz (Reuters) - The daughter of the woman killed by an Uber [UBER.UL] self-driving vehicle in Arizona has retained a personal injury lawyer, underlying the potential high stakes of the first fatality caused by an autonomous vehicle.\nFILE PHOTO: U.S. National Transportation Safety Board (NTSB) investigators examine a self-driving Uber vehicle involved in a fatal accident in Tempe, Arizona, U.S., March 20, 2018. A women was struck and killed by the vehicle on March 18, 2018. National Transportation Safety Board\/Handout via REUTERS\nThe law firm of Bellah Perez in Glendale, Arizona, said in a statement on Thursday it was representing the daughter of Elaine Herzberg, who died on Sunday night after being hit by the Uber self-driving SUV in the Phoenix suburb of Tempe.\n“As the first pedestrian death involving an Uber autonomous vehicle, the incident has sparked a national debate about the safety of self-driving cars, exposing the technology’s blind spots and raising questions of liability,” the law firm said.\nThe firm did not immediately return phone calls seeking more information.\nFall-out from the accident could stall the development and testing of self-driving vehicles, which are designed to perform far better than human drivers and sharply reduce the number of motor vehicle fatalities that occur each year.\nThe fatality also presents an unprecedented liability challenge because self-driving vehicles, which are still in the development stage, involve a complex system of hardware and software often made by outside suppliers. The specifics of how Uber’s technology operates are not known.\nOn Thursday a group of 10 Democratic senators cited the fatality in Tempe in a letter sent to Uber and 59 other companies including Ford Motor Co F.N and Tesla Inc TSLA.O highlighting how the use of forced arbitration clauses to settle customer disputes would prevent victims of accidents involving self-driving vehicles from exercising their legal rights.\nThe senators, led by Richard Blumenthal, a member of the Senate Commerce, Science and Transportation Committee, noted that Uber includes forced arbitration in its standard terms of service with users.\n“Had the victim been a passenger ... the victim’s family could have been denied recourse through the courts,” the letter read. The senators asked the companies to commit to not using the arbitration clauses in contracts related to self-driving cars.\n“When injury or death does occur, a forced arbitration clause would prevent consumers from exercising their fundamental legal rights as Americans,” the senators wrote.\nMany companies include forced arbitration clauses in contracts with customers, requiring that any disputes be settled in binding arbitration and barring customers from suing in a court of law. Arbitration rulings, generally, cannot be appealed.\nACCIDENT LIABILITY?\nHerzberg, 49, who was homeless, was jay-walking across a divided four-lane road with her bicycle when she was struck while in the far right-hand lane. A video taken from a dash-mounted camera inside the vehicle that was released by Tempe police on Wednesday showed the SUV traveling along a dark street when suddenly the headlights illuminate Herzberg in front of the SUV.\nShe later died from her injuries.\nOther footage showed the human driver who was behind the wheel mostly looking down and not at the road in the seconds before the accident.\nUber, like many other companies testing self-driving vehicles, has a human driver in each vehicle as a monitor and to act as a backup if necessary.\nFew details of the incident have emerged amid the investigations by police and federal safety regulators. Police have said the vehicle, a Volvo XC90 which was operating in autonomous mode, was traveling at about 40 mile per hour (65 km per hour) at the time of the collision and did not appear to brake.\nPolice have said that following their probe, they will submit the case to the Maricopa County Attorney’s office, which will determine if there is any basis for a case for potential criminal prosecution.\nOne key question for investigators will be how the vehicle’s technology failed to notice the pedestrian crossing the street in front of it, despite the darkness. Self-driving cars typically use a combination of sensors, including radar and light-sensing Lidar, to identify objects, including potential obstacles coming into range.\nIn company presentations, Uber has stated its self-driving technology includes sensors that provide a 360-degree view around the vehicle.\nWriting by Alexandria Sage; Additional reporting by David Shepardson in Washington, D.C.; Editing by Leslie Adler\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about the daughter of the woman killed by an Uber self-driving vehicle in Arizona retaining a personal injury lawyer. The incident marked the first fatality caused by an autonomous vehicle. The law firm of Bellah Perez in Glendale, Arizona, is representing the daughter of Elaine Herzberg, who died after being hit by the Uber self-driving SUV. The incident has sparked a national debate about the safety of self-driving cars and raised questions of liability. It could potentially impact the development and testing of self-driving vehicles. The details of the accident, including how Uber's technology operates, are not fully known. The incident has also led to discussions about forced arbitration clauses in contracts related to self-driving cars, preventing victims from exercising their legal rights. The victim, Elaine Herzberg, was jaywalking with her bicycle when she was struck. The incident is under investigation by police and federal safety regulators, and it will be reviewed for potential criminal prosecution. One key question is how the vehicle's technology failed to notice the pedestrian crossing the street despite using sensors like radar and Lidar for object detection.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-uber-idUSKBN1GY30I","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":81,"Time":"Published 5:20 PM GMT+2, January 31, 2023","Title":"Justice Department seeks Tesla automated driving documents","Content":"The U.S. Justice Department has requested documents from Tesla related to its Autopilot and “Full Self-Driving” features, according to a regulatory filing.\n“To our knowledge no government agency in any ongoing investigation has concluded that any wrongdoing occurred,” Tesla said in the filing Tuesday with the Securities and Exchange Commission.\nThe Austin, Texas, electric vehicle maker cautioned that if the government decides to pursue an enforcement action, it could possibly have a material adverse impact on its business.\nMessages were left Tuesday seeking comment from the Justice Department and from Tesla, which has disbanded its media relations department.\nOTHER NEWS\nA truck crash in Illinois kills 5, seriously injures 5 and forces an evacuation due to ammonia leak\nMan deliberately drives into a home and crashes into a police station in New Jersey, police say\nFatal 2021 jet crash was likely caused by parking brake left on during takeoff, NTSB says\nTesla Inc. is already facing multiple investigations by the National Highway Traffic Safety Administration for problems with its two driver-assist systems, Autopilot and “Full Self-Driving.”\nDespite their names, Tesla still says on its website that the cars can’t drive themselves. Teslas using “Full Self-Driving” can navigate roads in many cases, but experts say the system can make mistakes, which even CEO Elon Musk acknowledges. “We’re not saying it’s quite ready to have no one behind the wheel,” CEO Musk said in October.\nMichael Brooks, executive director of the nonprofit Center for Auto Safety, said the Justice Department could be looking at safety issues with the systems, or it could be investigating Tesla claims that the cars can drive themselves when they can’t.\n“When you get the car, it really can’t do everything that’s been promised,” Brooks said. “Tesla is putting a vehicle out on the road that is unable to perform to the capabilities claimed. Yet we have drivers relying on those promises and esentially not paying attention to the drive because they think it is more capable than it is.”\nThe systems have been under investigation by NHTSA since June of 2016 when a driver using Autopilot was killed after his Tesla went under a tractor-trailer crossing its path in Florida. A separate probe into Teslas that were using Autopilot when they crashed into emergency vehicles started in August 2021. At least 14 Teslas that have crashed into emergency vehicles while using the Autopilot system.\nIncluding the Florida crash, NHTSA has sent investigators to 35 Tesla crashes in which automated systems are suspected of being used. Nineteen people have died in those crashes, including two motorcyclists.\nThe agency also is investigating complaints that Teslas can brake suddenly for no reason.\n“Full Self-Driving” went on sale late in 2015, and Musk has used the name ever since. It currently costs $15,000 to activate the system. Tsl\nIn 2019 he promised a fleet of autonomous robotaxis by 2020, and he said in early 2022 that the cars would be autonomous that year.\nSince 2021, Tesla has been beta-testing “Full Self-Driving” using owners who haven’t been trained on the system but are actively monitored by the company. Tesla said this month that 400,000 owners are participating.\nAuto safety advocates and government investigators have long criticized Tesla’s monitoring system as inadequate. Three years ago the National Transportation Safety Board listed poor monitoring as a contributing factor in a 2018 fatal Tesla crash in California. The board recommended a better system, but said Tesla has not responded.\nNHTSA has noted in documents that numerous Tesla crashes have occurred in which drivers had their hands on the wheel but still weren’t paying attention. The agency has said that Autopilot is being used in areas where its capabilities are limited and that many drivers aren’t taking action to avoid crashes despite warnings from the vehicle.\nIn addition, the National Transportation Safety Board determined in 2020 that Tesla’s system to make sure drivers are paying attention is not adequate, and it should be limited to areas where it can safely operate.\nTesla shares were up just under 4% in Tuesday morning trading.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about the U.S. Justice Department seeking documents from Tesla related to its Autopilot and 'Full Self-Driving' features. Tesla is facing multiple investigations by the National Highway Traffic Safety Administration (NHTSA) for problems with its driver-assist systems, Autopilot and 'Full Self-Driving.' These investigations include cases where Teslas using Autopilot crashed into emergency vehicles, leading to fatalities. The Justice Department's inquiry could be related to safety issues with these systems or Tesla's claims about the cars' capabilities. Additionally, there have been concerns raised about Tesla's monitoring system, which has been criticized as inadequate by auto safety advocates and government investigators.\"\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/elon-musk-technology-accidents-business-f4f59a574cbe3fe2214c573ab81d47d3","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":94,"Time":"April 20, 2021","Title":"Insight: Tesla drives on Autopilot through a regulatory grey zone","Content":"BERKELEY, California, April 20 (Reuters) - The fatal crash of a Tesla with no one apparently behind the wheel has cast a new light on the safety of semi-autonomous vehicles and the nebulous U.S. regulatory terrain they navigate.\nPolice in Harris County, Texas, said a Tesla Model S smashed into a tree on Saturday at high speed after failing to negotiate a bend and burst into flames, killing one occupant found in the front passenger seat and the owner in the back seat.\nTesla Chief Executive Elon Musk tweeted on Monday that preliminary data downloaded by Tesla indicate the vehicle was not operating on Autopilot, and was not part of the automaker's \"Full Self-Driving\" (FSD) system.\nTesla's Autopilot and FSD, as well as the growing number of similar semi-autonomous driving functions in cars made by other automakers, present a challenge to officials responsible for motor vehicle and highway safety.\nAdvertisement · Scroll to continue\nU.S. federal road safety authority, the National Highway Traffic Safety Administration (NHTSA), has yet to issue specific regulations or performance standards for semi-autonomous systems such as Autopilot, or fully autonomous vehicles (AVs).\nThere are no NHTSA rules requiring carmakers to ensure systems are used as intended or to stop drivers misusing them. The only significant federal limitation is that vehicles have steering wheels and human controls required under federal rules.\nAdvertisement · Scroll to continue\nWith no performance or technical standards, systems such as Autopilot inhabit a regulatory grey area.\nThe Texas crash follows a string of crashes involving Tesla cars being driven on Autopilot, its partially automated driving system which performs a range of functions such as helping drivers stay in lanes and steer on highways.\nTesla has also rolled out what it describes as a \"beta\" version of its FSD system to about 2,000 customers since October, effectively allowing them to test how well it works on public roads.\nAdvertisement · Scroll to continue\nHarris County police are now seeking a search warrant for the Tesla data and said witnesses told them the victims intended to test the car’s automated driving.\nNO REQUIREMENTS\nAdding to the regulatory confusion is that traditionally NHTSA regulates vehicle safety while departments of motor vehicles (DMVs) in individual states oversee drivers.\nWhen it comes to semi-autonomous functions, it may not be apparent whether the onboard computer or the driver are controlling the car, or if the supervision is shared, says the U.S. National Transportation Safety Board (NTSB).\nAdvertisement · Scroll to continue\nCalifornia has introduced AV regulations but they only apply to cars equipped with technology that can perform the dynamic driving task without the active physical control or monitoring of a human operator, the state's DMV told Reuters.\nIt said Tesla's full self-driving system does not yet meet those standards and is considered a type of Advance Driver Assistance System that it does not regulate.\nThat leaves Tesla's Autopilot and its FSD system operating in regulatory limbo in California as the automaker rolls out new versions of the systems for its customers to test.\nNHTSA, the federal body responsible for vehicle safety, said this week it has opened 28 investigations into crashes of Tesla vehicles, 24 of which remain active, and at least four, including the fatal Texas accident, occurred since March.\nNHTSA has repeatedly argued that its broad authority to demand automakers recall any vehicle that poses an unreasonable safety risk is sufficient to address driver assistance systems.\nSo far, NHTSA has not taken any enforcement action against Tesla's advanced driving systems.\nWhite House spokeswoman Jen Psaki said NHTSA is \"actively engaged with Tesla and local law enforcement\" on the Texas crash.\nThe NTSB, a U.S. government agency charged with investigating road accidents, has criticized NHTSA's hands-off approach to regulating cars with self-driving features and AVs.\n\"NHTSA refuses to take action for vehicles termed as having partial, or lower level, automation, and continues to wait for higher levels of automation before requiring that AV systems meet minimum national standards,\" NTSB Chairman Robert Sumwalt wrote in a Feb. 1 letter to NHTSA.\n\"Because NHTSA has put in place no requirements, manufacturers can operate and test vehicles virtually anywhere, even if the location exceeds the AV control systems limitations,\" the letter said.\nREVIEWING REGULATIONS\nNHTSA told Reuters that with a new administration in place, it was reviewing regulations around AVs and welcomed the NTSB's input as it advanced policies on automated driving systems.\nIt said the most advanced vehicle technologies on sale required a fully attentive human driver at all times.\n\"Abusing these technologies is, at a minimum, distracted driving. Every State in the nation holds the driver responsible for the safe operation of the vehicle,\" NHTSA told Reuters.\nNTSB also says NHTSA does not have any method to verify whether carmakers have adopted system safeguards. For example, there are no federal regulations requiring drivers to touch the steering wheel within a specific time frame.\n\"NHTSA is drafting rules on autonomous vehicles, but it has been slow to regulate semi-autonomous vehicles,\" said Bryant Walker Smith, a law professor at the University of South Carolina. \"There is a growing awareness that they deserve more scrutiny priority and regulatory action.\"\nNew York has a law requiring drivers to keep at least one hand on the wheel at all times but no other states have legislation that could prevent the use of semi-autonomous cars.\nWhen it comes to AVs, 35 states have enacted legislation or state governors have signed executive orders covering AVs, according to the National Conference of State Legislatures.\nSuch rules allow companies such as Alphabet's (GOOGL.O) Google and General Motors (GM.N), among others, to test their Waymo and Cruise vehicles on public roads.\nBut regulations differ by state.\nAV regulations in Texas state that vehicles must comply with NHTSA processes, though there are no such federal regulations. The Texas Department of Public Safety, the regulator charged with overseeing AVs, did not respond to a request for comment.\nArizona's transport department requires companies to submit regular filings to verify, among other things, that vehicles can operate safely if the autonomous technology fails.\nWhile most automakers offer vehicles with various forms of assisted driving, there are no fully autonomous vehicles for sale to customers in the United States.\nRED FLAGS\nConcerns about the safety of autonomous driving technology, however, have been mounting in recent years and Tesla has warned about its limitations.\nIn February 2020, Tesla's director of autonomous driving technology, Andrej Karpathy, identified a challenge for its Autopilot system: how to recognize when a parked police car's emergency flashing lights are turned on.\n\"This is an example of a new task we would like to know about,\" Karpathy said at a conference during a talk about Tesla's effort to deliver FSD technology.\nIn just over a year since then, Tesla vehicles crashed into police cars parked on roads on four separate occasions and since 2016 at least three Tesla vehicles operating on Autopilot have been in fatal crashes.\nU.S. safety regulators, police and local government have investigated all four incidents, officials told Reuters.\nAt least three of the cars were on Autopilot, police said. In one of the cases, a doctor was watching a movie on a phone when his vehicle rammed into a police trooper in North Carolina.\nTesla did not immediately respond to a request for comment.\nAccidents and investigations have not slowed Musk's drive to promote Tesla cars as capable of driving themselves.\nIn a recent Tweet, Musk said Tesla is \"almost ready with FSD Beta V9.0. Step change improvement is massive, especially for weird corner cases & bad weather. Pure vision, no radar.\"\nTesla also says it has used 1 million cars on the road to collect image data and improve Autopilot, using machine learning and artificial intelligence.\nTesla's Karpathy said he has ridden in his Tesla for 20 minutes to get coffee in Palo Alto with no intervention.\n“It is not a perfect system but it is getting there,” he said in a “Robot Brains” podcast in March. “I definitely keep my hands on the wheel.”\nReporting by Hyunjoo Jin in Berkeley, California, David Shepardson in Washington and Tina Bellon in Austin, Texas; Editing by Joe White and David Clarke\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab\nTina Bellon\nThomson Reuters\nU.S.-based business and finance journalist covering Uber, Lyft, Tesla and larger auto and transportation trends with a focus on electrification, tech, safety and regulation. Data-driven reporter with a track record of uncovering corporate strategies through data analysis. Previous work includes breaking news on the U.S. legal team and covering politics and companies from the German bureau.","Relevant":true,"Overview":"Here is the analysis of the news in JSON format:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Title\": \"Tesla drives on Autopilot through a regulatory grey zone\",\n    \"Summary\": \"The news discusses a fatal crash involving a Tesla vehicle, highlighting concerns about the safety of semi-autonomous vehicles and the lack of clear regulations. The crash occurred in Harris County, Texas, where a Tesla Model S hit a tree at high speed, resulting in a fatality. The news also mentions that Tesla's CEO, Elon Musk, stated that preliminary data indicate the vehicle was not operating on Autopilot or part of the Full Self-Driving system.\",\n    \"Key_Details\": [\n      \"Fatal crash of a Tesla vehicle in Harris County, Texas, with one occupant found in the front passenger seat and the owner in the back seat.\",\n      \"Tesla CEO Elon Musk's statement regarding preliminary data on the vehicle's Autopilot usage.\",\n      \"Discussion of the regulatory challenges surrounding semi-autonomous and autonomous driving systems in the United States, including the lack of specific regulations from the National Highway Traffic Safety Administration (NHTSA).\",\n      \"Mention of NHTSA's investigations into Tesla vehicle crashes and concerns about the safety of autonomous driving technology.\",\n      \"Information about Tesla's efforts to develop its Full Self-Driving (FSD) system and its use of data collection to improve Autopilot.\"\n    ]\n  }\n}\nThe news is relevant to autonomous driving accidents as it discusses a fatal crash involving a Tesla vehicle and the regulatory challenges associated with semi-autonomous and autonomous driving systems in the United States. The specific information, as detailed as possible, has been extracted and presented in the JSON format.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/technology\/tesla-drives-autopilot-through-regulatory-grey-zone-2021-04-20\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":92,"Time":"10:00 PM GMT+2•May 23, 2016","Title":"We need more driverless car accidents","Content":"cameraImage Credits: arda savasciogullari\n(opens in a new window)\n\/ Shutterstock\n(opens in a new window)\nShawn DuBravac\nContributor\nShawn DuBravac is chief economist of the Consumer Technology Association and the author of Digital Destiny: How the New Age of Data Will Transform the Way We Live, Work, and Communicate.\nMore posts by this contributor\nAs technology lends you its ear, these technologies will determine what it hears\nCan the smartphone cure Zika?\nA driverless car was involved in a traffic accident on a California city street earlier this year. No one was hurt in the small fender bender, but the accident does signal we are making incredible leaps forward on the road toward driverless cars. It may sound counterintuitive, but this crash shows just how far autonomous technology has come in such a short time.\nThis wasn’t really an accident in the traditional sense — intentional software changes implemented just weeks earlier were likely a contributing factor. With any luck, we will continue to accelerate real-world experimentation and the possibility of more accidents to come.\nThe incident was partially caused by a subtle software update, implemented a few weeks prior in all of Google’s autonomous cars, that enabled them to “hug the rightmost side of the lane,” a common social norm that allows other drivers to pass on the left. According to the accident report, the Google autonomous vehicle was shifting within its lane to bypass an obstacle in its path when it made contact with a bus approaching from behind. The car was traveling slower than two miles per hour at the time of impact — the bus, about 15 mph.\nThe fact that Google was testing this new behavior to hug the right side of the lane shows that the technology has developed beyond simply following the rules of the road, but actually driving more “human like” — in line with the social elements of driving.\nAccidents like this are vital learning exercises. Google’s driverless vehicles cover more than 10,000 miles a day, in addition to the three million miles of computer-simulated driving taking place daily. But these real-life tests are crucial.\nThe only way to fully understand our world is to explore it by taking risks.\nDriverless cars won’t just change who or what is behind the wheel — they hold the potential to change where we drive, which in turn can change our commutes, vacation plans and how we connect with family and friends. Driverless cars are positioned to forever change the world in which we live… but before they can do that, they must fundamentally and fully understand that world. And the only way to fully understand our world is to explore it by taking risks.\nThe history of innovation is built upon pushing the frontier of what has been done before. From Lindbergh’s nonstop solo flight across the Atlantic in 1927 to Chuck Yeager breaking the sound barrier in 1947, we have long been demystifying the unknown in the name of progress. While shifting within the same lane may seem like a minute detail, autonomous car developers, like Lindbergh and Yeager before them, are fundamentally changing transportation.\nSubtle changes in technology are what will propel us forward. From air bags to automatic windshield wipers, on-board diagnostics to collision avoidance systems, our cars look vastly different than they did 30 years ago, thanks to incremental innovation over a long period of time.\nInnovation builds exponentially, but requires risk. The first seven U.S. astronauts were military test pilots. When they were selected in 1959, no one had come close to leaving the earth’s atmosphere. And at the time, no one knew whether any of these seven men would be successful.\nThese innovators pushed the frontier of what we knew to be possible. Back then we were pushing for discovery and exploration, but we were also in a space race with our Cold War rival, the Soviet Union. Countless Americans risked their lives, and some of the very first Americans in space gave the ultimate sacrifice, to win that race.\nMake no mistake: Today, we find ourselves in an even more important race. More than 1 million people are killed annually in traffic deaths globally, and almost all of these deaths are caused by human error. Autonomous vehicles hold the greatest promise to eradicating one of the most deadly forces on earth, but first we must push the limits.\nAutonomous vehicles need to learn to be aggressive to the right degree and in the right ways. Much of this can be determined within computer simulations, but some of it must be determined on open roads, where obstacles are dynamic and complex.\nThere will be accidents along the way. And from these incidents, we will gain treasure troves of intelligence that will push us further along the innovation curve.\nHopefully, we haven’t seen our last autonomous-vehicle accident; hopefully everyone will see them for what they are if\/when they occur: invaluable steps pushing us across uncrossed frontiers.","Relevant":true,"Overview":"Here is the JSON format output based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"A driverless car was involved in a traffic accident on a California city street earlier this year. No one was hurt in the small fender bender, but the accident does signal we are making incredible leaps forward on the road toward driverless cars. It may sound counterintuitive, but this crash shows just how far autonomous technology has come in such a short time. This wasn’t really an accident in the traditional sense — intentional software changes implemented just weeks earlier were likely a contributing factor. With any luck, we will continue to accelerate real-world experimentation and the possibility of more accidents to come. The incident was partially caused by a subtle software update, implemented a few weeks prior in all of Google’s autonomous cars, that enabled them to “hug the rightmost side of the lane,” a common social norm that allows other drivers to pass on the left. According to the accident report, the Google autonomous vehicle was shifting within its lane to bypass an obstacle in its path when it made contact with a bus approaching from behind. The car was traveling slower than two miles per hour at the time of impact — the bus, about 15 mph. The fact that Google was testing this new behavior to hug the right side of the lane shows that the technology has developed beyond simply following the rules of the road, but actually driving more “human like” — in line with the social elements of driving. Accidents like this are vital learning exercises. Google’s driverless vehicles cover more than 10,000 miles a day, in addition to the three million miles of computer-simulated driving taking place daily. But these real-life tests are crucial.\"\n}\nThe news is indeed about a driverless car accident, and the specific information about the case has been extracted as detailed as possible in the JSON format.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2016\/05\/23\/we-need-more-driverless-car-accidents\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":95,"Time":"16 August 2021","Title":"Tesla Autopilot: US opens official investigation into self-driving tech","Content":"Tesla Autopilot: US opens official investigation into self-driving tech\nPublished\n16 August 2021\nShare\nIMAGE SOURCE,\nREUTERS\nThe US federal agency in charge of road safety is opening an official investigation into Tesla's \"self-driving\" Autopilot system.\nThe National Highway Traffic Safety Administration (NHTSA) said it was acting following 11 Tesla crashes since 2018 involving emergency vehicles.\nIn some cases, the Tesla vehicles \"crashed directly into the vehicles of first responders\", it said.\nThe investigation will cover roughly 765,000 Tesla cars made since 2014.\nThat includes those in the Model Y, Model X, Model S and Model 3, the NHTSA said - the entire current range.\n'Control at all times'\nThe agency was primarily concerned with an apparent inability of Tesla vehicles to cope with vehicles stopped in the road - specifically emergency vehicles attending an incident.\nAmong the list of cases was one where a Tesla \"ploughed into the rear\" of a parked fire engine attending an accident, and another in which a parked police car was struck.\nThe NHTSA said it was opening its preliminary investigation into \"the technologies and methods used to monitor, assist, and enforce the driver's engagement\", while using Autopilot.\nIt said that in the 11 crashes that prompted its investigation, either Autopilot or a system called Traffic Aware Cruise Control had been active \"just prior\" to the collisions.\nTesla Autopilot 'tricked' to run without driver\nTesla crash driver 'was playing video game'\nThe assistive technology allows the car to automatically steer, accelerate and brake.\nBut it has come under fire for being misleading, as it does not automatically drive the car and drivers are required to maintain control and attention at all times.\nTesla has marketed the feature as an \"Autopilot\" and promised \"full self-driving\", which is now available to some users in a beta version.\nUsers have abused the system frequently in the past, with examples ranging from using their phones while the car drives unattended to switching car seats and leaving no driver at the wheel.\nMedia caption,\nThis driver received a ban after being caught in the passenger seat in England\nIn a statement, an NHTSA spokesperson said: \"No commercially available motor vehicles today are capable of driving themselves. Every available vehicle requires a human driver to be in control at all times.\"\nThe investigation's supporting documents do, however, note the challenging circumstances involved in many of the collisions.\n\"Most incidents took place after dark and the crash scenes encountered included scene control measures such as first responder vehicle lights, flares, an illuminated arrow board, and road cones,\" it reads.\nIt comes days ahead of an event to showcase the car company's software.\nChief executive Elon Musk had previously announced 19 August as \"Tesla AI Day\", which he said would showcase the progress of the firm's artificial intelligence systems - with a view to recruiting AI experts to the firm.\nTesla disbanded its public relations team in October 2020 and cannot be reached for comment.\nRelated Topics\nCar Technology\nTesla\nDriverless cars\nArtificial intelligence\nUnited States\nCar industry\nMore on this story\nTesla hit police car 'while using Autopilot'\nPublished\n30 May 2018\nTesla Autopilot 'tricked' to run without driver\nPublished\n23 April 2021\nTesla's 'Autopilot' misleading, Germany rules\nPublished\n15 July 2020\nTesla crash driver 'was playing video game'\nPublished\n26 February 2020","Relevant":true,"Overview":"Here is the JSON format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Tesla Autopilot: US opens official investigation into self-driving tech\",\n        \"Date_published\": \"16 August 2021\",\n        \"Summary\": \"The US federal agency in charge of road safety is opening an official investigation into Tesla's 'self-driving' Autopilot system. The National Highway Traffic Safety Administration (NHTSA) said it was acting following 11 Tesla crashes since 2018 involving emergency vehicles. In some cases, the Tesla vehicles 'crashed directly into the vehicles of first responders.' The investigation will cover roughly 765,000 Tesla cars made since 2014, including those in the Model Y, Model X, Model S, and Model 3. The agency is concerned about Tesla's ability to cope with vehicles stopped in the road, particularly emergency vehicles attending an incident. Autopilot or Traffic Aware Cruise Control had been active 'just prior' to the collisions in the 11 crashes that prompted the investigation.\"\n    }\n}\nThe news is indeed relevant to autonomous driving accidents, as it discusses an official investigation into Tesla's Autopilot system and mentions multiple Tesla crashes involving emergency vehicles. The specific information includes the news title, date of publication, and a summary of the key points related to the investigation and the incidents involving Tesla vehicles.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-58232137","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":96,"Time":"Updated 1:16 PM EST, Sun February 12, 2023","Title":"Super Bowl ad slams Tesla’s ‘Full Self-Driving’ tech","Content":"New York\nCNN\n — \nElectric carmaker Tesla will face a hit on Super Bowl Sunday, when an ad will play showing the alleged dangers of its Full Self-Driving technology.\nTesla confirms DOJ has requested documents on Autopilot, 'Full Self-Driving'\nThe commercial, which will be aired in Washington, DC, Austin, Tallahassee, Albany, Atlanta and Sacramento does not paint Tesla in the best light. The ad is part of a multimillion dollar advertising campaign by The Dawn Project. Its founder, Dan O’Dowd, is a California tech CEO who has dedicated millions of his own money (and a failed US Senate race) to the cause.\nThe ad cost $598,000, a Dawn Project spokesperson told CNN.\nIt shows a Tesla Model 3, which allegedly has the Full Self-Driving mode turned on, running over a child-sized dummy on a school crosswalk, and then a fake baby in a stroller, in a series of tests by the Dawn Project. In the ad, the car swerves into oncoming traffic, zooms past stopped school buses, and cruises through “do not enter” signs.\n“Tesla’s Full Self-Driving is endangering the public,” the ad said. “With deceptive marketing and woefully inept engineering.”\nThe Dawn Project says it wants to make computer-controlled systems safer for humanity, shooting its own videos as tests of Tesla’s alleged design flaws. In August, O’Dowd published a video showing a Tesla plowing into child-sized mannequins. Some Tesla fans posted their own videos in defense, using their own dummies or even their own children – YouTube has taken down several test videos involving actual children, citing safety risks.\nO’Dowd received a cease and desist letter from Tesla over the video, claiming he and the Dawn Project were “disparaging Tesla’s commercial interests and disseminating defamatory information to the public.”\nO’Dowd responded to the cease-and-desist with a 1,736-word post in which he pushed back at the suggestion his posts were defamatory, defended his tests and returned barbs from Musk and some Tesla supporters.\nO’Dowd, who sold software to the military, is undertaking a campaign of millions of dollars to ban Tesla’s Full Self-Driving feature. He is running national ads and posting online videos displaying the possible dangers of Musk’s technology. He also ran an unsuccessful one-issue campaign for the US Senate on the same message.\nThough officially in beta mode, Full Self-Driving is available to any user in North America who wants to purchase the $15,000 feature.\nTesla did not immediately respond to CNN’s request for comment. Tesla’s “Full Self-Driving” system is intended to someday work on city streets, but despite its wide rollout, is still officially in a developmental “beta” program. No car for sale on the market is yet able to drive itself.\nAutopilot is a suite of driver-assist features, while Full Self-Driving steers the car on city streets, but could also stop for traffic signals and make turns.\nTesla contends it is not aware of any ongoing government investigation that has concluded any wrongdoing occurred, and said its Autopilot, with its automated steering designed to keep a car within a lane, is safer than normal driving.\n“Tesla’s reckless deployment of Full Self-Driving software on public roads is a major threat to public safety. Elon Musk has released software that will run down children in school crosswalks, swerve into oncoming traffic and hit a baby in a stroller to all Tesla owners in North America,” O’Dowd said in a statement.\nConsumer Reports calls Ford's automated driving tech much better than Tesla's\nTesla said it “has received requests from the Department of Justice for documents related to Tesla’s Autopilot and FSD features” in a January 31 public filing.\nFederal investigators are looking into a Musk tweet about disabling driver alerts on Tesla’s “Full Self Driving” driver assist system, joining several other National Highway Traffic Safety Administration probes.\nOn December 31, Musk replied to a tweet by @WholeMarsBlog which said “users with more than 10,000 miles on FSD Beta should be given the option to turn off the steering wheel nag.”\n“Agreed, update coming in Jan,” Musk replied.\nThe National Highway Traffic Safety Administration announced last summer it was escalating its Tesla probe to an “engineering analysis,” a step toward seeking a recall. NHTSA first investigated Tesla’s driver-assist technology after reports Autopilot-engaged vehicles were crashing into emergency vehicles stopped at the scene of earlier crashes.\nO’Dowd is the founder and CEO of Green Hills Software. Some of Musk’s defenders claim O’Dowd has a conflict of interest as one of its customers is Intel-owned Mobileye, which makes a computer chip to run driver-assisted software, the Washington Post reported.\nO’Dowd told the Washington Post Mobileye is one of his hundreds of customers and that his main motivation is safety.\nCNN’s Matt McFarland contributed to this story.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous driving accidents. It discusses an advertisement criticizing Tesla's Full Self-Driving technology, which depicts a Tesla Model 3 allegedly with Full Self-Driving mode turned on, running over a child-sized dummy on a school crosswalk and a fake baby in a stroller in a series of tests. The ad also shows the car swerving into oncoming traffic, passing stopped school buses, and disregarding 'do not enter' signs. The Dawn Project, the organization behind the ad, claims that Tesla's Full Self-Driving is endangering the public with deceptive marketing and engineering issues. Additionally, the news mentions government investigations and safety concerns related to Tesla's Autopilot and Full Self-Driving features, including concerns about disabling driver alerts and previous incidents involving Autopilot-engaged vehicles. Overall, the news is relevant to autonomous driving accidents, and it provides specific information about the case.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2023\/02\/12\/business\/super-bowl-ad-tesla-full-self-driving\/index.html","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":93,"Time":null,"Title":"Uber self-driving test car involved in accident resulting in pedestrian death","Content":"An autonomous Uber test SUV driving in Tempe, Arizona was involved in a fatal collision last night, and the Uber vehicle was in autonomous mode at the time the accident occurred, according to Tempe Police (via Daisuke Wakabayashi).\nThe Uber SUV was driving northbound, and a woman crossed in its path outside of a crosswalk, at which point she was struck by the vehicle. She was later taken to hospital, as first reported by local Arizona ABC news affiliate ABC 15, where she died as a result of her injuries.\nAs is required for Uber’s autonomous test vehicles operating on public roads, the AV had a safety driver at the wheel. Safety drivers are in place in order to be able to take control of the autonomous test vehicle in case the self-driving system should fail or appear to be at risk of endangering others on the road. No other passengers were in the vehicle at the time of this accident.\nUber has paused all of its AV testing operations as a result of this accident, across all cities where it operates, including Pittsburgh, Toronto, San Francisco and Phoenix.\nUber provided the following statement regarding the incident to TechCrunch:\nOur hearts go out to the victim’s family. We are fully cooperating with local authorities in their investigation of this incident.\nThis is the first time an autonomous vehicle operating in self-driving mode has resulted in a human death, and that has huge implications for the future of AVs and their use on roads. It’s possible the safety driver involved could be held legally responsible, as their role is to ensure safe operation of the vehicle, but in many ways the outcome of this incident will define the path forward for AV regulation.\nUber CEO Dara Khosrowshahi expressed sympathy for the victim’s family on Twitter, and reiterated that Uber is working with local authorities to determine what exactly occurred.","Relevant":true,"Overview":"Here is the JSON format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Uber self-driving test car involved in accident resulting in pedestrian death\",\n        \"Location\": \"Tempe, Arizona\",\n        \"Description\": \"An autonomous Uber test SUV driving in Tempe, Arizona was involved in a fatal collision last night. The Uber vehicle was in autonomous mode at the time the accident occurred. The incident involved a woman crossing outside of a crosswalk and being struck by the autonomous Uber SUV. She was later taken to the hospital and unfortunately died due to her injuries. A safety driver was present in the vehicle, as required for Uber's autonomous test vehicles, but no other passengers were in the vehicle at the time of the accident. Uber has temporarily suspended all of its autonomous vehicle testing operations in multiple cities, including Pittsburgh, Toronto, San Francisco, and Phoenix, as a result of this accident. Uber is cooperating with local authorities in their investigation.\"\n    }\n}\nThe news is relevant to autonomous driving accidents, and the specific information about the case has been extracted as detailed as possible.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2018\/03\/19\/uber-self-driving-test-car-involved-in-accident-resulting-in-pedestrian-death\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":100,"Time":"Updated 7:14 PM EST, Tue November 19, 2019","Title":"Feds blame distracted test driver in Uber self-driving car death","Content":"Washington, DC\nCNN Business\n — \nA federal investigation into a fatal crash involving an Uber (UBER) self-driving car concluded the probable cause was a safety driver distracted by their phone. The National Transportation Safety Board investigation also determined that an inadequate safety culture at Uber (UBER) contributed to the March 2018 crash in Tempe, Arizona.\nInvestigators described it as an avoidable crash during a board meeting in Washington, DC, Tuesday. The investigators found that an alert vehicle operator would have had two to four seconds to detect and avoid pedestrian Elaine Herzberg, who was crossing a street when struck by Uber’s self-driving vehicle.\nUber death leaves questions about self-driving car liability unanswered\nThe test driver behind the wheel of Uber’s self-driving car was supposed to intervene if the autonomous driving software failed. But the driver was glancing away from the road during 34% of the fatal trip, including 23 glances in the final three minutes before the crash, according to the investigation. A camera in the car recorded the driver.\nThe NTSB found that Uber had no safety plan for its self-driving operation, or equivalent guiding document at the time of the crash. Uber’s self-driving software wasn’t designed to expect that pedestrians outside crosswalks may be crossing the street. The board also said Uber lacked appropriate oversight for vehicle operators.\n“This is about one fatality, but it’s about a lot more than that,” NTSB chairman Robert Sumwalt told reporters afterward. “We felt by focusing on this we could have much broader ramifications for improving safety.”\nThe NTSB made safety recommendations to the National Highway Traffic Safety Administration, the state of Arizona and the American Association of Motor Vehicle Administrators. It called on the NHTSA to require companies testing self-driving vehicles to submit a safety self-assessment report to the agency, and a plan for evaluating the reports.\nJennifer Homendy, an NTSB board member, described a “major failing” of the federal government to regulate the testing of self-driving vehicles. NHTSA has released guidelines for self-driving vehicles, which it calls a “Vision for Safety.”\n“They should rename it a ‘vision for lax safety,’” Homendy said. “In my opinion, they’ve put technology advancement before saving lives.”\nNHTSA said in a statement that it welcomed NTSB’s report and will carefully review it.\nNTSB also recommended that Arizona require self-driving companies to submit an application before testing autonomous vehicles.\nUber expressed remorse in a statement and that it would continue to improve the safety of its self-driving program.\nMost self-driving companies say this tech is crucial. Elon Musk disagrees.\n“Over the last 20 months, we have provided the NTSB with complete access to information about our technology and the developments we have made since the crash,” said Nat Beuse, who leads safety efforts in Uber’s self-driving division. “While we are proud of our progress, we will never lose sight of what brought us here or our responsibility to continue raising the bar on safety.”\nUber settled with the victim’s family shortly after the death.\nThe NTSB spoke highly of Uber’s willingness to contribute to the investigation. Sumwalt contrasted Uber’s approach with Tesla. NTSB ended Tesla’s participation in an investigation last year after Tesla released information about a crash before NTSB confirmed the information.\n“I appreciate the way Uber has been a good party,” he said. “I did notice that when I talked to their CEO, he did not hang up on me.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Feds blame distracted test driver in Uber self-driving car death\",\n\"Summary\": \"A federal investigation into a fatal crash involving an Uber self-driving car concluded the probable cause was a safety driver distracted by their phone. The National Transportation Safety Board investigation also determined that an inadequate safety culture at Uber contributed to the March 2018 crash in Tempe, Arizona. The investigators found that an alert vehicle operator would have had two to four seconds to detect and avoid pedestrian Elaine Herzberg, who was crossing a street when struck by Uber’s self-driving vehicle.\",\n\"Details\": [\n\"The news is about a fatal crash involving an Uber self-driving car in March 2018 in Tempe, Arizona.\",\n\"The probable cause of the crash was a safety driver distracted by their phone.\",\n\"The investigation also found that an inadequate safety culture at Uber contributed to the crash.\",\n\"An alert vehicle operator would have had two to four seconds to detect and avoid the pedestrian involved in the crash.\",\n\"The NTSB made safety recommendations to various organizations, including the National Highway Traffic Safety Administration, the state of Arizona, and the American Association of Motor Vehicle Administrators.\",\n\"Uber settled with the victim's family shortly after the death.\"\n]\n}\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2019\/11\/19\/tech\/uber-crash-ntsb\/index.html","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":99,"Time":"Updated 4:49 PM EST, Thu February 16, 2023","Title":"Tesla recalling nearly 363,000 vehicles equipped with ‘Full Self-Driving’","Content":"New York\nCNN\n — \nTesla is recalling all 363,000 US vehicles with its so-called “Full Self Driving” driver assist software due to safety risks, another blow to the feature that is central to the automaker’s business model.\n“Full self-driving,” as it currently stands, navigates local roads with steering, braking and acceleration, but requires a human driver prepared to take control at any moment, as the system makes judgment errors. The National Highway Traffic Safety Administration said that, based on its analysis, Tesla’s FSD feature “led to an unreasonable risk to motor vehicle safety based on insufficient adherence to traffic safety laws.” And it warned FSD could violate traffic laws at some intersections “before some drivers may intervene.”\n“The FSD Beta system may allow the vehicle to act unsafe around intersections, such as traveling straight through an intersection while in a turn-only lane, entering a stop sign-controlled intersection without coming to a complete stop, or proceeding into an intersection during a steady yellow traffic signal without due caution,” said the recall notice, posted on NHTSA’s website.\nTesla will attempt to fix the the FSD feature, which costs $15,000, through an over-the-air software update, the notice added.\nWhile Tesla CEO Elon Musk has not yet commented on the nature or scope of the problem, he tweeted that “the word “recall” for an over-the-air software update is anachronistic and just flat wrong!”\nBut NHTSA said in a statement that “manufacturers must initiate a recall for any repair, including a software update, that remedies an unreasonable risk to safety.” The federal agency said it will “continue to monitor the recall remedies for effectiveness.”\nThe notice said that the problems are present with all cars with the current version of the FSD software, which is available on all four Tesla models, the Model S, Model X, Model 3 and Model Y.\nIt also said Tesla has identified18 reports of incidents received between May 8, 2019, and September 12, 2022, that may be related to the conditions described above. It said Tesla is not aware of any injuries or deaths caused in those incidents. NHTSA itself has identified at least 273 crashes that involved one of Tesla’s driver assist systems.\nA Troubled Development\nFSD is considered key to the company’s basic business plan, given the premiums that drivers pay for the features, and it’s ability to attract buyers to chose Tesla cars in the first place. Tesla and Musk have repeatedly claimed that FSD, even in its current “beta” form, is safer than cars driven solely by humans. He told investors last month that Tesla has collected data from about 100 million miles of drivers using FSD outside of highways.\n“Our published data shows that improvement in safety,” he said. “It’s very clear. So we would not have released the FSD Beta if the safety statistics were not excellent.”\nBut other safety experts have questioned the validity of Tesla’s safety claims. There have been high-profile accidents of Tesla cars using FSD or its more rudimentary predecessor known as “Autopilot.” Some of those accidents included fatalities.\nNHTSA is also investigating that predecessor, Autopilot. That technology combines lane-keeping assist with adaptive cruise control to keep a car in a lane on a highway, as opposed to the promise of “full self-driving,” which Tesla says aims to one day be able to operate a vehicle without human supervision on a city street.\nWhile “this recall seeks to address a specific set of concerns identified by the agency,” NHTSA’s statement said that this recall does not address its earlier investigations. “Accordingly, the agency’s investigation into Tesla’s Autopilot and associated vehicle systems remains open and active.”\nLast month, Tesla disclosed in the company’s annual financial report that it “has received requests from the US Justice Department for documents related to Tesla’s Autopilot and FSD features.”\nMoving Deadlines\nMusk has repeatedly predicted that the company would soon build a truly self-driving car. But it has also repeatedly pushed back its own self-imposed deadlines. Tesla owners have filed a class-action lawsuit over the predictions and missed deadlines, which is still pending.\n“Mere failure to realize a long-term, aspirational goal is not fraud,” Tesla’s lawyers wrote in a November 28 court filing, asking that the suit be dismissed.\nTesla 'full self-driving' triggered an eight-car crash, a driver tells police\nMusk has said for years that the price of “full self-driving” would increase periodically as it develops and moves closer to regulatory approval. He tweeted in May 2020 that when “full self-driving” had that approval, the feature would “probably” be worth more than “$100,000.” But as recently as a July 2021 call with investors, Musk said it was “debatable” that the feature was worth the $10,000 Tesla was charging at that time.\nIn September, when CNN Business spoke with 13 people who have cars with the “full self-driving” beta, the overwhelming majority, 11 people, said they felt it wasn’t worth $15,000. And it’s been the subject of controversy for years, including a recent ad that played during the Super Bowl in a few markets.\nTesla does not appear close to regulatory approval for “full self-driving.” In August of 2022, the California DMV said that the name “full self-driving” is “a deceptive practice” and grounds for suspending or revoking Tesla’s license to sell vehicles in the state.\nTesla, which has disbanded its public relations staff and has not responded to press inquiries for several years, could not be reached for comment.\nCNN’s Matt McFarland contributed to this report.","Relevant":true,"Overview":"Here's the JSON format output based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Title\": \"Tesla recalling nearly 363,000 vehicles equipped with ‘Full Self-Driving’\",\n    \"Summary\": \"Tesla is recalling all 363,000 US vehicles with its so-called “Full Self Driving” driver assist software due to safety risks. The National Highway Traffic Safety Administration found that Tesla’s FSD feature led to an unreasonable risk to motor vehicle safety, citing issues with the system's behavior at intersections. Tesla plans to address the issue through an over-the-air software update. There have been 18 reports of incidents related to these conditions, but no reported injuries or deaths. The recall affects all four Tesla models. NHTSA has identified at least 273 crashes involving Tesla’s driver assist systems. The safety claims for FSD have been a subject of debate, and NHTSA is also investigating Tesla's predecessor, Autopilot. Tesla has faced legal scrutiny and criticism for missed deadlines related to self-driving promises.\"\n  }\n}\nThe news is indeed relevant to autonomous driving accidents, as it discusses Tesla's recall of vehicles equipped with \"Full Self-Driving\" software due to safety concerns and issues at intersections. The specific information includes the news title and a summary of the key details related to the recall and the safety concerns associated with Tesla's autonomous driving features.","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2023\/02\/16\/business\/tesla-fsd-recall\/index.html","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":97,"Time":"12:46 AM GMT+1•December 19, 2018","Title":"Uber to resume autonomous vehicle testing months after fatal accident","Content":"cameraImage Credits: Uber\nUber has been granted permission by the state of Pennsylvania to reinstate tests of its autonomous vehicles, as first reported by Reuters.\nA spokesperson for Uber confirmed to TechCrunch that the ride-hailing giant received a letter of authorization from the Pennsylvania Department of Transportation and clarified that the company has not yet resumed self-driving operations.\nUber halted testing of its self-driving cars following a fatal accident in Tempe, Arizona this March that left a pedestrian dead. An autonomous Uber SUV accompanied by a safety driver was driving northbound when it struck a woman, who was taken to the hospital where she later died as a result of her injuries.\nInvestigators later determined the driver, Rafaela Vasquez, had looked down at a phone 204 times during a 43-minute test drive, according to a 318-page police report released by the Tempe Police Department.\nIn the aftermath of the accident, Uber paused all of its AV testing operations in Pittsburgh, Toronto, San Francisco and Phoenix.\nMoving forward, Uber will test its self-driving cars more cautiously, per a recently released Uber safety report. The company will require that two employees are in the front seat of its cars at all times, that an automatic braking system is enabled and that its safety employees are more strictly monitored.\nUber, which first began developing its autonomous vehicle fleet in 2015 and initiated tests the following year, confidentially filed for an initial public offering two weeks ago. The company, currently valued at $72 billion, is expected to debut at a valuation as high as $120 billion early next year.","Relevant":true,"Overview":"Here is the JSON format analysis of the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Uber to resume autonomous vehicle testing months after fatal accident\",\n        \"Accident_details\": {\n            \"Location\": \"Tempe, Arizona\",\n            \"Date\": \"March\",\n            \"Fatalities\": {\n                \"Pedestrian\": {\n                    \"Status\": \"Died\",\n                    \"Cause\": \"Struck by autonomous Uber SUV\"\n                }\n            },\n            \"Driver_details\": {\n                \"Name\": \"Rafaela Vasquez\",\n                \"Actions\": {\n                    \"Number_of_times_looked_at_phone\": 204\n                }\n            }\n        },\n        \"Uber's Response\": {\n            \"Testing Suspension\": \"Uber halted testing of its self-driving cars\",\n            \"Safety Measures\": {\n                \"Two employees in front seat\": true,\n                \"Automatic braking system\": true,\n                \"Strict monitoring of safety employees\": true\n            }\n        },\n        \"Background\": {\n            \"Uber's Autonomous Vehicle Development\": {\n                \"Initiated\": 2015,\n                \"Testing started\": 2016\n            },\n            \"Company Valuation\": {\n                \"Current\": \"$72 billion\",\n                \"Expected\": \"$120 billion (early next year)\"\n            }\n        }\n    }\n}\nThe news is indeed about an autonomous driving accident involving Uber. The specific information about the case has been extracted and provided in JSON format as requested.\ncase_text_end=======","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2018\/12\/18\/uber-to-resume-autonomous-vehicle-testing-months-after-fatal-accident\/","query_name":"updated_file_autonomous_driving_accidents.xlsx"}
{"num":13,"Time":"May 12, 2023","Title":"More than 2 million Toyota users face risk of vehicle data leak in Japan","Content":"Companies\nToyota Motor Corp\nNippon Telegraph and Telephone Corp\nTOKYO, May 12 (Reuters) - Toyota Motor Corp (7203.T) said on Friday the vehicle data of 2.15 million users in Japan, or almost the entire customer base who signed up for its main cloud service platforms since 2012, had been publicly available for a decade due to human error.\nThe incident, which also affected customers of its luxury brand Lexus, comes as the world's biggest automaker by sales makes a push into vehicle connectivity and cloud-based data management which are seen as crucial to offering autonomous driving and other artificial intelligence-backed features.\nThe issue, which began in November 2013 and lasted until mid-April, stemmed from human error, leading to a cloud system being set to public instead of private, a Toyota spokesperson said. It could encompass details such as vehicle locations and identification numbers of vehicle devices, but there were no reports of malicious use, the company said.\n\"There was a lack of active detection mechanisms, and activities to detect the presence or absence of things that became public,\" the spokesperson said in response to why it took time to realise there had been an error.\nAdvertisement · Scroll to continue\nToyota said it would introduce a system to audit cloud settings, establish a system to continuously monitor settings, and thoroughly educate employees on data handling rules.\nAffected customers included those who signed up for the T-Connect service that provides a wide range of services including AI voice-enabled driving assistance, auto connection to call centres for vehicle management, and emergency support in such cases as a traffic accident or sudden illness.\nAdvertisement · Scroll to continue\nAlso affected were users of G-Link, a similar service for owners of Lexus vehicles.\nJapan's Personal Information Protection Commission has been informed about the incident, one of its officials said, but declined to provide further details, in line with its practice of not commenting on individual incidents.\nToyota said steps to block outside access to the data were taken after the issue was discovered and an investigation into all cloud environments managed by Toyota Connected Corp was being carried out.\nAdvertisement · Scroll to continue\nLarge leaks of personal data occasionally happen in Japan. In March, mobile carrier NTT DoCoMo (9432.T) said data of up to 5.29 million customers may have leaked via a company to which it outsourced work.\nThe incident adds to a raft of challenges facing Koji Sato who took over as Toyota CEO on April 1 from Akio Toyoda, grandson of the company's founder.\nSince he took office, Toyota has admitted safety test problems at its affiliate Daihatsu and received a shareholder proposal from a trio of European asset managers to improve disclosure of its lobbying on climate change.\nReporting by Kantaro Komiya; Editing by Clarence Fernandez\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab\nDaniel Leussink\nThomson Reuters\nDaniel Leussink is a correspondent in Japan. Most recently, he has been covering Japan’s automotive industry, chronicling how some of the world's biggest automakers navigate a transition to electric vehicles and unprecedented supply chain disruptions. Since joining Reuters in 2018, Leussink has also covered Japan’s economy, the Tokyo 2020 Olympics, COVID-19 and the Bank of Japan’s ultra-easy monetary policy experiment.\nKantaro Komiya\nThomson Reuters\nKantaro writes about everything from Japan's economic indicators to North Korea's missiles to global regulation on AI companies. His previous stories have been published in the Associated Press, Bloomberg, the Japan Times and Rest of World. A Tokyo native, Kantaro graduated from DePauw University in the United States and was the recipient of the Overseas Press Club Foundation 2020 Scholar Award.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Toyota Motor Corp has reported that the vehicle data of 2.15 million users in Japan, including customers of its luxury brand Lexus, had been publicly available for a decade due to human error. The incident started in November 2013 and lasted until mid-April, resulting from a cloud system being set to public instead of private. The exposed data could include details such as vehicle locations and identification numbers of vehicle devices. Toyota stated that there were no reports of malicious use. The company plans to introduce a system to audit cloud settings, establish continuous monitoring systems, and provide thorough employee education on data handling rules. The affected customers were those who signed up for services like T-Connect, which offers AI voice-enabled driving assistance and emergency support, and G-Link, a similar service for Lexus owners. Japan's Personal Information Protection Commission has been informed about the incident, and steps have been taken to block outside access to the data. An investigation into all cloud environments managed by Toyota Connected Corp is ongoing.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/toyota-flags-possible-leak-more-than-2-mln-users-vehicle-data-japan-2023-05-12\/","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":14,"Time":null,"Title":"Tesla recalls nearly 12,000 U.S. vehicles over software communication error","Content":"Companies\nTesla Inc\nWASHINGTON, Nov 2 (Reuters) - Tesla Inc is recalling nearly 12,000 U.S. vehicles sold since 2017 because a communication error may cause a false forward-collision warning or unexpected activation of the emergency brakes, the National Highway Traffic Safety Administration (NHTSA) said Tuesday.\nThe California automaker said the recall of 11,704 Model S, X, 3 and Y vehicles was prompted after a software update on Oct. 23 to vehicles in its limited early access version 10.3 Full-Self Driving (FSD) (Beta) population.\nFSD is an advanced driver assistance system that handles some driving tasks but Tesla says does not make vehicles autonomous.\nNHTSA said Tesla \"uninstalled FSD 10.3 after receiving reports of inadvertent activation of the automatic emergency braking system\" and then \"updated the software and released FSD version 10.3.1 to those vehicles affected.\"\nThe agency said it \"will continue its conversations with Tesla to ensure that any safety defect is promptly acknowledged and addressed.\"\nAdvertisement · Scroll to continue\nThe recall comes after NHTSA last month asked Tesla why it had not issued a recall to address software updates made to its Autopilot driver-assistance system to improve the vehicles' ability to detect emergency vehicles.\nTesla said the issue was prompted by a software communication disconnect between two onboard chips that prompted an issue that could produce \"negative object velocity detections when other vehicles are present.\"\nAdvertisement · Scroll to continue\nIf the automatic emergency braking system unexpectedly activates while driving, it could raise the risk of a rear-end\ncollision, Tesla said, but added it was not aware of any\ncrashes or injuries related to the issue.\nAfter the Oct. 24 reports, Tesla said canceled the FSD update on vehicles that had not installed it and disabled FCW and AEB on affected vehicles.\nThe same day, Tesla Chief Executive Elon Musk tweeted of FSD: \"Seeing some issues with 10.3, so rolling back to 10.2 temporarily. Please note, this is to be expected with beta software.\"\nAdvertisement · Scroll to continue\nOn Oct. 25, Tesla began deploying the over-the-air software update and re-enabled FCW and AEB features on vehicles with the update.\nTesla said as of Oct. 29, more than 99.8% of the vehicles - all but for 17 - had installed an update and no further action is necessary.\nNHTSA in August opened a formal safety probe into Tesla's Autopilot system in 765,000 U.S. vehicles after a series of crashes involving Tesla models and emergency vehicles.\nAdvertisement · Scroll to continue\nThe U.S. auto safety agency also asked Tesla in October about its \"Autosteer on City Streets” which the company also refers to as FSD first leased in October 2020, and raised concerns about limits on disclosure by drivers of safety issues.\nReporting by David Shepardson; Editing by Kirsten Donovan and Nick Macfie\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here is the JSON format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Tesla recalls nearly 12,000 U.S. vehicles over software communication error\",\n        \"Description\": \"Tesla Inc is recalling nearly 12,000 U.S. vehicles sold since 2017 because a communication error may cause a false forward-collision warning or unexpected activation of the emergency brakes, the National Highway Traffic Safety Administration (NHTSA) said Tuesday.\",\n        \"Details\": [\n            \"The recall affects 11,704 Model S, X, 3, and Y vehicles.\",\n            \"The issue was prompted by a software communication disconnect between two onboard chips, resulting in a risk of false forward-collision warnings and emergency brake activation.\",\n            \"The recall was prompted after a software update on Oct. 23 to vehicles in its limited early access version 10.3 Full-Self Driving (FSD) (Beta) population.\",\n            \"NHTSA requested Tesla to address software updates made to its Autopilot driver-assistance system to improve the vehicles' ability to detect emergency vehicles.\",\n            \"Tesla has uninstalled FSD 10.3 after reports of inadvertent activation of the automatic emergency braking system, followed by an updated software release (FSD version 10.3.1) for affected vehicles.\",\n            \"Tesla temporarily rolled back to version 10.2 due to issues with 10.3, according to a tweet by CEO Elon Musk.\",\n            \"As of Oct. 29, more than 99.8% of the vehicles had installed an update, and no further action is necessary.\",\n            \"NHTSA previously opened a formal safety probe into Tesla's Autopilot system in 765,000 U.S. vehicles after crashes involving Tesla models and emergency vehicles.\"\n        ]\n    }\n}\nThe news is indeed about a recall related to a software communication error in Tesla vehicles, which could lead to false forward-collision warnings and unexpected activation of emergency brakes. The JSON response includes specific information about the case as requested.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/tesla-recalling-nearly-12000-us-vehicles-over-software-communication-error-2021-11-02\/","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":24,"Time":"Published 6:27 AM EST, Wed January 26, 2022","Title":"UK could make carmakers, not drivers, responsible for self-driving errors","Content":"Britain should pass a law regulating self-driving vehicles and include sanctions for companies if anything goes wrong when their vehicles take over control from human drivers, two independent governmental bodies said on Wednesday in a report.\nThe report by the Law Commission of England and Wales and the Scottish Law Commission recommended introducing an “Automated Vehicles Act” and drawing “a clear distinction between features which just assist drivers, such as adaptive cruise control, and those that are self-driving.”\nGovernments around the world are wrestling with how to regulate self-driving vehicle technology and the thorny topic of accident liability.\nOnce a self-driving system is engaged, “the person in the driving seat would no longer be responsible for how the car drives,” the law commissions proposed in the report.\n“Instead, the company or body that obtained the authorization would face regulatory sanctions if anything goes wrong,” the commissions said.\nBritain’s law commissions review laws and make recommendations to government and parliament about suggested reforms. The Law Commission of England and Wales says about two thirds of its reform recommendations have been implemented.\nThe UK government wants to be at the forefront of rolling out autonomous driving technology and the transport ministry forecasts by 2035 around 40% of new UK cars could have self-driving capabilities, creating up to 38,000 new skilled jobs.\nLast year the government announced Britain would become the first country to regulate the use of self-driving vehicles at slow speeds on motorways.\nBut insurance companies have warned Britain’s goals could backfire unless automakers and regulators spell out the limitations of the technology available today.\n“Today’s report is a significant step, as it provides important legal recommendations and clarity for the safe deployment of vehicles with self-driving features onto the UK’s roads,” said Matthew Avery, chief research strategy officer at UK insurance group Thatcham Research, which consulted with the law commissions on the report.\nThe government’s Centre for Connected and Autonomous Vehicles (CCAV) asked for the commissions’ review of the legal framework for self-driving vehicles in 2018.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news discusses the recommendation of two independent governmental bodies, the Law Commission of England and Wales and the Scottish Law Commission, to pass an 'Automated Vehicles Act' in the UK. The act would make carmakers responsible for self-driving errors and introduce sanctions for companies if anything goes wrong when their vehicles take over control from human drivers. It also emphasizes that once a self-driving system is engaged, the person in the driving seat would no longer be responsible for how the car drives, and instead, the company or body that obtained authorization would face regulatory sanctions if anything goes wrong. The article also mentions the UK government's ambition to be a leader in autonomous driving technology and the concerns raised by insurance companies regarding the limitations of the technology.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2022\/01\/26\/business\/self-driving-cars-regulation\/index.html","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":33,"Time":"Published 2:52 PM EDT, Mon August 22, 2022","Title":"Tesla raises price of feature it calls ‘full self-driving’ to $15,000","Content":"Washington, DC\nCNN Business\n — \nTesla is raising the price of its controversial driver-assist feature it calls “full self-driving” to $15,000.\nTesla (TSLA) buyers can purchase it for $12,000 until Sept. 5. The $12,000 price dates to January. Following the latest price increase, the software will cost five times as much as when it was first introduced as a $3,000 add-on, even as it has developed slower than the automaker projected and faced criticism and government scrutiny.\nTesla CEO Elon Musk has said for years that the price of “full self-driving” would increase periodically as it develops and moves closer to regulatory approval. He tweeted in May 2020 that when “full self-driving” had that approval the feature would “probably” be worth more than “$100,000.” But Tesla does not appear close to regulatory approval for “full self-driving.” The California DMV said this month that the name “full self-driving” is “a deceptive practice” and grounds for suspending or revoking Tesla’s license to sell vehicles in the state.\nPeople are now testing Tesla's 'full self-driving' on real kids\nThe US National Highway Traffic Safety Administration is also investigating the more rudimentary predecessor of “full self-driving,” Autopilot. That technology combines lane-keeping assist with adaptive cruise control to keep a car in a lane on a highway, as opposed to the promise of “full self-driving,” which Tesla says aims to one day be able to operate a vehicle without human supervision on a city street. A recall of Autopilot software from vehicles featuring it is possible.\n“Full self-driving” is a critical part of Tesla’s vision of offering a robotaxi that the company claims can drive a million miles and make “probably something on the order of $30,000 per year,” Musk said in 2019. Tesla first said in 2016 that it felt its vehicles had the hardware for “full self-driving,” and only the software needed to be developed.\nTesla, like all companies developing autonomous driving features, has repeatedly missed self-imposed deadlines and been unable to match the hype that spurred billions of dollars of investment. Musk has said every year from 2015 to 2022 that self-driving Teslas were probably a year or two away.\nMusk said Sunday that his main goals for this year include a wide release of “full self-driving.”\n“Full self-driving,” as it currently stands, navigates local roads with steering, braking and acceleration, but requires a human driver prepared to take control at any moment, as the system makes judgment errors.\nA select group of Tesla owners have had early access to an unfinished version of “full self-driving” since October 2020. These testers have described being both impressed and concerned with the system’s performance. Some of their loved ones have not enjoyed the technology, which is rough around the edges. CNN Business’ own experience with the system in November 2021 on New York City streets found that the system would occasionally steer the vehicle into oncoming traffic. But many testers describe seeing gradual improvements.\nMusk said in April that more than 100,000 people are in the “beta” program, a reference to software that hasn’t been finalized.\nThe price increase coincides with a new software version of “full self-driving,” which the automaker says has many improvements including left turns and smoothness.\nTesla did not respond to a request for comment.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Tesla is raising the price of its controversial driver-assist feature known as 'full self-driving' from $12,000 to $15,000. This price increase marks a significant jump from the initial cost of $3,000 when it was first introduced as an add-on. Tesla CEO Elon Musk had previously stated that the price of 'full self-driving' would increase periodically as it develops and approaches regulatory approval, with a potential value of over $100,000. However, Tesla has not received regulatory approval for 'full self-driving' yet. The California DMV has criticized the name 'full self-driving' as deceptive, and the US National Highway Traffic Safety Administration is investigating Tesla's Autopilot technology. 'Full self-driving' currently requires a human driver to be ready to take control as it can make judgment errors while navigating local roads.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2022\/08\/22\/business\/tesla-fsd-price-increase\/index.html","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":18,"Time":"2:25 AM GMT+2•August 3, 2022","Title":"TuSimple addresses autonomous truck crash during Q2 earnings call","Content":"cameraImage Credits: TuSimple\nAutonomous trucking company TuSimple used its second-quarter earnings call to address an April crash during which one of the company’s autonomous trucks suddenly veered across the I-10 highway in Tuscon, slamming into a concrete barricade. \nThe crash first came to light via a YouTube video that showed footage of the crash along with a letter from the Federal Motor Carrier Safety Administration (FMCSA), dated May 26, alerting TuSimple to a “safety compliance investigation.” The accident was later reported on by The Wall Street Journal. \n“An error occurred when a test driver and safety engineer tried to reenter autonomous driving mode before the system computer was primed to do so, and the truck swerved, making contact with the highway barrier,” said Xiaodi Hou, TuSimple co-founder and CEO, during Tuesday’s earnings call. “No one was hurt. And the only evidence of the accident are a few scrapes and some minor damages on our truck.”\nHou noted that in the past seven years, TuSimple had accumulated 8.1 million miles of road testing with “precisely one incident.” When the incident happened on April 6, TuSimple grounded the entire fleet and began an independent investigation, said Hou. After determining the cause of the error, the company then upgraded all of its systems with an overhaul of its human machine interface to make sure the same problem would never happen again, the executive continued. \nThat internal report, which was reviewed by WSJ, revealed that the truck abruptly swerved left due to an outdated command, which was 2.5 minutes old and should have been erased from the system but wasn’t. \nResearchers at Carnegie Mellon University told WSJ that common safeguards, had they been in place, would have prevented the crash. For example, the truck shouldn’t be responding to a command that’s even a couple hundredths of a second old, let alone more than two minutes old. The system also shouldn’t be able to turn so sharply while traveling at 65 miles per hour, nor should a safety driver be able to engage a self-driving system that’s not properly functioning. \nThe National Highway Traffic Safety Administration has since joined the FMCSA’s investigation into the TuSimple highway crash.\nHou said that the two agencies have yet to find any anomalies in their investigation or give TuSimple any safety recommendations, but the investigation is not yet complete. \nDuring the earnings call, TuSimple repeated its plans to commercialize driver-out operations, in which no human safety operator is present in the vehicle. The company first completed a driver-out demonstration along an 80-mile stretch in Arizona in December, and has completed several more runs since. \nTuSimple said the crash wouldn’t affect its plans to begin driver-out operations for Union Pacific Railroad, but it’s unclear if the company is even on schedule for that at present. TuSimple was meant to launch fully autonomous freight hauling for Union Pacific in the spring of this year and scale to commercial viability by the end of 2023, but Hou said the company has encountered a complete road closure in front of the distribution center at its destination point, which has delayed the run by “a couple of weeks.” He also reiterated that the company’s deadline for driver-out in Texas is set for 2023, but did not specify if those will be initial test runs or fully commercial operations. TuSimple did not respond in time for requests for clarification.\nTuSimple Q2 financials\nTuSimple’s total revenue was $2.6 million in the second quarter, which is up 73% year-over-year and 13% sequentially. Wall Street analysts expected TuSimple’s revenues to come in at $4.06 million; moreover, they expected the company to beat those estimates. \nThe company attributed its growth, such as it was, to increased utilization of existing assets and year-over-year price increases. \nTuSimple’s net loss came in at $108.6 million, versus $116.5 million in the same quarter of last year. The company appears to have slimmed down on total operating expenses, which came in at $107.5 million this quarter versus $119.4 million last year. However, R&D spending was up 13% year-over-year at $85.5 million. TuSimple said the largest portion of R&D expense was $60.8 million related to hiring, including a stock-based compensation expense of $22.4 million. That said, sales, general and administrative spend was significantly lower than last year. \nTo prepare for driver-out operations and to expand its autonomous freight network, TuSimple invested a total of $3.8 million in purchases of property and equipment. The company ended the quarter with $1.16 billion in cash. \nUpdated full-year guidance\nTuSimple’s updated guidance on 2022 revenue remained unchanged at $9 million to $11 million. Generally, the company intends to spend less, and therefore lose less money this year. TuSimple’s adjusted EBITDA loss for the year is now expected to be between $360 million and $380 million, versus previous guidance of $400 million to $420 million. \nIn addition, TuSimple will spend less on stock-based compensation — attributable to a hiring slowdown — as well as purchases of property and equipment. The company is hoping to end the year with $950 million in cash versus previous guidance of $900 million. \nExecutive shakeups\nHou touched on some key leadership changes that had been announced in June, including chief financial officer Patrick Dillon leaving the company, to be temporarily replaced by Eric Tapia, TuSimple’s global controller and principal accounting officer. \nIn addition, Dr. Ersin Yumer, previously head of TuSimple’s autonomous freight network, was promoted to EVP of operations, and Dr. Lei Wang was promoted to EVP of technology. Both were promoted to support TuSimple’s driver-out operations. \nWorth noting\nIt’s worth noting that TuSimple would not address a question about the company’s tentative plans to sell off its China operations, something that was touched on during the first-quarter earnings call. \nAt the time, TuSimple told TechCrunch that the company’s stock price today doesn’t reflect the value of the China autonomous freight business, so it would be a good idea to split off APAC operations. A perusal through the company’s 10-Q revealed TuSimple is more likely looking to sell its China operations because it’s too expensive to keep it going, given the National Security Agreement the company agreed to as part of a review by the Committee on Foreign Investment in the United States. \nTapia, TuSimple’s interim CFO, also shared that the company is in the process of upgrading most of its older trucks to its newest AV hardware technology, a process which will continue through 2023 and will involve adding upgraded sensors to the vehicles. \n“While we plan to introduce some new trucks into the fleet, our ability to add a significant number of trucks is difficult, given the challenges in purchasing new or even slightly used trucks,” said Tapia. “Lastly, we plan to continue to invest in adding terminals to the [autonomous freight network], primarily around the Texas triangle. Our intention is to do this in a capital-light manner, partnering when possible.” \nIn 2020, TuSimple partnered with Navistar to build fully autonomous trucks, and has previously set a deadline to begin manufacturing by 2024 and deliver to certain customers, like DHL, by 2025. Hou and Tapia dodged one analyst’s repeated attempts to get clarity on this timeline.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous driving system errors. In the second-quarter earnings call, TuSimple addressed an April crash involving one of its autonomous trucks. The crash occurred when a test driver and safety engineer tried to reenter autonomous driving mode before the system computer was primed to do so, causing the truck to swerve and collide with a highway barrier. The crash did not result in injuries but caused some minor damage to the truck. TuSimple grounded its fleet and conducted an investigation, which revealed that an outdated command was responsible for the abrupt swerve. Researchers at Carnegie Mellon University pointed out that common safeguards were lacking, and the National Highway Traffic Safety Administration and the Federal Motor Carrier Safety Administration initiated an investigation. TuSimple has since upgraded its systems to prevent similar errors. The news also mentions TuSimple's financial performance and plans for commercializing driver-out operations.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/08\/02\/tusimple-addresses-autonomous-truck-crash-during-q1-earnings\/","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":5,"Time":"2:35 PM GMT+1•November 2, 2021","Title":"Tesla recalls 11,704 vehicles after identifying Full Self-Driving Beta software error","Content":"cameraImage Credits: Toru Hanai\/Bloomberg\n(opens in a new window)\n\/ Getty Images\nTesla voluntarily recalled nearly 11,704 vehicles after identifying a software error that could cause a false forward-collision warning or unexpected activation of the automatic emergency brake system, the National Highway Traffic and Safety Administration said.\nAll of the affected vehicles had early access to the automaker’s “Full Self-Driving Beta,” it’s advanced driver assistance system. The system, which is still in beta mode and requires the driver to be attentive at all times, has been released to thousands of customers in recent weeks. Tesla said it was not aware of any crashes or injuries as a result of the software error.\nThe over-the-air firmware update, which was released on October 23, introduced what the regulator called “a software communication disconnect” between two on-board chips. The following day, Tesla said it started receiving reports from customers.\n“This communication disconnect can result in the video neural networks that operate on that chip to run less consistently than expected,” the safety recall report said. “The inconsistency can produce negative object velocity detections when other vehicles are present, which in turn can lead to false [forward-collision warnings] and [automatic emergency braking] events.”\nAffected vehicles include certain Model S, Model X and Model 3 cars that were manufactured between 2017-2021, and certain Model Y models that were manufactured between 2020-2021. Tesla has released a separate over-the-air software update to address the issue and owners will be sent letters notifying them of the issue and resolution.\n“In a matter of hours, we investigated the reports and took actions to mitigate any potential safety risk,” Tesla said.\nThe formal recall is a marked departure from the California automaker’s recent interactions with the country’s top automotive safety agency, which includes releasing a software update for a separate bug identified in its Autopilot system last month, for which Tesla did not issue a recall. NHTSA sent a letter to the automaker on October 12 asking why it did not issue a recall.\n“As Tesla is aware, the Safety Act imposes an obligation on manufacturers of motor vehicles and motor vehicle equipment to initiate a recall by notifying NHTSA when they determine vehicles or equipment they produced contain defects related to motor vehicle safety or do not comply with an applicable motor vehicle safety standard,” the agency wrote.\nThe regulator opened a separate investigation into Autopilot in August after identifying 12 separate incidents in which a Tesla crashed into parked emergency vehicles.\nRegarding this recall, NHTSA said it would “continue its conversations with Tesla to ensure that any safety defect is promptly acknowledged and addressed.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Tesla has voluntarily recalled 11,704 vehicles due to a software error in its Full Self-Driving Beta system. The error could trigger false forward-collision warnings and automatic emergency braking events. The affected vehicles include certain Model S, Model X, Model 3 cars (manufactured between 2017-2021), and certain Model Y models (manufactured between 2020-2021). Tesla has released an over-the-air software update to address the issue. There have been no reported crashes or injuries related to this software error. This recall is notable because it marks a departure from Tesla's recent interactions with the National Highway Traffic and Safety Administration (NHTSA), which had previously asked Tesla to issue a recall for a separate bug in its Autopilot system. NHTSA is continuing its conversations with Tesla regarding safety concerns.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2021\/11\/02\/tesla-recalls-11704-vehicles-after-identifying-full-self-driving-beta-software-error\/","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":38,"Time":null,"Title":"U.S. investigates 416,000 Tesla vehicles over unexpected braking reports","Content":"Companies\nTesla Inc\nFollow\nWASHINGTON, Feb 17 (Reuters) - The National Highway Traffic Safety Administration (NHTSA) said Thursday it is opening a formal investigation into 416,000 Tesla vehicles over reports of unexpected brake activation tied to its driver assistance system Autopilot.\nThe preliminary evaluation covers 2021-2022 Tesla Model 3 and Model Y vehicles in the United States after the agency received 354 complaints about the issue over the past nine months. NHTSA said the vehicles under review have a advanced driver assistance system that Tesla calls Autopilot that allows them to brake and steer automatically within its lanes.\nNHTSA said: \"Complainants report that the rapid deceleration can occur without warning, at random, and often repeatedly in a single drive cycle.\"\nOwners say they have raised concerns with Tesla, which has dismissed the complaints saying the braking is normal, and some have called it \"phantom braking\".\nThe owner of a 2021 Tesla Model Y told NHTSA in October that while driving on a highway at 80 miles per hour \"the car braked hard and deceleated from 80 mph to 69 mph in less than a second. The braking was so violent, my head snapped forward and I almost lost control of the car.\"\nAdvertisement · Scroll to continue\nNHTSA in August opened a formal safety probe into Tesla's Autopilot system in 765,000 U.S. vehicles after a series of crashes involving Tesla models and emergency vehicles.\nEarlier this month, NHTSA confirmed it was reviewing consumer complaints that Tesla vehicles were activating the brakes unnecessarily. A preliminary evaluation is the first phase before NHTSA could issue a formal recall demand.\nIn May, Tesla chief executive Elon Musk said dropping a radar sensor from its partially automated driving system would address \"phantom braking,\" which some Tesla drivers have long complained about. Tesla, which disbanded its media relations department, did not respond to a request for comment.\nAdvertisement · Scroll to continue\nTesla has come under increasingly scrutiny from NHTSA, which is investigating several issues and the electric vehicle manufacturer has issued 10 recalls since October, including many for software issues and some under pressure from the agency.\nJonathan Adkins, executive director of the Governors Highway Safety Association, the group that represents state highway safety agencies, said on Twiter \"another day, another issue with Tesla. Good to see NHTSA being more active in reigning this company in.\"\nAdvertisement · Scroll to continue\nIn November, Tesla recalled nearly 12,000 U.S. vehicles sold since 2017 because a communication error may cause a false forward-collision warning or unexpected activation of the emergency brakes.\nThe recall was prompted after a software update on Oct. 23 to vehicles in its limited early access version 10.3 Full-Self Driving (FSD) (Beta) population.\nFSD is an advanced driver assistance system that handles some driving tasks but Tesla and NHTSA say it does not make vehicles autonomous.\nAdvertisement · Scroll to continue\nThe recall came after NHTSA in October asked Tesla why it had not issued a recall to address software updates made to its Autopilot driver-assistance system to improve the vehicles' ability to detect emergency vehicles.\nLast week, Tesla recalled 578,607 U.S. vehicles because pedestrians may not be able to hear a required warning sound of an approaching car due to loud music or other sounds played by its \"Boombox\" feature.\nMusk said last week on Twitter \"the fun police made us do it (sigh).\"\nReporting by David Shepardson; Editing by Toby Chopra and David Evans\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here is the JSON format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"U.S. investigates 416,000 Tesla vehicles over unexpected braking reports\",\n        \"Description\": \"The National Highway Traffic Safety Administration (NHTSA) is opening a formal investigation into 416,000 Tesla vehicles due to reports of unexpected brake activation tied to its driver assistance system Autopilot. The investigation covers 2021-2022 Tesla Model 3 and Model Y vehicles in the United States after the agency received 354 complaints about the issue over the past nine months. Complainants report that the rapid deceleration can occur without warning, at random, and often repeatedly in a single drive cycle. Owners have raised concerns, and Tesla has dismissed the complaints, referring to the braking as normal. The investigation follows a series of crashes involving Tesla models and emergency vehicles, and NHTSA is actively looking into consumer complaints about Tesla vehicles activating the brakes unnecessarily.\"\n    }\n}\nThe news is relevant to autonomous driving system errors as it discusses unexpected brake activation related to Tesla's Autopilot system. The specific information about the case is included in the JSON response.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/us-probes-416000-tesla-vehicles-over-unexpected-braking-reports-2022-02-17\/","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":43,"Time":"10 January 2022","Title":"Tesla adds chill and assertive self-driving modes","Content":"Tesla adds chill and assertive self-driving modes\nPublished\n10 January 2022\nShare\nIMAGE SOURCE,\nGETTY IMAGES\nTesla's automated driver assist feature has added an assertive driving mode.\nThe setting will follow other cars more closely, change lanes more frequently, not leave the overtaking lane, and perform rolling stops.\nSuch driver behaviour by humans is often discouraged by safety groups.\nHowever, it could sometimes be safer for an automated system to be more assertive, like a human driver, rather than being overly cautious, one motor safety expert said.\nThe three driving profiles - chill, average, and assertive - were first added in Tesla's October update. That update, however, was quickly pulled because of other issues, but the driving profiles feature has now been restored.\nA screenshot of the update was posted to Twitter by David Zipper, a technology writer and visiting fellow at Harvard Kennedy School, and was first reported by The Verge.\nSkip twitter post by David Zipper\nAllow Twitter content?\nThis article contains content provided by Twitter. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Twitter’s cookie policy\n, external\nand privacy policy\n, external\nbefore accepting. To view this content choose ‘accept and continue’.\nAccept and continue\nThe BBC is not responsible for the content of external sites.\nEnd of twitter post by David Zipper\nIt shows the assertive mode described as: \"Your Model X will have a smaller follow distance, perform more frequent speed lane changes, will not exit passing lanes and may perform rolling stops.\"\nPotentially safer\nThe list of behaviours has been criticised by some on social media as being less safe.\nBut Matthew Avery, from the UK's Thatcham Research, said that well-designed driverless systems are theoretically safer than human drivers because they eliminate human error.\nAs such, if a more assertive driving style encourages more drivers to take up self-driving systems than a very cautious style would, that could be a net gain for safety.\n\"If we want widespread adoption of automation, drivers are going to expect the vehicle to do and make the decisions that you would do as a human driver, not some very benign and very safe algorithm,\" he explained.\nHuman drivers come to an impasse regularly, such as when one has to pull over in a single-lane country road or at a four-way intersection, and one driver must make the move first. Two extremely careful automated cars might both wait for the other to act.\n\"This is what the manufacturers are trying to learn at the moment,\" Mr Avery explained.\n\"So, a degree of being slightly less cautious, If that means more people use the systems more of the time because they feel that they're more human-like, that's a good thing.\"\nMedia caption,\nWhat happens if you fall asleep in a self-driving car?\nBut he warned that it depends on how \"assertive\" the system is - and said it must avoid aggressive driving.\n\"It's a fine line between assertive and aggressive, but definitely there are situations when automation going through some very basic rules will eventually sort of stop because it just can't progress,\" he said.\n\"I don't think we're there yet. I don't think the technology is sophisticated enough.\"\n'Rolling stops'\nIn many jurisdictions, failing to come to a complete halt at a stop line is illegal, and can result in someone failing their driving test. As a habit, many drivers simply come to a slow crawl - or a rolling stop - instead, but it is considered a dangerous technique.\nTesla disables gaming while driving feature\nOutage locks Tesla drivers out of their cars\nThese appear to be part of both Tesla's average and assertive modes. The description that the car \"will not exit passing lanes\" also seems to contradict some regional rules.\nTesla's so-called \"Full Self-Driving\" feature is currently only available in the US as part of a limited test.\nBut in the US, where they drive on the right, several states - but not all - have made it illegal to leave the right-hand lane unless overtaking. Similarly, in the UK, the Highway Code says motorists should always stay in the left lane unless overtaking, and return to the left when safe to do so.\nIt is not clear if Tesla's system will account for national or state-based variations in the rules about staying in the overtaking lanes, or what the term rolling stops means in relation to stop signs.\nTesla to recall 475,000 cars in the US\nParis taxi firm suspends Tesla model after crash\nThe company has disbanded its media relations department and does not respond to queries from journalists.\nTesla's so-called full self-driving has been subject to much scrutiny, with crashes and incidents involving the technology gaining widespread media coverage.\nIt is not, despite its name, a self-driving technology, but is rather considered a driver assistance feature similar to other car-makers' lane assist technology. It is at level two on a five-point scale of automated systems.\nTesla owners must remain in control of the vehicle and alert at all times, ready to take over in an instant for safety.\nRelated Topics\nTesla\nDriverless cars\nRoad safety\nMore on this story\nTesla withdraws self-driving beta after problems\nPublished\n25 October 2021\nOutage locks Tesla drivers out of their cars\nPublished\n20 November 2021\nParis taxi firm suspends Tesla model after crash\nPublished\n15 December 2021\nTesla disables gaming while driving feature\nPublished\n24 December 2021\nTesla to recall 475,000 cars in the US\nPublished\n30 December 2021","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Tesla has added an assertive driving mode to its automated driver assist feature. This mode follows other cars more closely, changes lanes more frequently, does not leave the overtaking lane, and may perform rolling stops. While some consider these behaviors less safe, others argue that a more assertive driving style could encourage more drivers to use self-driving systems, potentially improving safety by reducing human errors. However, it's important to strike a balance between assertive and aggressive driving to ensure safety. The article also mentions regional differences in driving rules and the ongoing scrutiny of Tesla's 'Full Self-Driving' feature.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-59939536","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":48,"Time":"Updated 6:25 PM EST, Thu November 18, 2021","Title":"We tried Tesla’s ‘full self-driving.’ Here’s what happened","Content":"Washington, DC\nCNN\n — \nWhen a dozen small children crossed in front of our Tesla with “full self-driving,” I had good reason to be nervous.\nI’d spent my morning so far in the backseat of the Model 3 using “full self-driving,” the system that Tesla says will change the world by enabling safe and reliable autonomous vehicles. I’d watched the software nearly crash into a construction site, try to turn into a stopped truck and attempt to drive down the wrong side of the road. Angry drivers blared their horns as the system hesitated, sometimes right in the middle of an intersection. (We had an attentive human driver behind the wheel during all of our tests, to take full control when needed.)\nTesla owners say they are wowed -- and alarmed -- by 'full self-driving'\nThe Model 3’s “full self-driving” needed plenty of human interventions to protect us and everyone else on the road. Sometimes that meant tapping the brake to turn off the software, so that it wouldn’t try to drive around a car in front of us. Other times we quickly jerked the wheel to avoid a crash. (Tesla tells drivers to pay constant attention to the road, and be prepared to act immediately.)\nI hoped the car wouldn’t make any more stupid mistakes. After what felt like an eternity the kids finished crossing. I exhaled.\nWe were clear to make our turn. The car seemed overly hesitant initially, but then I noticed a bicyclist coming from our left. We waited.\nOnce the bicyclist crossed the intersection, the car pulled up and made a smooth turn.\nOver the past year I’ve watched more than a hundred videos of Tesla owners using “full self-driving” technology, and I’ve spoken to many of them about their experiences.\n“Full self-driving” is a $10,000 driver-assist feature offered by Tesla. While all new Teslas are capable of using the “full self-driving” software, buyers must opt into the costly addition if they want to access the feature. The software is still in Beta and is currently available to only a select group of Tesla owners, though CEO Elon Musk has claimed that a wider rollout is imminent. Musk promises “full self-driving” will be totally capable of getting a car to its destination in the near future.\nBut it doesn’t do that. Far from it.\nTesla owners have described the technology as impressive but also flawed. One moment it’s driving perfectly, the next moment it nearly crashes into something.\nJason Tallman, a Tesla owner who documents his “full self-driving” trips on YouTube, offered to let me experience it first-hand.\nWe asked Jason to meet us on Brooklyn’s Flatbush Avenue. It’s an urban artery that funnels thousands of cars, trucks, cyclists and pedestrians into Manhattan. For even experienced human drivers, it can be a challenge.\nHow Tesla can sell 'full self-driving' software that doesn't really drive itself\nCity driving is chaotic, with vehicles running red lights and pedestrians on nearly every block. It’s a far cry from the suburban neighborhoods and predictable highways around Tesla’s California offices, or the broad streets of Arizona, where Alphabet’s Waymo operates fully autonomous vehicles.\nCruise, GM’s self-driving company, recently completed its first fully autonomous rides in San Francisco. But they were conducted after 11 p.m. at night, when traffic is light and few pedestrians or cyclists are present.\nBrooklyn offered us a chance to see how close Tesla’s autonomous driving software was to replacing human drivers. It’s the sort of place where humans drive because they have to, not the sort of place selected by a corporate headquarters. It’s where self-driving cars might have the biggest impact.\nAt one point we were cruising along in the right lane of Flatbush. A construction site loomed ahead. The car continued full speed ahead toward a row of metal fencing.\nI felt deja vu as I recalled a video in which a Tesla owner slammed on the brakes after his car appeared set on crashing headlong into a construction site.\nBut this time I was sitting in the back seat. I instinctively threw up my right arm like the Heisman Trophy, as if to protect myself in a collision.\nThat was a moment I wished “full self-driving” would be quick to change lanes. In other cases, I wished it would chill out on its aggressive turns.\n“Full self-driving” sometimes makes jerky turns. The wheel starts to turn, but then shifts back, before again turning in its intended direction. The staggered turns generally don’t seem to be a bother on sweeping suburban curves, but in a dense city largely built before cars, it’s uncomfortable.\nElon Musk says Tesla is pushing ahead with 'full self-driving' a month after fatal crash\nThere’s also the braking, which can feel random. At one point a car came close to rear ending us following braking that surprised me. Getting honked at was common. I never quite felt like I knew what “full self-driving” would do next. Asking “full self-driving” to navigate Brooklyn felt like asking a student driver to take on a road test they weren’t ready for yet.\nWhat “full self-driving” could do well was impressive, but the experience was ultimately unnerving. I can’t imagine using “full self-driving” regularly in a city. I noticed I was reluctant to ever look down at the Model 3’s dashboard, such as for checking our speed, because I didn’t want to take my eyes off the road.\nTesla owners routinely tell me how Autopilot, the highway-focused predecessor to “full self-driving” makes their trips less stressful. They arrive at destinations feeling less fatigued. Some have told me they’re more likely to go on long road trips because of Autopilot.\nBut “full self-driving” felt like the inverse. I felt like I needed to be constantly on guard to prevent the car from doing something wrong.\nUltimately, seeing “full self-driving” in Brooklyn reminded me of the importance of the finer points of driving, which is tough for an artificial intelligence powered car to master. Things like pulling slightly into the intersection on a narrow road to make a left turn, so traffic behind you has room to pull around. “Full self-driving” just sat in place as frustrated drivers behind us honked.\nFor now, “full self-driving” seems closer to a party trick to show friends than a must-have feature.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about Tesla's 'full self-driving' system, and it highlights various instances where the system exhibited errors and required human intervention. The article describes situations where the software nearly crashed into a construction site, attempted to drive into a stopped truck, and hesitated in the middle of intersections. It also mentions that Tesla owners have described the technology as impressive but flawed, with jerky turns, unpredictable braking, and a lack of readiness for city driving. The 'full self-driving' feature is currently in Beta and is not fully autonomous, despite Tesla's claims.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2021\/11\/18\/cars\/tesla-full-self-driving-brooklyn\/index.html","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":50,"Time":"6:09 PM GMT+2•September 20, 2022","Title":"Tesla extends FSD access amid regulator scrutiny","Content":"cameraImage Credits: Tesla\nTesla is extending access to its advanced driver assistance system, Full Self-Driving (FSD) Beta version 10.69.2.2, to 160,000 owners in the U.S. and Canada, according to a tweet Monday from Tesla CEO Elon Musk.\nAround 100,000 Tesla owners currently have access to the controversial driver assist system, which, for a price tag of $15,000, promises to automate certain driving tasks such as assisted steering on highways and city streets, smart vehicle summoning, automatic parking and recognizing and reacting to traffic lights and stop signs. The expansion comes at a time when federal and state regulators, as well as Tesla owners and other concerned citizens, are coming down on FSD and Autopilot, Tesla’s less advanced driver assist system, after a series of crashes and other potentially life-threatening errors.\nThe National Highway Traffic and Safety Administration (NHTSA) is currently investigating 16 crashes in which Tesla owners were potentially engaging the ADAS when they crashed into stationary emergency vehicles. In July, California’s Department of Motor Vehicles accused Tesla of falsely advertising its systems.\nJust last week, a Tesla owner filed a proposed class action lawsuit against the company for deceitfully advertising Autopilot and FSD as either fully functioning or close to being “solved” since 2016, despite knowing full well that the systems cannot perform as advertised. The plaintiff alleged that Tesla’s ADAS — which can cost up to $15,000 for FSD — causes vehicles to run red lights, miss turns and veer into traffic.\nMusk has admitted that Tesla is basically worthless without FSD, but in the same breath he’ll say that the company is close to “solving” full self-driving by the end of this year. However, despite actually dangerous mistakes the software has made, FSD still seems to fail at reaching human-levels for certain basic driving tasks.\n“Loving FSD Beta — but — could the vehicle initiate the turn signal before switching into the turn lane?” tweeted one cautious Tesla owner. “It uses the signal for speed-based lane changes but not for merging into a turn lane.”\n“Elon when will the tesla speed up when noticing traffic is going faster? Sometimes it’s going slow,” from Tesla Owners Silicon Valley. Musk promised that update would come next month.\nMusk also noted on Twitter that the Autopilot\/AI team is also working on Optimus, Tesla’s humanoid robot, alongside “actually smart summon\/autopark,” both of which have end of month deadlines. Tesla’s AI Day is scheduled for September 30, and Musk is expected to provide updates on a range of technology, including Optimus.\nDuring Tesla’s first-quarter earnings this year, Musk said Optimus, which will be designed to take the drudgery out of household tasks, will be “worth more than the car business, worth more than FSD.”\nWhile Musk has several times underscored the importance that Optimus will have in the company’s future, the fact that the same team responsible for Autopilot is working on a robot implies that Optimus is still just a bit of a side project, rather than a development program.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous driving systems (ADS) errors. Tesla is extending access to its Full Self-Driving (FSD) Beta version 10.69.2.2 to 160,000 owners in the U.S. and Canada. The article also mentions the controversy surrounding FSD and Autopilot, Tesla's less advanced driver assist system, due to a series of crashes and potentially life-threatening errors. The National Highway Traffic and Safety Administration (NHTSA) is investigating 16 crashes involving Tesla vehicles using ADAS. Additionally, a Tesla owner has filed a lawsuit alleging that Tesla's ADAS, including FSD, causes vehicles to run red lights and make errors. Elon Musk's comments about FSD's progress are also mentioned.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/09\/20\/tesla-extends-fsd-access-amid-regulator-scrutiny\/","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":19,"Time":"Published 4:58 AM GMT+2, February 17, 2023","Title":"Tesla recalls ‘Full Self-Driving’ to fix unsafe actions","Content":"DETROIT (AP) — U.S. safety regulators have pressured Tesla into recalling nearly 363,000 vehicles with its “Full Self-Driving” system because it can misbehave around intersections and doesn’t always follow speed limits.\nThe recall, part of part of a larger investigation by the National Highway Traffic Safety Administration into Tesla’s automated driving systems, is the most serious action taken yet against the electric vehicle maker.\nIt raises questions about CEO Elon Musk’s claims that he can prove to regulators that cars equipped with “Full Self-Driving” are safer than humans, and that humans almost never have to touch the controls.\nMusk at one point had promised that a fleet of autonomous robotaxis would be in use in 2020. The latest action appears to push that development further into the future.\nOTHER NEWS\nTikTok Shop launches in the U.S. as the company bets big on e-commerce\nGoogle reaches tentative settlement with all 50 states over alleged app store monopoly\nPeople are losing more money to scammers than ever before. Here’s how to keep yourself safe\nThe safety agency says in documents posted on its website Thursday that Tesla will fix the concerns with an online software update in the coming weeks. The documents say Tesla is doing the recall but does not agree with an agency analysis of the problem.\nThe system, which is being tested on public roads by as many as 400,000 Tesla owners, can make unsafe actions such as traveling straight through an intersection while in a turn-only lane, failing to come to a complete stop at stop signs, or going through an intersection during a yellow traffic light without proper caution, NHTSA said. The problems happen in “certain rare circumstances,” the agency wrote.\nIn addition, the system may not adequately respond to changes in posted speed limits, or it may not account for the driver’s adjustments in speed, the documents said.\n“FSD beta software that allows a vehicle to exceed speed limits or travel through intersections in an unlawful or unpredictable manner increases the risk of a crash,” the agency said in documents.\nMusk complained Thursday on Twitter, which he now owns, that calling an over-the-air software update a recall is “anachronistic and just flat wrong!” A message was left Thursday seeking further comment from Tesla, which has disbanded its media relations department.\nTesla has received 18 warranty claims that could be caused by the software from May of 2019 through Sept. 12, 2022, the documents said. But the Austin, Texas, electric vehicle maker told the agency it is not aware of any deaths or injuries.\nIn a statement, NHTSA said it found the problems during tests performed as part of an investigation into Tesla’s “Full Self-Driving” and “Autopilot” software that take on some driving tasks. The investigation remains open, and the recall doesn’t address the full scope of what NHTSA is scrutinizing, the agency said.\nDespite the names “Full Self-Driving” and “Autopilot,” Tesla says on its website that the cars cannot drive themselves and owners must be ready to intervene at all times.\nNHTSA’s testing found that Tesla’s FSD beta testing, “led to an unreasonable risk to motor vehicle safety based on insufficient adherence to traffic safety laws.”\nRaj Rajkumar, a professor of computer engineering at Carnegie Mellon University, doubts that Tesla can fix all of the problems cited by NHTSA with a software update. The automaker, he says, relies only on cameras and artificial intelligence to make driving decisions, a system that will make mistakes.\n“Cameras can miss a lot of things,” Rajkumar said. “These are not straightforward issues to fix. If they could have fixed it, they would have fixed it a long time back.”\nMost other companies with self-driving vehicles use laser sensors and radar in addition to cameras to make sure vehicles see everything. “One sensing modality is not perfect by any metric,” Rajkumar said.\nHe questioned whether NHTSA will require testing before the software update is sent out to make sure it works. The agency said that it works closely with automakers as they develop recall remedies “to ensure adequacy.”\nIn documents, NHTSA says that on Jan. 25, as part of regular communications with Tesla, it told the automaker about concerns with FSD, and it asked Tesla to do a recall. On Feb. 7, Tesla decided to do the recall out of an abundance of caution, “while not concurring with the agency’s analysis.”\nThe recall is another in a list of problems that Tesla has with the U.S. government. In January, the company disclosed that the U.S. Justice Department had requested documents from Tesla about “Full Self-Driving” and “Autopilot.”\nNHTSA has been investigating Tesla’s automated systems since June of 2016 when a driver using Autopilot was killed after his Tesla went under a tractor-trailer crossing its path in Florida. A separate probe into Teslas that were using Autopilot when they crashed into emergency vehicles started in August 2021. At least 14 Teslas have crashed into emergency vehicles while using the Autopilot system.\nNHTSA has sent investigators to 35 Tesla crashes in which automated systems are suspected of being used. Nineteen people have died in those crashes, including two motorcyclists.\nThe agency also is investigating complaints that Teslas can brake suddenly for no reason.\nSince January of 2022, Tesla has issued 20 recalls, including several that were required by NHTSA. The recalls include one from January of last year for “Full Self-Driving” vehicles being programmed to run stop signs at slow speeds.\n“Full Self-Driving” went on sale late in 2015, and Musk has used the name ever since. It currently costs $15,000 to activate the system.\nThe recall announced Thursday covers certain 2016-2023 Model S and Model X vehicles, as well as 2017 through 2013 Model 3s, and 2020 through 2023 Model Y vehicles equipped with the software, or with installation pending.\nShares of Tesla closed Thursday down 5.7%. The stock has rallied about 64% in the year to date, reversing 2022’s hefty loss.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Tesla has been forced to recall nearly 363,000 vehicles with its 'Full Self-Driving' system due to safety concerns. The recall is a result of U.S. safety regulators' investigation into Tesla's automated driving systems, which have been found to misbehave around intersections and not always follow speed limits. The recall raises questions about CEO Elon Musk's claims regarding the safety of 'Full Self-Driving' and the development of autonomous robotaxis. Tesla will address the concerns with an online software update in the coming weeks. The issues include making unsafe actions at intersections, failing to stop at stop signs, and not responding adequately to posted speed limits.\"\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/tesla-recalls-full-self-driving-cars-875b54d4b71e97d43a17e968d7b856ae","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":54,"Time":"7:00 PM GMT+1•February 16, 2023","Title":"Tesla recalls 362,758 vehicles over Full Self-Driving software safety concerns","Content":"cameraImage Credits: Tesla\nTesla has issued a recall of its Full Self-Driving software, an advanced driver-assistance system that federal safety regulators say could allow vehicles to act unsafe around intersections and cause crashes.\nTesla said it is recalling certain 2016–2023 Model S, Model X; 2017–2023 Model 3; and 2020–2023 Model Y vehicles equipped with Full Self-Driving Beta (FSD Beta) software or those pending installation. The recall, which was posted on the National Highway Traffic Safety Administration’s website, affects as many as 362,758 vehicles equipped with the software, according to the notice.\nTesla will release an over-the-air software update, free of charge, to fix the issue.\nTesla vehicles come standard with a driver-assistance system branded as Autopilot. For an additional $15,000, owners can buy “full self-driving,” or FSD — a feature that CEO Elon Musk has promised for years will one day deliver full autonomous driving capabilities. Tesla vehicles are not self-driving.\nInstead, FSD includes a number of automated driving features that still require the driver to be ready to take control at all times. It includes the parking feature Summon, as well as Navigate on Autopilot, an active guidance system that navigates a car from a highway on-ramp to off-ramp, including interchanges and making lane changes. The system is also supposed to handle steering on city streets and recognize and react to traffic lights and stop signs.\nAccording to the notice, the FSD Beta system may allow the vehicle to act unsafe around intersections, such as traveling straight through an intersection while in a turn-only lane, entering a stop sign–controlled intersection without coming to a complete stop, or proceeding into an intersection during a steady yellow traffic signal without due caution. In addition, the system may respond insufficiently to changes in posted speed limits or not adequately account for the driver’s adjustment of the vehicle’s speed to exceed posted speed limits, the notice said.\nThe issue was discovered during the NHTSA’s engineering analysis and testing of the system. The NHTSA said it found that in certain situations, Tesla’s Autosteer on City Streets feature led to an unreasonable risk to motor vehicle safety based on insufficient adherence to traffic safety laws. As required by law and after discussions with the NHTSA, Tesla launched a recall to repair those defects, the agency said in an emailed statement.\nThe NHTSA reached out to Tesla on January 25 as part of “regular ongoing communications” between Tesla and the agency relating to Autopilot and FSD Beta, according to the Defect Information Report, which includes a chronology of events leading up to the recall. At that time, the NHTSA advised Tesla that it had identified potential concerns related to certain operational characteristics of FSD Beta in four specific roadway environments.\nTesla met with the NHTSA “numerous times” over the next several days to discuss the agency’s concerns and Tesla’s proposed over-the-air improvements in response, according to the report.\n“NHTSA will continue to monitor the recall remedies for effectiveness,” the agency said. “This recall seeks to address a specific set of concerns identified by the agency.”\nThe agency added that the recall does not address the full scope of the NHTSA’s EA22-002 investigation into Tesla’s Autopilot and associated vehicle systems. That investigation remains open and active.\nFSD beta software — from its name and Musk’s promises around its capabilities to its rollout and safety concerns — has been controversial. It’s also attracted scrutiny from regulators.\nIn January, Tesla disclosed that the U.S. Department of Justice asked for documents related to its branded Full Self-Driving and Autopilot advanced driver-assistance system.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Tesla has issued a recall of its Full Self-Driving software due to safety concerns raised by federal safety regulators. The recall affects certain 2016–2023 Model S, Model X; 2017–2023 Model 3; and 2020–2023 Model Y vehicles equipped with Full Self-Driving Beta (FSD Beta) software or those pending installation, totaling approximately 362,758 vehicles. The recall was prompted by concerns that the FSD Beta system could lead to unsafe behavior around intersections, including failing to stop at stop signs, ignoring traffic signals, and not adjusting speed limits correctly. The issue was discovered during the NHTSA's engineering analysis and testing, and Tesla will release a free over-the-air software update to address the problem. The NHTSA's investigation into Tesla's Autopilot and associated vehicle systems remains open and active.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2023\/02\/16\/tesla-recalls-362758-vehicles-over-fsd-beta-software-safety-concerns\/","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":59,"Time":"MAY 24, 2018","Title":"Uber disabled emergency braking in self-driving car: U.S. agency","Content":"By David Shepardson\nWASHINGTON (Reuters) - Uber had disabled an emergency braking system in a self-driving vehicle that struck and killed a woman in Arizona in March even though the car had identified the need to apply the brakes, the National Transportation Safety Board said in a preliminary report released on Thursday.\nThe report into the first fatal crash caused by a self-driving vehicle also disclosed that the modified 2017 Volvo XC90’s radar systems observed the pedestrian six seconds before impact but “the self-driving system software classified the pedestrian as an unknown object, as a vehicle, and then as a bicycle with varying expectations of future travel path.”\nAt 1.3 seconds before impact, the self-driving system determined emergency braking was needed. But Uber said, according to the NTSB, that automatic emergency braking maneuvers in the Volvo XC90 were disabled while the car was under computer control in order to “reduce the potential for erratic vehicle behavior.”\nThe report gives new fuel to opponents in Congress who have stalled a bill designed to speed the deployment of self-driving cars on U.S. roads and puts a spotlight on the fact that the National Highway Traffic Safety Administration, which is also investigating, does not test self-driving vehicles or certify them before they are deployed on U.S. roads.\nUber Technologies Inc, which voluntarily suspended testing after the crash in the city of Tempe, said on Wednesday it planned to end testing in Arizona and focus on limited testing in Pittsburgh and two cities in California. Uber aims to resume its self-driving operations this summer, likely with smaller routes and fewer cars, the company said.\nThe company did not directly comment on the NTSB findings but noted it recently named a former NTSB chairman, Christopher Hart, to advise on Uber’s safety culture.\n“As their investigation continues, we’ve initiated our own safety review of our self-driving vehicles program,” the company said on Thursday, adding it planned to announce changes in the coming weeks.\nAll aspects of the self-driving system were operating normally at the time of the crash, and there were no faults or diagnostic messages, the NTSB said.\nElaine Herzberg, 49, was walking her bicycle outside the crosswalk on a four-lane road when she was struck by the Uber vehicle traveling 39 miles per hour (63 kph).\nA safety operator behind the wheel appeared to be looking down, and not at the road, moments before the crash, according to video from inside the car released by police. The operator told the NTSB she was not looking at a mobile phone but monitoring the vehicle’s self-driving systems.\nTempe police said on Wednesday it had completed its investigation and turned the findings over to prosecutors to review. Police did not release the results of the probe.\nThe NTSB said the Uber vehicle required the operator to intervene and take action, but the system was not designed to alert the operator. The report said the operator engaged the steering wheel less than a second before impact and began braking less than a second after impact.\nThe report noted that Herzberg tested positive for methamphetamine and marijuana, and that she did not look in the direction of the vehicle until just before impact.\nFILE PHOTO: U.S. National Transportation Safety Board (NTSB) investigators examine a self-driving Uber vehicle involved in a fatal accident in Tempe, Arizona, U.S., March 20, 2018. A women was struck and killed by the vehicle on March 18, 2018. National Transportation Safety Board\/Handout via REUTERS ATTENTION EDITORS - THIS IMAGE WAS PROVIDED BY A THIRD PARTY. - RC124083F3B0\n‘RECKLESS’\nWilliam Wallace, senior policy analyst for Consumers Union, the advocacy division of Consumer Reports, called Uber “reckless” and said the NTSB report “makes it clear that a self-driving car was tested on public roads when it wasn’t safe enough to be there, and it killed a pedestrian.” He added that the system “was far too dangerous to be tested off a closed track.”\nSome cities expressed hesitation about immediately allowing Uber to return to testing.\nPittsburgh mayoral spokesman Timothy McNulty said the mayor still “wants a full federal investigation of the accident and for Uber to agree to his demands specific to Pittsburgh testing before he would welcome them back to the city.”\nSacramento, California, is interested in having Uber and other developers test in the city but wants to ensure the companies follow all regulations.\n“The NTSB report really shines a light on the importance of safety and security of vehicles, so I think it hardens our stance a little bit on safety and security,” said Louis Stewart, Sacramento chief innovation officer.\nA spokesman for Ontario’s Ministry of Transportation said on Thursday it would ensure it was satisfied with the steps Uber has taken to ensure the safety of its automated vehicles, before resuming testing in Toronto.\nArizona’s governor in March suspended Uber’s permit for the testing, citing safety concerns.\nUber has said it considers self-driving technology important to the future of its ride services, although it is not clear how it fits into the plans of new Chief Executive Dara Khosrowshahi. He has revamped the company structure and cut expenses as Uber prepares for an initial public offering next year.\nThe NTSB did not say when it would release its final report on the accident. The agency typically issues its final conclusions at least a year after an accident.\nIt is also investigating a series of crashes involving Tesla Inc’s semi-autonomous “Autopilot” system after faulting the system last year after a fatal crash in Florida.\nReporting by David Shepardson; Additional reporting by Heather Somerville in San Francisco and Jim Finkle in Toronto; Editing by Dan Grebler and Peter Cooney\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about Uber's self-driving car involved in a fatal accident in Arizona. The National Transportation Safety Board (NTSB) reported that Uber had disabled an emergency braking system in the self-driving vehicle that struck and killed a pedestrian. The report also mentioned that the self-driving system software initially classified the pedestrian as an unknown object, then as a vehicle, and finally as a bicycle. At 1.3 seconds before impact, the self-driving system determined that emergency braking was needed, but Uber had disabled automatic emergency braking maneuvers in the vehicle to 'reduce the potential for erratic vehicle behavior.' The report revealed that the safety operator behind the wheel was not looking at the road moments before the crash, and the self-driving system did not alert the operator. The report noted that the Uber vehicle required operator intervention but was not designed to alert the operator. The operator engaged the steering wheel less than a second before impact and began braking less than a second after impact. Additionally, the report mentioned that the pedestrian had tested positive for methamphetamine and marijuana and did not look in the direction of the vehicle until just before impact. The news also highlighted Uber's decision to end testing in Arizona and focus on limited testing in other locations, as well as the reaction of various cities and organizations regarding the safety of self-driving technology.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-uber-crash-idUSKCN1IP26K","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":49,"Time":"Tue 2 Nov 2021 15.33 CET","Title":"Tesla software glitch prompts safety recall for nearly 12,000 US vehicles","Content":"Nearly 12,000 Tesla vehicles sold in the US since 2017 are the subject of a safety recall because a communication error may cause a false forward-collision warning or unexpected activation of the emergency brakes, the National Highway Traffic Safety Administration (NHTSA) said on Tuesday.\nThe California automaker said the concern over 11,704 Model S, X, 3 and Y vehicles was prompted after a software update on 23 October to vehicles in its limited early access version 10.3 Full-Self Driving (FSD) (Beta) population.\n‘The smartest person in any room anywhere’: in defence of Elon Musk, by Douglas Coupland\nRead more\nFSD is an advanced driver assistance system that handles some driving tasks but Tesla says does not make vehicles autonomous.\nNHTSA said Tesla “uninstalled FSD 10.3 after receiving reports of inadvertent activation of the automatic emergency braking system” and then “updated the software and released FSD version 10.3.1 to those vehicles affected”.\nThe agency said it “will continue its conversations with Tesla to ensure that any safety defect is promptly acknowledged and addressed”.\nThe recall comes after NHTSA last month asked Tesla why it had not issued a recall to address software updates made to its autopilot driver-assistance system to improve the vehicles’ ability to detect emergency vehicles.\nTesla said the issue was prompted by a software communication disconnect between two onboard chips that prompted an issue that could produce “negative object velocity detections when other vehicles are present”.\nIf the automatic emergency braking system unexpectedly activates while driving, it could raise the risk of a rear-end collision, Tesla said, but added it was not aware of any crashes or injuries related to the issue.\nAfter the 24 October reports, Tesla said canceled the FSD update on vehicles that had not installed it and disabled FCW and AEB on affected vehicles.\nThe same day, the Tesla chief executive, Elon Musk, tweeted of FSD: “Seeing some issues with 10.3, so rolling back to 10.2 temporarily. Please note, this is to be expected with beta software.”\nOn 25 October, Tesla began deploying the over-the-air software update and re-enabled FCW and AEB features on vehicles with the update.\nTesla said as of 29 October, more than 99.8% of the vehicles – all but for 17 – had installed an update and no further action is necessary.\nNHTSA in August opened a formal safety probe into Tesla’s autopilot system in 765,000 US vehicles after a series of crashes involving Tesla models and emergency vehicles.\nThe US auto safety agency also asked Tesla in October about its “Autosteer on City Streets” which the company also refers to as FSD first leased in October 2020, and raised concerns about limits on disclosure by drivers of safety issues.","Relevant":true,"Overview":"Here is the JSON format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"Tesla has issued a safety recall for nearly 12,000 US vehicles due to a software glitch. The recall was prompted by a communication error that may cause a false forward-collision warning or unexpected activation of the emergency brakes. The affected vehicles include Model S, X, 3, and Y vehicles, and the issue arose after a software update to vehicles in the limited early access version 10.3 Full-Self Driving (FSD) (Beta) population. The National Highway Traffic Safety Administration (NHTSA) had initiated conversations with Tesla regarding the safety defect, and Tesla took action by updating the software to address the issue. Tesla reported that it was not aware of any crashes or injuries related to the problem. This recall follows previous concerns raised by NHTSA about Tesla's autopilot system and related software updates.\"\n}\nThe news is indeed about autonomous driving system errors, specifically a software glitch in Tesla vehicles, as requested.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2021\/nov\/02\/tesla-recall-nearly-12000-us-vehicles-software-glitch","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":63,"Time":"11:32 PM GMT+1•December 14, 2016","Title":"Uber says self-driving car ran red light due to “human error”","Content":"Update 7 PM ET- An Uber spokesperson says the incident in the video above was definitely caused by an error on the part of a human driver – full statement:\nThis incident was due to human error. This is why we believe so much in making the roads safer by building self-driving Ubers. This vehicle was not part of the pilot and was not carrying customers. The driver involved has been suspended while we continue to investigate.\nUber is aware of the incident depicted in the video above, which appears to show one of its self-driving test SUVs running a red light in San Francisco, where it launched a live pilot of the cars in service early on Wednesday; the company tells TechCrunch it is investigating what exactly occurred. The video, published by the San Francisco Examiner, was captured by a dashcam mounted inside a vehicle operated by Luxor Cab, one of SF’s licensed cab companies.\nWhen contacted and asked for more information about the incident caught on the taxi’s camera, an Uber spokesperson provided the following statement:\nSafety is our top priority. This incident has been reported and we are looking into what happened.\nUber’s self-driving test vehicles in San Francisco have a human driver behind the wheel, as well as a co-pilot technician in the front passenger seat taking measurements and observing data about the rides. The human driver is there to take control of the vehicle in case of any incidents, including ones like this, in theory, where a vehicle might fail to come to a complete stop at an intersection.\nIt’s also possible the human driver was in control of the vehicle at the time of the incident in the video, which would obviously present its own set of problems. Early on in the company’s Pittsburgh trials, reports emerged of incidents with the Ford Fusion self-driving test vehicles it’s using in that city, including wrong-way driving.\nThat the cars are not able to fully drive themselves is part of the reason Uber has argued that it does not need to seek from the state’s Department of Motor Vehicles a permit to test its vehicles on California roads (a position which the DMV contests).\nWe’ve asked Uber to update us if they discover more about the conditions surrounding the incident in the video and will update when and if that becomes available.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Uber acknowledges that one of its self-driving test SUVs ran a red light in San Francisco. The incident was attributed to human error, and the vehicle was not part of the pilot program and was not carrying customers. The human driver is present in the self-driving test vehicles to take control in case of incidents, like failing to stop at an intersection. Uber is investigating the incident further.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2016\/12\/14\/uber-looking-into-incident-of-self-driving-car-running-a-red-light-captured-on-dashcam\/","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":71,"Time":"September 29, 2023","Title":"In Tesla trial over Autopilot fatality, lawyer cites 'experimental vehicles'","Content":"Companies\nTesla Inc\nFollow\nSAN FRANCISCO, Sept 28 (Reuters) - The lawyer representing victims of a fatal crash of a Tesla Model 3 car in his opening statement in court on Thursday blamed the company's Autopilot driver assistant system, saying that \"a car company should never sell consumers experimental vehicles.\"\nAn attorney for Tesla (TSLA.O), in the first U.S. trial over allegations that its Autopilot feature led to a death, however, said the crash was the result of \"a classic human error.\"\nThe trial, in a California state court, stems from a civil lawsuit alleging the Autopilot system caused owner Micah Lee’s Model 3 to suddenly veer off a highway east of Los Angeles at 65 miles per hour (105 kph), strike a palm tree and burst into flames, all in the span of seconds.\nThe 2019 crash killed Lee and seriously injured his two passengers, including an 8-year-old boy who was disemboweled, according to court documents. The lawsuit, filed against Tesla by the passengers and Lee's estate, accuses Tesla of knowing that Autopilot and other safety systems were defective when it sold the car.\nAdvertisement · Scroll to continue\nTesla said its Autopilot driver assistant system is not designed to make a sharp turn on a highway, as it defended the system’s safety.\nJonathan Michaels, an attorney for the plaintiffs, in his opening statement at the trial in Riverside, California, said that when the 37-year-old Lee bought Tesla's “full self-driving capability package” for $6,000 for his Model 3 in 2019, the system was in \"beta,\" meaning it was not yet ready for release.\nAdvertisement · Scroll to continue\n\"A car company should never sell consumers experimental vehicles,\" Michaels said.\nMichaels said the car's steering wheel made a sharp, 43-degree turn on a freeway, adding that “excessive steering command is a known issue at Tesla.\"\nTesla denied the claims, saying its Autopilot system puts \"guardrails\" on the angle of the steering wheel at high speeds, making it capable of steering only a little bit left or right on highways.\nAdvertisement · Scroll to continue\nTesla also blamed the driver for being intoxicated.\n\"The case is not about Autopilot,\" Michael Carey, an attorney for Tesla, said. \"Autopilot makes a road safer. It is a good thing,\" he said. \"It is a classic human error that caused the crash.\"\nThe electric-vehicle maker also claims it was not clear whether Autopilot was engaged at the time of the crash.\nTesla has been testing and rolling out its Autopilot and more advanced Full Self-Driving (FSD) system, which Chief Executive Elon Musk has touted as crucial to his company's future but which has drawn regulatory and legal scrutiny.\nAdvertisement · Scroll to continue\nTesla won a bellwether trial in Los Angeles in April over a Tesla crash related to its Autopilot feature with a strategy of saying that it tells drivers that its technology requires human monitoring, despite the \"Autopilot\" name. In that incident in 2019, a Model S swerved into a curb and injured the driver. Jurors told Reuters after the verdict that they believed Tesla warned drivers about its system and that driver distraction was to blame.\nThe stakes are higher in the trial this week, and in other cases, because people died. Tesla and plaintiff attorneys jousted in the run-up about what evidence and arguments each side could make.\nTesla, for instance, won a bid to exclude some of Musk’s public statements about Autopilot. However, attorneys for the crash victims can argue that Lee’s blood alcohol content was below the legal limit, according to court filings.\nThe trial, in Riverside County Superior Court, is expected to last a few weeks.\nReporting by Dan Levine and Hyunjoo Jin in San Francisco Editing by Matthew Lewis\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here's the JSON format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Summary\": \"The news is relevant to autonomous driving system errors.\",\n    \"Details\": [\n      {\n        \"Event\": \"In a Tesla Model 3 crash trial, the lawyer representing victims blamed Tesla's Autopilot system.\",\n        \"Description\": \"The lawyer argued that the crash was caused by the Autopilot driver assistant system and criticized Tesla for selling 'experimental vehicles' to consumers.\",\n        \"Date\": \"September 28, 2023\",\n        \"Location\": \"San Francisco\"\n      },\n      {\n        \"Trial Details\": \"The trial is the first U.S. trial over allegations that Tesla's Autopilot feature led to a fatal crash involving a Model 3.\",\n        \"Crash Description\": \"The crash involved a Tesla Model 3 that veered off a highway at high speed, struck a palm tree, and burst into flames, resulting in fatalities and serious injuries.\",\n        \"Lawsuit\": \"The lawsuit accuses Tesla of knowing that Autopilot and other safety systems were defective when selling the car.\",\n        \"System Status\": \"The lawyer argued that when the car's owner purchased Tesla's 'full self-driving capability package' in 2019, the system was in 'beta' and not ready for release.\"\n      },\n      {\n        \"Tesla's Defense\": \"Tesla defended its Autopilot system, stating that it is not designed for sharp turns on a highway and blamed the driver for being intoxicated.\",\n        \"Safety Claims\": \"Tesla claimed that its Autopilot system includes safety measures to limit the angle of the steering wheel at high speeds.\",\n        \"Trial Focus\": \"Tesla insisted that the case is not about Autopilot itself, and that Autopilot makes roads safer, attributing the crash to 'classic human error.'\"\n      },\n      {\n        \"Additional Information\": \"Tesla's Autopilot and Full Self-Driving systems have faced regulatory and legal scrutiny, and this trial has higher stakes due to fatalities.\"\n      }\n    ]\n  }\n}\nBased on the news, it is relevant to autonomous driving system errors, specifically concerning allegations against Tesla's Autopilot system in a fatal crash involving a Model 3. The JSON response provides detailed information about the case, arguments from both sides, and the context of the trial.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/tesla-trial-over-autopilot-fatality-kicks-off-california-2023-09-28\/","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":75,"Time":"Published 6:32 PM EST, Wed January 18, 2023","Title":"Human error more likely in JFK near collision after alert system failure ruled out","Content":"CNN\n — \nA source familiar with the near-collision of two airliners on a John F. Kennedy International Airport runway tells CNN that special flashing lights which warn pilots against taxiing across a runway were in fact working.\nThe new development further raises the possibility that human error contributed to the Friday evening incident.\nRegulators conducting interviews in JFK airport near-crash probe\nThe incident involved an American Airlines 777 that improperly crossed an active runway at the busy airport. “S**t!” an air traffic controller called out over the radio, urgently directing a Delta Air Lines 737 to stop its takeoff from the same runway where the American plane rolled across its direct path. The Delta plane stopped within 1,000 feet of the American jet.\nOn Wednesday, the source told CNN that following the incident, airport staff “went out immediately” to confirm Runway Status Lights were working properly.\nJFK is one of 20 airports in the United States equipped with the fully automated Federal Aviation Administration system that warns pilots “when it is unsafe to enter, cross, or takeoff from a runway.”\nOn Tuesday, the National Transportation Safety Board told CNN that “interviews are ongoing” as part of its investigation into the incident. The FAA is also investigating.\n- CNN’s Gregory Wallace contributed to this report","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to a near-collision incident at John F. Kennedy International Airport involving an American Airlines 777 that improperly crossed an active runway. The incident was further highlighted by the fact that special flashing lights, which warn pilots against taxiing across a runway, were confirmed to be working properly. The incident also included an air traffic controller urgently directing a Delta Air Lines 737 to stop its takeoff from the same runway, with the Delta plane stopping within 1,000 feet of the American jet. The National Transportation Safety Board and FAA are conducting investigations into the incident to determine if human error played a role.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2023\/01\/18\/business\/jfk-near-collision-human-error\/index.html","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":79,"Time":"5 October 2017","Title":"Drink-drive rules 'need updating for autonomous car era'","Content":"Drink-drive rules 'need updating for autonomous car era'\nPublished\n5 October 2017\nShare\nIMAGE SOURCE,\nBBC SPORT\nImage caption,\nWill occupants in self-drive cars be considered as drivers?\nDrivers under the influence of alcohol or drugs should be exempt from drink-drive laws if they are using autonomous cars, an Australian report has suggested.\nThe National Transport Commission (NTC) has recommended the change, comparing it to someone getting into a taxi.\nCurrent laws could be a \"barrier\" to the adoption of such vehicles, it said.\nMany countries are considering updates to the laws of the road to accommodate autonomous vehicles.\nThe NTC has been tasked with looking at the legislative changes necessary as self-drive vehicles become common on Australia's roads.\nSuch cars have already been trialled in the country, and commercial rollouts are expected by 2020.\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nA Google self-driving car being tested in California\nThe report considers many aspects of the law changes necessary to accommodate such vehicles, but one of the key issues it addresses is deciding who would be responsible - the person in the vehicle or the autonomous driving system (ADS) that is operating it.\n\"The NTC believes that the introduction of automated vehicles will have overall safety benefits for the road network by reducing the risk of human error,\" the report says.\n\"Enabling people to use an automated vehicle to drive them home despite having consumed alcohol has the potential to improve road-safety outcomes by reducing the incidence of drink-driving.\"\nIt does not recommend drivers under the influence of alcohol or drugs be exempt if they are in semi-autonomous vehicles or cars that allow a switch-over to manual driving.\nBen Gardner, an associate at law firm Pinsent Masons, said that the technology had a long way to go before such changes would become necessary.\n\"The technology is not quite there for full autonomous vehicles and, as long as we require a human to gain control if needed, it would not be right for them to get drunk,\" he said.\nA recent report from Pew Research indicated 87% of US adults favoured policies that would see a person in the driver's seat who could take control of an autonomous vehicle if needed.\nAnd 83% thought such vehicles should travel in dedicated lanes.\nMore on this story\nDriverless cars 'confused' by kangaroos\nPublished\n27 June 2017\nWill you need a driving licence in the age of self-driving cars?\nPublished\n31 July 2017","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news discusses the need to update drink-drive rules for the autonomous car era, with a recommendation from the National Transport Commission (NTC) in Australia to exempt drivers under the influence of alcohol or drugs when using autonomous cars. The report also addresses the potential benefits of automated vehicles in reducing the risk of human error and improving road safety by reducing drink-driving incidents. However, it does not recommend such exemptions for semi-autonomous vehicles or cars with manual driving capabilities. The report also mentions that the technology for full autonomous vehicles may not be advanced enough yet, according to an associate at law firm Pinsent Masons. Additionally, it highlights public opinion on the matter, with a significant percentage of US adults favoring the presence of a person in the driver's seat who can take control if needed and the use of dedicated lanes for autonomous vehicles.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-41511690","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":80,"Time":"12 April 2022","Title":"Self-driving car stopped by San Francisco police","Content":"Self-driving car stopped by San Francisco police\nPublished\n12 April 2022\nShare\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nCruise has been developing autonomous-driving technology\nBy Chris Vallance\nTechnology Reporter\nSan Francisco police officers were faced with a new challenge after stopping an autonomous vehicle with no-one inside.\nOfficers approached the car, operated by Cruise, because it had been driving without headlights.\nIn the incident, captured on film, an officer remarks that there \"ain't nobody in it\" before the car moves to what Cruise said was a safer location.\nThe company said the headlights issue was a result of human error.\nSkip instagram post by b.rad916\nAllow Instagram content?\nThis article contains content provided by Instagram. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Meta’s Instagram cookie policy\n, external\nand privacy policy\n, external\nbefore accepting. To view this content choose ‘accept and continue’.\nAccept and continue\nThe BBC is not responsible for the content of external sites.\nEnd of instagram post by b.rad916\nCruise, which is owned by General Motors and develops self-driving technology, has been testing autonomous vehicles (AVs) in the US.\nEarlier this year it started allowing members of the public who join a waiting list to hail free night-time rides.\nCruise tweeted: \"Our AV yielded to the police vehicle, then pulled over to the nearest safe location, as intended. An officer contacted Cruise personnel and no citation was issued.\"\nSkip twitter post by cruise\nAllow Twitter content?\nThis article contains content provided by Twitter. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Twitter’s cookie policy\n, external\nand privacy policy\n, external\nbefore accepting. To view this content choose ‘accept and continue’.\nAccept and continue\nThe BBC is not responsible for the content of external sites.\nEnd of twitter post by cruise\nThe company has produced a YouTube video for law-enforcement and other \"first responders\", setting out how they should interact with AVs.\nIt says in the video that the vehicles have microphones that can identify the sounds of sirens.\nThe \"AV can detect lights and sirens so it will come to a stop\".\nThe video explains that officers should call a dedicated phone number to reach the company's \"escalation team\" before approaching a vehicle.\n\"For example, the escalation team can do a number of tasks remotely, including unlocking the vehicle... and ensuring that the vehicle remains in a safe stationary position.\"\nLegal challenges\nThe law commissions for England and Wales and the Scottish Law Commission have called for legal changes to deal with self-driving vehicles.\nThe independent bodies, which monitor and review UK legislation, recommended that if anything goes wrong, the company behind the autonomous driving system would be responsible, rather than the driver.\nDriverless car glitches have inevitably cropped up as the technology develops - including the experience of people living in a quiet San Francisco street, who were deluged with an influx of self-driving vehicles.\nOther car giants, including Elon Musk's electronic car company Tesla, are working on their own driverless technology.\nRelated Topics\nDriverless cars\nUnited States\nMore on this story\nMajor legal changes needed for driverless cars\nPublished\n26 January 2022\nDriverless passenger shuttle trial under way\nPublished\n30 November 2021\nWaymo self-driving taxi confused by traffic cones\nPublished\n17 May 2021","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"San Francisco police officers stopped an autonomous vehicle operated by Cruise because it had been driving without headlights. In a captured video of the incident, an officer noticed that there was nobody inside the autonomous car, and the car moved to a safer location as per Cruise's claim. Cruise stated that the headlight issue was due to human error. The company mentioned that their autonomous vehicles have microphones to detect sirens and can come to a stop when necessary. The law commissions for England and Wales and the Scottish Law Commission have recommended legal changes that would hold the company responsible for self-driving vehicle errors instead of the driver.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-61080666","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":76,"Time":"10:05 PM GMT+2•April 20, 2022","Title":"Billionaire CEO’s US Senate run focuses on one issue: Banning ‘self-driving’ Teslas","Content":"cameraImage Credits: Tesla\nDan O’Dowd, the billionaire founder of The Dawn Project and Green Hills Software, is running for U.S. Senate on a single issue. It’s not unemployment or inflation or even education.\nIt’s about making computers safer for humans, starting with a ban on the Tesla-branded “Full Self Driving” beta software that some 100,000 owners currently have access to as part of a safety mission that takes inspiration from Ralph Nader.\n“Today I launched my campaign for U.S. Senate to make computers safe for humanity. The first danger I am tackling is @ElonMusk‘s reckless deployment of unsafe @Tesla Full Self-Driving cars on our roads,” O’Dowd tweeted April 19. \nThe tweet included a 60-second ad that shows clips of various Tesla vehicles equipped with the FSD software nearly hitting pedestrians, cyclists or making other errors while an ominous voice explains the failures and at one point says “sometimes the Tesla doesn’t seem to want the driver to take over.”\nThe ad will be broadcast nationally, according to O’Dowd’s campaign website, which could give a whole new group of consumers insight into the product. \nTesla vehicles come standard with a driver-assistance system branded as Autopilot. For an additional $12,000, owners can buy “full self-driving,” or FSD — software that Tesla CEO Elon Musk has repeatedly promised will one day deliver full autonomous driving capabilities.\nFSD, which has steadily increased in price and has added new functions, has been available as an option for years. However, Tesla vehicles are not self-driving. FSD includes the parking feature Summon as well as Navigate on Autopilot, an active guidance system that navigates a car from a highway on-ramp to off-ramp, including interchanges and making lane changes.\nThe latest FSD Beta is supposed to automate driving on highways as well as city streets. This is still a Level 2 driver-assistance system that requires the driver to pay attention, have their hands on the wheel and take control at all times. Numerous videos posted since last fall showing owners’ experiences provide a mixed picture of the software’s capability. In some videos, the vehicles handle city driving; in many others, drivers are seen taking control due to missed turns, being too close to the curb, failure to creep forward, veering off suddenly toward pedestrians and in at least one incident, hitting a stationary object.\nTesla began using personal driving data last September to determine whether owners who paid for its “Full Self-Driving” software would be able to access the latest beta version. Musk has provided frequent updates about the FSD software, stating as recently as a few weeks ago that the beta software would be rolled out to all customers in North America this year.\nO’Dowd says he is “running to draw attention to this important safety issue: Tesla’s Full Self-Driving is defective and should be banned from our roads immediately.”\nOn his campaign website, O’Dowd notes that “connecting the power grid, hospitals, and millions of cars to the Internet with software riddled with bugs and security defects has turned these systems into potential weapons of mass destruction at the mercy of hackers.” But the first danger, in O’Dowd’s view is the deployment of Tesla’s branded FSD software.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous driving system errors. The billionaire CEO, Dan O’Dowd, is running for the U.S. Senate on a single issue, which is making computers safer for humans, starting with a ban on Tesla's 'Full Self Driving' beta software. The news highlights concerns about the safety of Tesla's autonomous driving features, including instances where Tesla vehicles equipped with the FSD software nearly hit pedestrians, cyclists, or made other errors. It also mentions that Tesla's FSD is still a Level 2 driver-assistance system that requires the driver to pay attention and take control at all times, despite being marketed as 'Full Self Driving.' O’Dowd's campaign is focused on drawing attention to the safety issues associated with Tesla's FSD software and advocating for its immediate ban from the roads.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/04\/20\/billionaire-ceos-us-senate-run-focuses-on-one-issue-banning-self-driving-teslas\/","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":86,"Time":"28 April 2021","Title":"'Self-driving' cars to be allowed on UK roads this year","Content":"'Self-driving' cars to be allowed on UK roads this year\nPublished\n28 April 2021\ncomments\nComments\nShare\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nDrivers will be allowed to take their hands off the wheel if ALKS is enabled\nBy Cristina Criddle\nTechnology reporter\n\"Self-driving\" vehicles could be allowed on UK roads by the end of this year, the government has said.\nThe Department for Transport said automated lane-keeping systems (ALKS) would be the first type of hands-free driving legalised.\nThe technology controls the position and speed of a car in a single lane and it will be limited to 37mph (60km\/h)\nBut insurers have warned the government's definition of ALKS as \"self-driving\" is misleading.\nPreviously, the government had said these new laws would be in place by spring this year and told the BBC there was no delay in its suggested timeframes.\nHands-free\nFollowing a consultation last year, the government has now said that vehicles with ALKS technology can be legally defined as self-driving, “as long as they receive GB type approval and that there is no evidence to challenge the vehicle’s ability to self-drive.”\nThe government confirmed that drivers will not be required to monitor the road or keep their hands on the wheel when the vehicle is driving itself.\nBut the driver will need to stay alert and be able take over when requested by the system within 10 seconds.\nIf a driver fails to respond, the vehicle will automatically put on its hazard lights to warn nearby vehicles, slow down and eventually stop.\nThe Highway Code is now consulting on what rules will be put into new laws to make sure the technology is safely used.\nMedia caption,\nWhat happens if you fall asleep in a self-driving car?\n“This is a major step for the safe use of self-driving vehicles in the UK, making future journeys greener, easier and more reliable while also helping the nation to build back better,” said Transport Minister Rachel Maclean.\n“But we must ensure that this exciting new tech is deployed safely, which is why we are consulting on what the rules to enable this should look like.”\nHuman Error\nThe technology could improve road safety by reducing human error, the Society of Motor Manufacturers and Traders said.\n“Automated driving systems could prevent 47,000 serious accidents and save 3,900 lives over the next decade through their ability to reduce the single largest cause of road accidents – human error,” said chief executive Mike Hawes.\n“Technologies such as Automated Lane Keeping Systems will pave the way for higher levels of automation in future – and these advances will unleash Britain’s potential to be a world leader in the development and use of these technologies, creating essential jobs while ensuring our roads remain among the safest on the planet.”\nAutopilot\nTesla's so-called \"Autopilot\" uses lane technology similar to ALKS. It is considered \"level two\" on the five defined levels of self-driving cars.\nThe next step - level three - would not need the driver's attention at all times, and in theory, the driver could do other things such as check email or even watch a movie - until the car prompts them to take over again. This is would include ALKS.\nHowever, there have been several incidents involving this technology when drivers did not pay enough attention to the road.\nIn 2018, a Nottingham resident was banned from driving after climbing into the passenger seat of his Tesla on the motorway, letting it do the driving.\nA fatal crash in the United States was caused, in part, by the driver playing a video game while leaving his car in \"Autopilot\" mode, before it drove into a concrete barrier.\nA recent report found that the feature can be “easily tricked” into operating without a driver, adding to the mounting criticisms over the marketing of these features as \"self-driving\", and whether that is misleading to customers.\n'Tragic deaths'\nThatcham Research, which conducts safety tests for motor insurers, urged caution over defining ALKS as 'self-driving', as the government has done.\n\"ALKS as currently proposed by the government are not automated,\" said Matthew Avery, director of research at Thatcham Research. \"They are assisted driving systems as they rely on the driver to take back control.\n\"Aside from the lack of technical capabilities, by calling ALKS automated our concern also is that the UK government is contributing to the confusion and frequent misuse of assisted driving systems that have unfortunately already led to many tragic deaths.\n\"Consumers will expect the car to do the job of a driver, which current models can't do.\"\nThe autonomous car industry and the UK government share a sunny optimism about progress towards our self-driving future - but in recent years many of their predictions have failed to come to pass.\nIn his 2017 Budget the then Chancellor Philip Hammond promised that driverless cars would be on British roads by 2021.\nWhen I emailed a Treasury official to ask whether that really meant an autonomous vehicle with no safety driver the answer came back in block capitals YES - THIS WOULD MEAN FULL SELF-DRIVING CARS WITH NOBODY BEHIND THE WHEEL.\nThat has obviously not happened and even today's announcement takes us only a short way down that road.\nSystems such as Tesla's Autopilot have been keeping cars in lane on British roads for some years - and at speeds far higher than the 37mph envisaged in the new regulations.\nThe danger is that calling this a \"self-driving\" feature will make motorists over confident. The road to full autonomy has plenty of technical and regulatory challenges - and the trickiest section of the journey is where drivers can begin to let the car take over but still need to stay alert.\nThe government is keen to push the idea that the UK can be a leader in self-driving technology - but safety experts are urging caution about the pace at which the robots can be allowed to take the wheel.\nRelated Topics\nCar Technology\nTesla\nDriverless cars\nTransport\nRoad safety\nMotoring\nMore on this story\nHands-free driving could be made legal on UK roads\nPublished\n19 August 2020\nWill robot drivers rule the road?\nPublished\n20 May 2019\nUK wants fully autonomous cars on road\nPublished\n6 February 2019\nView comments","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about the UK government allowing 'self-driving' vehicles with automated lane-keeping systems (ALKS) on UK roads by the end of the year. ALKS technology controls the position and speed of a car in a single lane and will be limited to 37mph (60km\/h). Drivers will not be required to monitor the road or keep their hands on the wheel when the vehicle is driving itself, but they must stay alert and be able to take over when requested by the system within 10 seconds. The news also mentions that the Highway Code is consulting on rules to ensure safe technology use. Additionally, the Society of Motor Manufacturers and Traders believes that such technology could reduce road accidents caused by human error.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-56906145","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":83,"Time":"7:28 PM GMT+1•February 1, 2018","Title":"California DMV disengagement report reveals self-driving improvements","Content":"California’s Department of Motor Vehicles releases an annual report detailing the number of disengagements reported by companies it has licensed to test autonomous vehicles on public roads in the state. This year, the report reveals some interesting details about the progress of some of those companies, including Waymo, GM’s Cruise and Tesla (sort of – you’ll see what I mean).\nCruise’s numbers were very positive, relatively speaking. The company’s reported around a 1400% improvement in performance, with the number of average miles between disengagements climbing from around 300 miles between each to aver 4,600.\nTo step back, the CA DMV classifies a disengagement as any event where a human safety driver has to assume manual control of the vehicle in order to 1. intervene in the interest of safety, or 2. take control because the AV system has failed or disengaged on its own.\nCruise’s increase sheds light on why the company and GM seem so confident it’ll be able to launch an actual initial commercial service using autonomous vehicles sometime in 2019, a date it revealed it was targeting earlier this year. Jalopnik reported that one of the company’s disengagements wasn’t in the report, but the company noted that the system wasn’t taken over by the driver due to either safety concerns or a system failure in that instance, hence its omission.\n“Cruise’s rapid rate of improvement, in the most complex testing environments, is why we’re confident in our ability to safely deploy self-driving cars in 2019,” Cruise said about the report in a statement to TechCrunch. For example, in November 2017 alone, our reported disengagement rate is equivalent to driving from SF to Detroit and back, in dense traffic the entire time.”\nWaymo also saw its performance increase based on the year-over-year disengagement data, and the company tells us that has now driven over 4 million miles across the U.S., including 2 million miles driven just last year. Its total disengagement rate dropped to around 0.18 events per 1,000 miles driven, which is about 5,555 miles between disengagements on average – better than Cruise’s rate, though Cruise has claimed its focus on testing in dense urban settings like San Francisco makes its training more challenging.\nAs for Tesla, it says it didn’t spend any time during the year testing AVs on California roads despite having permission to do so. Tesla’s Autopilot software is considered a Level 2 semi-automated technology, so it doesn’t fall under the category of use covered by the DMV’s AV rules. Tesla also said that the data it gathers from all its cars on the road, combined with AV testing performed “in other settings, including on public roads in various other locations around the world” lets it develop its autonomous tech “more efficiently” than others focused on a few specific spots.\nMore TechCrunch\nMistral AI makes its first large language model free for everyone\nTinder goes ultra-premium, Amazon invests in Anthropic and Apple explains its new AirPods\nWhy Solana, Polygon and Aptos expect the enterprise to drive mass adoption\nPitch Deck Teardown: Point.me's $10M Series A deck","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous driving systems. It discusses the California Department of Motor Vehicles' annual report on disengagements reported by companies testing autonomous vehicles. The report mentions improvements in performance, including a 1400% improvement in Cruise's disengagement rate, Waymo's increased performance, and Tesla's approach to autonomous technology development. It also highlights Cruise's confidence in deploying self-driving cars in 2019 based on their rapid rate of improvement.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2018\/02\/01\/california-dmv-disengagement-report-reveals-self-driving-improvements\/","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":90,"Time":"Published 11:17 AM EDT, Wed September 7, 2022","Title":"Tesla’s ‘full self-driving’ isn’t worth $15,000, say many who bought it","Content":"Washington, DC\nCNN\n — \nTesla’s price for the feature it calls “full self-driving” has increased again. This time to $15,000, making it an unusually expensive option for a car.\n“After wide release of FSD Beta 10.69.2, price of FSD will rise to $15k in North America on September 5,” Musk tweeted last month.\nThe release of the new software version was delayed on Monday until “probably end of week. Needs a bit more polish,” Musk said, but the price increase took effect.\nSome Tesla owners who paid far less for an earlier, unfinished version of the same “full self-driving” software that now retails for $15,000 caution that the new price does not make sense for most buyers.\nCNN Business spoke with 13 people who have cars with the “full self-driving” beta and the overwhelming majority, 11 people, said they felt it wasn’t worth $15,000. These drivers say the software has improved, but not enough to warrant the price. They describe at times having to pay more attention while using “full self-driving” than when driving on their own, as the car can be unpredictable. More than 100,000 Tesla vehicles have been sold with the feature.\nTesla buyers can purchase “full self-driving” outright when they buy their car, or they may opt to pay a monthly subscription fee to use the feature. Many of the Tesla drivers who spoke with CNN Business suggested that, were they in the market today for a Tesla, they would opt for the monthly subscription fee for “full self-driving,” which still costs $199 a month or purchase for the more affordable Enhanced Autopilot, which combines features like automated parking and automated lane changes on highways, but isn’t so clearly an unfinished, imperfect product.\nThree people told CNN Business that they feel that “full self-driving” is worth the high price to them because they personally relish testing new technologies and are excited about the promise of what “full self-driving” might eventually become.\n“I really admire the optimism of all these people,” Brian Moody, executive editor of the online car marketplace AutoTrader.com, said of those who believe “full self-driving” may soon handle all driving responsibilities despite years of delays. “I wish I could be that way.”\nMoody told CNN Business that “full self-driving” is especially expensive compared to other available driver-assist packages like GM’s SuperCruise and Nissan ProPilot Assist. SuperCruise generally costs about $2,500 – plus a monthly $25 connectivity fee after the third year – and ProPilot Assist can be under $2,000 in some cases.\nThe promised functionality of these driver-assist packages is more modest. While they steer and keep up with traffic on some highways, Musk says that “full self-driving” will someday allow Tesla owners to operate their vehicles as robotaxis that do not need a human behind the wheel.\nIt’s not unheard of for options to reach the $10,000 threshold, but such high-priced options typically include a range of add-ons like an adaptive suspension, engine upgrade and adaptive headlights, he said.\nMoody said he expects consumers to pay for the monthly subscription of “full self-driving” rather than pay $15,000 for the promise of the feature’s future potential.\nTaking the full plunge\nFour years ago, Wisam Al-Rawi bought a Tesla equipped with the driver-assist software the company calls “full self-driving.” He says he was happy to pay a $2,500 premium for “full self-driving” in anticipation that his Tesla would one day drive itself. But last month Al-Rawi, frustrated by the software’s flaws, had enough. He asked Tesla to remove it.\nNow the California resident is surprised to see that Tesla is raising the price of “full self-driving” to $15,000 even as it falls short of the automaker’s projections.\n“I would never even pay $10,000 for it,” Al-Rawi told CNN Business. “They overpromised like crazy.”\nBut Tesla feels that the feature, which the company says remains in testing (or “beta”) despite its wide release, will be worth much more than even the price it’s selling for now.\n“The FSD price will continue to rise as the software gets closer to full self-driving capability with regulatory approval,” Musk wrote on Twitter in 2020. “[At] that point, the value of FSD is probably somewhere in excess of $100,000.”\nBut “full self-driving” wasn’t close to what Al-Rawi heard from Musk, who has claimed every year from 2015 to 2022 that self-driving Teslas were probably a year or two away.\nAl-Rawi said it was stressful to use the “full self-driving” beta because the software would sometimes try to hit curbs or drive on the wrong side of the road. His car would brake unexpectedly at times, triggering road rage from others.\n“He promised your car can be a taxi. This is going to be generating income,” Al-Rawi said. “Unfortunately, none of this happened.”\nAl-Rawi still believes that Tesla’s driver-assist features are better than any other automaker. He calls the automaker’s lane-keeping feature “fantastic.” The “full self-driving” feature, which navigates local roads with steering, braking and acceleration, is where Al-Rawi had problems.\nAl-Rawi is far from the only one to be concerned about Teslas and unexpected braking. The National Highway Traffic Safety Administration, which regulates motor vehicles, launched an investigation in February following hundreds of complaints of unexpected braking.\nBut the limitations of “full self-driving” haven’t led many Tesla owners to regret their purchase.\n“There’s nothing in FSD that’s actually that useful, certainly not for this kind of money,” Logan Rossignol, a Tesla “full self-driving” beta tester, told CNN Business. “You’re still paying for a promise. And what’s that’s worth to you is up to you.”\nRossignol feels the investment was worth it for him because he is a software developer and enjoys experimenting with technology. “Full self-driving” changes regularly due to over-the-air software updates. Drivers wake up some mornings with a new version of the technology to test and play with.\nSunny Gulati, who said he paid $8,000 for “full self-driving,” says he feels an “inherent joy” from watching his Tesla try to navigate roads. He said he no longer envies other cars on the road. The system doesn’t have to be perfect for him to love it.\nHe justified the purchase of “full self-driving” in part because he thought it could one day make having a second car unnecessary. Gulati said that he wouldn’t have been able to afford $15,000 for “full self-driving,” and would opt for the monthly subscription fee if purchasing today.\nStaying mobile\nDon Burke bought his Model S in 2019 because he wanted a vehicle that might one day drive itself.\nThe Virginia resident said he has a form of muscular dystrophy. His legs are weakening. He will be in a wheelchair in a few years.\nBurke spoke with CNN Business during a multi-month road trip around the United States.\nHe’s driving as much as 600 miles a day and says that Tesla’s driver-assist technology makes him a much safer driver. He said he feels humans are terrible drivers, pointing to the many deaths on roads today, and believes machines will be better.\nBurke said that if he was in the market again for a Tesla, the $15,000 price for “full self-driving” would be worth it to him.\n“I love being able to be mobile,” Burke said. “I desperately want a machine that I can own that can drive me to where I need to be when I get in a wheelchair.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to the topic of Tesla's 'full self-driving' feature. The article discusses how Tesla has raised the price of the 'full self-driving' feature to $15,000 and includes opinions from Tesla owners who have expressed concerns about the value of the feature. Some owners feel that the software is not worth the high price, citing issues with its performance, while others find value in it for various reasons, such as testing new technology or accommodating mobility challenges. The article also mentions that Tesla's 'full self-driving' feature remains in testing (or 'beta') despite its wide release and mentions concerns about unexpected braking issues.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2022\/09\/07\/business\/tesla-fsd-price-increase\/index.html","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":87,"Time":"Thu 15 Dec 2016 13.25 CET","Title":"Uber blames humans for self-driving car traffic offenses as California orders halt","Content":"California regulators ordered Uber to remove its self-driving vehicles from the road on the same day that the company’s vehicles were caught running red lights – violations the company immediately blamed on “human error”.\n“It is essential that Uber takes appropriate measures to ensure safety of the public,” the California department of motor vehicles (DMV) wrote to Uber on Wednesday after it defied government officials and began piloting the cars in San Francisco without permits. “If Uber does not confirm immediately that it will stop its launch and seek a testing permit, DMV will initiate legal action.”\nAn Uber spokesperson said two red-light violations were due to mistakes by the people required to sit behind the steering wheel and said the company has suspended the drivers.\nA video posted by Charles Rotter, an operations manager at Luxor, a traditional cab company, shows one of Uber’s computer-controlled cars plowing through a pedestrian crosswalk in downtown about four seconds after the light turned red. Elsewhere, a photo from a San Francisco writer showed one of the Uber vehicles entering an intersection against a red light.\n“People could die,” Rotter said in an interview later. “This is obviously not ready for primetime.”\nPeople could die. This is obviously not ready for primetime\nCharles Rotter\nThe traffic violations and threat of legal action are a significant blow to Uber in its home town, where the California department of motor vehicles has said that Uber requires permits to test the technology on its roads.\nDespite that stated mandate from a government agency, Uber declared in a blogpost that it did not believe it needed a “testing permit” to launch self-driving vehicles in San Francisco, arguing that the rules don’t apply since the cars have people in them monitoring movements.\n“Most states see the potential benefits, especially when it comes to road safety,” wrote Anthony Levandowski, head of Uber’s advanced technology group. His post announcing the Wednesday launch noted the Volvo XC90s’ “core safety capabilities”.\nIn his letter to Levandowski, the DMV’s deputy director, Brian Soublet, noted that 20 companies have already been approved to test self-driving vehicles in California.\n“They are obeying the law and are responsibly testing and advancing their technology,” he said, adding, “This technology holds the promise of true safety benefits on our roadways, but must be tested responsibly.”\nThe self-driving vehicles of the popular car-sharing company were first unveiled in Pittsburgh in September. The vehicles have technology that allows them to navigate on their own, though licensed drivers sit behind the wheel and can take control as necessary.\nAnnie Gaus, a San Francisco writer and producer, said she was riding to work on Wednesday in a Lyft, Uber’s biggest competitor, when she saw one of the Uber self-driving vehicles nearly crash into her.\nJust passed a 'self-driving' Uber that lurched into the intersection on Van Ness, on a red, nearly hitting my Lyft.\n— Annie Gaus (@AnnieGaus) December 14, 2016\nShe tweeted a photo of the car in the intersection after it ran the red light.\n(Not enough time to get a good shot, but...whoops!) pic.twitter.com\/XK49nMF2Q4\n— Annie Gaus (@AnnieGaus) December 14, 2016\n“The Uber car sort of jutted out into the intersection,” she told the Guardian by phone, noting that she and her Lyft driver were both taken aback. “It was close enough that we were both kind of like, ‘Woah.’ It’s close enough that you kind of react and are sort of rattled.”\nGaus, who has written about technology and has contributed to the Guardian, said the red-light violations on day one of the pilot seem to highlight how the implementation of the technology in a place like San Francisco may be premature.\n“I don’t think anybody has a good understanding of how this works in a city context.”\nAn Uber spokesperson said both cars running red lights were not part of the pilot and were not carrying customers.\n“These incidents were due to human error. This is why we believe so much in making the roads safer by building self-driving Ubers,” the statement said. “The drivers involved have been suspended while we continue to investigate.”\nThe company did not immediately respond to questions about the state’s order to remove the cars from the road.\nIt’s unclear how law enforcement may address these kinds of violations.\nAsked how the San Francisco police department would respond to a self-driving Uber running a red light, officer Giselle Talkoff said: “I don’t even know. I guess we could pull them over.”\nAfter the Guardian sent Talkoff footage of the Uber violation, first reported by the San Francisco Examiner, she said the police were not investigating the specific incident. But she noted officers would follow up in cases in which there was an injury or if they witnessed a violation in person – though she said she wasn’t sure if the “secondary driver” or the company would be held accountable.\n“There was a person that was walking very closely,” she said of the footage, pointing out that a pedestrian was entering the street when the Uber car ran the red.\nTalkoff further noted that there aren’t state or federal laws governing self-driving cars.\n“First comes technology, then comes policy. It’s going to be a matter of setting some precedents,” she said, adding, “The companies that are putting these vehicles on the road should have their vehicles operate with due regard to the rules of the road.”\nA sergeant with the police traffic division said his department was not even aware that Uber had started using autonomous cars in San Francisco.\nRotter argued that the technology company should be held responsible for sending the vehicles out on the road despite government objections.\n“What this company has done is start operating illegally and push for permission later.”\nContact the author: sam.levin@theguardian.com","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about Uber's self-driving car traffic offenses in California. The California Department of Motor Vehicles (DMV) ordered Uber to remove its self-driving vehicles from the road due to violations, including running red lights. Uber initially blamed these violations on 'human error' by the drivers required to sit behind the steering wheel. The violations raised concerns about the safety of self-driving technology in urban settings like San Francisco, where the incidents occurred. The DMV highlighted the need for responsible testing of self-driving technology, and there was debate about whether Uber needed testing permits for its self-driving vehicles. Law enforcement's response to self-driving car violations, the absence of state or federal laws governing self-driving cars, and the company's actions were also discussed in the news.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/dec\/14\/uber-self-driving-cars-run-red-lights-san-francisco","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":89,"Time":"Tue 7 Jun 2016 17.42 CEST","Title":"Insurer launches UK's 'first driverless car policy'","Content":"The company Adrian Flux has launched what it claims to be the UK’s first personal driverless car insurance policy.\nThe policy is designed for consumers who already have driverless features in their cars, such as self-parking, or are thinking of buying a car with autopilot features. Fully self-driving cars are not expected to be on the road until 2020 at the earliest, when Volvo has said it plans to launch such vehicles.\nGerry Bucke, the general manager of Adrian Flux, said: “We understand this driverless policy to be the first of its kind in the UK – and possibly the world … More than half of new cars sold last year featured autonomous safety technology, such as self-parking or ABS [anti-lock braking systems], which effectively either take control or take decisions on behalf of the driver. And it’s only going to continue. Driverless technology will become increasingly common in our cars over the next few years.”\nTesla Autopilot software, released late last year, and Nissan Motor’s Infiniti Q50 technology allow drivers to take their hands off the wheel in certain circumstances.\nAdrian Flux said said its policy would be updated as both the liability debate and driverless technology evolved. It is estimated that human error is at least partly responsible for more than 90% of current road fatalities.\nAs a result, a move to self-driving cars is expected to reduce car insurance premiums in the long term. Norfolk-based Adrian Flux, which has 600,000 customers, said that with the potential reduction in accidents, the new policy could be cheaper than standard ones, although other factors, such as location and overnight parking, remained important.\nBucke said: “We already provide discounts for cars fitted with assistive technology, such as autonomous braking, as it has been proved to reduce accidents, and therefore claims.”\nThe driverless policy has additional features to a standardone. Customers will be covered for loss or damage in case of: failure to install vehicle software updates and security patches, subject to an increased policy excess; satellite failure or outages affecting navigation systems, or failure of the manufacturer’s vehicle operating system or other authorised software; loss or damage caused by a failure to manually override the system to prevent an accident should the system fail; and loss or damage if the car gets hacked.\nThe modern transport bill announced in last month’s Queen’s speech will extend compulsory cover to accidents where the car, rather than the driver, is at fault. The Association of British Industry welcomed the move and is working with the government on insurance and liability issues, to determine when manufacturers rather than drivers are responsible.\nAnother major issue is the self-driving car’s vulnerability to hacking. The Jeep manufacturer, Chrysler, was forced to recall cars after researchers showed they could take control of the car via simple text messages.\nTesting of driverless cars in the UK has already begun, with the government funding a number of projects. The first trial of self-driving cars in public pedestrianised areas is with the Lutz Pathfinder in Milton Keynes. It involves electric two-seater pods made by the Coventry-based engineering firm RDM and equipped with autonomous control systems developed by the University of Oxford’s mobile robotics group. Volvo has announced it will start testing self-driving cars on public roads in London next year.\nA recent survey of nearly 1,800 customers by Adrian Flux revealed very few would consider buying one.\nOf those who answered “not likely” to owning a self-driving vehicle, just over 45% said they didn’t like the idea of giving up control to a computer, while 36% said they enjoyed driving too much to hand over the wheel.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous driving systems as it discusses the launch of the UK's first personal driverless car insurance policy by Adrian Flux. The policy is designed for consumers who have driverless features in their cars, such as self-parking or autopilot features. It also mentions Tesla Autopilot software and Nissan Motor's Infiniti Q50 technology, which allow drivers to take their hands off the wheel in certain circumstances. The policy covers various aspects related to autonomous driving, such as software updates, satellite failures, and the potential for hacking. The article also touches upon the reduction in car insurance premiums due to the expected decrease in accidents with the adoption of self-driving cars. Finally, it mentions ongoing testing and trials of self-driving cars in the UK.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/business\/2016\/jun\/07\/uk-driverless-car-insurance-policy-adrian-flux","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":97,"Time":"February 8, 2022","Title":"Two U.S. senators raise 'significant concerns' with Tesla driving systems","Content":"Companies\nTesla Inc\nFollow\nWASHINGTON, Feb 8 (Reuters) - Two U.S. Senate Democrats wrote to Tesla Inc's (TSLA.O) chief executive on Tuesday, raising \"significant concerns\" about the electric-car maker's Autopilot and Full Self-Driving (Beta) systems which have prompted scrutiny from government safety regulators.\n\"We are deeply troubled by Tesla’s design choices that seemingly encourage unsafe driving habits,\" senators Richard Blumenthal and Ed Markey wrote, finding fault with Tesla's decision to allow vehicles using its Full-Self Driving (Beta) system to roll through stop signs at low speeds.\nUnder pressure from regulators, Tesla last week agreed to recall about 54,000 U.S. vehicles to revise software to prevent vehicles from disregarding stop signs. Tesla Chief Executive Officer Elon Musk said on Twitter last week \"there were no safety issues\" with the function.\nTesla did not immediately comment on Tuesday.\nTesla has been testing the improved version of its automated driving software on public roads, but the carmaker and the National Highway Traffic Safety Administration (NHTSA) have said the features do not make the cars autonomous.\nAdvertisement · Scroll to continue\nTesla faces a number of other investigations.\n\"These complaints and investigations paint a troubling picture: Tesla repeatedly releases software without fully considering its risks and implications, creating grave dangers for all on the roads,” the senators said, asking Musk to answer a series of questions by Feb. 22 regarding its design and programming decisions.\nIn November, Tesla recalled nearly 12,000 U.S. vehicles sold since 2017 for another software update because a communication error could cause a false forward-collision warning or unexpected activation of the emergency brakes.\nAdvertisement · Scroll to continue\nNHTSA said last week it was reviewing complaints from consumers that Tesla vehicles were activating the brakes unnecessarily.\nNHTSA last month sought additional information from Tesla in its probe into 580,000 vehicles over the automaker’s decision to allow games to be played by passengers on the front center touch screen.\nIn August, NHTSA opened a formal safety probe into Tesla’s Autopilot in 765,000 U.S. vehicles after about a dozen crashes involving Tesla models and emergency vehicles.\nAdvertisement · Scroll to continue\nIn their letter, Blumenthal and Markey asked about specific safety features. \"Are all Tesla vehicles that come with Autopilot or FSD capabilities equipped with an effective camera-based driver monitoring system\" to prevent misuse, they asked.\nReporting by David Shepardson in Washington Editing by Mark Porter and Matthew Lewis\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Two U.S. senators, Richard Blumenthal and Ed Markey, have raised 'significant concerns' about Tesla Inc's Autopilot and Full Self-Driving (Beta) systems. They criticized Tesla's design choices that seemingly encourage unsafe driving habits, particularly allowing vehicles using its Full-Self Driving (Beta) system to roll through stop signs at low speeds. Tesla agreed to recall about 54,000 U.S. vehicles to revise software to prevent this behavior. The senators also mentioned other investigations and recalls related to Tesla's software updates, including a communication error causing false warnings and the activation of emergency brakes. The senators have asked Tesla CEO Elon Musk to answer questions regarding the design and programming decisions of these systems.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/two-us-senators-raise-significant-concerns-with-tesla-driving-systems-2022-02-08\/","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":85,"Time":"Updated 10:18 PM GMT+2, August 10, 2023","Title":"US probing Virginia fatal crash involving Tesla suspected of running on automated driving system","Content":"DETROIT (AP) — U.S. auto safety regulators have sent a team to investigate a fatal crash in Virginia involving a Tesla suspected of running on a partially automated driving system.\nThe latest crash, which occurred in July, brings to 35 the number of Tesla crashes under investigation by the National Highway Traffic Safety Administration since June of 2016. In all the cases, the agency suspects the Teslas were operating on a partially automated driving system such as Autopilot. At least 17 people have died.\nThe safety agency said in documents Thursday that the Tesla ran beneath a heavy truck, but gave no further details.\nThe Fauquier County Sheriff’s office in Virginia said in a statement that on July 19, a Tesla ran underneath the side of a tractor-trailer pulling out of a truck stop, killing the Tesla driver. The department says the truck driver was charged with reckless driving.\nOTHER NEWS\nA bus carrying children overturns in northwest England, killing the driver and a 14-year-old girl\nHundreds attend funeral for high school band director who died in bus crash\nTraffic deaths declined 3.3% in the first half of the year, but Fed officials see more work ahead\nSheriff’s office spokesman Jeffrey Long said the possible role of automated driving systems in the crash is under investigation. The sheriff’s office “is investigating the crash to determine the cause and any potential culpability,” Long said in an email. “The NHTSA is also involved and will contribute their expertise toward any investigative conclusion.”\nMessages were left Thursday seeking comment from Tesla.\nRecent crashes NHTSA is investigating include a July 5 head-on collision between a Tesla Model 3 and Subaru Impreza in South Lake Tahoe, California. The driver of the Subaru and an infant traveling in the Tesla were killed.\nNHTSA also sent investigators to a March 15 crash in Halifax County, North Carolina, that injured a 17-year-old student. The State Highway Patrol said at the time that the driver of the 2022 Tesla Model Y, a 51-year-old male, failed to stop for the bus, which was displaying all of its activated warning devices.\nThe U.S. safety agency has been looking into a string of crashes involving Teslas that are suspected of operating on partially automated systems such as Autopilot and “Full Self-Driving.” Neither system can drive itself despite the names. Tesla says on its website that drivers must be ready to intervene at any time.\nThe agency hasn’t made public the results of the special crash investigation teams. At least two of the investigations involved Teslas running beneath tractor-trailers crossing in front of them.\nIn addition to the special crash investigations, NHTSA has opened at least six formal investigations into Tesla safety problems during the past three years.\nInvestigators are looking into Teslas that can crash into parked emergency vehicles while running on the Autopilot driver-assist system, emergency braking for no reason, suspension failures, steering wheels that can fall off, steering failures, and front seat belts that may not be connected properly.\nAutopilot can keep a car in its lane and away from vehicles in front of it, while Tesla says “Full Self-Driving” can take on most driving tasks and is being tested on public roads by owners. In each case, Tesla tells owners they must be ready to intervene at all times.\nMichael Brooks, executive director of the nonprofit Center for Auto Safety, said Autopilot is faulty and should be recalled. Many Tesla drivers clearly aren’t paying attention and are relying too much on the system to drive the vehicles, he said.\nThe recall “should be a slam dunk,” Brooks said. “That’s why I think it’s baffling why this is taking so long, and why we having to keep watching people die.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"U.S. auto safety regulators are investigating a fatal crash in Virginia involving a Tesla suspected of running on a partially automated driving system. The crash occurred in July and marks the 35th Tesla crash under investigation by the National Highway Traffic Safety Administration since June 2016. At least 17 people have died in these incidents. The Tesla ran beneath a heavy truck, resulting in the death of the Tesla driver. The Fauquier County Sheriff’s office charged the truck driver with reckless driving. The possible role of automated driving systems in the crash is under investigation by both the sheriff’s office and the NHTSA.\"\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/tesla-crash-investigation-autopilot-virginia-fatal-fdd1071648f60135b78e8b6b66c59088","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":98,"Time":"11:22 PM GMT+1•December 16, 2022","Title":"Cruise’s autonomous driving tech comes under scrutiny from safety regulators","Content":"cameraImage Credits: Cruise\nU.S. safety regulators have opened a preliminary investigation into the robotaxis developed and operated by GM self-driving subsidiary Cruise.\nThe National Highway Traffic and Safety Administration said it opened the investigation after learning of incidents when these robotaxis “may have engaged in inappropriately hard braking or became immobilized while operating on public roads.” The preliminary investigation covers all Cruise AVs.\nReuters was the first to report the formal safety probe.\nCruise has received the proper permits from California regulators to operate and charge for driverless rides in certain areas of San Francisco. The company is awaiting the last remaining approval from the state’s Public Utilities Commission to expand its service area to all of San Francisco.\nAs Cruise has ramped up its driverless operations, so has the public’s attention. While numerous videos and posts focus on the thrill of riding in a driverless car, not all of the public’s documentation has been positive. Numerous videos and images have been posted on social media, Reddit and other public forums documenting Cruise robotaxis seemingly stuck in intersections and blocking traffic in San Francisco.\nHowever, NHTSA didn’t learn of the hard-braking crash events from social media. Cruise reported the events via the agency’s Standing General Order, which requires manufacturers to report certain crashes involving vehicles equipped with automated driving systems or SAE Level 2 advanced driver assistance systems. This is the third investigation NHTSA has opened into an automated driving systems developer; the first two were for Pony.ai (a recall query and an audit query), according to the agency. There have been multiple investigations into Tesla’s advanced driver assistance system.\nNHTSA said three hard-braking crashes were reported by Cruise through the Standing General Order. Two crashes involved injuries. Cruise said in all three of these incidents, the vehicle was supervised, which means there was a trained safety operator behind the wheel. None of the incidents resulted in police citations, according to the company.\nCruise also said it has already met with NHTSA to discuss each one of the events mentioned in their filing, and provided the agency with briefings and the information they requested. The agency said the investigation was launched to determine the scope and severity of the potential problem and fully assess the potential safety-related issues posed by these two types of incidents.\n“Cruise’s safety record is publicly reported and includes having driven nearly 700,000 fully autonomous miles in an extremely complex urban environment with zero life-threatening injuries or fatalities,” Cruise spokesperson Hannah Lindow wrote in an emailed statement to TechCrunch. “This is against the backdrop of over 40,000 deaths each year on American roads. There’s always a balance between healthy regulatory scrutiny and the innovation we desperately need to save lives, which is why we’ll continue to fully cooperate with NHTSA or any regulator in achieving that shared goal.”\nLindlow noted that in each of these instances, the robotaxi was predicting and responding to the behavior of aggressive or erratic road actors, and was working to minimize collision severity and risk of harm.\nNHTSA didn’t provide further insight into the cases of immobilized Cruise vehicles. However, Cruise told TechCrunch that the company designed its technology to err on the side of being conservative. Whenever the technology isn’t extremely confident in how to proceed, the vehicle will turn on hazard lights and come to a safe stop. If needed, Cruise personnel are physically dispatched to retrieve the vehicle as quickly as possible, the company said, adding this is rare and has not resulted in collisions.\nThe robotaxi may become immobilized because a door is left open, there’s an issue with vehicle hardware or software or there is an out-of-ordinary external event on the road like a spontaneous fireworks display in the street, according to the company. A spokesperson said the company communicates with the CPUC and the state’s Department of Motor Vehicles, the agency that regulates autonomous vehicles, around how, why and when it does this.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about safety regulators opening a preliminary investigation into the autonomous driving technology used in robotaxis developed and operated by GM's self-driving subsidiary, Cruise. The investigation was prompted by incidents where these robotaxis were reported to have engaged in inappropriately hard braking or became immobilized while operating on public roads. Cruise has received permits to operate driverless rides in specific areas of San Francisco and is awaiting further approval to expand its service area. The investigation follows Cruise's report of three hard-braking crashes, two of which involved injuries, and all incidents occurred with a trained safety operator behind the wheel. Cruise has cooperated with NHTSA, and the investigation aims to determine the scope and severity of the potential problem. Cruise emphasized its safety record, having driven nearly 700,000 fully autonomous miles in complex urban environments with no life-threatening injuries or fatalities. They stated that their technology is designed to err on the side of caution and will stop the vehicle if it's not confident in how to proceed. Immobilization may occur due to various reasons, including issues with hardware, software, or external events on the road.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/12\/16\/cruises-autonomous-driving-tech-comes-under-scrutiny-from-safety-regulators\/","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":91,"Time":"Sun 3 Jan 2021 14.24 CET","Title":"'Peak hype': why the driverless car revolution has stalled","Content":"B\ny 2021, according to various Silicon Valley luminaries, bandwagoning politicians and leading cab firms in recent years, self-driving cars would have long been crossing the US, started filing along Britain’s motorways and be all set to provide robotaxis in London.\n1 January has not, however, brought a driverless revolution. Indeed in the last weeks of 2020 Uber, one of the biggest players and supposed beneficiaries, decided to park its plans for self-driving taxis, selling off its autonomous division to Aurora in a deal worth about $4bn (£3bn) – roughly half what it was valued at in 2019.\nThe decision did not, Uber’s chief executive protested, mean the company no longer believed in self-driving vehicles. “Few technologies hold as much promise to improve people’s lives with safe, accessible, and environmentally friendly transportation,” Dara Khosrowshahi said. But more people might now take that promise with a pinch of salt.\n00:00:29\nUber dashcam footage shows lead up to fatal self-driving crash – video\nProf Nick Reed, a transport consultant who ran UK self-driving trials, says: “The perspectives have changed since 2015, when it was probably peak hype. Reality is setting in about the challenges and complexity.”\nAutomated driving, says Reed, could still happen in the next five years on highways with clearly marked lanes, limited to motorised vehicles all going in the same direction. Widespread use in cities remains some way further out, he says: “But the benefits are still there.”\nThe most touted benefit is safety, with human error blamed for more than 90% of road accidents. Proponents also say autonomous cars would be more efficient and reduce congestion.\nLooking back, Reed says “the technology worked … people had the sense, it does the right thing most of the time, we are 90% of the way there. But it is that last bit which is the toughest. Being able reliably to do the right thing every single time, whether it’s raining, snowing, fog, is a bigger challenge than anticipated.”\nWaymo, the Google spin-off that has led the field, could be a case in point: having quickly wowed the world with footage of self-driving cars, the subsequent steps have appeared small.\nIn October last year it announced the public could now hail fully driverless taxis, “in the near term” without a safety driver in any car – although the range remains limited to the sunny suburbs of Phoenix, Arizona, whose every centimetre has been mapped by Waymo computers.\n00:05:04\nWaymo trials its self-driving cars outside Phoenix, Arizona – video\nElsewhere, robotaxis have stalled. Like Uber, the cab firm Addison Lee had staked out bold ambitions, signing up with the UK autonomy pioneer Oxbotica in 2018 to get robotaxis into London by 2021.\nThat deal was quietly dropped in March last year, under new ownership. Addison Lee’s chief executive, Liam Griffin, said: “Driverless cars are best left to the OEMs [manufacturers], and don’t form part of our current plans.”\nThe launch of an autonomous taxi service by Ford has also been postponed at least a year to 2022 because of the pandemic.\n“Globally, Covid-19 has delayed trials and launches of connected and automated vehicles,” says Mike Hawes, the chief executive of the Society of Motor Manufacturers and Traders.\nRegulatory changes could still allow developments such as Automated Lane Keeping Systems being rolled out in 2021 across everyday cars.\n“ALKS is the first version of automated driving technology which could prevent some 47,000 serious accidents over the next decade, while creating up to 420,000 new jobs,” Hawes said.\nThe system could let the car take control on UK motorways this year – although insurers are trying to talk the government out of giving the go-ahead.\nAlexandra Smyth, who leads on autonomous systems at the Royal Academy of Engineering, said: “There’s lots of progress and interesting developments with regulations and codes of practice – all important components that sit alongside the technology itself. But realistically there are still going to be errors and things that don’t perform as we hoped. Public trust will be one of the major hurdles.”\nFears were stoked after Uber’s self-driving car killed a pedestrian in Arizona in 2018. And despite Elon Musk’s continuing bold claims for Tesla, and reports that Apple is still secretly pushing to develop a personal autonomous vehicle by 2024, the law is unlikely to permit drivers to relinquish the wheel soon.\nAccording to Christian Wolmar, the author of Driverless Cars: On a Road to Nowhere?, problems such as social acceptance, cybersecurity and cost have never been addressed.\nHe says: “People do not want to replace the car outside the front door with an app it’s just not a viable concept. I think more and more people are sceptical of the model that we’ll all be in robocars soon. Instead, the industry is now talking about specific uses.”\nIf Oxbotica’s Paul Newman, one of the Oxford University professors pioneering Britain’s autonomous industry, has any doubts over the long term, he is not showing it – although he says the level of autonomy where “occasionally there might be a remote assist” is a far more achievable ambition than a world where the machines can entirely get on with it.\nOxbotica is running a fleet of autonomous Ford Mondeos on public roads in a trial in Oxford – but the technological progress, he says, is not about robotaxis: “It’s purely about the software, it’s agnostic about the vehicles.”\nThe driverless car world, he says, is “a great moonshot”: cars are a huge market but also the hardest to transform, long after autonomous mining or rail or shuttle services are in place.\nNewman compares the progress with mobiles phones, recalling the first he saw, wielded by Danny Glover in the 1987 film Lethal Weapon, which was the size of a small suitcase.\n“Is a future of driverless cars coming? Assuredly as mobile phones. This is the normal cycle that technology goes through. We’re still moving along that graph,” he said.\n“We’ve gone through the flashy stage, when we’ve said it’s six months away … Now we’ve got engineers saying this is properly hard.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news discusses the current state of the driverless car revolution. It mentions that Uber abandoned its plans for self-driving taxis in 2020, and the driverless car industry has faced challenges and delays. Waymo, another prominent player, has limited its fully driverless taxi service to Phoenix, Arizona. Addison Lee and Ford have also faced setbacks in their autonomous taxi plans. The COVID-19 pandemic has contributed to delays in trials and launches of autonomous vehicles. The article also mentions the potential rollout of Automated Lane Keeping Systems (ALKS) and highlights the importance of public trust and regulatory changes. Overall, it provides insights into the challenges and progress of autonomous driving technology.\"\n}\ncase_text_end=======","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2021\/jan\/03\/peak-hype-driverless-car-revolution-uber-robotaxis-autonomous-vehicle","query_name":"updated_file_autonomous_driving_system_errors.xlsx"}
{"num":2,"Time":"Updated 5:18 PM EST, Mon December 12, 2022","Title":"Tesla: Our ‘failure’ to make actual self-driving cars ‘is not fraud’","Content":"CNN\n — \nTesla CEO Elon Musk has said numerous times since 2015 that Tesla cars would be entirely self-driving in two years, or less. But years after his self-imposed deadlines have blown by, it still hasn’t happened. Even when equipped with a $15,000 technology package that is literally called “Full Self Driving Capability,” a Tesla car can’t actually drive by itself.\nNow, lawyers for Tesla are arguing that while the company may have failed to live up to these lofty goals, that doesn’t mean it perpetuated a fraud, as alleged in a class-action lawsuit filed in September.\n“Mere failure to realize a long-term, aspirational goal is not fraud,” Tesla’s lawyers wrote in a November 28 court filing, asking that the suit be dismissed.\nThe lawsuit cited numerous times when Musk and others at Tesla had stated that, within a year or two, the cars would be fully self-driving thanks to software updates. For instance, in a 2016 Tweet, Musk stated that a Tesla car would be able to drive itself across the United States “by next year,” the suit said.\nLater that year, Tesla published a video, also cited in the lawsuit, which the automaker said showed one of its cars driving itself. The video was misleading, the lawsuit said, because according to Tesla employees involved in the video, numerous attempts had to be made before the car could be shown to maneuver through the route without obvious problems.\nThe lawsuit, filed by the California firm of Cotchett, Pitre & McCarthy, also cited numerous cases of crashes involving the use of Tesla’s driver assist technology.\nEven among Advanced Driver Assistance Systems, a type of technology now widely available from many automakers, Tesla’s Autopilot system falls behind those now available in global markets from other companies including Mercedes-Benz and Honda, the lawsuit alleges.\nAutopilot is Tesla’s name for it’s driver assistance system that helps keep the car in its lane on a highway. It is supposed to allow it to automatically avoid other cars, though it has had significant issues seeing stopped emergency vehicles on the side of the road. Tesla has said the system is designed to respond to moving vehicles, since reacting to non-moving objects could be dangerous. it can also change lanes while avoiding other cars.\nMany other car companies offer similar systems. The name Autopilot has come under criticism from safety groups and even governments for implying too great a degree of independent control. Tesla and Musk have defended the name because, in airplanes, the autopilot system still requires an active and alert pilot at the controls. A German court ruled in 2020 that Tesla should not be allowed to advertise its driver-assist technology as “Autopilot,” although the complaint was later thrown out, allowing for use of the term.\nFull Self Driving Capability is Tesla’s name for an additional system that, in limited release beta versions, allowed the car to independently handle steering, braking and accelerating in more complex environments such as city streets. According to Tesla’s Website, the feature no longer handles steering on city streets but only the gas and brakes.\nTesla and Musk have said that, with future software improvements that will be delivered “over the air” like smartphone updates, cars with these systems will, one day soon, be able to drive themselves with no involvement from a person.\nThat is not the case yet, though, and Tesla has missed its own predictions about when this would be possible.\nJust failing to meet Musk’s own expectations isn’t evidence that anyone purposely tried to deceive consumers, which would constitute fraud, Tesla said in its filing. Moreover, according to Tesla, buyers should have been well aware of the limitations of these systems before purchasing them, based on Tesla’s disclaimers on its website and owners’ manuals. Also, Tesla said, the plaintiffs didn’t show that, even if Tesla’s systems were involved in crashes, they were involved in more crashes than an unaided human driver would have when operating under the same conditions.\nTesla also listed a number of other more technical reasons the suit should be dismissed including that most of the plaintiffs, who were Tesla owners, had agreed to an arbitration clause when ordering their cars so these claims should not be tried in public courts or in class-action lawsuits. Tesla also argued too much time had elapsed since the cars were purchased and that the plaintiffs hadn’t really been harmed by the fact the cars never drove themselves, despite the system being called “Full Self-Driving” and costing some owners $15,000.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news discusses Tesla's failure to deliver fully self-driving cars despite promises made by CEO Elon Musk since 2015. It mentions a class-action lawsuit filed against Tesla, arguing that the company did not commit fraud by failing to achieve its self-imposed deadlines. The lawsuit cites instances where Musk claimed that Tesla cars would be fully self-driving within a year or two due to software updates, as well as cases of crashes involving Tesla's driver assist technology. The article also highlights that Tesla's Autopilot system has been criticized for its performance compared to systems from other automakers. Despite the name 'Full Self Driving Capability,' the feature does not yet handle steering on city streets, and Tesla has missed its own predictions about when cars would be able to drive themselves. Tesla argues that the plaintiffs should have been aware of the system's limitations and that the suit should be dismissed for various reasons, including arbitration agreements and the plaintiffs' lack of harm.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2022\/12\/12\/business\/tesla-fsd-autopilot-lawsuit\/index.html","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":6,"Time":"6:55 PM GMT+2•March 27, 2018","Title":"Nvidia suspends all autonomous vehicle testing","Content":"Nvidia is temporarily stopping testing of its autonomous vehicle platform in response to last week’s fatal collision of a self-driving Uber car with a pedestrian. TechCrunch confirmed this with the company, which offered the following statement:\nUltimately [autonomous vehicles] will be far safer than human drivers, so this important work needs to continue. We are temporarily suspending the testing of our self-driving cars on public roads to learn from the Uber incident. Our global fleet of manually driven data collection vehicles continue to operate.\nUpdate: Shortly afterwards, the statement was apparently improved on internally and the following appended (brackets mine, replacing acronyms):\nThe accident was tragic. It’s a reminder of how difficult [self-driving car] technology is and that it needs to be approached with extreme caution and the best safety technologies. This tragedy is exactly why we’ve committed ourselves to perfecting this life-saving technology.\nLikely someone pointed out that it wasn’t particularly charming to respond to a fatal system failure in an autonomous vehicle by saying that “ultimately” they’ll be safer, even if it’s true.\nReuters first reported the news.\nThe manually driven vehicles, to be clear, are not self-driving ones with safety drivers, but traditionally controlled vehicles with a full autonomous sensor suite on them to collect data.\nToyota also suspended its autonomous vehicle testing out of concern for its own drivers’ well-being. Uber of course ceased its testing operations at once.","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"Nvidia has temporarily suspended testing of its autonomous vehicle platform in response to a recent fatal collision involving a self-driving Uber car and a pedestrian. The company acknowledged the importance of continuing work on autonomous vehicles for their eventual safety improvement. They mentioned temporarily halting the testing of self-driving cars on public roads to learn from the Uber incident. Additionally, Nvidia highlighted the need for extreme caution and the implementation of the best safety technologies in developing self-driving car technology, emphasizing the tragic nature of the accident. Toyota also suspended its autonomous vehicle testing due to concerns for driver safety, and Uber had already ceased its testing operations.\"\n}\nThe news is relevant to autonomous vehicle testing, and the specific information has been extracted as detailed as possible.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2018\/03\/27\/nvidia-reportedly-suspending-all-autonomous-vehicle-testing\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":1,"Time":"Tue 12 Jan 2016 23.00 CET","Title":"Google reports self-driving car mistakes: 272 failures and 13 near misses","Content":"Google’s self-driving cars might not yet have caused a single accident on public roads, but it’s not for want of trying.\nBetween September 2014 and November 2015, Google’s autonomous vehicles in California experienced 272 failures and would have crashed at least 13 times if their human test drivers had not intervened, according to a document filed by Google with the California Department of Motor Vehicles (DMV).\nWhen California started handing out permits for the testing of self-driving cars on public roads, it had just a few conditions. One was that manufacturers record and report every “disengagement”: incidents when a human safety driver had to take control of a vehicle for safety reasons.\nGoogle lobbied hard against the rule. Ron Medford, director of safety for the company’s self-driving car project, wrote at the time: “This data does not provide an effective barometer of vehicle safety. During testing most disengages occur for benign reasons, not to avoid an accident.”\nCrash involving self-driving Google car injures three employees\nRead more\nThe first annual reports were due on 1 January, and Google is the first company to share its data publicly. The figures show that during the 14-month period, 49 Google self-driving cars racked up over 424,000 autonomous miles and suffered 341 disengagements, when either the cars unexpectedly handed control back to their test drivers, or the drivers intervened of their own accord. The reports include both Google’s own prototype “Koala” cars and its fleet of modified Lexus RX450h.\nA Lexus RX450h modified with Google’s autonomous car software and sensors. Photograph: Tony Avelar\/AP\nIn 272 of those disengagements, the car detected a technology failure such as a communications breakdown, a strange sensor reading or a problem in a safety-critical system such as steering or braking.\nGoogle calls these “immediate manual control” disengagements. As the name suggests, the test driver is given audio and visual signals to alert them that they should take over driving without delay. Google test drivers typically responded to these warnings in 0.8 seconds.\nDrivers took over ‘many thousands of times’\nIn the remaining 69 disengagements, the human driver took control of the car on their own initiative, simply by grabbing the steering wheel or pressing the accelerator or brake pedal. The car automatically cedes control when this happens. Drivers do this fairly regularly if they suspect the car is doing (or is about to do) something hazardous or in response to other road users.\nHowever, Google admits that its drivers actually took over from their vehicles “many thousands of times” during the period. The company is reporting only 69 incidents because Google thinks California’s regulations require it only to report disengagements where drivers were justified in taking over, and not those where the car would have coped on its own.\nThe company decides this by replaying each disengagement in its online simulator over and over again. Google says that its powerful software, which now drives over 3m virtual miles each day, can accurately predict the behaviour of other drivers, pedestrians and cyclists and can thus determine whether the test driver’s intervention was required for safety.\nThis isn’t representative of where the technology will be when we’re ready to release it\nChris Urmson, director of Google’s self-driving car program\nBryant Walker Smith, assistant professor in the School of Law at the University of South Carolina, says the DMV could reasonably ask for more information. “Google could be clearer on how it draws the line between those driver-initiated disengagements that it reports and those that it does not,” he says. “The DMV is entitled to interpret its own rule, and it may have questions on this point.”\nConsumer Watchdog, a California-based campaign group, said the report shows that self-driving cars still need a human driver behind the wheel. Privacy project director John M Simpson said: “It’s unfathomable that Google is pushing back against those sensible safety protecting regulations. How can Google propose a car with no steering wheel or brakes when its own tests show that in 15 months the robot technology failed and handed control to the driver 272 times and a driver decided to intervene 69 times?\n“Release of the disengagement report was a positive step, but Google should also release any video it has of the disengagement incidents, as well as any technical data it collected.”\nIn 56 of the 69 driver disengagements reported to the DMV, Google calculated that its car would probably not have come into contact with another object. But, admits Google in its report, “we identified some aspect of the [car]’s behavior that could be a potential cause of contacts in other environments or situations if not addressed. This includes proper perception of traffic lights, yielding properly to pedestrians and cyclists, and violations of traffic laws.”\nGoogle classified the final 13 disengagements as “simulated contacts”: situations that would have resulted in a crash had the human driver not taken over. “In these cases, we believe a human driver could have taken a reasonable action to avoid the contact but the simulation indicated the [car] would not have taken that action,” the company says.\nAll self-driving car firms have filed disengagement reports\nThe report could be seen as a blow to Google’s insistence that self-driving cars should be fully autonomous. It latest prototypes are designed to operate without any driving controls for their human occupants to take over in an emergency (although those currently on public roads do have backup controls fitted).\n“It demonstrates that it is valuable to have a safety driver in the vehicle while testing, which is something we’ve always believed,” said Chris Urmson, director of Google’s self-driving car program. “But if you look at [regular] drivers, they’re effectively untrained in America. Expecting them to vigilantly monitor a system that operates as well as this does is a really a very challenging problem.”\nThe latest software for the Tesla Model S gives it autopilot capability. Photograph: The Washington Post\/Getty Images\nGoogle is not the only company to have filed a disengagement report with the DMV. Volkswagen\/Audi, Mercedes-Benz, Delphi, Tesla, Bosch and Nissan have all filed reports, which are currently under review by the department to confirm that they contain all the required information. The DMV told the Guardian that it does not currently have an expected date to complete its analysis of the data or draw conclusions from it.\nWhile Google has been testing its self-driving cars since 2008, the company will not be releasing disengagement data from before 2014. “This is the period we’re required to share with the DMV. Any data we would have from before that is just outdated,” Urmson says.\nGoogle notes that disengagements have been getting less common over the period of the report. However, Urmson cautions against expecting disengagements to drop regularly, year on year. “We’re continually adding capabilities to our vehicles, pushing them into more challenging situations,” he says. “Over the long view, we’d expect disengagements to be improving, but as we test in more challenging weather or driving situations, you could expect locally this to not look as good. And it really isn’t representative of where the technology will be when we’re ready to release it.”\nGoogle’s parent company, Alphabet, is reported to be planning to spin out its self-driving car technology into its own business later this year.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Google's self-driving cars experienced 272 failures and 13 near misses between September 2014 and November 2015. These failures would have resulted in crashes if human test drivers had not intervened. The failures include technology issues such as communications breakdowns, sensor readings, and safety-critical system problems. Additionally, there were 341 disengagements, where either the cars unexpectedly handed control back to their test drivers or the drivers intervened themselves. Google reported that drivers took over 'many thousands of times' during this period, but only 69 incidents were reported as per California regulations. Google's software was used to determine if the driver's intervention was justified for safety. In some cases, disengagements were related to potential issues like traffic light perception, yielding to pedestrians and cyclists, and violations of traffic laws. The report suggests that self-driving cars might still require human drivers behind the wheel for safety.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/jan\/12\/google-self-driving-cars-mistakes-data-reports","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":13,"Time":"JANUARY 12, 2016","Title":"Google says fewer tech glitches in its self-driving cars","Content":"By Alexandria Sage\nDETROIT (Reuters) - Google said on Tuesday the rate of software failures in its self-driving cars was declining as the technology was learning from its mistakes, bringing the tech company closer to its goal of fully autonomous vehicles.\nIn 424,000 miles of autonomous driving, Google said its cars had experienced 272 episodes in which the human test driver had to assume control of the vehicle, an occurrence called “disengagement”, when the autonomous technology failed.\n“As we continue to develop and refine the self-driving software, we are seeing fewer disengagements” despite more miles driven, Google said in a 33-page report submitted by law to the California Department of Motor Vehicles on Dec. 31.\nCalifornia regulations call for test drivers and steering wheels in autonomous vehicles.\nGoogle’s development of self-driving cars has spurred outsized interest around the world, but the company best known for search has disclosed little about its strategy, business plans or ultimate goals.\nWithout the need to watch the road, people in cars will talk on phones, watch entertainment, purchase consumer goods, among many other options, providing valuable data to Google.\nThe report, covering the period Sept. 24, 2014, when Google began testing on roads of Palo Alto, California, to Nov. 30, 2015, found disengagements occurred about every 785 miles in the fourth quarter of 2014. A year later, that had expanded to 5,318 miles between episodes.\nEighty-nine percent of disengagements occurred on city streets, where more obstacles and stop-and-go traffic make autonomous driving more difficult.\nProject director Chris Urmson said Google deliberately tests cars in different weather and times of the day, which explained why some months saw more episodes than others.\nGoogle, a unit of Alphabet Inc, said it kept the threshold for measuring disengagements low to gather as much data as possible to refine the technology.\nThere were another 69 episodes in which the test driver chose to take control of the vehicle rather than the car signaling to the driver to take control.\nUsing a simulator to replay the situation, Google found in 13 of these instances its cars would have hit another object had the test driver not taken control. Google said two involved traffic cones, and three were due to “another driver’s reckless behavior.”\nUrmson said the California DMV had not seen the report when it issued draft rules in December restricting how autonomous vehicles could operate for the next three years.\nReporting By Alexandria Sage; Editing by Meredith Mazzilli\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Google has reported a decline in software failures in its self-driving cars. In 424,000 miles of autonomous driving, there were 272 instances of 'disengagement,' where the human test driver had to take control of the vehicle due to autonomous technology failures. The report covers the period from September 24, 2014, to November 30, 2015, and it shows that disengagements occurred about every 785 miles in the fourth quarter of 2014, but a year later, it had expanded to 5,318 miles between episodes. Eighty-nine percent of disengagements happened on city streets, where driving conditions are more challenging. Google noted that they deliberately tested cars in various weather conditions and times of the day to gather data to refine the technology. There were also 69 episodes where the test driver chose to take control, with 13 instances where the cars would have hit objects if the driver hadn't intervened.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autoshow-detroit-google-idINKCN0UQ2LQ20160112","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":10,"Time":"September 14, 2022","Title":"Tesla is sued by drivers over alleged false Autopilot, Full Self-Driving claims","Content":"Sept 14 (Reuters) - Tesla Inc (TSLA.O) was sued on Wednesday in a proposed class action accusing Elon Musk's electric car company of misleading the public by falsely advertising its Autopilot and Full Self-Driving features.\nThe complaint accused Tesla and Musk of having since 2016 deceptively advertised the technology as fully functioning or \"just around the corner\" despite knowing that the technology did not work or was nonexistent, and made vehicles unsafe.\nBriggs Matsko, the named plaintiff, said Tesla did this to \"generate excitement\" about its vehicles, attract investments, boost sales, avoid bankruptcy, drive up its stock price and become a \"dominant player\" in electric vehicles.\n\"Tesla has yet to produce anything even remotely approaching a fully self-driving car,\" Matsko said.\nThe lawsuit filed in federal court in San Francisco seeks unspecified damages for people who since 2016 bought or leased Tesla vehicles with Autopilot, Enhanced Autopilot and Full Self-Driving features.\nAdvertisement · Scroll to continue\nTesla did not immediately respond to requests for comment. It disbanded its media relations department in 2020.\nThe lawsuit followed complaints filed on July 28 by California's Department of Motor Vehicles accusing Tesla of overstating how well its advanced driver assistance systems (ADAS) worked. read more\nRemedies there could include suspending Tesla's license in California, and requiring restitution to drivers.\nAdvertisement · Scroll to continue\nTesla has said Autopilot enables vehicles to steer, accelerate and brake within their lanes, while Full Self-Driving lets vehicles obey traffic signals and change lanes.\nIt has also said both technologies \"require active driver supervision,\" with a \"fully attentive\" driver whose hands are on the wheel, \"and do not make the vehicle autonomous.\"\nMatsko, of Rancho Murieta, California, said he paid a $5,000 premium for his 2018 Tesla Model X to obtain Enhanced Autopilot.\nAdvertisement · Scroll to continue\nHe also said Tesla drivers who receive software updates \"effectively act as untrained test engineers\" and have found \"myriad problems,\" including that vehicles steer into oncoming traffic, run red lights, and fail to make routine turns.\nThe National Highway Traffic Safety Administration has since 2016 opened 38 special investigations of Tesla crashes believed to involve ADAS. Nineteen deaths were reported in those crashes.\nAdvertisement · Scroll to continue\nThe case is Matsko v Tesla Inc et al, U.S. District Court, Northern District of California, No. 22-05240.\nReporting by Jonathan Stempel in New York; editing by Jonathan Oatis\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"Tesla Inc (TSLA.O) was sued in a proposed class action on September 14, 2023, accusing Elon Musk's electric car company of misleading the public by falsely advertising its Autopilot and Full Self-Driving features. The complaint alleges that Tesla and Musk deceptively advertised the technology as fully functioning or 'just around the corner' since 2016, despite knowing that the technology did not work or was nonexistent. The lawsuit seeks unspecified damages for people who purchased or leased Tesla vehicles with Autopilot, Enhanced Autopilot, and Full Self-Driving features since 2016. Additionally, there have been complaints about Tesla's advanced driver assistance systems (ADAS) overstating their performance, which could result in remedies such as suspending Tesla's license in California and requiring restitution to drivers. The National Highway Traffic Safety Administration has opened 38 special investigations of Tesla crashes believed to involve ADAS since 2016, with 19 reported deaths in those crashes.\"\n}\nThe news is indeed relevant to autonomous vehicle failures, and the specific information has been extracted as detailed as possible from the provided news article.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/tesla-is-sued-by-drivers-over-alleged-false-autopilot-full-self-driving-claims-2022-09-14\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":19,"Time":"9:36 PM GMT+1•March 17, 2015","Title":"Google[x] Head Astro Teller Says Moonshots Are All About Embracing Failure","Content":"Google’s ‘Captain of Moonshots’ Astro Teller admitted to a packed crowd at SXSW that a number of Google[x] projects have experienced a series of “bumps and scrapes” over the last five years. But he also highlighted how the Google lab attempts to overcome those issues and “embrace failure” in projects that aren’t going so well.\nThe original moonshot proposal for the secret Google lab was to get people to the moon. It has since spun out 10 different projects that involve everything from biotechnology, robots and cancer detection nanotechnology.\nBut it has not been without a lot of problems. One example is Google Glass, which Teller said he regrets launching as if it were a finished product.\n“We made one great decision and one not-so-great decision. The great decision was the Explorer program. The thing we did not do well is that we allowed and somewhat encouraged too much exposure to the program. We did things to encourage people to think that [Glass] was a finished product,” he said.\nIt turns out building autonomous vehicles is also pretty difficult. For Google’s self-driving car project, the lab gave a bunch of Lexus SUVs to non-Google employees. They drove fine on the highway but not on city streets. The mistake Google made was in assuming humans would pay attention the whole time and be a reliable backup. That didn’t work out so well all the time.\n“People do really stupid things while driving,” he said. Afterward, Google switched to making the cars in-house, and that project is still underway.\nWe did things to encourage people to think that [Glass] was a finished product\nAstro Teller\nHe also shared Google’s “maniacal interest” in seeing projects through, even those which don’t seem to be working. Project Wing, a self-flying delivery vehicle, fell under that category. The team believed the idea to be a failure 18 months after the fact.\n“Fifty percent of the team knew it was a bad project after a few months, 80 percent knew after a year-and-a-half but they didn’t kill it,” he said. The team is still at it on that one, too. “I look forward to telling you more about that later this year,” Teller said.\nGoogle’s Project Loon, a plan to deliver Internet around the world using giant 20-meter balloons, had problems because the balloons leaked when they hit a certain altitude. The team resorted to studying things like the fluffiness of researcher’s socks to figure out what the problem was. “It turned out that actually mattered,” Teller told the crowd.\nThese moonshots are all about embracing failure, according to the head of Google[x].\n“You make a ton of progress by making a ton of mistakes,” he said. “The longer you work on something, the more you don’t really want to know what the world is going to tell you. The longer you put off that learning you will unconsciously put off that news because it is disheartening to hear that what you have been working on is not working.”\nGoogle’s self-driving car project now travels several thousand miles of city streets every day to find ways to get stuck. “It’s not about ensuring you don’t have these bumps and scrapes. It’s about ensuring we learn from them,” Teller said.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about the challenges and failures faced by Google[x] in various projects, including the self-driving car project. The self-driving car project encountered difficulties when Lexus SUVs, given to non-Google employees, did not perform well on city streets because humans didn't pay enough attention. Google later switched to making the cars in-house, and the project is still ongoing. Additionally, the article discusses other projects like Google Glass, Project Wing, and Project Loon, all of which faced their own sets of challenges and failures. The overall theme of the news is about embracing failure in moonshot projects.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2015\/03\/17\/googlex-head-astro-teller-says-moonshots-are-all-about-embracing-failure\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":21,"Time":null,"Title":"Australian miner Fortescue says two driverless trucks involved in low-speed incident","Content":"By Melanie Burton\nMELBOURNE (Reuters) - Australian iron ore miner Fortescue said on Friday that one of its driverless trucks, traveling at low speed, ran into another that was parked at its remote Western Australian operations in an incident earlier this week.\nAn autonomous truck readies to pick up a load of iron ore at Australia's Fortescue Metals Group (FMG) Chichester Hub, which includes the Christmas Creek iron ore mine, in the Pilbara region, located southeast of the coastal town of Port Hedland in Western Australia, November 29, 2018. REUTERS\/Melanie Burton\nThe miner said in a statement that no one was hurt or at risk of being injured in the Feb. 11 incident. Fortescue is retrofitting 100 huge mining trucks with autonomous haulage systems (AHS) at its remote Chichester hub, aiming to more than double its self-driving fleet.\n“This was not the result of any failure of the autonomous system,” Fortescue Chief Executive Elizabeth Gaines said in the statement. The miner is conducting a full investigation into the incident and expects that to conclude in the near future.\n“On Monday, 11 February an AHS truck made contact with a parked AHS truck at slow speed,” the statement said, without disclosing the speed at which the moving vehicle was traveling or details of any damage to the trucks. “No manned vehicles or people were involved.”\nAnalysts said that minor accidents with autonomous vehicles had been reported in the mining industry before. All of Australia’s iron ore miners have transitioned into using some autonomous vehicles to cut costs as they can operate without breaks and drive more efficiently.\n“These things happen (across the industry) from time to time,” said UBS analyst Glyn Lawcock in Sydney.\nEarlier on Friday the West Australian paper, without citing sources, reported the moving truck backed into the stationary vehicle.\nFortescue, which said its AHS trucks have safely traveled more than 24.7 million km (14.8 million miles) since 2012, declined to comment directly on the West Australian’s report.\nA spokesman for the firm, which reports earnings on Feb. 20, said, “We already have a good understanding of the incident and expect the investigation to conclude in the near term.”\nThe Western Australian Department of Mines was notified by Fortescue and has begun an investigation, Director of Mines Safety Andrew Chaplyn said in a statement.\n“The department will review the company’s internal investigation report to determine what further actions may be required,” he said.\nReporting by Melanie Burton; Editing by Kenneth Maxwell and Christian Schmollinger\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Australian miner Fortescue reported an incident involving two driverless trucks at its remote Western Australian operations on Feb. 11. The incident occurred when one autonomous truck made contact with another parked autonomous truck at low speed. Fortunately, no one was injured, and the company emphasized that it was not the result of any failure of the autonomous system. Fortescue is conducting a full investigation into the incident, and the Western Australian Department of Mines has also started its own investigation. The incident was reported in the context of Fortescue retrofitting 100 mining trucks with autonomous haulage systems at its Chichester hub to expand its self-driving fleet. Minor accidents involving autonomous vehicles in the mining industry have been reported before, and all of Australia's iron ore miners have been adopting autonomous vehicles to cut costs and improve efficiency.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-australia-fortescue-autonomous-idUSKCN1Q408G","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":3,"Time":"Sun 27 Mar 2022 15.00 CEST","Title":"How self-driving cars got stuck in the slow lane","Content":"“I\nwould be shocked if we do not achieve full self-driving safer than a human this year,” said Tesla chief executive, Elon Musk, in January. For anyone who follows Musk’s commentary, this might sound familiar. In 2020, he promised autonomous cars the same year, saying: “There are no fundamental challenges.” In 2019, he promised Teslas would be able to drive themselves by 2020 – converting into a fleet of 1m “robotaxis”. He has made similar predictions every year going back to 2014.\nFrom late 2020, Tesla expanded beta trials of its “Full Self-Driving” software (FSD) to about 60,000 Tesla owners, who must pass a safety test and pay $12,000 for the privilege. The customers will pilot the automated driver assistance technology, helping to refine it before a general release.\nWith the beta rollout, Tesla is following the playbook of software companies, “where the idea is you get people to iron out the kinks”, says Andrew Maynard, director of the Arizona State University risk innovation lab. “The difficulty being that when software crashes, you just reboot the computer. When a car crashes, it’s a little bit more serious.”\nPlacing fledgling technology into untrained testers’ hands is an unorthodox approach for the autonomous vehicle (AV) industry. Other companies, such as Alphabet-owned Waymo, General Motors-backed Cruise and AV startup Aurora, use safety operators to test technology on predetermined routes. While the move has bolstered Tesla’s populist credentials with fans, it has proved reputationally risky. Since putting its tech into the hands of the people, a stream of videos documenting reckless-looking FSD behaviour has racked up numerous views online.\nThere’s the video of a car in FSD mode veering sharply into oncoming traffic, prompting the driver to swerve off the road into a field. The one that shows a car repeatedly attempting to turn on to train tracks and into pedestrians. Another that captures the driver struggling to regain control of the car after the system prompts him to take over. What would appear to be the first crash involving FSD was reported to the US National Highway Traffic Safety Administration (NHTSA) in November last year; no one was injured, but the vehicle was “severely damaged”.\nTesla boss Elon Musk has promised the arrival of self-driving cars several times over the years. Photograph: Stephen Lam\/Reuters\nFSD is proficient at driving on motorways, where it’s “straightforward, literally”, says Taylor Ogan, a Tesla FSD owner and chief executive of Snow Bull Capital. On more complex, inner-city streets, he says the system is more unpredictable. Continuous software updates are supposed to iron out glitches. For example, the NHTSA forced Tesla to prevent the system from executing illegal “rolling stops” (moving slowly through a stop sign without ever coming to a full stop, while an “unexpected braking” problem is the subject of a current inquiry. In Ogan’s experience of trialling FSD though, “I haven’t even seen it get better. It just does crazier things more confidently.”\nMaynard says the “learner driver” metaphor holds for some of FSD’s issues, but falls apart when the technology engages in indisputably non-human behaviour. For example, a lack of regard for getting dangerously close to pedestrians and the time a Tesla ploughed into a bollard that FSD failed to register. Similar problems have emerged with Tesla’s Autopilot software, which has been implicated in at least 12 accidents (with one death and 17 injuries) owing to the cars being unable to “see” parked emergency vehicles.\nThere’s reason to believe that the videos that make their way online are some of the more flattering ones. Not only are the testers Tesla customers, but an army of super-fans acts as an extra deterrent to sharing anything negative. Any reports of FSD behaving badly can trigger a wave of outrage; any critical posts on the Tesla Motors Club, a forum for Tesla drivers, are inevitably greeted by people blaming users for accidents or accusing them of wanting Tesla to fail. “People are terrified that Elon Musk will take away the FSD that they paid for and that people will attack them,” says Ogan.\nThis helps to shield Tesla from criticism, says Ed Niedermeyer, the author of Ludicrous: The Unvarnished Story of Tesla Motors, who was “bombarded by an online militia” when he started reporting on the company. “Throughout Tesla’s history, this faith and sense of community… has been absolutely critical to Tesla’s survival,” he says. The proof, he adds, is that Musk can claim again and again to be a year from reaching full autonomous driving without losing the trust of fans.\nB\nut it’s not just Tesla that has missed self-imposed autonomous driving deadlines. Cruise, Waymo, Toyota and Honda all said they would launch fully self-driving cars by 2020. Progress has been made, but not on the scale anticipated. What happened?\n“Number one is that this stuff is harder than manufacturers realised,” says Matthew Avery, director of research at Thatcham Research. While about 80% of self-driving is relatively simple – making the car follow the line of the road, stick to a certain side, avoid crashing – the next 10% involves more difficult situations such as roundabouts and complex junctions. “The last 10% is really difficult,” says Avery. “That’s when you’ve got, you know, a cow standing in the middle of the road that doesn’t want to move.”\nIt’s the last 20% that the AV industry is stuck on, especially the final 10%, which covers the devilish problem of “edge cases”. These are rare and unusual events that occur on the road such as a ball bouncing across the street followed by a running child; complicated roadworks that require the car to mount the kerb to get past; a group of protesters wielding signs. Or that obstinate cow.\nSelf-driving cars rely on a combination of basic coded rules such as “always stop at a red light” and machine-learning software. The machine-learning algorithms imbibe masses of data in order to “learn” to drive proficiently. Because edge cases only rarely appear in such data, the car doesn’t learn how to respond appropriately.\nAn Uber self-driving car at its Pittsburgh technology centre in 2016. Photograph: Angelo Merendino\/Getty\nThe thing about edge cases is that they are not all that rare. “They might be infrequent for an individual driver, [but] if you average out over all the drivers in the world, these kinds of edge cases are happening very frequently to somebody,” says Melanie Mitchell, computer scientist and professor of complexity at the Santa Fe Institute.\nWhile humans are able to generalise from one scenario to the next, if a self-driving system appears to “master” a certain situation, it doesn’t necessarily mean it will be able to replicate this under slightly different circumstances. It’s a problem that so far has no answer. “It’s a challenge to try to give AI systems common sense, because we don’t even know how it works in ourselves,” says Mitchell.\nMusk himself has alluded to this: “A major part of real-world AI has to be solved to make unsupervised, generalised full self-driving work,” he tweeted in 2019. Failing a breakthrough in AI, autonomous vehicles that function on a par with humans probably won’t be coming to market just yet. Other AV makers use high-definition maps – charting the lines of roads and pavements, placement of traffic signs and speed limits – to partly get around this problem. But these maps need to be constantly refreshed to keep up with ever-changing conditions on roads and, even then, unpredictability remains.\nThe edge-case problem is compounded by AV technology that acts “supremely confidently” when it’s wrong, says Philip Koopman, associate professor of electrical and computer engineering at Carnegie Mellon University. “It’s really bad at knowing when it doesn’t know.” The perils of this are evident in analysing the Uber crash in which a prototype AV killed Elaine Herzberg as she walked her bicycle across a road in Arizona, in 2018. An interview with the safety operator behind the wheel at the time describes the software flipping between different classifications of Herzberg’s form – “vehicle”, “bicycle”, “other” – until 0.2 seconds before the crash.\nT\nhe ultimate aim of AV makers is to create cars that are safer than human-driven vehicles. In the US, there is about one death for every 100m miles driven by a human (including drunk driving). Koopman says AV makers would have to beat this to prove their technology was safer than a human. But he also believes somewhat comparable metrics used by the industry, such as disengagement data (how often a human needs to take control to prevent an accident), elide the most important issues in AV safety.\n“Safety isn’t about working right most of the time. Safety is all about the rare case where it doesn’t work properly,” says Koopman. “It has to work 99.999999999% of the time. AV companies are still working on the first few nines, with a bunch more nines to go. For every nine, it’s 10 times harder to achieve.”\nSome experts believe AV makers won’t have to completely crack human-level intelligence to roll out self-driving vehicles. “I think if every car was a self-driving car, and the roads were all mapped perfectly, and there were no pedestrians around, then self-driving cars would be very reliable and trustworthy,” says Mitchell. “It’s just that there’s this whole ecosystem of humans and other cars driven by humans that AI just doesn’t have the intelligence yet to deal with.”\nCruise Origin founder Kyle Vogt at the company’s launch. Photograph: Stephen Lam\/Reuters\nUnder the right conditions, such as quiet roads and favourable weather, self-driving cars can mostly function well. This is how Waymo is able to run a limited robotaxi service in parts of Phoenix, Arizona. However, this fleet has still been involved in minor accidents and one vehicle was repeatedly stumped by a set of traffic cones despite a remote worker providing assistance. (A Waymo executive claimed they were not aware of these incidents happening more than with a human driver.)\nDespite the challenges, the AV industry is speeding ahead. The Uber crash had a temporarily sobering effect; manufacturers suspended trials afterwards owing to negative press and Arizona’s governor suspended Uber’s testing permit. Uber and another ride-hailing company, Lyft, both then sold their self-driving divisions.\nBut this year has marked a return to hubris – with more than $100bn invested in the past 10 years, the industry can hardly afford to shirk. Carmakers General Motors and Geely and AV company Mobileye have said people may be able to buy self-driving cars as early as 2024. Cruise and Waymo both aim to launch commercial robotaxi operations in San Francisco this year. Aurora also plans to deploy fully autonomous vehicles in the US within the next two to three years.\nS\nome safety experts are concerned by the lack of regulation governing this bold next step. At present, every company “basically gets one free crash”, says Koopman, adding that the regulatory system in the US is predicated on trust in the AV maker until a serious accident occurs. He points to Uber and AV startup Pony.ai, whose driverless test permit was recently suspended in California after a serious collision involving one of its vehicles.\nA side-effect of Tesla sharing its technology with customers is that regulators are taking notice. Tesla has so far avoided the more stringent requirements of other AV makers, such as reporting crashes and systems failures and using trained safety professionals as testers, because of the claim that its systems are more basic. But California’s Department of Motor Vehicles, the state’s autonomous driving regulator, is considering changing the system, in part because of the dangerous-looking videos of the technology in action, as well as investigations into Tesla by the NHTSA.\nThe dearth of regulation so far highlights the lack of global consensus in this space. The question, says Maynard, is “is the software going to mature fast enough that it gets to the point where it’s both trusted and regulators give it the green light, before something really bad happens and pulls the rug out from the whole enterprise?”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n\"The news discusses the challenges and issues faced by self-driving cars, particularly those developed by Tesla.\",\n\"Elon Musk, Tesla's CEO, has made several promises about achieving full self-driving capabilities but has faced delays and setbacks.\",\n\"Tesla expanded beta trials of its 'Full Self-Driving' software to about 60,000 owners, which has raised concerns.\",\n\"Videos have surfaced showing reckless behavior of Tesla's Full Self-Driving system, including veering into oncoming traffic and attempting to turn onto train tracks.\",\n\"There have been reports of a crash involving Full Self-Driving technology, although no one was injured.\",\n\"The article also mentions that other companies like Waymo and Cruise use safety operators for testing, unlike Tesla's approach of relying on customer testers.\",\n\"The challenges of self-driving cars include handling complex inner-city streets, 'edge cases' (unusual and rare events on the road), and the limitations of machine learning in dealing with such situations.\",\n\"The lack of regulation and the need for trust in autonomous vehicle technology are also discussed.\",\n\"Despite the challenges, the autonomous vehicle industry is still moving forward with ambitious goals and investments.\"\n]\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2022\/mar\/27\/how-self-driving-cars-got-stuck-in-the-slow-lane","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":22,"Time":null,"Title":"California says self-driving cars must have driver for now","Content":"By Alexandria Sage\nSAN FRANCISCO (Reuters) - California, the largest car market in the United States, issued draft rules on Wednesday for self-driving cars, requiring a licensed driver inside the vehicle in case of failure in a plan that stresses safety.\nThe regulations by the Department of Motor Vehicles are intended to help nurture the state's nascent but fast-growing autonomous vehicle technology industry while allowing traditional car companies and new entrants like Alphabet Inc GOOGL.O and Apple Inc AAPL.O to safely deploy their self-driving cars already in development.\nThe rules, which will face a period of public comment before being finalised, set out a path to take the industry from the current stage of vehicle testing to actually rolling them out to consumers.\nCalifornia has been at the forefront of the fast-growing autonomous vehicle industry, fuelled by technology companies in Silicon Valley, and is one of a handful of states to have passed regulations enabling self-driving car testing on public roads.\nCurrently, 11 companies have permits to drive autonomous vehicles on public roads in the state, provided there is a licensed driver in the car, with Ford being the most-recent entrant.\nThe proposed regulations require certification and third-party testing for carmakers, as well as regular reports back to the DMV for a period of three years. Data from that testing will be used to inform future regulation, the DMV said.\nManufacturers are also required to disclose the data they collect, other than from safety systems, and obtain approval to collect it. Concerns that self-driving cars could be a way for major data collectors like Google to collect information on consumers have fuelled privacy concerns.\nGoogle - which is operating its self-driving cars on the streets of Palo Alto, California and Austin, Texas - and other carmakers and suppliers have said the technology to build self-driving cars should be ready by 2020.\nEditing by Stephen R. Trousdale and Alan Crosby\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"California has issued draft rules for self-driving cars, requiring a licensed driver inside the vehicle in case of failure. The regulations aim to nurture the state's autonomous vehicle technology industry while allowing traditional car companies and technology companies like Alphabet Inc and Apple Inc to safely deploy their self-driving cars. Currently, 11 companies have permits to drive autonomous vehicles on public roads in California with a licensed driver in the car, including Ford. The proposed regulations also require certification, third-party testing, and data disclosure by carmakers, addressing privacy concerns related to data collection by self-driving cars.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/california-regulations-self-driving-idINKBN0TZ2SI20151216","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":18,"Time":"Tue 9 Aug 2022 19.08 CEST","Title":"Tesla’s self-driving technology fails to detect children in the road, group claims","Content":"A safe-technology campaign group opposed to Tesla’s self-driving technology has claimed to have run tests that show the software represents a potentially lethal threat to child pedestrians, in the latest in a series of claims and investigations to hit the world’s leading electric carmaker.\nThe Dawn Project says its test track results revealed that the latest version of Tesla’s Full Self-Driving (FSD) Beta software failed to detect a stationary, child-sized mannequin at an average speed of 25mph. The claim is the central plank of an ad campaign urging the public to press Congress to ban Tesla’s auto-driving technology.\nThe Dawn Project was founded by the software entrepreneur Dan O’Dowd, an outspoken critic of Tesla boss Elon Musk and what he says is Tesla’s “reckless” deployment of unsafe self-driving vehicles.\nCalifornia tech titan’s Senate run has one target: Tesla’s self-driving software\nRead more\nO’Dowd described the test results – which have been published on the Dawn Project’s website – as “deeply disturbing” and that Tesla’s software represented “a lethal threat to all Americans.”\nHe added: “Over 100,000 Tesla drivers are already using the car’s Full Self-Driving mode on public roads, putting children at great risk in communities across the country.”\nO’Dowd argued that the results show the need to prohibit self-driving cars until Tesla proves the vehicles “will not mow down children in crosswalks”.\nTesla has repeatedly hit back at claims that its self-driving technology is too underdeveloped to guarantee the safety of either the car’s occupants or other road users.\nO’Dowd has drawn accusations that he is little more than a competitor to Tesla because his company bills itself as an expert in making particular software used in automated driving systems. O’Dowd insists his Green Hills software doesn’t compete with Tesla, saying it doesn’t make self-driving cars. But he has acknowledged some car companies use his company’s software for certain components.\nAfter a fiery crash in Texas in 2021 that killed two, Musk tweeted that the autopilot feature, a less sophisticated version of FSD, was not switched on at the moment of collision.\nAt the company’s shareholder meeting earlier this month Musk said that Full Self-Driving has greatly improved, and he expected to make the software available by the end of the year to all owners that request it. But questions about its safety continue to mount.\nIn June, the National Highway Traffic Safety Administration (NHTSA), said it was expanding an investigation into 830,000 Tesla cars across all four current model lines. The expansion came after analysis of a number of accidents revealed patterns in the car’s performance and driver behavior.\nThe NHTSA said the widened investigation would aim to examine the degree to which Tesla’s autopilot technology and associated systems “may exacerbate human factors or behavioral safety risks by undermining the effectiveness of the driver’s supervision”.\nA second NHTSA investigation is also under way to determine if the removal of the forward-looking radar sensor on some newer Teslas is causing the vehicles to apply their brakes for no reason, which is called “phantom braking” and can lead to wrecks.\nSince 2016, the agency has investigated 30 crashes involving Teslas equipped with automated driving systems, 19 of them fatal. NHTSA’s Office of Defects Investigation is also looking at the company’s autopilot technology in at least 11 crashes where Teslas hit emergency vehicles.\nMany such wrecks aren’t investigated by the NHTSA. And in nearly 400 crashes involving cars with driver-assist systems reported by automakers between July 2021 and this past May, more Teslas were involved than all other manufacturers combined.\nThis article was amended on 11 August 2022 to provide further context of, and give appropriate significance to, the Dawn Project’s stated political opposition to Tesla’s self-driving technology. The headline was changed to indicate that the Dawn Project’s test results are claimed by the group, and have not been independently verified.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about autonomous vehicle failures involving Tesla's self-driving technology. The Dawn Project, a campaign group opposed to Tesla's self-driving technology, conducted tests that showed the software failed to detect a stationary, child-sized mannequin at an average speed of 25mph. This failure is at the center of an ad campaign urging the public to press Congress to ban Tesla's auto-driving technology. The article also discusses concerns about the safety of Tesla's self-driving technology, including investigations by the National Highway Traffic Safety Administration (NHTSA) into accidents involving Tesla cars with automated driving systems.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2022\/aug\/09\/tesla-self-driving-technology-safety-children","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":26,"Time":null,"Title":"U.S. probes touchscreen failures in Tesla Model S cars","Content":"By David Shepardson\nWASHINGTON (Reuters) - The U.S. National Highway Traffic Safety Administration (NHTSA) said Tuesday it had opened an investigation into 63,000 Tesla Model S cars after reports of media-control unit failures that led to the loss of the use of touchscreens.\nA Tesla logo adorns a 'Model S' car in the dealership in Berlin, Germany, November 18, 2015. REUTERS\/Hannibal Hanschke\/File Photo\nThe auto safety agency said the preliminary evaluation, covering 2012-2015 model year vehicles, comes after it received 11 complaints alleging premature failure of the media-control unit due to memory wear-out.\nA complete unit failure results in the loss of audible and visual touchscreen features, such as infotainment, navigation, and web browsing, and loss of the rear-camera image display when in reverse gear, the agency said.\nTesla did not immediately respond to a request for comment.\nNHTSA said the failure does not affect vehicle-control systems.\nTesla used the same unit in 159,000 2012-2018 Model S and 2016-2018 Model X vehicles built by Tesla through early 2018.\nThe memory control unit uses an Nvidia Corp Tegra 3 processor with an integrated 8GB eMMC NAND flash memory device, NHTSA said. Nvidia did not immediately respond to a request for comment.\nThe flash devices have a finite lifespan based on the number of program or erase cycles, NHTSA said.\nFailures resulting from memory wear-out “are likely to occur after periods of progressively degraded performance (e.g., longer power-up times, more frequent touchscreen resets, intermittent loss of cellular connectivity, loss of navigation),” the agency said.\nSome complaints said the failure could result in a loss of charging ability and that other safety alerts could be impacted. One driver said he could not clear fogged windows because he could not change climate controls.\nAnother complaint said the failure disabled safety monitors associated with Tesla’s driver-assistance system Autopilot.\nThe complaints said Tesla requires owners to pay to replace the unit once the warranty expires.\nReporting by David Shepardson; Editing by Bernadette Baum\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news article:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Investigation_title\": \"U.S. probes touchscreen failures in Tesla Model S cars\",\n    \"Investigation_source\": \"Reuters\",\n    \"Investigation_date\": \"Tuesday\",\n    \"Description\": \"The U.S. National Highway Traffic Safety Administration (NHTSA) has opened an investigation into 63,000 Tesla Model S cars due to reports of media-control unit failures. These failures have resulted in the loss of the use of touchscreens. The investigation covers model years 2012-2015 and was initiated after receiving 11 complaints about the premature failure of the media-control unit due to memory wear-out. Complete unit failure leads to the loss of audible and visual touchscreen features, including infotainment, navigation, web browsing, and the rear-camera image display when in reverse gear. It was noted that this failure does not affect vehicle-control systems.\"\n  }\n}\nThe news is indeed relevant to autonomous vehicle technology, specifically regarding touchscreen failures in Tesla Model S cars, and the specific information has been extracted as detailed as possible.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/tesla-safety-idINKBN23V1X4","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":28,"Time":"DECEMBER 20, 2018","Title":"Uber resuming self-driving car testing in Pennsylvania","Content":"By David Shepardson\nWASHINGTON (Reuters) - Uber Technologies Inc [UBER.UL] on Thursday is resuming limited self-driving car testing on public roads in Pittsburgh, nine months after it suspended the program following a deadly accident in Arizona.\nFILE PHOTO: Uber's logo is displayed on a mobile phone, September 14, 2018. REUTERS\/Hannah Mckay\/File Photo\nThe return of testing comes days after the state of Pennsylvania granted Uber permission to resume testing.\nThe relaunch is a significant reduction from Uber’s previous self-driving operations. The company had operated its fleet in autonomous mode on public roads at high speeds, after dark, in areas crowded with pedestrians and with a single backup driver in the front seat.\nThis time, the cars will not operate at night or in wet weather and will not exceed speeds of 25 miles per hour, the company has said.\nUber will resume testing with two employees in the front seat, enable an automatic braking system at all times, and more strictly monitor safety employees after it said it made improvements to the vehicles’ self-driving software. The company for the time being will not resume picking up passengers in the robot cars, a service Uber launched in 2016.\nReuters reported earlier this month that Uber planned to initially begin driving “a handful” of cars on a mile loop between two company offices in Pittsburgh, where Uber first debuted its autonomous vehicles in 2016.\nUber also said it is resuming manual driving of its self-driving cars in Toronto and San Francisco that will be controlled by a driver and monitored by a second person in the passenger seat. Manual driving helps the company gather data on new driving scenarios, Uber said.\nUber said it “will only pursue a return to road for self-driving in these cities in coordination with federal, state, and local authorities.”\nEric Meyhofer, who heads Uber’s Advanced Technologies Group, said in a blog post that Uber has “reviewed and improved our testing program to ensure that our vehicles are considerate and defensive drivers.” He added that vehicles “must pass a series of more than 70 scenarios without safety-related failures on our test track” before testing on public roads.\nIn March, authorities in Arizona suspended Uber’s ability to test its self-driving cars after one of its cars hit and killed a woman crossing the street at night in the Phoenix suburb of Tempe, Uber’s largest testing hub. Uber also voluntarily halted its entire autonomous car testing program.\nThe crash was the first death attributed to a self-driving vehicle. The death prompted significant safety concerns about the nascent self-driving car industry, which is racing to get vehicles into commercial use.\nThe National Transportation Safety Board (NTSB) and National Highway Traffic Safety Administration are investigating the fatal crash.\nThe NTSB said Uber had disabled an emergency braking system in the modified Volvo test vehicle. Uber said the Volvo automatic emergency braking system will be activated at all times.\nReporting by David Shepardson; Editing by Christopher Cushing\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Uber is resuming limited self-driving car testing on public roads in Pittsburgh, Pennsylvania, after a nine-month suspension following a deadly accident in Arizona. The testing conditions have been significantly reduced compared to previous operations, with restrictions on nighttime and wet weather operations, a speed limit of 25 miles per hour, and two employees in the front seat. Uber will also enable an automatic braking system at all times and more strictly monitor safety employees after improvements to the vehicles’ self-driving software. However, Uber will not resume picking up passengers with self-driving cars for the time being. The resumption of manual driving of self-driving cars in Toronto and San Francisco, controlled by a driver and monitored by a second person, is also mentioned.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-uber-selfdriving-idUSKCN1OJ1OD","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":23,"Time":"9:33 PM GMT+1•March 19, 2018","Title":"Here’s how Uber’s self-driving cars are supposed to detect pedestrians","Content":"A self-driving vehicle made by Uber has struck and killed a pedestrian. It’s the first such incident and will certainly be scrutinized like no other autonomous vehicle interaction in the past. But on the face of it it’s hard to understand how, short of a total system failure, this could happen, when the entire car has essentially been designed around preventing exactly this situation from occurring.\nSomething unexpectedly entering the vehicle’s path is pretty much the first emergency event that autonomous car engineers look at. The situation could be many things — a stopped car, a deer, a pedestrian — and the systems are one and all designed to detect them as early as possible, identify them and take appropriate action. That could be slowing, stopping, swerving, anything.\nUber’s vehicles are equipped with several different imaging systems which work both ordinary duty (monitoring nearby cars, signs and lane markings) and extraordinary duty like that just described. No less than four different ones should have picked up the victim in this case.\nTop-mounted lidar. The bucket-shaped item on top of these cars is a lidar, or light detection and ranging, system that produces a 3D image of the car’s surroundings multiple times per second. Using infrared laser pulses that bounce off objects and return to the sensor, lidar can detect static and moving objects in considerable detail, day or night.\nThis is an example of a lidar-created imagery, though not specifically what the Uber vehicle would have seen.\nHeavy snow and fog can obscure a lidar’s lasers, and its accuracy decreases with range, but for anything from a few feet to a few hundred feet, it’s an invaluable imaging tool and one that is found on practically every self-driving car.\nThe lidar unit, if operating correctly, should have been able to make out the person in question, if they were not totally obscured, while they were still more than a hundred feet away, and passed on their presence to the “brain” that collates the imagery.\nFront-mounted radar. Radar, like lidar, sends out a signal and waits for it to bounce back, but it uses radio waves instead of light. This makes it more resistant to interference, since radio can pass through snow and fog, but also lowers its resolution and changes its range profile.\nTesla’s Autopilot relies mostly on radar.\nDepending on the radar unit Uber employed — likely multiple in both front and back to provide 360 degrees of coverage — the range could differ considerably. If it’s meant to complement the lidar, chances are it overlaps considerably, but is built more to identify other cars and larger obstacles.\nThe radar signature of a person is not nearly so recognizable, but it’s very likely they would have at least shown up, confirming what the lidar detected.\nShort and long-range optical cameras. Lidar and radar are great for locating shapes, but they’re no good for reading signs, figuring out what color something is and so on. That’s a job for visible-light cameras with sophisticated computer vision algorithms running in real time on their imagery.\nThe cameras on the Uber vehicle watch for telltale patterns that indicate braking vehicles (sudden red lights), traffic lights, crossing pedestrians and so on. Especially on the front end of the car, multiple angles and types of camera would be used, so as to get a complete picture of the scene into which the car is driving.\nDetecting people is one of the most commonly attempted computer vision problems, and the algorithms that do it have gotten quite good. “Segmenting” an image, as it’s often called, generally also involves identifying things like signs, trees, sidewalks and more.\nThat said, it can be hard at night. But that’s an obvious problem, the answer to which is the previous two systems, which work night and day. Even in pitch darkness, a person wearing all black would show up on lidar and radar, warning the car that it should perhaps slow and be ready to see that person in the headlights. That’s probably why a night-vision system isn’t commonly found in self-driving vehicles (I can’t be sure there isn’t one on the Uber car, but it seems unlikely).\nSafety driver. It may sound cynical to refer to a person as a system, but the safety drivers in these cars are very much acting in the capacity of an all-purpose failsafe. People are very good at detecting things, even though we don’t have lasers coming out of our eyes. And our reaction times aren’t the best, but if it’s clear that the car isn’t going to respond, or has responded wrongly, a trained safety driver will react correctly.\nWorth mentioning is that there is also a central computing unit that takes the input from these sources and creates its own more complete representation of the world around the car. A person may disappear behind a car in front of the system’s sensors, for instance, and no longer be visible for a second or two, but that doesn’t mean they ceased existing. This goes beyond simple object recognition and begins to bring in broader concepts of intelligence such as object permanence, predicting actions and the like.\nIt’s also arguably the most advanced and closely guarded part of any self-driving car system and so is kept well under wraps.\nIt isn’t clear what the circumstances were under which this tragedy played out, but the car was certainly equipped with technology that was intended to, and should have, detected the person and caused the car to react appropriately. Furthermore, if one system didn’t work, another should have sufficed — multiple failbacks are only practical in high-stakes matters like driving on public roads.\nWe’ll know more as Uber, local law enforcement, federal authorities and others investigate the accident.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"A self-driving vehicle made by Uber struck and killed a pedestrian. It's the first such incident and will certainly be scrutinized like no other autonomous vehicle interaction in the past. The news discusses various technologies employed by Uber's self-driving vehicles, including lidar, radar, and optical cameras, and how these systems are designed to detect objects and pedestrians in the vehicle's path. It also mentions the role of safety drivers and the central computing unit in ensuring safety. The circumstances of the tragedy are not clear yet, and further investigations are ongoing.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2018\/03\/19\/heres-how-ubers-self-driving-cars-are-supposed-to-detect-pedestrians\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":25,"Time":"9:33 PM GMT+2•June 22, 2021","Title":"How Amazon-owned Zoox designed its self-driving vehicles to prevent crashes","Content":"cameraImage Credits: Zoox\nThe hubbub surrounding the autonomous vehicle industry often focuses on venture capital rounds, speculation about IPOs and acquisitions. But the industry’s future also hinges on the high-stakes task of proving the technology can operate safer than human drivers do today and gaining the public’s trust. In short: safety matters.\nZoox issued a safety report Tuesday that aims to give new insight into its custom electric autonomous vehicle and describes in greater detail various design details aimed at preventing crashes and protecting if they do.\n“As you know, and something everybody keeps talking about, is that part of the rationale for doing AVs is because of safety, safety, safety, but they never get to the next bullet [point] right? What are you going to actually do to prevent those crashes, to save those lives?” Mark Rosekind, the company’s chief safety innovation officer and former head of the National Highway Traffic Safety Administration, told TechCrunch in a recent interview.\nRosekind says this latest report answers those questions.\nZoox is a bit different from its rivals. It isn’t just developing the self-driving software stack. The company is responsible for creating the on-demand ride-sharing app and the vehicle itself. Zoox also plans to own, manage and operate its robotaxi fleet.\nZoox unveiled in December the electric, autonomous robotaxi it built from the ground up — a cube-like vehicle loaded with sensors, no steering wheel and a moonroof that is capable of transporting four people at speeds of up to 75 miles per hour. At the time, Zoox shared a few specs on the four-seat vehicle, including the face-to-face symmetrical seating configuration, similar to what a train traveler might encounter, and the 133 kilowatt-hour battery that the company said allows it to operate for up to 16 continuous hours on a single charge. But not everything was revealed, particularly details about how it would protect occupants in the vehicle as well as the pedestrians, cyclists and other drivers it will be sharing the road with.\nTo be clear, Zoox is not the only AV company issuing safety reports. Voluntary safety self-assessment reports, or VSSAs, have become fairly common in the industry. These voluntary safety reports, which are included in NHTSA’s Automated Driving Systems VSSA Disclosure Index, are supposed to cover 12 areas, including the vehicle’s design, crash simulation scenarios and benchmarks for testing, as well as protective measures for occupants and other road users.\nZoox’s first safety report came out in 2018, which outlined the company’s “prevent and protect” philosophy. This latest one reveals how Zoox plans to meet its safety goals, including specific details on the design of the vehicle. And more safety reports are coming — per a few hints in this latest one — including details about its collision avoidance system and the lighting system the vehicle uses for communicating with other road users.\nZoox has designed and included more than 100 safety innovations into its purpose-built vehicle. Rosekind shared details on nine of them that fall into three categories: driving control, no single point of failure and rider protection.\nDriving control\nImage Credits: Zoox\nZoox’s vehicle has independent braking and an active suspension system, which means that each of the brakes has its own electronic control unit, allowing for more control over traction on the road and weight distribution. All of that translates to shorter stopping distances.\nThe vehicle also has four-wheel steering, which Rosekind noted doesn’t exist on any AV car on the road today, and is bidirectional. Four-wheel steering allows the vehicle to simultaneously adjust where it is headed and its position within the lane.\n“Once our software has determined the path for the vehicle, it’ll stay on that path down to centimeters accuracy — even at speed through a curb, Rosekind explained.\nThe four-wheel steering combined with the vehicle’s symmetrical design allows for it to travel bidirectionally. The bidirectional capability means no more U-turns or three-point turns, two maneuvers that are more complex, time consuming and can make occupants more vulnerable to oncoming traffic. \nNo single point of failure\nRosekind said the company’s design objective was that there would be no single point of failure for its safety critical systems. For instance, the vehicle has two powertrains. The motors, drive systems and batteries work in conjunction with each other. If one component in the system fails, the other one will take over.\nThe vehicle also has two batteries as well as a safety diagnostics system that monitors all of the hardware, software and firmware. Sensors like lidar and radar are also placed on the four corners of the vehicle — each one provides a 270-degree field of view.\nThe diagnostic system goes beyond monitoring and will mitigate a failure or performance problem that it identifies. For instance, if a sensor has degraded performance from damage or debris, it will activate a cleaning system on the vehicle or turn it from bidirectional to unidirectional, placing the sensor in a position where it basically doesn’t matter if it is obscured, Rosekind explained. \n“Fail-safe operational means it’s going to continue the ride, let you out, and then go take care of whatever the issue is, or pull over to a safe spot,” he said.\nRider protection\nImage Credits: Zoox\nZoox’s goal is for its vehicle to meet five-star crash protection for every seat in the vehicle. The vehicles are currently going through crash testing now, Rosekind said, adding that it is “going quite well and almost complete.”\nThe company also designed a new kind of airbag system that contains five different airbags. Curtain airbags are on each side of the vehicle, a frontal one is divided in two parts to protect the head, neck and chest. There are also rear and side seat airbags.\nWithin the system is an airbag control unit that can monitor where a collision is coming as well as the velocity and determine which airbags and in what order to deploy. Instead of every airbag deploying at once, they will inflate based on the collision location and the severity of the impact.\nFinally, the vehicle has sensors in the seat, the buckle and even the coating on the webbing of the seatbelt to be able to tell if passengers are using the seatbelt. The vehicle will not start until everybody’s buckled up, Rosekind said.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Amazon-owned Zoox has released a safety report detailing various design details aimed at preventing crashes and protecting occupants. The report discusses features such as independent braking and an active suspension system, bidirectional four-wheel steering, a no single point of failure design with two powertrains and multiple sensors, and a comprehensive airbag system. It also mentions the vehicle's goal of meeting five-star crash protection for every seat and its sensors to ensure passengers are using seatbelts.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2021\/06\/22\/how-amazon-owned-zoox-designed-its-self-driving-vehicles-to-prevent-crashes-and-protect-if-they-do\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":34,"Time":"July 21, 2023","Title":"Tesla's Elon Musk optimistic on progress for self-driving, robots","Content":"SAN FRANCISCO, July 19 (Reuters) - Tesla (TSLA.O) Chief Executive Elon Musk on Wednesday set new targets for artificial intelligence products including self-driving software and using humanoid robots in factories, though he acknowledged he's been optimistic before.\nThe electric vehicle maker is in early talks with a major automaker to license its full self-driving technology, Musk added.\nThe value of Tesla vehicles would rise in perhaps \"the single biggest step change in history\" once regulators approved self-driving, he said at an earnings briefing. Musk has also said that Tesla robots, in pilot phase, could become a huge product. He said they could help out on Tesla's factory floors as soon as next year, although only about 10 have been built to date.\nRising interest rates and competition from new EV makers have forced Tesla to cut vehicle prices to gain market share, hurting margins.\nBut Musk said Tesla will keep pushing to expand sales volume at the cost of profit margins, betting on the long-term value from FSD. \"Autonomy will make all of these numbers look silly,\" he said.\nTesla's move to license its technology comes after years of failed promises by many to create software that lets cars drive themselves.\nAdvertisement · Scroll to continue\nThe licensing announcement was not surprising, given industry failures, Ark Invest's Tasha Keeney said on Twitter. \"Autonomy is hard, it requires vast amounts of data, and I believe many automakers will fail to achieve it on their own.\"\nTesla has completed over 300 million miles in the beta version of FSD, over half of which was in the past quarter, according to an earnings presentation.\nBut Musk was more cautious than usual.\nAdvertisement · Scroll to continue\n\"People have sort of made fun of me and perhaps quite fairly have made fun of me, my predictions about achieving full self-driving have been optimistic in the past,\" he said.\n\"I'm the boy who cried FSD, but I think we'll be better than human by the end of this year,\" he said. \"I've been wrong in the past, I may be wrong this time.\"\nReporting by Abhirup Roy in San Francisco; Editing by Peter Henderson and Sam Holmes\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Tesla's Chief Executive Elon Musk discussed new targets for artificial intelligence products, including self-driving software and the use of humanoid robots in factories. He mentioned that Tesla is in early talks with a major automaker to license its full self-driving technology. Musk also expressed optimism about the value of Tesla vehicles rising significantly once regulators approve self-driving technology. He mentioned the pilot phase of Tesla robots and their potential use on factory floors. However, he acknowledged past optimism about achieving full self-driving and was more cautious this time, saying, 'I've been wrong in the past, I may be wrong this time.'\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/teslas-elon-musk-optimistic-progress-self-driving-robots-2023-07-20\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":29,"Time":"Wed 7 Jun 2023 22.17 CEST","Title":"Self-driving Waymo car kills dog amid increasing concern over robotaxis","Content":"A Waymo self-driving car killed a small dog in San Francisco while in “autonomous mode” last month, the latest in a string of incidents that have led city officials to protest a planned expansion of the company’s driverless taxi services.\nMy week navigating the awkward teenage years of self-driving cars\nRead more\nThe 21 May collision occurred just before 11am on Toland Street in San Francisco as the car was in “autonomous mode” with a test driver in the front seat. A Waymo spokesperson said that an initial review showed that the system correctly identified the dog when it ran out from behind a parked car, but “was not able to avoid contact” due to how fast the canine was moving. The company said it was investigating the matter.\n“We send our sincere condolences to the dog’s owner,” the Waymo spokesperson said. “The trust and safety of the communities we are in is the most important thing to us and we’re continuing to look into this on our end.”\nThe car, a Jaguar, sustained minor damage, according to a California DMV report.\nThe collision comes as Waymo, which is owned by the Google parent company, Alphabet, and its competitor, Cruise, seek to dramatically scale up their robotaxi operations throughout the city. Cruise, operated by General Motors, currently has permission to charge fares for driverless taxi rides between 10pm and 6am in some parts of the city, but its cars must avoid the densest downtown areas. Waymo only has authority to charge fares if a safety driver is present in the car.\nThe California public utilities commission, which regulates the companies’ robotaxi services, is on the cusp of approving the companies’ requests to operate their self-driving taxis at all hours and throughout the entire city. The commission is set to vote on 29 June on whether to approve draft resolutions that would allow Waymo and Cruise to offer round-the-clock robotaxi service.\nBut the resolutions have faced fierce opposition from local officials. The San Francisco municipal transportation agency, San Francisco county transportation authority and the mayor’s office of disability authored a letter pushing back on the commission’s plan.\nThe city has recorded “dozens – perhaps hundreds or thousands – of incidents” in which autonomous vehicles were reported to drive erratically, block traffic and interfere with emergency response operations, officials wrote.\nLast year, a viral video showed police stopping one of Cruise’s autonomous vehicles for driving without headlights. When police pulled the car over, the driverless vehicle sped away to the other side of the intersection. In another incident, nearly 20 of Cruise’s self-driving cars blocked traffic for more than two hours, forcing the company’s employees to physically move the cars off the street.\nSign up to Headlines US\nFree newsletter\nGet the most important US headlines and highlights emailed direct to you every morning\nPrivacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.\nOther incidents include autonomous vehicles trapping other cars at the curb, entering construction zones and crime scenes and failing to follow directions from traffic control officers. San Francisco officials also pointed to a lack of data transparency and adequate reporting and monitoring as reasons for the commission to deny the companies’ requests.\n“This approach is backwards and is inconsistent with the commission’s power and duty to protect not only passenger safety but the safety of the general public,” they wrote.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"A Waymo self-driving car killed a small dog in San Francisco while in 'autonomous mode' last month. The 21 May collision occurred just before 11am on Toland Street in San Francisco as the car was in 'autonomous mode' with a test driver in the front seat. Waymo spokesperson stated that the system correctly identified the dog when it ran out from behind a parked car but 'was not able to avoid contact' due to how fast the canine was moving. The car, a Jaguar, sustained minor damage, according to a California DMV report. The collision has raised concerns about autonomous vehicle safety in the city, as city officials are protesting the planned expansion of Waymo's driverless taxi services.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/us-news\/2023\/jun\/07\/waymo-car-kills-dog-self-driving-robotaxi-san-francisco","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":31,"Time":"2:17 AM GMT+2•July 31, 2019","Title":"Drone crash near kids leads Swiss Post and Matternet to suspend autonomous deliveries","Content":"cameraImage Credits: Matternet Inc.\n(opens in a new window)\nunder a license.\n(Update: With the investigation concluded and recommended improvements underway, the Matternet-Swiss Post partnership will take flight again in January 2020.)\nA serious crash by a delivery drone in Switzerland has grounded the fleet and put a partnership on ice. Within a stone’s throw of some kids, the incident raised grim possibilities for the possibilities of catastrophic failure of payload-bearing autonomous aerial vehicles.\nThe drones were operated by Matternet as part of a partnership with the Swiss Post (i.e. the postal service), which was using the craft to dispatch lab samples from one medical center for priority cases. As far as potential applications of drone delivery, it’s a home run — but twice now the craft have crashed, first with a soft landing and the second time a very hard one.\nThe first incident, in January, was the result of a GPS hardware error; the drone entered a planned failback state and deployed its emergency parachute, falling slowly to the ground. Measures were taken to improve the GPS systems.\nThe second failure in May, however, led to the drone attempting to deploy its parachute again, only to sever the line somehow and plummet to earth, crashing into the ground some 150 feet from a bunch of kindergartners. No one was hit, but this narrowly avoided being a worst-case scenario for the service: not just a craft failing, but the emergency systems failing as well, and immediately over a bunch of children. The incident was documented last month but not widely reported.\nFalling from a few hundred feet, the 12-kilogram (about 26 pounds) drone and payload could easily have seriously injured or even killed someone — this is why there are very strict regulations about flying over populated areas and crowds. Matternet noted that the area the drone came down in is not populated or a school, but rather a forested area of Zurich more appropriate for flying over.\nObviously they grounded the fleet following this incident and will not spin up again until Matternet addresses the various issues involved. How was it even possible, for instance, that the parachute line was capable of being cut by something on the drone?\nIEEE Spectrum first noted the news stateside. The company issued the following statement on the matter:\nThis is the first time ever that our vehicle parachute system has failed. As stated in the report, the flight termination system was triggered nominally per the drone’s specification, but the parachute cord was severed during the parachute deployment.\nAt Matternet, we take the safety of our technology and operations extremely seriously. A failure of the parachute safety mechanism system is unacceptable and we are taking all the appropriate measures to address it.\nSwiss Post and Matternet reacted to the incident immediately by grounding all the operations involving this vehicle type. Our experts analyzed the incident and proposed the appropriate mitigations which are being evaluated by FOCA. We will restart operations once Matternet and Swiss Post, FOCA and our hospital customers in Switzerland are satisfied that the appropriate mitigations have been applied.\nDrone delivery is a promising field, but situations like this don’t do it any favors when regulators take a look. Despite sunny predictions from the industry, there is a huge amount of work yet to be done in terms of flight proving the technology, and although two failures out of some 3,000 may not sound like a lot, if one of those failures is an uncontrolled fall that nearly takes out some kids, that could set back the entire industry.\n(This story has been slightly updated to accommodate a new statement from Matternet.)","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"A serious crash by a delivery drone in Switzerland has grounded the fleet and put a partnership on ice. The incident involved a Matternet delivery drone operated in partnership with Swiss Post. The first incident in January was caused by a GPS hardware error, while the second failure in May led to the drone attempting to deploy its parachute but severing the line and crashing into the ground near a group of kindergartners. Thankfully, no one was injured, but it raised concerns about the safety of payload-bearing autonomous aerial vehicles. Matternet has grounded its fleet until they address the issues involved, including the parachute safety mechanism system failure.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2019\/07\/30\/drone-crash-near-kids-leads-swiss-post-and-matternet-to-suspend-autonomous-deliveries\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":37,"Time":"August 28, 2023","Title":"Focus: Tesla braces for its first trial involving Autopilot fatality","Content":"SAN FRANCISCO, Aug 28 (Reuters) - Tesla Inc (TSLA.O) is set to defend itself for the first time at trial against allegations that failure of its Autopilot driver assistant feature led to death, in what will likely be a major test of Chief Executive Elon Musk's assertions about the technology.\nSelf-driving capability is central to Tesla’s financial future, according to Musk, whose own reputation as an engineering leader is being challenged with allegations by plaintiffs in one of two lawsuits that he personally leads the group behind technology that failed. Wins by Tesla could raise confidence and sales for the software, which costs up to $15,000 per vehicle.\nTesla faces two trials in quick succession, with more to follow.\nThe first, scheduled for mid-September in a California state court, is a civil lawsuit containing allegations that the Autopilot system caused owner Micah Lee’s Model 3 to suddenly veer off a highway east of Los Angeles at 65 miles per hour, strike a palm tree and burst into flames, all in the span of seconds.\nThe 2019 crash, which has not been previously reported, killed Lee and seriously injured his two passengers, including a then-8-year old boy who was disemboweled. The lawsuit, filed against Tesla by the passengers and Lee's estate, accuses Tesla of knowing that Autopilot and other safety systems were defective when it sold the car.\nAdvertisement · Scroll to continue\nMUSK 'DE FACTO LEADER' OF AUTOPILOT TEAM\nThe second trial, set for early October in a Florida state court, arose out of a 2019 crash north of Miami where owner Stephen Banner’s Model 3 drove under the trailer of an 18-wheeler big rig truck that had pulled into the road, shearing off the Tesla's roof and killing Banner. Autopilot failed to brake, steer or do anything to avoid the collision, according to the lawsuit filed by Banner's wife.\nAdvertisement · Scroll to continue\nTesla denied liability for both accidents, blamed driver error and said Autopilot is safe when monitored by humans. Tesla said in court documents that drivers must pay attention to the road and keep their hands on the steering wheel.\n\"There are no self-driving cars on the road today,\" the company said.\nThe civil proceedings will likely reveal new evidence about what Musk and other company officials knew about Autopilot’s capabilities – and any possible deficiencies. Banner’s attorneys, for instance, argue in a pretrial court filing that internal emails show Musk is the Autopilot team's \"de facto leader\".\nAdvertisement · Scroll to continue\nTesla and Musk did not respond to Reuters’ emailed questions for this article, but Musk has made no secret of his involvement in self-driving software engineering, often tweeting about his test-driving of a Tesla equipped with \"Full Self-Driving\" software. He has for years promised that Tesla would achieve self-driving capability only to miss his own targets.\nTesla won a bellwether trial in Los Angeles in April with a strategy of saying that it tells drivers that its technology requires human monitoring, despite the \"Autopilot\" and \"Full Self-Driving\" names. The case was about an accident where a Model S swerved into the curb and injured its driver, and jurors told Reuters after the verdict that they believed Tesla warned drivers about its system and driver distraction was to blame.\nAdvertisement · Scroll to continue\nSTAKES HIGHER FOR TESLA\nThe stakes for Tesla are much higher in the September and October trials, the first of a series related to Autopilot this year and next, because people died.\n\"If Tesla backs up a lot of wins in these cases, I think they're going to get more favorable settlements in other cases,\" said Matthew Wansley, a former General Counsel of nuTonomy, an automated driving startup and Associate Professor of Law at Cardozo School of Law.\nOn the other hand, \"a big loss for Tesla - especially with a big damages award\" could \"dramatically shape the narrative going forward,\" said Bryant Walker Smith, a law professor at the University of South Carolina.\nIn court filings, the company has argued that Lee consumed alcohol before getting behind the wheel and that it is not clear whether Autopilot was on at the time of crash.\nJonathan Michaels, an attorney for the plaintiffs, declined to comment on Tesla’s specific arguments, but said \"we're fully aware of Tesla's false claims including their shameful attempts to blame the victims for their known defective autopilot system.\"\nIn the Florida case, Banner's attorneys also filed a motion arguing punitive damages were warranted. The attorneys have deposed several Tesla executives and received internal documents from the company that they said show Musk and engineers were aware of, and did not fix, shortcomings.\nIn one deposition, former executive Christopher Moore testified there are limitations to Autopilot, saying it \"is not designed to detect every possible hazard or every possible obstacle or vehicle that could be on the road,\" according to a transcript reviewed by Reuters.\nIn 2016, a few months after a fatal accident where a Tesla crashed into a semi-trailer truck, Musk told reporters that the automaker was updating Autopilot with improved radar sensors that likely would have prevented the fatality.\nBut Adam (Nicklas) Gustafsson, a Tesla Autopilot systems engineer who investigated both accidents in Florida, said that in the almost three years between that 2016 crash and Banner’s accident, no changes were made to Autopilot’s systems to account for cross-traffic, according to court documents submitted by plaintiff lawyers.\nThe lawyers tried to blame the lack of change on Musk. \"Elon Musk has acknowledged problems with the Tesla autopilot system not working properly,\" according to plaintiffs' documents. Former Autopilot engineer Richard Baverstock, who was also deposed, stated that \"almost everything\" he did at Tesla was done at the request of \"Elon,\" according to the documents.\nTesla filed an emergency motion in court late on Wednesday seeking to keep deposition transcripts of its employees and other documents secret. Banner’s attorney, Lake \"Trey\" Lytal III, said he would oppose the motion.\n\"The great thing about our judicial system is Billion Dollar Corporations can only keep secrets for so long,\" he wrote in a text message.\nReporting by Dan Levine and Hyunjoo Jin in San Francisco, Editing by Peter Henderson and Grant McCool\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n{\n\"Title\": \"Tesla braces for its first trial involving Autopilot fatality\",\n\"Trial_Details\": [\n{\n\"Date\": \"Mid-September\",\n\"Location\": \"California state court\",\n\"Case_Description\": \"Allegations that the Autopilot system caused owner Micah Lee’s Model 3 to suddenly veer off a highway east of Los Angeles at 65 miles per hour, strike a palm tree and burst into flames, all in the span of seconds. The 2019 crash killed Lee and seriously injured his two passengers, including a then-8-year-old boy who was disemboweled. The lawsuit accuses Tesla of knowing that Autopilot and other safety systems were defective when it sold the car.\"\n},\n{\n\"Date\": \"Early October\",\n\"Location\": \"Florida state court\",\n\"Case_Description\": \"Arising out of a 2019 crash north of Miami where owner Stephen Banner’s Model 3 drove under the trailer of an 18-wheeler big rig truck that had pulled into the road, shearing off the Tesla's roof and killing Banner. Autopilot failed to brake, steer, or do anything to avoid the collision, according to the lawsuit filed by Banner's wife.\"\n}\n],\n\"Tesla's Response\": \"Tesla denied liability for both accidents, blamed driver error, and said Autopilot is safe when monitored by humans. Tesla said drivers must pay attention to the road and keep their hands on the steering wheel.\"\n}\n]\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/tesla-braces-its-first-trial-involving-autopilot-fatality-2023-08-28\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":36,"Time":"Mon 26 Sep 2016 21.36 CEST","Title":"Passengers in Uber's self-driving cars waived right to sue for injury or death","Content":"Anyone requesting an Uber ride in a 12-sq mile area in the center of Pittsburgh might now be randomly allocated a self-driving Ford Fusion rather than a human-operated vehicle.\nBut passengers riding in Uber’s computer-controlled cars today might be surprised at just how experimental the technology is. According to documents obtained by the Guardian under public records laws, until as recently as June anyone not employed by Uber riding in one of its autonomous vehicles (AVs) had to sign a legal document waiving the company of any liability for their injury or death.\nOne senior Pittsburgh police officer signed a waiver on 23 June: “I acknowledge that some or all of the AVs in which I ride are in a development phase, are experimental in nature, and that riding in an AV may involve the potential for death, serious injury, and\/or property loss.”\nThe document was required by Uber’s Advanced Technologies Center (ATC), a research hub that in early 2015 hired dozens of robotics experts from nearby Carnegie Mellon University. Since then the ATC has been working furiously to catch up with the likes of Google and Tesla in self-driving technology – a task Uber CEO Travis Kalanick has described as “basically existential for us”.\nHow driverless cars could change our whole future\nJohn Naughton\nRead more\nOver the summer, ATC ran a “Friends & Family” program that allowed contacts and family members to ride in the same self-driving cars that are now being used in the public trial.\nThe legal document obtained by the Guardian details what might have gone wrong on any ride. “Risks associated with riding in an AV may include, without limitation, those caused by equipment failure, development vehicle operators or other safety drivers, actions of other motorists, weather, temperature, road conditions, negligence or human error,” it says.\n“Nobody has set up software that can reliably drive a car safely without a human,” Kalanick told Bloomberg Businessweek in August. “We are focusing on that.” In the current self-driving Ubers, a human is always in the driver’s seat to cope with technical failures and unexpected situations – such a family of geese crossing the road.\nAt the moment, self-driving Ubers can be hailed between 7am and 10pm, and only in a downtown area that has previously been meticulously mapped to within centimeters. Uber would not confirm whether the pilot program would operate in challenging weather conditions like heavy rain, fog, snow or ice. All rides are currently free during the trial.\nWhile transporting passengers does not materially affect the way autonomous vehicles navigate or drive, it does allow Uber to see how riders react to being driven by a machine.\nUber admits it is filming passengers, inside and outside the car, to inform how it develops the technology. Picking up and dropping off customers may be trickier without a human driver to wave to, for example, or to point out a safe place to stop.\nDuring the Friends & Family program, users were obliged to assume complete responsibility for riding in Uber’s AVs, including “any risks that may arise from the negligence or carelessness of [Uber and the ATC], operation of the AVs and\/or dangerous or defective equipment”. They also had to release the company of liability from beyond the grave, with a clause that stated: “I hereby agree on behalf of myself, my executors, administrators, heirs [and] next of kin”.\nSelf-driving car design needs to involve regulators more, US government argues\nRead more\nPublic passengers today do not have to sign any such waiver, and Uber confirmed to the Guardian that all riders in self-driving Ubers are covered by $5m of accident insurance, in line with pending legislation in Pennsylvania.\n“It’s possible that, by not charging for rides, Uber’s pilot program avoids Pennsylvania’s rules for taxicabs,” says Bryant Walker-Smith, a professor at the University of South Carolina. “Courts generally hold that providers of transport for hire have a heightened responsibility for the safety of their passengers. However, I’d expect a court to hold Uber to a similarly high standard if its program were to result in injury.”\nThe blistering pace of Uber’s technology development has taken even self-driving experts by surprise. Anthony Levandowski, who built Google’s first self-driving car, went on to found an autonomous trucking start-up called Otto. In September, he joined Uber as a vice-president in charge of self-driving technology.\nSpeaking to the Guardian shortly before he was hired, Levandowski said: “Uber’s effort seems to be very logical. But it will take a long time for the tech to penetrate and be ubiquitous everywhere. It could take 25 years.”\nKalanick wants driverless self-driving Ubers much quicker than that. “We are going commercial,” he told Bloomberg Businessweek. “This can’t just be about science.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous vehicles as it discusses Uber's self-driving cars and the requirement for passengers to sign a legal document waiving liability for injury or death. It mentions that the technology is experimental, and riders may face risks associated with equipment failure, human error, and other factors. Additionally, the article mentions that self-driving Ubers currently have a human driver in the seat to handle technical failures and unexpected situations.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/sep\/26\/uber-self-driving-passengers-pittsburgh-injury-death-waiver","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":43,"Time":"JUNE 5, 2014","Title":"GM top executives spared in internal report on safety failure","Content":"By Ben Klayman\nWARREN Mich. (Reuters) - General Motors Co on Thursday issued a report detailing how for 11 years it turned a blind eye to an ignition-switch problem linked to at least 13 deaths but largely pinned the blame on what the report described as incompetent lower-level employees, leaving top brass untouched.\nA General Motors logo is seen on a Denali vehicle for sale at the GM dealership in Carlsbad, California January 4, 2012. REUTERS\/Mike Blake \/Files\nThe report, which will be the subject of upcoming congressional hearings, describes shortcomings of GM engineers, including a failure to understand “how the car was built.” Meanwhile, according to the 325-page report, the highest levels of the company were not made aware.\nProviding a rare peek into the operations of one of the world’s biggest automotive companies, the internal investigation said GM had a long-running corporate culture in which nobody took responsibility for problems.\nThe “GM nod” was how CEO Mary Barra described that culture, “when everyone nods in agreement to a proposed plan of action, but then leaves the room and does nothing,” the document said.\nIn February, GM finally began recalling vehicles for repairs. So far, 2.6 million vehicles have been identified. This recall, coupled with others announced by GM this year, has cost the company about $1.7 billion so far.\nBy 2011, three years before the recalls began, outside lawyers were warning GM’s in-house counsel that they needed to act, the report said.\nBarra said 15 employees found to have “acted inappropriately” have been fired. She did not name all the individuals, but said more than half of them had been in senior or executive roles.\nDuring April congressional hearings, Barra was unable to answer many questions, saying the internal investigation would find answers. But at Thursday’s news conference, she still left some questions unanswered, including why GM redesigned the flawed ignition switch but failed to follow normal procedures of assigning a new part number. That has led some critics to believe someone was covering up the change.\nBarra, who has served as CEO for about five months, said disciplinary action was taken against five others.\nDemocratic Senator Edward Markey of Massachusetts, who is pushing legislation to clamp down on automaker defects, said of the GM report: “We need more than an accounting of past mistakes” and “an internal investigation alone is not nearly enough to ensure that a decade-long tragedy like this never happens again.”\nSince early this year, the Detroit automaker has been enveloped in a scandal over why it took more than a decade to begin recalling low-cost Chevrolet Cobalts, Saturn Ions and other cars with ignition-switch problems that were causing them to stall during operation. Because of the engine stalls, air bags failed to deploy during crashes - some of them fatal - and drivers had difficulty operating their vehicles because power steering and brake systems also malfunctioned.\n“As years passed and fatalities mounted, engineers investigating the non-deployments and attempting to understand their cause were neither diligent nor incisive,” the GM report said.\nAlso infused through the document is the notion that GM engineers misdiagnosed the safety problem by failing to connect dots that would have linked the cars’ system failures.\nBut at the same time, GM “heard over and over from various quarters, including customers, dealers, the press and their own employees that the car’s ignition switch led to moving stalls” but employees “failed to take action or acted too slowly.”\nRISING CRASH COUNT\nSome new details also emerged about fatalities related to GM’s cars.\nThe report said GM had identified 54 frontal-impact crashes, involving the deaths of more than a dozen people, in which air bags did not deploy as a possible result of the faulty ignition switch.\nOnly last week, GM raised the count to 47, from 35, and has now raised it again, leading to questions about whether the 13 deaths linked to the defect will grow, as consumer advocates have predicted.\nAs expected, Barra also confirmed that GM will soon set up a fund to compensate victims of crashes linked to the faulty ignition switches.\nGM officials told reporters that the number of fatalities related to the part defect may rise but added that Kenneth Feinberg, who is looking into victim compensation, will determine that number.\nReuters reported on Monday that at least 74 people have died in crashes similar to those GM has linked to the faulty switches, based on an analysis of government data.\nCentral to the GM investigation, as well as probes by the U.S. Congress, Department of Justice and other federal and state agencies, is why the automaker did not announce vehicle recalls to fix the problem until this past February.\nGM said its investigation, conducted over 70 days, reviewed 41 million documents and more than 230 people were interviewed.\nCONSUMERS NOT PUNISHING GM\nThe move to spare the highest executives from blame drew some sharp criticism.\n“How do you truly fix a culture of carelessness and cover-up without cutting the head off the snake?” said Robert Hilliard, a lawyer for a plaintiff in a lawsuit against GM related to the ignition-switch defect.\nBarra previewed the report at the company’s technical center in Warren, Michigan, where she received a standing ovation from an estimated crowd of more than 1,000 employees. She emphasized that GM already has taken steps to beef up its internal safety operations.\nHer remarks were broadcast to GM’s 220,000 workers globally. Despite all the negative publicity in recent months, GM sales have been robust.\nThe Detroit automaker reported this week that U.S. sales in May were up 13 percent compared with a year ago, making for the best total sales since August 2008.\nWhile Barra noted a pattern of “incompetence and neglect” that she blamed on individuals who failed to “disclose critical pieces of information,” she added that there was “no conspiracy by the corporation to cover up facts.”\nFurthermore, Barra said the internal investigation “found no evidence that any employee made a trade-off between safety and cost” in failing to deal with the safety problem.\nThe full report was made public on Thursday.\n‘DEEPLY DISTURBING’ REPORT\nWith the submission of GM’s internal investigation, Congress is expected to announce a new round of hearings soon. Further details on a compensation fund for victims and their families are expected by early August.\nU.S. Representative Fred Upton, who chairs the House of Representatives’ Energy and Commerce Committee, said the findings of GM’s investigation were “deeply disturbing.”\nUpton, of Michigan, added that his committee will continue its investigation with an eye on “what legislative remedies may be necessary.”\nDemocratic Senator Richard Blumenthal of Connecticut said the GM report “seems like the best report money can buy. It absolves upper management, denies deliberate wrongdoing and dismisses corporate culpability.”\nThe internal report concluded that Barra, executives who reported directly to her, the board of directors and former CEO Dan Akerson did not know about the defective switches before December.\nIt also found that GM’s general counsel, Michael Millikin, was not responsible for the mishandling of defects and the recall delay. Millikin, who led the internal probe with former U.S. prosecutor Anton Valukas, is still employed by GM, Barra said.\nMore than one lawyer in GM’s legal department was fired as part of the 15 employees let go, including senior safety lawyer William Kemp, according to a source familiar with the matter who asked not to be named. A company spokesman declined to identify anyone fired beyond former engineers Ray DeGiorgio, who in 2002 dubbed the flawed part “the switch from hell,” and Gary Altman.\nGM’s use of Valukas came under withering attack because his law firm, Jenner & Block, has had ties to the company since 2002.\nBlumenthal, a former prosecutor, said those ties “undermine the credibility of this report” and underscored the need for federal investigations to press ahead.\nOn May 16, GM was slapped with a $35 million fine for its delayed response to the defect, which is the maximum that can be imposed by the U.S. Department of Transportation. Also, the National Highway Traffic Safety Administration has been leveling a $7,000-per-day fine on GM for missing an April 3 deadline for fully responding to the agency’s request for information about the ignition switch problem. These fines could be dwarfed, however, by potential actions the Justice Department could take. Earlier this year, Toyota Motor Corp was fined $1.2 billion for concealing problems related to sudden acceleration of some of its vehicles.\nThe NHTSA said GM’s probe appeared to support the agency’s findings that company practices “stood in the way of safety at a time when airbags were failing to work properly in millions of GM products.”\nAdditional reporting by Paul Lienert in Detroit and Marilyn Thompson, Susan Heavey and Karey Van Hall in Washington; Writing by Richard Cowan; editing by Lisa Von Ahn and Matthew Lewis\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about General Motors Co. issuing a report detailing how it turned a blind eye to an ignition-switch problem linked to at least 13 deaths for 11 years. The report describes shortcomings of GM engineers, including a failure to understand 'how the car was built.' The internal investigation reveals that GM had a long-running corporate culture in which nobody took responsibility for problems. CEO Mary Barra described this culture as the 'GM nod,' where everyone agrees to a plan but takes no action. The report also mentions that 15 employees found to have 'acted inappropriately' have been fired, with more than half of them in senior or executive roles. The report discusses the delay in recalling vehicles for repairs, which has cost the company about $1.7 billion so far. It also highlights the failure of engineers to connect dots related to safety problems. Furthermore, the report mentions that GM had identified 54 frontal-impact crashes, involving the deaths of more than a dozen people, in which airbags did not deploy as a possible result of the faulty ignition switch. GM plans to set up a fund to compensate victims of crashes linked to the faulty ignition switches, and the number of fatalities related to the defect may rise.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-gm-recall\/gm-top-executives-spared-in-internal-report-on-safety-failure-idINKBN0EG2JG20140605","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":44,"Time":"OCTOBER 2, 2015","Title":"NHTSA upgrades probe on Ford's police interceptor vehicles","Content":"By Reuters Staff\n(Reuters) - The National Highway Traffic Safety Administration (NHTSA) is upgrading its investigation on 20,275 Ford Motor Co’s Explorer Police Interceptor vehicles over sudden brake failures.\nIn April, the NHTSA had first opened a preliminary investigation into reports of brake failures in the model year 2015 vehicles.\nThe U.S. vehicle safety regulator is now conducting an engineering analysis, a step required before it can formally demand Ford for a recall.\nThe Sacramento police fleet has reported seven front hose failures in five different Explorer Police Interceptor vehicles used for its emergency vehicle operation course training, according to a document filed with the NHTSA\nNone of the failures resulted in any crashes so far, the document said.\nReporting by Radhika Rukmangadhan in Bengaluru; Editing by Shounak Dasgupta\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Based on the provided news article, here is the JSON-format response:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"The National Highway Traffic Safety Administration (NHTSA) is upgrading its investigation on 20,275 Ford Motor Co’s Explorer Police Interceptor vehicles over sudden brake failures. In April, the NHTSA had first opened a preliminary investigation into reports of brake failures in the model year 2015 vehicles. The U.S. vehicle safety regulator is now conducting an engineering analysis, a step required before it can formally demand Ford for a recall. The Sacramento police fleet has reported seven front hose failures in five different Explorer Police Interceptor vehicles used for its emergency vehicle operation course training, according to a document filed with the NHTSA. None of the failures resulted in any crashes so far, the document said.\"\n}\nThe news is indeed relevant to autonomous vehicle failures, specifically regarding brake failures in Ford's Explorer Police Interceptor vehicles, and the provided information details the case.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-ford-probe-nhtsa-idUSKCN0RW1RJ20151002","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":41,"Time":"Published 11:35 AM EDT, Thu September 16, 2021","Title":"Some Tesla owners are losing trust in Elon Musk’s promises of ‘full self-driving’","Content":"Washington, DC\nCNN\n — \nFrustrated Tesla owners continue to wait for “full self-driving,” an expensive and long-delayed software feature that isn’t even guaranteed to help their cars’ resale values. Some of the company’s earliest backers of the “full self-driving” option are even beginning to lose faith in the promise of ever enjoying a truly autonomous Tesla.\nYears-long delays, buggy beta software, and the risk of no return on their investment in the option package have left some Tesla owners disappointed. Tesla CEO Elon Musk’s prognostications, and Tesla’s actual reality have diverged so much that some owners describe to CNN Business that they’ve lost confidence in his predictions. Some otherwise satisfied Tesla owners describe feeling duped into buying “full self-driving” ahead of its polished release, because Musk warned that the price would increase.\nCNN Business interviewed eight Tesla owners to discuss “full self-driving.” The option has been offered since 2016, when Tesla claimed all its new vehicles had the hardware for “full self-driving.” Paying extra for the “full self-driving” software seemed like a worthy investment as Musk claimed it would be finalized in 2018. But while the software’s price has gone from $3,000 to $10,000 since then, most Tesla owners have never come close to experiencing a self-driving Tesla.\nTesla is following in the steps of an unlikely rival: Subaru\nOwners with “full self-driving” today don’t get the big autonomous vision Musk has long promoted, but instead a set of features that drivers with only Autopilot don’t have. They include automated lane changes on highways and “Navigate on Autopilot,” which guides a Tesla from highway on-ramp to off-ramp. There’s also a parking assist feature as well as “smart summon,” in which the car can slowly drive through a parking lot to pick up passengers, and a feature to identify stop signs and traffic lights.\nFor years, Musk made grandiose claims about how soon Tesla’s cars will drive themselves. Musk and Tesla have fallen short of these deadlines repeatedly, but he’s continued to make optimistic predictions.\n“It’s financially insane to buy anything other than a Tesla,” Musk said in April 2019. “If you buy a car that does not have the hardware necessary for full self-driving, it is like buying a horse.”\nElon Musk, at a press event on the grounds of the Tesla Gigafactory on August 13, 2021 in Gr'nheide, Brandenburg,Germany. The first vehicles are to roll off the production line in Gr'nheide near Berlin from the end of 2021. The US company plans to build around 500,000 units of the compact Model 3 and Model Y series here each year.\nPatrick Pleul\/picture-alliance\/dpa\/AP Images\nRecently Musk himself has questioned the value of a “full self-driving” subscription, which Tesla began also offering this summer.\n“We need to make full self-driving work in order for it to be a compelling value proposition,” Musk said on Tesla’s most recent earnings call. “Right now does it make sense for somebody to do an FSD subscription? I think it’s debatable.”\nLast month Musk said that in late September Tesla owners would be able to access the latest “full self-driving” software, which 2,000 Tesla owners have been testing as of March of this year. But Tesla owners say they have heard similar stories before.\nThis March Musk said that in about 10 days, Tesla owners would have an option to download the latest beta version of the “full self-driving” software. Two weeks later Musk said the option would hopefully arrive in April. Then in April he said the software would hopefully be available in May. In May, Musk said Tesla was “maybe a month or two away … but these things are hard to predict.” Then in August, Musk said the best guess for the public to have access to the latest software would be in about four weeks.\nAlex Costos of Plainville, Connecticut, who bought a Tesla Model 3 in 2018, heard Musk’s predictions. He described himself to CNN Business as a “Tesla fanboy,” and said he used to wear its merchandise everywhere.\nCostos said he bought “full self-driving” software in 2019, thinking he would get good value out of it even though it wasn’t usable yet. But two years later, that money feels lost, he told CNN Business. Costos is not part of the group of Tesla owners testing the beta version of “full self-driving.”\nCostos said he’s close to trading in his Model 3 for a new car and hasn’t yet really been able to enjoy “full self-driving.”\n“It stops at every light whether it’s green or red. That’s not working as intended,” Costos said of Tesla’s publicly available features of “full self-driving,” which generally requires drivers to tap the accelerator to go through an intersection.\nOther Tesla owners have described weaknesses in the company’s autonomous driving technology, such as confusing the moon for a traffic signal, or thinking an advertisement of a person on the back of a bus is a pedestrian.\nOne Tesla owner filed a lawsuit last month in the Albuquerque, New Mexico district court, claiming that Tesla fraudulently concealed its engineering failures and made false and misleading representations about “full self-driving.”\nCostos is especially concerned about the trade-in value of his Model 3. He does not expect dealerships will pay him a premium because his car has “full self-driving.” The software isn’t guaranteed to carry over to a future buyer of the car, making dealers less likely to pay him extra for it, he said. Vroom, a used car website, told CNN Business that there’s no guarantee “full self-driving” will transfer to a new owner, so it doesn’t add any value to models that come in with the feature.\nFord, VW, Nissan, Toyota, Subaru, Honda and GM’s driver-assist packages that compete with Tesla will carry over to new owners of vehicles, company spokespersons told CNN Business.\nCars at charging stations at a Tesla car dealership in Maplewood, Minnesota on June 13, 2021.\nMichael Siluk\/UCG\/Universal Images Group\/Getty Images\n“The longer you’re an owner the more you realize they’re kind of like any other car company. They have all the issues other car companies have, and other ones those companies don’t have,” Costos said.\nTesla generally does not engage with the professional news media and did not respond to a request for comment.\nPushing owners’ buttons\nBrian Self, a Tesla owner in Bristol, Tennessee, told CNN Business that he loves his Model S, but has been unhappy with how Tesla has handled the upcoming broad release of “full self-driving.”\n“He’s put out random tweets for three years that say, ‘Oh, yeah, full self-driving is getting ready to come out,” Self said. “You can’t believe them anymore.”\nSelf said he didn’t think it’s fair that a select group of Tesla owners who have paid for “full self-driving” driving access have received early access to the software. He said he felt that Tesla owners who purchased “full self-driving” should all have the option of receiving the latest version of the software.\nSelf said it appeared Tesla is using the limited release version of “full self-driving” as a form of advertising. Many of the Tesla owners selected to use the beta software have large social media presences.\nTesla first released the beta version of the latest “full self-driving” in October 2020 to a select group. Videos from the beta testers show the technology has limitations and requires vigilant human supervision. But the beta testers have also described that the system is generally improving.\nEven so, it’s gotten to the point that Musk’s delayed timelines have become a joke in the Tesla community.\n“Elon might say, ‘Oh, yeah, we’ll fix this, it’ll be out next week,’” Self said. “And everyone says — ‘Elon time two weeks, actual time two months.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous vehicle technology, particularly Tesla's 'full self-driving' feature. The article discusses how Tesla owners are becoming frustrated with the long delays, buggy software, and unfulfilled promises regarding 'full self-driving.' Some owners feel that they were misled into purchasing this feature, which has not lived up to Elon Musk's optimistic predictions. There are complaints about the functionality of the current 'full self-driving' features, including issues like stopping at every traffic light and misidentifying objects. One Tesla owner has even filed a lawsuit claiming that Tesla concealed engineering failures and made false representations about 'full self-driving.' Additionally, concerns have been raised about the trade-in value of Tesla vehicles with 'full self-driving' as it may not transfer to future buyers. Overall, the article provides specific information about the dissatisfaction and skepticism among Tesla owners regarding autonomous driving technology.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2021\/09\/16\/cars\/tesla-fsd-delay\/index.html","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":49,"Time":"MARCH 20, 2018","Title":"Factbox: How United States, others regulate autonomous vehicle testing","Content":"By Reuters Staff\n(Reuters) - An Uber self-driving sport utility vehicle struck and killed a woman crossing a street in Arizona on Sunday, the first fatality involving an autonomous vehicle and a potential blow to the technology expected to transform transportation.\nThe ride services company said it was suspending North American tests of its self-driving vehicles. Safety advocates have called for a national moratorium on the testing of all so-called robot cars on public roads.\nHere is a brief look at how the United States and some other countries regulate testing of autonomous vehicles.\n- So far, 21 U.S. states plus the District of Columbia have enacted legislation related to autonomous vehicles, according to the National Conference of State Legislatures. (here)\n- Nevada was the first to authorize operation of autonomous vehicles in 2011.\n- Arizona has opened its arms to companies testing self-driving vehicles as a means to economic growth and jobs. Republican Governor Doug Ducey reached out to Uber in 2016 after California regulators cracked down on the company over its failure to obtain testing permits.\n- China has issued licenses to automakers that allow self-driving vehicles to be road tested, the state-owned Xinhua news agency has reported. The licenses allow operators to test drive the vehicles on a 5.6-km (3.5-mile) public road in Jiading District of Shanghai, Xinhua said. Shanghai has regulations on road tests for such smart cars and has said it would promote the application and commercialization of vehicles using artificial intelligence technology and internet-linked functions, Xinhua reported.\n- South Korea’s transport ministry began the provisional permit system in February 2016 and has approved provisional permits for 41 self-driving cars as of March including 14 for Hyundai Motor Co 005380.KS, two for Kia Motors 000270.KS , and one for Audi-Volkswagen VOWG_p.DE. Audi-Volkswagen is the only foreign company with a provisional permit in South Korea.\nSouth Korea said it was aware of the Uber incident and that it makes “frequent improvements to the permit system and could make changes in light of the incident going forward.”\n- In Japan, all vehicles on roads require a driver to be sitting in the driver’s seat, according to the Transport Ministry. As long as someone is in the driver’s seat during an automated driving test and has his or her hands close to the wheel at all times, no special permit is needed to conduct self-driving tests.\nA ministry spokesman said of the Uber incident, “We will be following it closely, but it’s too early to say how it will affect our approach to self-driving tests.”\n- Singapore requires that all autonomous test vehicles undergo a vehicle safety assessment before they are approved for on-road trials. Test vehicles can only be driven in autonomous mode within an approved test-site. All test vehicles are required to have a qualified safety driver who is ready to take control, until autonomous vehicle trials demonstrate that the technology is ready for fully autonomous operations. All trial participants are also required to have third-party insurance for test vehicles and must share data.\nCompiled by Matthew Lewis; Reporting by Joseph White in Detroit, Joyce Lee in Seoul, Twinnie Siu in Hong Kong\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"An Uber self-driving sport utility vehicle struck and killed a woman crossing a street in Arizona on Sunday, the first fatality involving an autonomous vehicle and a potential blow to the technology expected to transform transportation. The ride services company said it was suspending North American tests of its self-driving vehicles.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-uber-factbox\/factbox-how-united-states-others-regulate-autonomous-vehicle-testing-idUSKBN1GW2PY","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":42,"Time":"10:40 PM GMT+1•January 17, 2022","Title":"New York Times ad warns against Tesla’s “Full Self-Driving”","Content":"cameraImage Credits: The Dawn Project\nA full page advertisement in Sunday’s New York Times took aim at Tesla’s “Full Self-Driving” software, calling it “the worst software ever sold by a Fortune 500 company” and offering $10,000, the same price as the software itself to the first person who could name “another commercial product from a Fortune 500 company that has a critical malfunction every 8 minutes.”\nThe ad was taken out by The Dawn Project, a recently founded organization aiming to ban unsafe software from safety critical systems that can be targeted by military-style hackers, as part of a campaign to remove Tesla Full Self-Driving (FSD)  from public roads until it has “1,000 times fewer critical malfunctions.”\nThe founder of the advocacy group, Dan O’Dowd, is also the CEO of Green Hill Software, a company that builds operating systems and programming tools for embedded safety and security systems. At CES, the company said BMW’s iX vehicle is using its real-time OS and other safety software, and it also announced the availability of its new over-the-air software product and data services for automotive electronic systems.\nDespite the potential competitive bias of The Dawn Project’s founder, Tesla’s FSD beta software, an advanced driver assistance system that Tesla owners can access to handle some driving function on city streets, has come under scrutiny in recent months after a series of YouTube videos that showed flaws in the system went viral.\nThe NYT ad comes just days after the California Department of Motor Vehicles told Tesla it would be “revisiting” its opinion that the company’s test program, which uses consumers and not professional safety operators, doesn’t fall under the department’s autonomous vehicle regulations. The California DMV regulates autonomous driving testing in the state and requires other companies like Waymo and Cruise that are developing, testing and planning to deploy robotaxis to report crashes and system failures called “disengagements. Tesla has never issued those reports.\nTesla CEO Elon Musk has since vaguely responded on Twitter, claiming Tesla’s FSD has not resulted in accident or injury since its launch. The U.S. National Highway Traffic Safety Administration (NHTSA) is investigating a report from the owner of a Tesla Model Y, who reported his vehicle went into the wrong lane while making a left turn in FSD mode, resulting in the vehicle being struck by another driver.\nEven if that was the first FSD crash, Tesla’s Autopilot, the automaker’s ADAS that comes standard on vehicles, has been involved in around a dozen crashes.\nAlongside the NYT ad, The Dawn Project published a fact check of its claims, referring to its own FSD safety analysis that studied data from 21 YouTube videos totaling seven hours of drive time.\nThe videos analyzed included beta versions 8 (released December 2020) and 10 (released September 2021), and the study avoided videos with significantly positive or negative titles to reduce bias. Each video was graded according to the California DMV’s Driver Performance Evaluation, which is what human drivers must pass in order to gain a driver’s license. To pass a driver’s test, drivers in California must have 15 or fewer scoring maneuver errors, like failing to use turn signals when changing lanes or maintaining a safe distance from other moving vehicles, and zero critical driving errors, like crashing or running a red light.\nThe study found that FSD v10 committed 16 scoring maneuver errors on average in under an hour and a critical driving error about every 8 minutes. There as an improvement in errors over the nine months between v8 and v10, the analysis found, but at the current rate of improvement, “it will take another 7.8 years (per AAA data) to 8.8 years (per Bureau of Transportation data) to achieve the accident rate of a human driver.”\nThe Dawn Project’s ad makes some bold claims that should be taken with a grain of salt, particularly because the sample size is far too small to be taken seriously from a statistical standpoint. If, however, the seven hours of footage is indeed representative of an average FSD drive, the findings could be indicative of a larger problem with Tesla’s FSD software and speak to the broader question of whether Tesla should be allowed to test this software on public roads with no regulation.\n“We did not sign up for our families to be crash test dummies for thousands of Tesla cars being driven on the public roads…” the ad reads.\nFederal regulators have started to take some action against Tesla and its Autopilot and FSD beta software systems.\nIn October, NHTSA sent two letters to the automaker targeting the its use of non-disclosure agreements for owners who gain early access to FSD beta, as well as the company’s decision to use over-the-air software updates to fix an issue in the standard Autopilot system that should have been a recall. In addition, Consumer Reports issued a statement over the summer saying the FSD version 9 software upgrade didn’t appear to be safe enough for public roads and that it would independently test the software. Last week, the organization published its test results, which revealed that “Tesla’s camera-based driver monitoring system fails to keep a driver’s attention on the road.” CR found that Ford’s BlueCruise, on the other hand, issues alerts when the driver’s eyes are diverted.\nSince then, Tesla has rolled out many different versions of its v10 software – 10.9 should be here any day now, and version 11 with “single city\/highway software stack” and “many other architectural upgrades” coming out in February,  according to CEO Elon Musk.\nReviews of the latest version 10.8 are skewed, with some online commenters saying it’s much smoother, and many others stating that they don’t feel confident in using the tech at all. A thread reviewing the newest FSD version on the Tesla Motors subreddit page shows owners sharing complaints about the software, with one even writing, “Definitely not ready for the general public yet…”\nAnother commenter said it took too long for the car to turn right onto “an entirely empty, straight road…Then it had to turn left and kept hesitating for no reason, blocking the oncoming lane, to then suddenly accelerate once it had made it onto the next street, followed by a just-as-sudden deceleration because it changed its mind about the speed and now thought a 45 mph road was 25 mph.”\nThe driver said it eventually had to disengage entirely because the system completely ignored an upcoming left turn, one that was to occur at a standard intersection “with lights and clear visibility in all directions and no other traffic.”\nThe Dawn Project’s campaign highlights a warning from Tesla that its FSD “may do the wrong thing at the worst time.”\n“How can anyone tolerate a safety-critical product on the market which may do the wrong thing at the worst time,” said the advocacy group. “Isn’t that the definition of defective? Full Self-Driving must be removed from our roads immediately.”\nNeither Tesla nor The Dawn Project could be reached for comment.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about concerns and criticisms regarding Tesla's Full Self-Driving (FSD) software. The Dawn Project, an organization, has taken out an ad in The New York Times criticizing Tesla's FSD software, calling it 'the worst software ever sold by a Fortune 500 company.' The ad offers a $10,000 reward for anyone who can name another commercial product from a Fortune 500 company with a critical malfunction every 8 minutes. The Dawn Project is campaigning to remove Tesla's FSD from public roads until it has '1,000 times fewer critical malfunctions.' The news also mentions that Tesla's FSD software has come under scrutiny due to viral videos showing flaws in the system, and the California Department of Motor Vehicles is revisiting its opinion on Tesla's test program's compliance with autonomous vehicle regulations. There are mentions of Tesla's CEO Elon Musk's responses on Twitter and investigations by the U.S. National Highway Traffic Safety Administration (NHTSA). Additionally, The Dawn Project conducted a safety analysis of Tesla's FSD software based on YouTube videos, which found significant errors and questioned the software's safety. The news also highlights regulatory actions and criticisms from organizations like Consumer Reports regarding Tesla's FSD software.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/01\/17\/new-york-times-ad-warns-against-teslas-full-self-driving\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":40,"Time":"Sun 25 Mar 2018 09.00 CEST","Title":"Death by robot: the new mechanised danger in our changing world","Content":"Elaine Herzberg\nWas killed last Sunday by an Uber autonomous car that hit the 49-year-old at approximately 40mph as she was crossing the road in Tempe, Arizona. Police confirmed there was an operator in the Volvo SUV at the time of the collision, and stated that it didn’t appear the car had slowed down.\nCars on the assembly line at Ford’s plant in Flat Rock, Michigan. Photograph: Bill Pugliano\/Getty Images\nRobert Williams\nCited as the first human killed by a robot, Williams was an assembly worker at Ford’s Flat Rock plant in Michigan. He was crushed to death by the arm of a one-tonne robot on the production line on 25 January 1979. His family was awarded $10m in damages. The court concluded that he was struck on the head due to inadequate safety measures.\nRobot-assisted surgery is becoming more common. Photograph: Alamy\nSurgery patients\nWhile robot-assisted surgery has allowed surgeons to perform operations remotely with minimal invasiveness, a 2016 US study found that it was linked to the deaths of 144 people between 2008 and 2013. Causes of death included parts falling into patients’ bodies, machines turning off\/on at the wrong time and loss of video feeds. “Despite widespread adoption of robotic systems… a non-negligible number of technical difficulties and complications are still being experienced during procedures,” the study said.\nA Remotec Andros F-6A bomb-disposal robot similar to the one used to kill Micah Johnson. Photograph: Damian Berg\/US Navy\/EPA\nMicah Johnson\nBelieved to be the first person killed by a police robot. In July 2016, Johnson was involved in a standoff with Dallas law enforcement after fatally shooting five officers and injuring seven others. Police attached a block of C-4 explosive to a bomb-disposal robot and detonated it when it was near the suspect. “We saw no other option,” said the Dallas police chief.\nAn Oerlikon GDF robot cannon. Photograph: Rheinmetall Air Defence\nNine South African soldiers\nIn October 2007, an Oerlikon GDF-005 semi-autonomous anti-aircraft gun jammed and exploded before firing 250 high-explosive rounds at soldiers who were training with it. The weapon is capable of tracking targets and reloading automatically. “I think it is bad luck more than anything else,” one expert told the New Scientist.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Elaine Herzberg was killed last Sunday by an Uber autonomous car that hit the 49-year-old at approximately 40mph as she was crossing the road in Tempe, Arizona. Police confirmed there was an operator in the Volvo SUV at the time of the collision, and stated that it didn’t appear the car had slowed down.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/mar\/25\/death-by-robot-mechanised-danger-in-our-changing-world","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":56,"Time":"SEPTEMBER 18, 2015","Title":"BoE considers how to deal with failing insurers better","Content":"By Huw Jones\nLONDON, Sept 18 (Reuters) - The Bank of England is looking at whether better rules are needed to handle failed insurers so that policyholders don’t face disruption.\nInsurers in Britain held 1.9 trillion pounds ($3 trillion) in assets at the end of 2014, broadly equivalent to the country’s economic output.\nResearchers at the BoE, whose Prudential Regulation Authority supervises insurers like Aviva and Prudential , said that unlike with banks, there is no tailor made, legally-backed regime for protecting all policyholders when winding down a failed insurers.\n“Even though the current regime to manage insurance companies’ failures in the United Kingdom is generally robust, there remain some potential challenges,” the BoE said in its Quarterly Bulletin on Friday.\nBritain’s financial compensation scheme covers motor and employers’ liability insurance cover but not reinsurance, marine, aviation, transport business and credit insurance.\n“These are important lines of business for the real economy and the failure of a large provider could therefore lead to disruptions for existing policyholders if no competitor were willing to acquire the portfolio of policies underwritten,” the bulletin said.\n“The Bank of England and other UK authorities are working with international partners to ascertain whether the current framework for dealing with insolvent insurance companies provides adequate protection or needs to be reviewed.”\nInsurers have insisted they don’t pose a threat to financial stability and played no role in the 2007-09 financial crisis though the near collapse of U.S. insurer AIG triggered closer regulatory scrutiny.\nEarlier this month, the European Union’s insurance watchdog EIOPA said it too was looking at what rules were needed to deal with an insurer in trouble to avoid disruption to policyholders and the wider market.\nAs regulators complete work on making banks more stable after the financial crisis, they are now turning their attention to other parts of the market, such as insurers and asset managers.\nWhen an insurance company fails in Britain, regulators can intervene to stop it writing new business and allow the existing policies to mature or “run off”, or transfer the policies to another insurer.\nThere are over 100 insurance companies in run-off in Britain but the worry for the BoE is if a run-off is not possible and another insurer isn’t be willing to take over the policies. ($1 = 0.6399 pounds) (Reporting by Huw Jones)\nBREAKINGVIEWS\nReuters Breakingviews is the world's leading source of agenda-setting financial insight. As the Reuters brand for financial commentary, we dissect the big business and economic stories as they break around the world every day. A global team of about 30 correspondents in New York, London, Hong Kong and other major cities provides expert analysis in real time.\n\nSign up for a free trial of our full service at https:\/\/www.breakingviews.com\/trial and follow us on Twitter @Breakingviews and at www.breakingviews.com. All opinions expressed are those of the authors.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about the Bank of England considering better rules for handling failed insurers to prevent policyholders from facing disruptions. The article discusses the lack of a tailor-made, legally-backed regime for protecting policyholders in the event of insurer failure, especially in cases involving reinsurance, marine, aviation, transport business, and credit insurance. The Bank of England is collaborating with international partners to assess the current framework for dealing with insolvent insurance companies and whether it requires revision. Regulators are concerned about the potential disruption to policyholders and the wider market when insurance companies fail, and they are exploring ways to address this issue.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/boe-insurance-regulations-idUKL5N11O1HL20150918","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":59,"Time":"MARCH 20, 2018","Title":"Factbox - How United States, others regulate autonomous vehicle testing","Content":"By Reuters Staff\n(Reuters) - An Uber self-driving sport utility vehicle struck and killed a woman crossing a street in Arizona on Sunday, the first fatality involving an autonomous vehicle and a potential blow to the technology expected to transform transportation.\nThe ride services company said it was suspending North American tests of its self-driving vehicles. Safety advocates have called for a national moratorium on the testing of all so-called robot cars on public roads.\nHere is a brief look at how the United States and some other countries regulate testing of autonomous vehicles.\n- So far, 21 U.S. states plus the District of Columbia have enacted legislation related to autonomous vehicles, according to the National Conference of State Legislatures. (here)\n- Nevada was the first to authorize operation of autonomous vehicles in 2011.\n- Arizona has opened its arms to companies testing self-driving vehicles as a means to economic growth and jobs. Republican Governor Doug Ducey reached out to Uber in 2016 after California regulators cracked down on the company over its failure to obtain testing permits.\n- China has issued licenses to automakers that allow self-driving vehicles to be road tested, the state-owned Xinhua news agency has reported. The licenses allow operators to test drive the vehicles on a 5.6-km (3.5-mile) public road in Jiading District of Shanghai, Xinhua said. Shanghai has regulations on road tests for such smart cars and has said it would promote the application and commercialization of vehicles using artificial intelligence technology and internet-linked functions, Xinhua reported.\n- South Korea’s transport ministry began the provisional permit system in February 2016 and has approved provisional permits for 41 self-driving cars as of March including 14 for Hyundai Motor Co, two for Kia Motors , and one for Audi-Volkswagen. Audi-Volkswagen is the only foreign company with a provisional permit in South Korea.\nSouth Korea said it was aware of the Uber incident and that it makes “frequent improvements to the permit system and could make changes in light of the incident going forward.”\n- In Japan, all vehicles on roads require a driver to be sitting in the driver’s seat, according to the Transport Ministry. As long as someone is in the driver’s seat during an automated driving test and has his or her hands close to the wheel at all times, no special permit is needed to conduct self-driving tests.\nA ministry spokesman said of the Uber incident, “We will be following it closely, but it’s too early to say how it will affect our approach to self-driving tests.”\n- Singapore requires that all autonomous test vehicles undergo a vehicle safety assessment before they are approved for on-road trials. Test vehicles can only be driven in autonomous mode within an approved test-site. All test vehicles are required to have a qualified safety driver who is ready to take control, until autonomous vehicle trials demonstrate that the technology is ready for fully autonomous operations. All trial participants are also required to have third-party insurance for test vehicles and must share data.\nCompiled by Matthew Lewis; Reporting by Joseph White in Detroit, Joyce Lee in Seoul, Twinnie Siu in Hong Kong\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"An Uber self-driving sport utility vehicle struck and killed a woman crossing a street in Arizona on Sunday, the first fatality involving an autonomous vehicle and a potential blow to the technology expected to transform transportation. The ride services company said it was suspending North American tests of its self-driving vehicles.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/uk-autos-selfdriving-uber-factbox-idUKKBN1GW2RE","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":54,"Time":"8:55 PM GMT+2•April 11, 2022","Title":"Autonomous Cruise car encounter with police raises policy questions","Content":"cameraImage Credits: Cruise\nNo technology is perfect. Even self-driving cars trained to obey traffic laws are bound to run into issues that cause them to commit a citable offense. Such was the case with a Cruise-operated hatchback in San Francisco last weekend, which was pulled over by local law enforcement for failing to switch on its headlights. While the car came to a stop, as video of the incident shows, there’s policy to be established when it comes to interactions between autonomous vehicles and police.\nOriginally published on Instagram, the video shows the car — one of Cruise’s Chevy Bolts — in the city’s Richmond District pulling over to the side of the road when signaled to do so by an officer, ahead of an intersection. The policeperson walks toward the car and attempts unsuccessfully to open the driver-side door, at which point the Cruise vehicle begins to drive down the road — only to pull over again and activate its hazards. Police approach the car a second time in a presumed effort to figure out how to turn on the headlights.\nWelcome to the future. Cop pulls over driverless car (because no lights?) Then Cruise goes on the lamb. (via https:\/\/t.co\/mtmsIeOAUP) pic.twitter.com\/ecQ5xXuSnS\n— Seth Weintraub (@llsethj) April 10, 2022\nCruise, which a little over two months ago began letting San Francisco residents hail rides in its driverless vehicles, asserts that the pulled-over vehicle acted as intended. The headlights indeed malfunctioned — and have been fixed, according to the company — but the car yielded to police and then pulled over to the “nearest safe location.” One of the officers contacted Cruise after the traffic stop, and no citation was issued.\nBut the episode raises questions about procedure where self-driving vehicles are involved with the police.\nAs it turns out, the vehicle in the video wasn’t entirely autonomous. Cruise in 2019 began testing computer vision and sound detection AI to help its cars respond to emergency vehicles. However, a spokesperson told TechCrunch that Cruise personnel directed the car to pull over at the second location — across the intersection — when it became clear that the car was the subject of a traffic stop and the officer was clear of the car. When asked, the spokesperson declined to reveal whether Cruise vehicles would behave differently if the stop happened on a highway versus a city street, for example, saying only that the company has a team of people to feed Cruise vehicles with information in instances where the vehicles experience problems.\nWe work closely with the SFPD on how to interact with our vehicles, including a dedicated phone number for them to call in situations like this.\n— cruise (@Cruise) April 10, 2022\nThe most common contact people in the U.S. have with police takes the form of a traffic stop. But not all drivers are treated equally. Black drivers are almost twice as likely to be pulled over as white drivers, far too often with deadly consequences. Several experts argue that autonomous cars will de-escalate interactions with the police by, for instance, reducing investigations associated with hit-and-run offenses. But others, including Elizabeth E. Joh, a law professor at UC Davis, expect that autonomous vehicles will increase police powers if police are allowed to seize cars remotely.\n“An autonomous car would be a programmable car. Perhaps speed enforcement will be a thing of the past, either because cars will be programmed to stay within the legal speed limit, or because such violations will be automatically enforced with a ticket sent to drivers’ electronic dashboards,” Joh writes in an essay on the subject. “But even in this version of the future, police will still seize cars. A person inside the autonomous car may have an outstanding arrest warrant. The police may suspect that the car contains contraband or evidence of a crime. By no longer requiring human control, autonomous cars may even encourage more types of crime to take place within them.”\nWhile no company developing autonomous vehicles has indicated that they’d provide law enforcement this sort of access, police might make the case for it, citing the confusion that can arise from incidents like the Cruise traffic stop. Cruise’s isn’t the first autonomous car to get pulled over — that distinction goes to a Google prototype in 2015. But as autonomous vehicle developers increasingly deploy their fleets on public roads, the issue is more likely to come to the fore.\n“The police and the public will interact differently when there is little human involvement in driving,” Joh continues in the essay. “[B]ecause so much of the Fourth Amendment is premised upon human drivers and human police officers, that means that a future of automated car stops will pose novel and difficult questions of law and policy that we should begin to address now.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about an incident involving a Cruise-operated autonomous vehicle in San Francisco. The vehicle was pulled over by the police for failing to switch on its headlights. The incident was captured on video and showed the car pulling over as signaled by an officer, but it subsequently began driving down the road again before pulling over once more and activating its hazard lights. Cruise claims that the vehicle acted as intended, and the malfunctioning headlights have been fixed. No citation was issued, but the incident raises questions about interactions between autonomous vehicles and the police. It's worth noting that the vehicle in the video was not entirely autonomous, as Cruise personnel directed it to pull over at a different location when it became clear that it was subject to a traffic stop.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/04\/11\/autonomous-cruise-car-encounter-with-police-raises-policy-questions\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":62,"Time":"Updated 5:04 PM EDT, Tue November 1, 2022","Title":"Self-driving cars were supposed to take over the road. What happened?","Content":"Washington, DC\nCNN Business\n — \nFor over a decade, companies from Google to General Motors have poured billions of dollars into the pursuit of what was seen as the Holy Grail of driving technology: the fully self-driving car. Such vehicles would usher in an era of consumer safety and convenience, experts promised, and would be an immensely valuable product for carmakers.\nBut recently many of the main players in the autonomous vehicle game have been scaling back or outright abandoning their lofty ambitions. Last week Ford and Volkswagen pulled the plug on their self-driving effort, Argo AI, the latest admission from a hype-fueled industry that building a good self-driving car that’s also a profitable business may not happen anytime soon.\nWhen it all began\nArgo AI CEO and co-founder Bryan Salesky was part of a famed Carnegie Mellon University team that developed a primitive self-driving vehicle that won a Pentagon race in 2007.\nFour students on the team went on to co-found self-driving companies, which have raised billions since: Salesky, who founded Argo; Dave Ferguson at Nuro; and Drew Bagnell and Chris Urmson at Uber-backed Aurora.\n“We started out doing this stuff because it was cool and it was a neat idea, but we weren’t quite sure how it was going to be used,” Salesky said in a 2019 interview.\nThe team’s self-driving Chevrolet Tahoe caught the eye of Google co-founders Larry Page and Sergey Brin, who went on to launch a self-driving car program in 2009, later dubbed Waymo, that ignited an industry. Competitors like Uber jumped in, fearful that being left out of self-driving would destroy their business’s future. (Uber launched its program in 2015 before selling it in 2020 following a costly lawsuit with Alphabet.)\nA Waymo robotaxi is shown in Arizona.\nCaitlin O'Hara\/Bloomberg\/Getty Images\nGeneral Motors bought a self-driving company in 2016. Ford followed suit a year later with Argo AI. Uber rushed to scoop up engineers from Salesky’s alma mater, Carnegie Mellon.\nMore than $10 billion was invested in self-driving cars since 2010, McKinsey estimated.\nProponents of self-driving believed they were onto something big. Driving is the deadliest of the common forms of transportation, and a leading cause of death for many age groups. More Americans have died in traffic crashes than fighting in all of its wars.\nInvestors swooned at the potential to make billions while saving millions of lives.\nThis business would be huge, they said. “$7 trillion as autonomous vehicles become mainstream,” claimed Intel.\nBig talk, smaller results\nUrmson, while leading Google’s self-driving car project before founding Aurora, talked of his preteen son never needing to get a driver’s license. Optimism ran wild.\nWhy would anyone even own a car? Autonomous taxis would be even cheaper than walking, said one industry analyst.\n“Autonomous vehicles are able to acquire new skills faster than humans,” Salesky said in 2021.\nBut there was one problem. The technology could do impressive things but mastering all the situations we face as human drivers is tough. The billions invested in the technology haven’t yet been able to drive better than a human in some situations.\nThat’s been a problem for the long list of companies that set aggressive targets for self-driving cars. General Motors in 2017 promised mass production of fully autonomous vehicles in 2019. Lyft said in 2016 that half its rides would be self-driving by 2021. Ford talked up 2021 too, promising full self-driving vehicles deployed broadly.\nBut there’s a funny saying from the software world, known as the 90-90 Rule. Once 90% of the work is done, you only have 90% to go. The ability of self-driving car software to steer the vehicle within a highway lane is great, but being able to do so, even for thousands of miles at a stretch, isn’t enough. A self-driving car, without a steering wheel or pedals, would have to be able to drive itself in literally every situation possible.\nWith millions of miles being driven by humans every day, the number of tricky and unusual situations, called edge cases, is vast. Unusual events are individually, by definition, uncommon. But the sheer number of unusual events encountered by millions of drivers all over the world means that “unusual events” are quite common.\n“It’s really, really hard,” Waymo’s then-CEO John Krafcik said in 2018 of self-driving technology. “You don’t know what you don’t know until you’re really in there and trying to do things.”\nIndustry leaders have backed off their big claims. Waymo has said it will be decades before the technology is everywhere in the world.\nTesla CEO Elon Musk now largely stands alone for sticking with aggressive predictions. He’s said every year since 2017 that the software capable of full self-driving will probably arrive in the “next year.” The software hasn’t arrived though. Instead, the company has drawn increasing scrutiny from regulators for its big talk. It’s being investigated for the rear-ending of emergency vehicles stopped on roads, and appeared unable to identify motorcycles at times.\nHumbled but not defeated\nIn recent years many industry leaders have quietly acknowledged and tried to tweak their business plan. Urmson and Bagnell shifted to the development of self-driving trucks that drive mostly on highways, which are much easier to master.\nFerguson left the Google self-driving car project to start a company building fully autonomous vehicles without a person inside for goods delivery. It’s a far simpler challenge as there’s no worry about protecting people inside the vehicles. Even Google itself started working on self-driving trucks.\nCompanies developing lidar, widely seen as a key component for self-driving vehicles, as well as self-driving companies, have seen their stocks plummet recently.\nLidar companies Velodyne, Quanergy, Luminar, and Ouster have all seen their stocks pummeled this year. Autonomous truck companies Aurora, TuSimple, and Embark have all had huge dives too this year.\nTesla, which has promised “full self-driving” for years, has delivered a less ambitious driver-assist feature that’s enjoyed by some, but it is also a beta product with lots of room for improvement. It’s also facing several government investigations related to the technology.\nOnly VW and Ford’s Argo AI and GM’s Cruise stayed focused exclusively on true self-driving cars.\nNow only one of them remains.\nCruise is offering some rides to passengers in San Francisco at night.\nDavid Paul Morris\/Bloomberg\/Getty Images\nGM’s Cruise offers a nighttime ridehail service in San Francisco, which completed 84 trips in June, 224 in July and 416 in August. (Driving in daytime is much harder as there’s more traffic including pedestrians and cyclists.) Ride fees are similar to riding with Uber or Lyft.\nCruise has lost $1.4 billion this year, and it cannot stand to lose hundreds of thousands of dollars per trip forever. It plans to expand service to offer rides in Austin and Phoenix later this year.\nCruise CEO Kyle Vogt said last week that “next year marks the beginning of our rapid scaling phase.”\nThere will be plenty of kinks to work out. The local government has raised safety and traffic concerns about Cruise’s expansion plans.\nSome longtime industry observers caution that self-driving technology still has a long way to go, a line of thinking Ford CEO Jim Farley echoed Wednesday.\n“Things have changed,” Farley said of Ford’s 2017 plans to broadly deploy self-driving cars in 2021.\nThe company is instead focusing on advanced driver-assist technology.\nAutomakers, aside from Tesla, largely rely on Mobileye, an Israeli tech company, for driver-assist technology. Intel spun out Mobileye last week, raising $861 million. The successful IPO suggests there’s still a robust market for driver-assist technology, even if it falls short of fully autonomous vehicles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news discusses the scaling back and abandonment of self-driving car ambitions by major players in the autonomous vehicle industry, including Ford and Volkswagen discontinuing their self-driving effort, Argo AI. It highlights the challenges and limitations of self-driving technology, such as the difficulty of mastering all situations faced by human drivers, known as 'edge cases.' The article also mentions industry leaders revising their business plans, shifting focus to self-driving trucks and driver-assist technology. GM's Cruise is mentioned as one of the companies still focused on true self-driving cars, though it faces challenges and concerns about safety and traffic. Ford is mentioned as shifting its focus to advanced driver-assist technology. Overall, the news is relevant to autonomous vehicle failures and provides specific information about the industry's struggles and changes in direction.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2022\/11\/01\/business\/self-driving-industry-ctrp\/index.html","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":60,"Time":"August 29, 2023","Title":"Tesla faces more questions from feds on Autopilot safety","Content":"Companies\nTesla Inc\nFollow\nCarparts.Com Inc\nFollow\nGeneral Motors Co\nFollow\nShow more companies\nAug 29 (Reuters) - The U.S. auto safety regulator investigating Tesla's Autopilot driver assistance system is demanding an explanation for a software change that allows drivers to keep their hands off the wheel for longer, risking collisions, documents released on Tuesday showed.\nThe National Highway Traffic Safety Administration (NHTSA) ordered Tesla to answer questions about the Autopilot change and produce documents in a so-called special order dated July 26. The NHTSA order did not compel Tesla to recall vehicles.\n\"The resulting relaxation of controls...could lead to greater driver inattention and failure of the driver to properly supervise Autopilot,\" NHTSA said in its letter to Tesla.\nThe NHTSA order underscores the challenges regulators face keeping up with safety-critical vehicle technology that can be changed overnight with a software upgrade delivered over the air.\nNHTSA asked when the software update was introduced, the number of vehicles affected, Tesla's reason for installing, and collisions or near-misses involving vehicles with the software.\nAdvertisement · Scroll to continue\n\"NHTSA is concerned that this feature was introduced to consumer vehicles and, now that the existence of this feature is known to the public, more drivers may attempt to activate it,\" the agency wrote.\nIn April, Chief Executive Elon Musk tweeted that Tesla is gradually reducing the steering wheel nag - alerts aimed at making sure drivers using Tesla's \"Full Self-Driving\" (FSD) system keep their hands on the wheel. He also this week said \"Yeah\" in response to an X message that \"No steering wheel nag will be a *game changer* for FSD user satisfaction.\"\nAdvertisement · Scroll to continue\nloading\nloading\nMusk will soon face a major test of his assertions about the technology's safety when Tesla defends itself later this year in a trial over allegations that failure of its Autopilot driver assistant feature led to death.\nTesla shares were up 7% in late trading Tuesday, outpacing the broader market.\nThe agency's acting head Ann Carlson told Reuters last week that a resolution of the Autopilot investigation will come soon.\nAdvertisement · Scroll to continue\nSpeaking broadly about the problem of driver attention, Carlson said it is \"really important that driver monitoring systems take into account that humans over-trust technology.\"\nMajor automakers are racing each other to deploy technology that allows motorists to take their hands off the wheel under certain conditions.\nGeneral Motors (GM.N), Mercedes-Benz (MBGn.DE) and other automakers are challenging Tesla's Autopilot technology. But those automakers use more robust monitoring systems than Tesla's to assure drivers keep their eyes on the road and are ready to re-take control.\nAdvertisement · Scroll to continue\nThe agency is investigating the performance of Autopilot after identifying more than a dozen crashes in which Tesla vehicles hit stationary emergency vehicles. It is also investigating whether Tesla vehicles adequately ensure drivers are paying attention when using the driver assistance system.\nIn June 2022, NHTSA upgraded an earlier probe of 830,000 Tesla vehicles into an engineering analysis - a required step before it could potentially demand a recall.\nAutopilot is intended to enable cars to steer, accelerate and brake automatically within their lane, while \"Enhanced Autopilot\" and “Full Self-Driving” allow vehicles to do more sophisticated driving tasks. Tesla says those technologies “require active driver supervision and do not make the vehicle autonomous.”\nThe special order requests documents and explanations by Aug. 25 to help with NHTSA's inquiry. Late responses can incur a penalty of $26,315 per day.\nReporting by Hyunjoo Jin and Joseph White; Editing by Peter Henderson and Lisa Shumaker\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab\nJoseph White\nThomson Reuters\nJoe White is a global automotive correspondent for Reuters, based in Detroit. Joe covers a wide range of auto and transport industry subjects, writes The Auto File, a three-times weekly newsletter about the global auto industry. Joe joined Reuters in January 2015 as the transportation editor leading coverage of planes, trains and automobiles, and later became global automotive editor. Previously, he served as the global automotive editor of the Wall Street Journal, where he oversaw coverage of the auto industry and ran the Detroit bureau. Joe is co-author (with Paul Ingrassia) of Comeback: The Fall and Rise of the American Automobile Industry, and he and Paul shared the Pulitzer Prize for beat reporting in 1993.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous vehicle safety concerns. The U.S. auto safety regulator, National Highway Traffic Safety Administration (NHTSA), is investigating Tesla's Autopilot driver assistance system for a software change that allows drivers to keep their hands off the wheel for longer, potentially risking collisions. The NHTSA ordered Tesla to answer questions about the Autopilot change, including when the software update was introduced, the number of vehicles affected, Tesla's reasons for installing it, and any collisions or near-misses involving vehicles with the software. The NHTSA expressed concerns about the feature leading to driver inattention and a failure to properly supervise Autopilot. Additionally, there is mention of a trial later in the year over allegations that Autopilot failure led to a death, indicating the seriousness of the safety concerns surrounding autonomous driving technology.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/nhtsa-issues-special-order-regarding-teslas-driver-monitoring-system-autopilot-2023-08-29\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":61,"Time":"Sat 1 Jul 2017 04.23 CEST","Title":"Volvo admits its self-driving cars are confused by kangaroos","Content":"Volvo’s self-driving car is unable to detect kangaroos because hopping confounds its systems, the Swedish carmaker says.\nThe company’s “Large Animal Detection system” can identify and avoid deer, elk and caribou, but early testing in Australia shows it cannot adjust to the kangaroo’s unique method of movement.\nThe managing director of Volvo Australia, Kevin McCann, said the discovery was part of the development and testing of driverless technology, and wouldn’t pose problems by the time Volvo’s driverless cars would be available in 2020.\nThe road ahead: self-driving cars on the brink of a revolution in California\nRead more\n“Any company that would be working on the autonomous car concept would be having to do the same developmental work,” he said. “We brought our engineers into Australia to begin the exercise of gathering the data of how the animals can move and behave so the computers can understand it more.”\nEarlier this month, Volvo’s Australian technical manager, David Pickett, told the ABC the troubles had arisen because their cars’ object detection systems used the ground as a reference point.\nThis meant a kangaroo’s hopping was making it difficult to judge how close they were.\n“When it’s in the air, it actually looks like it’s further away, then it lands and it looks closer,” he said.\nMcCann added: “Autonomous cars are a continuing development. A driverless car does not yet exist, and developing technology to recognise kangaroos is part of that development.\nVolvo’s Trent Victor runs through some of the features of the self-driving XC90 during a trial in Adelaide. Photograph: Rick Goodman\/AAP\n“We are developing a car that can recognise kangaroos,” he said.\nVolvo’s detection system was designed in Sweden, where it was tested in areas populated with moose, before trials at a nature reserve in Canberra revealed the problem with kangaroos.\nKangaroos cause more accidents than any other animal in Australia – the marsupials are responsible for about 90% of collisions between vehicles and animals – although most are not serious.\nA spokeswoman for Robert Bosch Australia, which develops component technology for driverless cars, said their system could theoretically recognise kangaroos.\n“Although it hasn’t been tested in a kangaroo-specific environment, there was an instance where black swans were interfering, and so they had to build into the car the ability to recognise animals,” Amy Kaa said.\nVolvo plans to release its first autonomous cars by 2020 and has pledged zero fatalities or serious injuries from all its cars by that time.\n“The whole development process has to take in as many variations of conditions as possible,” McCann said. “It’s a fairly drawn-out process. We don’t even refer to it specifically as kangaroo detection, it’s what we call small animal detection.”\nThe carmaker offers now semi-autonomous features in its S90 and XC90 models, which it says give “a taste of the future of autonomous driving”.\nThe cars can automatically maintain a safe distance from the vehicle in front, and spot potential collisions in urban environments. McCann said a feature called “run-off road assist” would keep passengers safe in near-collisions.","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"Volvo's self-driving cars are facing challenges in detecting kangaroos due to their unique method of movement. The company's 'Large Animal Detection system' can identify and avoid other animals like deer, elk, and caribou, but it struggles with kangaroos. This issue was discovered during early testing in Australia. The cars' object detection systems use the ground as a reference point, making it difficult to judge the distance of hopping kangaroos accurately. Volvo Australia's managing director, Kevin McCann, explained that this challenge is part of the ongoing development and testing of driverless technology. They are working to gather data on how kangaroos move and behave so that their autonomous cars can better understand and respond to them. While Volvo plans to release its first autonomous cars by 2020, they are committed to addressing this issue as part of their development process.\"\n}\nThe news is relevant to autonomous vehicle failures, specifically regarding Volvo's self-driving cars' difficulties in detecting kangaroos and the measures they are taking to address this issue.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2017\/jul\/01\/volvo-admits-its-self-driving-cars-are-confused-by-kangaroos","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":67,"Time":"APRIL 17, 2019","Title":"Exclusive: Apple in talks with potential suppliers of sensors for self-driving cars - sources","Content":"By Stephen Nellis\nSAN FRANCISCO (Reuters) - Apple Inc has held talks with at least four companies as possible suppliers for next-generation lidar sensors in self-driving cars, evaluating the companies’ technology while also still working on its own lidar unit, three people familiar with the discussions said.\nApple's headquarters in Cupertino, California in a 2017 aerial photo. REUTERS\/Noah Berger\nThe moves provide fresh evidence of Apple’s renewed ambitions to enter the autonomous vehicle derby, an effort it calls Project Titan. The talks are focused on next-generation lidar, a sensor that provides a three-dimensional look at the road.\nApple is seeking lidar units that would be smaller, cheaper and more easily mass produced than current technology, the three people said. The iPhone maker is setting a high bar with demands for a “revolutionary design,” one of the people familiar with the talks said. The people declined to name the companies Apple has approached.\nThe sensor effort means Apple wants to develop the entire chain of hardware to guide autonomous vehicles and has joined automakers and investors in the race to find winning technologies.\nCurrent lidar systems, including units from Velodyne Inc mounted on Apple’s fleet of self-driving test vehicles, use laser light pulses to render precise images of the environment around the car. But the systems can cost $100,000 and use mechanical parts to sweep the laser scanners across the road.\nThat makes them too bulky and prone to failure for use in mass-produced vehicles. The shortcomings have spurred $1 billion in investment at dozens of startups and mature companies alike to make lidar smaller, cheaper and more robust.\nApple’s interest in next-generation lidar sensors comes as it has sharply increased its road testing while bringing on key hires from Tesla Inc and Alphabet Inc’s Google.\nIt remains unclear whether the goal of Apple’s Project Titan is to build its own vehicle or supply the hardware and software elements of self-driving car while pairing with a partner for the entire vehicle.\nBut what is clear from Apple’s interest in cheaper lidar systems is that it wants to control the “perception stack” of sensors, computers and software to drive an autonomous vehicle, regardless of who makes the vehicle, another person familiar with the talks said. The three people familiar with the talks declined to be identified because the discussions are not public.\nIn addition to evaluating potential outside suppliers, Apple is believed to have its own internal lidar sensor under development, two of the people said.\nAlphabet-owned Waymo has taken a similar path, assembling a sensor and computer system while inking deals to buy vehicles from Fiat Chrysler Automobiles.\nApple gets “a lot of optionality by working on the perception stack,” said the second person familiar with the talks. “Bringing a passenger car to the market is really, really hard, and there’s no reason right now they need to jump into it.”\nREDUCING COSTS\nThe designs Apple is seeking could potentially be made with conventional semiconductor manufacturing techniques, all four people familiar with the talks said.\nThat has the potential to lower prices from the many thousands to the hundreds of dollars as the sensors are produced in larger numbers, similar to chips in phones and other devices. Apple also wants sensors that can see several hundred meters (yards) down the road.\nThe long-distance requirement shows Apple is interested in fully self-driving vehicles, versus the more limited features such as adaptive cruise control used today, two people familiar with the matter said.\n“They’re not happy with most of what they see,” the first person familiar with the matter said. “They’re looking for a revolutionary design.”\nA third person familiar with the matter said Apple is seeking a “design-oriented” sensor that would be sleek and unobtrusive enough to fit into the overall lines of a vehicle.\nApple declined to comment.\nApple once investigated building its own vehicle. The company had a team of more than a dozen engineers dedicated to detailed work such as ensuring doors closed quietly instead of slamming shut, a fourth person briefed on the matter said.\nApple last year re-hired Doug Field, an Apple veteran who was serving as Tesla’s engineering chief, to work on Project Titan. The project has about 1,200 people, according to a count in court documents.\nField has been putting his stamp on the effort, laying off about 190 workers but also bringing on key hires such as Michael Schwekutsch, who oversaw electric drive train technology at Telsa. Apple also ramped up its testing miles in California, driving nearly 80,000 last year compared to 800 the year before.\nReporting by Stephen Nellis in San Francisco; Editing by Greg Mitchell and Cynthia Osterman\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Apple Inc is in talks with at least four companies as possible suppliers for next-generation lidar sensors in self-driving cars. They are seeking lidar units that would be smaller, cheaper, and more easily mass-produced than current technology. Apple's goal is to control the 'perception stack' of sensors, computers, and software to drive an autonomous vehicle, and they have also been conducting road testing and hiring key personnel from companies like Tesla and Google. Additionally, Apple is believed to have its own internal lidar sensor under development. The designs they are seeking could potentially lower sensor prices from thousands to hundreds of dollars, and they are interested in fully self-driving vehicles with sensors that can see several hundred meters down the road.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-apple-autonomous-exclusive-idUSKCN1RT16V","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":69,"Time":"June 8, 2023","Title":"Focus: UK self-driving car dreams could break down at the starting line","Content":"Companies\nAxa Sa\nFollow\nBp Plc\nFollow\nMicrosoft Corp\nFollow\nShow more companies\nLONDON, June 8 (Reuters) - Britain could lose out on investments in autonomous vehicles (AVs) and see startups shift testing elsewhere if promised laws to regulate the technology are not passed before the next general election, startups and insurance companies said.\nDespite the government's vision to be a world leader in AV technology, truly driverless cars are not currently permitted on Britain's roads, making it difficult for start-ups to commercialise their vehicles and insurers to assess their risks.\nThe UK government said last August it would advance a bill in the current parliamentary session, which is expected to end this autumn, providing detailed regulations by 2025.\nThat has not yet happened, with political turmoil forcing the government to water down ambitions for this session.\nWith a national election due by January 2025, industry wants the government to bring forward legislation next session, or risk lengthy delays.\nAdvertisement · Scroll to continue\n\"There is a window of opportunity for the UK to at least be leading on something,\" said Claudio Gienal, head of UK and Ireland operations for global insurer AXA (AXAF.PA), which believes the technology could prevent accidents.\n\"Whoever moves first will have the advantage of attracting investment, skills and expertise.\"\n\"But if you're second or third, why should people come here?\" added Gienal, who wrote to Britain's finance minister in April urging the government to pass a bill in the next parliamentary session.\nAdvertisement · Scroll to continue\nThe British government's vision is built on forecasts that by 2035 around 40% of new cars could have self-driving capabilities and leadership in AV technology could create up to 38,000 new skilled jobs in a market worth 42 billion pounds ($53 billion) to Britain by that date.\nTransport Minister Mark Harper in December said there would no longer be a Transport Bill this session, and did not mention a separate AV bill in an outline of the ministry's legislative agenda.\nAdvertisement · Scroll to continue\nIain Stewart, who chairs parliament's transport committee, said there was not enough time for an AV law this session, but it could happen in the next session.\nLast month Junior Minister Jesse Norman said he shared AV startups' concerns.\n\"We are making the case as vigorously as we can... for this to be a priority for the government,\" he told lawmakers.\nFailure to enact regulations could cede ground to other countries like France, Germany and the United Arab Emirates, which are putting regulations in place, or several U.S. states including California.\nAdvertisement · Scroll to continue\nStartups are actively lobbying the government to pass the bill, while insurers need to know who is liable so they can insure driverless cars.\nThey fear a self-driving bill will be crowded out by other vote-winning priorities in the run-up to the election.\n\"We've got to see the legislation move forward in this next (parliamentary) session to make this a reality,\" said Kaity Fischer, commercial vice president at London-based Wayve. \"Or we would be forced to move to other markets for deployment.\"\nWayve has raised around $260 million so far from investors, including Microsoft (MSFT.O).\n\"CHANGE OUR PLANS\"\nSo far, the UK government has been seen as highly supportive of AV startups, with its Centre for Connected and Autonomous Vehicles securing over 400 million pounds in private and government funding for more than 90 projects.\nLast year, two independent governmental bodies spelled out well-received recommendations for an AV law.\nAshley Feldman, programme and policy manager for transport and smart cities at industry group TechUK, said a delay would force startups to test and start generating revenues elsewhere.\n\"It's important that these startups get to commercialisation quickly,\" he said.\nBristol-based Fusion Processing recently launched a 14-mile autonomous bus route linking Edinburgh and nearby Fife in partnership with bus operator Stagecoach, bus maker Alexander Dennis and partly funded by the government - with a safety driver at the wheel.\nFusion wants to start UK commercial driverless bus services in 2025.\n\"If that doesn't happen, then we would have to change our plans,\" CEO Jim Hutchinson said, including testing in other countries.\nOxford-based AV software firm Oxbotica has raised about $225 million from investors and is working on AV projects with customers including BP (BP.L) and British online supermarket and technology group Ocado (OCDO.L).\nFounder Paul Newman said he was optimistic the government will pass its promised bill, but added it is \"extremely important that we get this done very, very soon.\"\nReporting By Nick Carey, editing by Ben Klayman and Sharon Singleton\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about the potential impact of delayed legislation on autonomous vehicles (AVs) in the UK. It discusses concerns that the UK may lose investments in AV technology and startups could shift their testing and operations to other countries if promised laws to regulate the technology are not passed before the next general election. The UK government had initially aimed to be a world leader in AV technology, but truly driverless cars are not currently permitted on Britain's roads, making it challenging for startups to commercialize their vehicles and for insurers to assess their risks. The article also mentions the industry's desire for the government to bring forward legislation in the next session to avoid lengthy delays and to attract investment and expertise. Failure to enact regulations could potentially lead to other countries like France, Germany, and the United Arab Emirates taking the lead in AV technology. Startups and insurers are actively lobbying for the passage of the bill, as they fear it might be crowded out by other priorities in the run-up to the election.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/uk-self-driving-car-dreams-could-break-down-starting-line-2023-06-08\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":65,"Time":"Tue 20 Mar 2018 18.07 CET","Title":"UK self-driving car trials continue despite death of US pedestrian","Content":"Britain is pushing ahead with tests of self-driving cars on public roads despite mounting public concern over safety after a pedestrian was killed by one in the US.\nThe country’s biggest carmaker, Jaguar Land Rover, has been experimenting with autonomous cars on roads in the Midlands and is set to demonstrate more of the cars’ features, including an emergency braking warning system, on urban streets this week.\nGovernment-backed trials using small autonomous vehicles in south London are due to end on Friday, with organisers reporting widespread public unease about the implications for road safety and cybersecurity.\nA self-driving Uber car killed a woman in Tempe, Arizona on Sunday night – the first time a self-driving vehicle has killed someone that was not its occupant. Elaine Herzberg, 49, was wheeling her bicycle when she was struck by the Volvo, and later died of her injuries in hospital.\nUS government safety investigators were sent to examine the crash site and Uber has suspended its test fleets of self-driving cars across the US and Canada.\nPolice in Arizona said initial video footage suggested Herzberg walked out suddenly. One previous death involving autonomous cars, a Tesla Model S owner killed in Florida in 2016 when his car crashed on autopilot, was blamed on the driver’s inattention, but investigators highlighted design flaws in the vehicle.\nMany in the motor and insurance industries expect safety benefits from autonomous cars since more than 90% of accidents involve human error. In 2016, the latest full year for which data is available, 448 pedestrians were killed by vehicles on UK roads, and more than 6,000 in the US. But fears remain over how driverless cars will interact with humans on the roads.\nChristian Wolmar, the author of Driverless Cars: a Road to Nowhere, said the Arizona accident would have a big impact: “We don’t know precisely what happened, but it is clear Uber are worried by withdrawing all their cars. Driverless cars will not be accepted if there is a perception that they are not 100% safe. Of course new technology has blips, but this one, that no one has particularly asked for, is being sold on the basis that it’s so much safer.”\nNoel Sharkey, emeritus professor of artificial intelligence at the University of Sheffield, said: “Autonomous vehicles present us with a great future opportunity to make our roads safer. But the technology is just not ready yet and needs to mature before it goes on the road. Too many mistakes and the public may turn its back on the technology.”\nIn London, members of the public have been using low-speed autonomous pods on cycle paths and walkways around the Greenwich peninsula as the culmination of a three-year Gateway study into people’s responses to driverless technology.\nGateway said that under half of 1,300 public responses were positive about the new technology, with those uncertain or opposed citing cybersecurity and road safety fears. A spokesman said: “The lesson is you absolutely have to build in security and road safety from the get-go.”\nThe pods have had one reported collision, hitting a barrier with the roads minister, Jesse Norman, onboard. He has nonetheless pledged to keep the UK in the vanguard of developing autonomous technology, recently confirming an overhaul of road laws to include self-driving cars. Greenwich is expected to to allow Ford and Jaguar Land Rover autonomous cars on its streets in the next phase of testing.\nGatwick announced it would be testing autonomous vehicles to shuttle staff across the airfield, which it said could lead to “an Uber-like service” for ground staff to hail.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous vehicle failures. It mentions the death of a pedestrian in the US due to a self-driving Uber car in Tempe, Arizona. The incident resulted in the suspension of Uber's test fleets of self-driving cars across the US and Canada. There are also concerns expressed about the safety and maturity of autonomous vehicle technology, with experts and members of the public raising cybersecurity and road safety fears.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/mar\/20\/uk-self-driving-car-trials-to-continue-despite-fatal-collision-in-us","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":74,"Time":"12:00 AM GMT+2•April 25, 2023","Title":"Max Q: A successful failure","Content":"Hello and welcome back to Max Q!\nIn this issue:\nStarship’s successful failure\nRocket Lab’s Electron goes suborbital\nNews from Orbit Fab and more\nSpaceX launches fully stacked Starship for the first time\nSpaceX launched a fully stacked Starship for the first time on Thursday morning, with the vehicle passing through Max Q and flying for around three minutes. Let me be clear: This was a remarkable result for a first flight test, especially given that this is the biggest and most powerful rocket human beings have ever built. To fixate on the conclusion of the launch — the fairly spectacular explosion after the rocket’s upper stage failed to separate from the booster — is to miss the significant success story of the Starship testing program so far.\nThe flight test likely yielded a ton of useful data to inform subsequent Starship tests — data that the company wouldn’t be able to get through further ground testing. Sometimes you just need to fly the damn rocket.\nImage Credits: SpaceX\nRocket Lab adds a suborbital launch option aimed at hypersonic defense customers\n“Rocket Lab is adding a new offering to its suite of services: hypersonic suborbital launches. The new service will be available starting sometime in the first half of this year, when the company will fly its first mission for a “confidential customer” who contracted use of the new suborbital rocket.\nRocket Lab’s suborbital vehicle is derived from its successful Electron orbital launch vehicle, but gets a new name by way of the backronym “HASTE” (hypersonic accelerator suborbital test electron). The first of these is already being prepared for launch at Rocket Lab’s U.S. launch facility in Wallops Island, Virginia.” – Darrell Etherington\nImage Credits: Rocket Lab\nMore news from TC and beyond\nAstranis closed $200 million in new funding as it gears up for its first launch on a SpaceX Super Heavy this week. (Bloomberg)\nAxiom Space is launching a new program aimed at giving countries that don’t have a domestic space agency easier access to space. (Axiom)\nAWS announced the 14 startups that will be participating in its space accelerator. (AWS)\nLockheed Martin successfully demonstrated rendezvous and proximity operations, technologies that could be used for on-orbit servicing, earlier this year. (Via Satellite)\nOrbit Fab, a startup that wants to establish on-orbit refueling services, closed a $28.5 million Series A. (TechCrunch)\nRocket Lab plans to reuse a Rutherford engine, which powers the Electron rocket, for the first time in an upcoming launch. (Rocket Lab)\nSpace Capital’s latest quarterly investment report highlights how the private markets are continuing to reset after the sky-high valuations and fund flow of the past few years. (Space Capital)\nSpace Forge, a U.K.-based startup that focuses on in-space manufacturing, plans to expand its presence in the U.S. with manufacturing facilities for its satellites. (Space Forge)\nSpaceX reduced Starlink monthly customer service fees to customers in multiple European countries. (Starlink Insider)\nSpaceX’s “successful failure” highlights how Starship missions are in need of a timeline reset. (TechCrunch)\nVirgin Orbit has every intention of launching again, despite filing for bankruptcy and laying off 85% of its staff. (The Washington Post)\nMax Q is brought to you by me, Aria Alamalhodaei. If you enjoy reading Max Q, consider forwarding it to a friend. ","Relevant":true,"Overview":"Based on the provided news article, it does mention SpaceX's Starship launch and the fact that it experienced an explosion during the flight test. However, the article primarily focuses on the success of the Starship testing program and the valuable data obtained from the flight test.\nSo, based on the content of the news article, here is the JSON-format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to SpaceX's Starship launch, which experienced an explosion during the flight test. However, the article emphasizes the overall success of the Starship testing program and the value of the data obtained from the test.\"\n}\nPlease note that while there was an incident during the Starship launch, the article highlights it as part of the testing process and not necessarily as an autonomous vehicle failure.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2023\/04\/24\/max-q-a-successful-failure\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":76,"Time":"Tue 28 Feb 2023 02.59 CET","Title":"Elon Musk overstated Tesla’s autopilot and self-driving tech, new lawsuit says","Content":"Elon Musk is facing yet another lawsuit as shareholders of Tesla accuse the chief executive and his company of overstating the effectiveness and safety of their electric vehicles’ autopilot and full self-driving technologies.\nTesla recalls 362,000 vehicles over self-driving software flaws that risk crashes\nRead more\nShareholders have alleged in the proposed class-action lawsuit that Tesla defrauded them over four years with false and misleading statements that concealed how its technologies – suspected as a possible cause of multiple fatal crashes – “created a serious risk of accident and injury”. The case was filed on Monday in a San Francisco federal court.\nThe case centers on the financial fallout of Tesla’s failed autopilot features, citing when the company’s share price fell after reports that the National Highway Traffic Safety Administration and the Securities and Exchange Commission had begun investigating the technologies.\nThe share price also fell 5.7% on 16 February 2023 after NHTSA forced a recall of more than 362,000 Tesla vehicles equipped with full self-driving beta software because they could be unsafe around intersections.\n“As a result of defendants’ wrongful acts and omissions, and the precipitous decline in the market value of the company’s common stock, plaintiff and other class members have suffered significant losses and damages,” the complaint said.\nTesla, which does not have a media relations department, did not immediately respond to requests for comment.\nThe suit, which is led by shareholder Thomas Lamontagne seeks unspecified damages for Tesla shareholders from 19 February 2019 to 17 February 2023. Chief financial officer Zachary Kirkhorn and his predecessor Deepak Ahuja are also defendants.\nMusk has been sued in the past over how his decisions affect company shares, including a suit over tweets he sent about taking Tesla private – which shareholders claimed cost them millions of dollars. That case was decided in Musk’s favor earlier this month. In another lawsuit continuing this week, shareholders claim they received “misleading” information that led them to approve an exorbitant pay package for the billionaire.\nThe legal action Musk faces is not limited to Tesla. The executive, who also helms Twitter, SpaceX and Neuralink, is facing lawsuits spanning all of his companies, including a recent suit filed by Twitter shareholders who say the executive’s antics when purchasing the social media firm contributed to volatility in the company’s price.\nAs these suits continue, Musk is expected at Tesla’s 1 March investor day to promote the company’s artificial intelligence capability and plans to expand its vehicle lineup.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a lawsuit against Elon Musk and Tesla by shareholders, accusing them of overstating the effectiveness and safety of their electric vehicles' autopilot and full self-driving technologies. Shareholders have alleged that Tesla defrauded them with false and misleading statements concealing the serious risk of accidents and injuries associated with their technologies. The lawsuit also mentions a recall of 362,000 Tesla vehicles due to self-driving software flaws that risk crashes, as well as the decline in Tesla's share price following investigations by regulatory agencies.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2023\/feb\/27\/elon-musk-tesla-lawsuit-autopilot","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":88,"Time":"12:33 AM GMT+2•May 8, 2021","Title":"Tesla refutes Elon Musk’s timeline on ‘full self-driving’","Content":"cameraImage Credits: Patrick T. Fallon \/ Bloomberg \/ Getty Images\nWhat Tesla CEO Elon Musk says publicly about the company’s progress on a fully autonomous driving system doesn’t match up with “engineering reality,” according to a memo that summarizes a meeting between California regulators and employees at the automaker.\nThe memo, which transparency site Plainsite obtained via a Freedom of Information Act request and subsequently released, shows that Musk has inflated the capabilities of the Autopilot advanced driver assistance system in Tesla vehicles, as well the company’s ability to deliver fully autonomous features by the end of the year. \nTesla vehicles come standard with a driver assistance system branded as Autopilot. For an additional $10,000, owners can buy “full self-driving,” or FSD — a feature that Musk promises will one day deliver full autonomous driving capabilities. FSD, which has steadily increased in price and capability, has been available as an option for years. However, Tesla vehicles are not self-driving. FSD includes the parking feature Summon as well as Navigate on Autopilot, an active guidance system that navigates a car from a highway on-ramp to off-ramp, including interchanges and making lane changes. Once drivers enter a destination into the navigation system, they can enable “Navigate on Autopilot” for that trip.\nTesla vehicles are far from reaching that level of autonomy, a fact confirmed by statements made by the company’s director of Autopilot software CJ Moore to California regulators, the memo shows.\n“Elon’s tweet does not match engineering reality per CJ,” according to the memo summarizing the conversation between regulators with the California Department of Motor Vehicles’ autonomous vehicles branch and four Tesla employees, including Moore.\nTesla’s decision to scrap its PR department could create a PR nightmare\nThe memo, which was written by California DMV’s Miguel Acosta, states that Moore described Autopilot — and the new features being tested — as a Level 2 system. That description matters in the world of automated driving.\nThere are five levels of automation under standards created by SAE International. Level 2 means two primary functions — like adaptive cruise and lane keeping — are automated and still have a human driver in the loop at all times. Level 2 is an advanced driver assistance system, and has become increasingly available in new vehicles, including those produced by Tesla, GM, Volvo and Mercedes. Tesla’s Autopilot and its more capable FSD were considered the most advanced systems available to consumers. However, other automakers have started to catch up.\nLevel 4 means the vehicle can handle all aspects of driving in certain conditions without human intervention and is what companies like Argo AI, Aurora, Cruise, Motional, Waymo and Zoox are working on. Level 5, which is widely viewed as a distant goal, would handle all driving in all environments and conditions.\nHere is an important bit via Acosta’s summarization:\nDMV asked CJ to address from an engineering perspective, Elon’s messaging about L5 capability by the end of the year. Elon’s tweet does not match engineering reality per CJ. Tesla is at Level 2 currently. The ratio of driver interaction would need to be in the magnitude of 1 or 2 million miles per driver interaction to move into higher levels of automation. Tesla indicated that Elon is extrapolating on the rates of improvement when speaking about L5 capabilities. Tesla couldn’t say if the rate of improvement would make it to L5 by end of calendar year.\nPortions of this commentary were redacted. However, Plainsite was able to copy and paste the redacted part, which shows up as white space on a PDF, into another document.\nThe comments in the memo are contrary to what Musk has said repeatedly in the public sphere.\nMusk is frequently asked on Twitter and in quarterly earnings calls for progress reports on FSD, including questions about when it will be rolled out via software updates to owners who have purchased the option. In a January earnings call, Musk said he was “highly confident the car will be able to drive itself with reliability in excess of a human this year.” In April 2021, during the company’s first quarter earnings call, Musk said “it’s really quite, quite tricky. But I am highly confident that we will get this done.”\nThe memo released this week provided other insights into Tesla’s push to test and eventually unlock greater levels of autonomy, including the number of vehicles testing a beta version of “Navigate on Autopilot on City Streets,” a feature that is meant to handle driving in urban areas and not just highways. Regulators also asked the Tesla employees if and how participants were being trained to test this feature, and how the sales team ensures that messaging about the vehicle capabilities and limitations are communicated.\nAs of the March meeting, there were 824 vehicles in a pilot program testing a beta version of “city streets.”  About 750 of those vehicles were being driven by employees and 71 by non-employees. Pilot participants are located across 37 states, with the majority of participants in California. As of March 2021, pilot participants have driven more than 153,000 miles using the City Streets feature, the memo states. The memo noted that Tesla planned to expand this pool of participants to approximately 1,600 later that month.\nTesla told the DMV that it is working on developing a video for the participants and that the next group of participants will include referrals from existing participants. “The new participants will be vetted by Tesla by looking at insurance telematics based on the VINs registered to that participant,” according to the memo.\nTesla also told the DMV that it is able to track when there are failures or when the feature is deactivated. Moore described these as “disengagements,” a term also used by companies testing and developing autonomous vehicle technology. The primary difference worth noting here is that these companies only use employees who are trained safety drivers, not the public.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about Tesla's CEO Elon Musk's statements about the company's progress on a fully autonomous driving system not matching 'engineering reality.' The memo obtained through a Freedom of Information Act request reveals that Musk has inflated the capabilities of Tesla's Autopilot and its ability to deliver fully autonomous features by the end of the year. The memo indicates that Tesla's Autopilot is described as a Level 2 system, which means it has two primary automated functions with a human driver in the loop. It also mentions the discrepancy between Elon Musk's messaging about Level 5 (full autonomy) capability by the end of the year and the engineering reality, with Tesla currently at Level 2. Additionally, the memo provides insights into Tesla's push to test and expand autonomy, including the number of vehicles in pilot programs and plans for participant vetting.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2021\/05\/07\/tesla-refutes-elon-musks-timeline-on-full-self-driving\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":91,"Time":null,"Title":"Daughter of Uber autonomous vehicle victim retains lawyer","Content":"By David Schwartz\nPHOENIX, Ariz (Reuters) - The daughter of the woman killed by an Uber [UBER.UL] self-driving vehicle in Arizona has retained a personal injury lawyer, underlying the potential high stakes of the first fatality caused by an autonomous vehicle.\nFILE PHOTO: U.S. National Transportation Safety Board (NTSB) investigators examine a self-driving Uber vehicle involved in a fatal accident in Tempe, Arizona, U.S., March 20, 2018. A women was struck and killed by the vehicle on March 18, 2018. National Transportation Safety Board\/Handout via REUTERS\nThe law firm of Bellah Perez in Glendale, Arizona, said in a statement on Thursday it was representing the daughter of Elaine Herzberg, who died on Sunday night after being hit by the Uber self-driving SUV in the Phoenix suburb of Tempe.\n“As the first pedestrian death involving an Uber autonomous vehicle, the incident has sparked a national debate about the safety of self-driving cars, exposing the technology’s blind spots and raising questions of liability,” the law firm said.\nThe firm did not immediately return phone calls seeking more information.\nFall-out from the accident could stall the development and testing of self-driving vehicles, which are designed to perform far better than human drivers and sharply reduce the number of motor vehicle fatalities that occur each year.\nThe fatality also presents an unprecedented liability challenge because self-driving vehicles, which are still in the development stage, involve a complex system of hardware and software often made by outside suppliers. The specifics of how Uber’s technology operates are not known.\nOn Thursday a group of 10 Democratic senators cited the fatality in Tempe in a letter sent to Uber and 59 other companies including Ford Motor Co F.N and Tesla Inc TSLA.O highlighting how the use of forced arbitration clauses to settle customer disputes would prevent victims of accidents involving self-driving vehicles from exercising their legal rights.\nThe senators, led by Richard Blumenthal, a member of the Senate Commerce, Science and Transportation Committee, noted that Uber includes forced arbitration in its standard terms of service with users.\n“Had the victim been a passenger ... the victim’s family could have been denied recourse through the courts,” the letter read. The senators asked the companies to commit to not using the arbitration clauses in contracts related to self-driving cars.\n“When injury or death does occur, a forced arbitration clause would prevent consumers from exercising their fundamental legal rights as Americans,” the senators wrote.\nMany companies include forced arbitration clauses in contracts with customers, requiring that any disputes be settled in binding arbitration and barring customers from suing in a court of law. Arbitration rulings, generally, cannot be appealed.\nACCIDENT LIABILITY?\nHerzberg, 49, who was homeless, was jay-walking across a divided four-lane road with her bicycle when she was struck while in the far right-hand lane. A video taken from a dash-mounted camera inside the vehicle that was released by Tempe police on Wednesday showed the SUV traveling along a dark street when suddenly the headlights illuminate Herzberg in front of the SUV.\nShe later died from her injuries.\nOther footage showed the human driver who was behind the wheel mostly looking down and not at the road in the seconds before the accident.\nUber, like many other companies testing self-driving vehicles, has a human driver in each vehicle as a monitor and to act as a backup if necessary.\nFew details of the incident have emerged amid the investigations by police and federal safety regulators. Police have said the vehicle, a Volvo XC90 which was operating in autonomous mode, was traveling at about 40 mile per hour (65 km per hour) at the time of the collision and did not appear to brake.\nPolice have said that following their probe, they will submit the case to the Maricopa County Attorney’s office, which will determine if there is any basis for a case for potential criminal prosecution.\nOne key question for investigators will be how the vehicle’s technology failed to notice the pedestrian crossing the street in front of it, despite the darkness. Self-driving cars typically use a combination of sensors, including radar and light-sensing Lidar, to identify objects, including potential obstacles coming into range.\nIn company presentations, Uber has stated its self-driving technology includes sensors that provide a 360-degree view around the vehicle.\nWriting by Alexandria Sage; Additional reporting by David Shepardson in Washington, D.C.; Editing by Leslie Adler\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about the daughter of the woman killed by an Uber self-driving vehicle in Arizona retaining a personal injury lawyer. The incident is the first fatality caused by an autonomous vehicle. The law firm of Bellah Perez in Glendale, Arizona, is representing the daughter of Elaine Herzberg, who died after being hit by the Uber self-driving SUV. The incident has sparked a national debate about the safety of self-driving cars and questions of liability. It is mentioned that the accident could stall the development and testing of self-driving vehicles, and it presents an unprecedented liability challenge due to the complexity of hardware and software involved. There is also a mention of a group of 10 Democratic senators citing the fatality in a letter sent to Uber and other companies, highlighting concerns about forced arbitration clauses in contracts related to self-driving cars. The news provides details about the accident, including the circumstances in which Herzberg was struck, the presence of a human driver as a monitor in the vehicle, and the use of sensors in self-driving technology.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-uber-idUSKBN1GY30I","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":77,"Time":"Published 3:42 AM GMT+2, August 17, 2021","Title":"US probing Autopilot problems on 765,000 Tesla vehicles","Content":"DETROIT (AP) — The U.S. government has opened a formal investigation into Tesla’s Autopilot partially automated driving system after a series of collisions with parked emergency vehicles.\nThe investigation covers 765,000 vehicles, almost everything that Tesla has sold in the U.S. since the start of the 2014 model year. Of the crashes identified by the National Highway Traffic Safety Administration as part of the probe, 17 people were injured and one was killed.\nNHTSA says it has identified 11 crashes since 2018 in which Teslas on Autopilot or Traffic Aware Cruise Control have hit vehicles at scenes where first responders have used flashing lights, flares, an illuminated arrow board or cones warning of hazards. The agency announced the action Monday in a posting on its website.\nThe probe is another sign that NHTSA under President Joe Biden is taking a tougher stance on automated vehicle safety than under previous administrations. Previously the agency was reluctant to regulate the new technology for fear of hampering adoption of the potentially life-saving systems.\nThe investigation covers Tesla’s entire current model lineup, the Models Y, X, S and 3 from the 2014 through 2021 model years.\nThe National Transportation Safety Board, which also has investigated some of the Tesla crashes dating to 2016, has recommended that NHTSA and Tesla limit Autopilot’s use to areas where it can safely operate. The NTSB also recommended that NHTSA require Tesla to have a better system to make sure drivers are paying attention. NHTSA has not taken action on any of the recommendations. The NTSB has no enforcement powers and can only make recommendations to other federal agencies.\n“Today’s action by NHTSA is a positive step forward for safety,” NTSB Chair Jennifer L. Homendy said in a statement Monday. “As we navigate the emerging world of advanced driving assistance systems, it’s important that NHTSA has insight into what these vehicles can, and cannot, do.”\nLast year the NTSB blamed Tesla, drivers and lax regulation by NHTSA for two collisions in which Teslas crashed beneath crossing tractor-trailers. The NTSB took the unusual step of accusing NHTSA of contributing to the crash for failing to make sure automakers put safeguards in place to limit use of electronic driving systems.\nThe agency made the determinations after investigating a 2019 crash in Delray Beach, Florida, in which the 50-year-old driver of a Tesla Model 3 was killed. The car was driving on Autopilot when neither the driver nor the Autopilot system braked or tried to avoid a tractor-trailer crossing in its path.\n“We are glad to see NHTSA finally acknowledge our long standing call to investigate Tesla for putting technology on the road that will be foreseeably misused in a way that is leading to crashes, injuries, and deaths,” said Jason Levine, executive director of the nonprofit Center for Auto Safety, an advocacy group. “If anything, this probe needs to go far beyond crashes involving first responder vehicles because the danger is to all drivers, passengers, and pedestrians when Autopilot is engaged.”\nAutopilot has frequently been misused by Tesla drivers, who have been caught driving drunk or even riding in the back seat while a car rolled down a California highway.\nA message was left seeking comment from Tesla, which has disbanded its media relations office. Shares of Tesla Inc., based in Palo Alto, California, fell 4.3% Monday.\nNHTSA has sent investigative teams to 31 crashes involving partially automated driver assist systems since June of 2016. Such systems can keep a vehicle centered in its lane and a safe distance from vehicles in front of it. Of those crashes, 25 involved Tesla Autopilot in which 10 deaths were reported, according to data released by the agency.\nTesla and other manufacturers warn that drivers using the systems must be ready to intervene at all times. In addition to crossing semis, Teslas using Autopilot have crashed into stopped emergency vehicles and a roadway barrier.\nThe probe by NHTSA is long overdue, said Raj Rajkumar, an electrical and computer engineering professor at Carnegie Mellon University who studies automated vehicles.\nTesla’s failure to effectively monitor drivers to make sure they’re paying attention should be the top priority in the probe, Rajkumar said. Teslas detect pressure on the steering wheel to make sure drivers are engaged, but drivers often fool the system.\n“It’s very easy to bypass the steering pressure thing,” Rajkumar said. “It’s been going on since 2014. We have been discussing this for a long time now.”\nThe crashes into emergency vehicles cited by NHTSA began on Jan. 22, 2018 in Culver City, California, near Los Angeles when a Tesla using Autopilot struck a parked firetruck that was partially in the travel lanes with its lights flashing. Crews were handling another crash at the time.\nSince then, the agency said there were crashes in Laguna Beach, California; Norwalk, Connecticut; Cloverdale, Indiana; West Bridgewater, Massachusetts; Cochise County, Arizona; Charlotte, North Carolina; Montgomery County, Texas; Lansing, Michigan; and Miami, Florida.\n“The investigation will assess the technologies and methods used to monitor, assist and enforce the driver’s engagement with the dynamic driving task during Autopilot operation,” NHTSA said in its investigation documents.\nIn addition, the probe will cover object and event detection by the system, as well as where it is allowed to operate. NHTSA says it will examine “contributing circumstances” to the crashes, as well as similar crashes.\nAn investigation could lead to a recall or other enforcement action by NHTSA.\n“NHTSA reminds the public that no commercially available motor vehicles today are capable of driving themselves,” the agency said in a statement. “Every available vehicle requires a human driver to be in control at all times, and all state laws hold human drivers responsible for operation of their vehicles.”\nThe agency said it has “robust enforcement tools” to protect the public and investigate potential safety issues, and it will act when it finds evidence “of noncompliance or an unreasonable risk to safety.”\nIn June, NHTSA ordered all automakers to report any crashes involving fully autonomous vehicles or partially automated driver assist systems.\nTesla uses a camera-based system, a lot of computing power, and sometimes radar to spot obstacles, determine what they are, and then decide what the vehicles should do. But Carnegie Mellon’s Rajkumar said the company’s radar was plagued by “false positive” signals and would stop cars after determining overpasses were obstacles.\nNow Tesla has eliminated radar in favor of cameras and thousands of images that the computer neural network uses to determine if there are objects in the way. The system, he said, does a very good job on most objects that would be seen in the real world. But it has had trouble with parked emergency vehicles and perpendicular trucks in its path.\n“It can only find patterns that it has been quote-unquote trained on,” Rajkumar said. “Clearly the inputs that the neural network was trained on just do not contain enough images. They’re only as good as the inputs and training. Almost by definition, the training will never be good enough.”\nTesla also is allowing selected owners to test what it calls a “full self-driving” system. Rajkumar said that should be investigated as well.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about the U.S. government opening a formal investigation into Tesla's Autopilot partially automated driving system after a series of collisions with parked emergency vehicles. The investigation covers 765,000 vehicles, including Tesla's entire current model lineup from the 2014 through 2021 model years. The investigation was prompted by 11 crashes since 2018 involving Teslas on Autopilot or Traffic Aware Cruise Control hitting vehicles at scenes where first responders have used flashing lights, flares, an illuminated arrow board, or cones warning of hazards. Of these crashes, 17 people were injured, and one person was killed. The National Transportation Safety Board (NTSB) has recommended limiting Autopilot's use to safer areas and ensuring drivers pay attention, but NHTSA has not taken action on these recommendations. NHTSA's investigation will assess technologies, methods, and contributing circumstances related to Autopilot operation and could lead to a recall or other enforcement action.\"\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/technology-business-61557d668b646e7ef48c5543d3a1c66c","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":5,"Time":"16 September 2020","Title":"Uber's self-driving operator charged over fatal crash","Content":"Uber's self-driving operator charged over fatal crash\nPublished\n16 September 2020\nShare\nIMAGE SOURCE,\nREUTERS\nImage caption,\nThe self-driving Volvo hit a pedestrian at 39mph, despite the presence of a safety driver\nThe back-up driver of an Uber self-driving car that killed a pedestrian has been charged with negligent homicide.\nElaine Herzberg, aged 49, was hit by the car as she wheeled a bicycle across the road in Tempe, Arizona, in 2018.\nInvestigators said the car's safety driver, Rafael Vasquez, had been streaming an episode of the television show The Voice at the time.\nMs Vasquez pleaded not guilty, and was released to await trial.\nUber will not face criminal charges, after a decision last year that there was \"no basis for criminal liability\" for the corporation.\nThe accident was the first death on record involving a self-driving car, and resulted in Uber ending its testing of the technology in Arizona.\n'Visually distracted'\nLengthy investigations by police and the US National Transportation Safety Board (NTSB) found that human error was mostly to blame for the crash.\nMs Vasquez was in the driver's seat, and had the ability to take over control of the vehicle in an emergency.\nDash-cam footage released by police showed Ms Vasquez looking down, away from the road, for several seconds immediately before the crash, while the car was travelling at 39mph (63km\/h).\nPolice say that although her first name was listed on her driver's licence as Rafael, Ms Vasquez identifies as a woman and goes by Rafaela.\nMedia caption,\nUber dash-cam footage shows moment before fatal impact\nRecords from the streaming service Hulu also seemed to show that her device had been streaming a television show at the time.\nA police report from June 2018 labelled the fatal collision as \"entirely avoidable\" if the driver had been watching the road.\nThe NTSB, meanwhile, identified the probable cause of the accident as failure of the operator to monitor their surroundings, and the automated system, \"because she was visually distracted throughout the trip by her personal cell phone\".\nSelf-driving Uber in fatal crash had 'safety flaws'\nUber warned about self-drive car crashes\nNTSB vice chairman Bruce Landsberg wrote in the report: \"On this trip, the safety driver spent 34% of the time looking at her cell phone while streaming a TV show.\"\nMs Vasquez was charged on 27 August, and made her first appearance in court on 15 September. The trial is now set for February next year.\nIn May 2018, when Elaine Herzberg was killed, confidence in autonomous vehicle technology was at an all-time high.\nEveryone from Elon Musk to the British Chancellor Philip Hammond was telling us that robo-taxis and other autonomous vehicles would be on the roads within a couple of years, cutting congestion and delivering a big boost to road safety.\nBut the accident in Arizona punctured that confidence.\nIt showed that however smart the machine learning in the autonomous systems, mixing robots with humans as cars made the journey towards full autonomy was going to prove a real challenge.\nNot only did Uber have to halt its testing programme for a while, but rivals such as Google's Waymo became notably more cautious in their trials. Only today it is being reported that the Chinese tech giant Baidu is pushing back the full rollout of its robo-taxis until 2025, partly because of confusion about regulations.\nAs long as \"self-driving\" cars still need a human safety driver behind the wheel, there will be confusion about whose fault it is when something goes wrong - but going fully autonomous is such a huge leap that even the boldest tech firm is likely to be very cautious about going first.\nDespite the decision not to levy criminal charges against Uber itself, the company did not escape criticism.\nThe NTSB report said that Uber's \"inadequate safety risk assessment procedures\" and \"ineffective oversight of vehicle operators\" were contributing factors. It accused the company of having an \"inadequate safety culture\".\nThe vehicle's automatic systems failed to identify Ms Herzberg and her bicycle as an imminent collision danger in the way they were supposed to, the NTSB found.\nDays before the crash, an employee had warned his superiors that the vehicles were unsafe, were routinely in accidents, and raised concerns about the training of operators.\nFollowing the crash, authorities in Arizona suspended Uber's ability to test self-driving cars on the state's public roads, and Uber ended its tests in the state. It received permission to carry out tests in the state of California earlier this year.\nMore on this story\nSelf-driving Uber in fatal crash had 'safety flaws'\nPublished\n6 November 2019\nUber self-drive crash 'mostly due to human error'\nPublished\n20 November 2019\nUber 'not criminally liable' for car death\nPublished\n6 March 2019\nFootage shows moment of fatal Uber crash. Video, 00:00:29\nFootage shows moment of fatal Uber crash\nPublished\n22 March 2018\n0:29\nUber crash driver was 'watching TV'\nPublished\n22 June 2018\nUber warned about self-drive car crashes\nPublished\n13 December 2018\nUber issued permit to test self-driving cars again\nPublished\n5 February 2020","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about an incident involving an Uber self-driving car that resulted in a fatal crash in Tempe, Arizona, in 2018. The back-up driver, Rafael Vasquez, was charged with negligent homicide. The incident occurred when the self-driving Volvo hit a pedestrian, Elaine Herzberg, who was wheeling a bicycle across the road. Investigators found that the safety driver, Rafael Vasquez, had been visually distracted, streaming a television show on her cell phone moments before the crash. The incident led to the end of Uber's self-driving technology testing in Arizona. The NTSB determined that the probable cause of the accident was the failure of the operator to monitor their surroundings due to distraction by her personal cell phone. Uber faced criticism for inadequate safety procedures and oversight, and the incident raised concerns about the safety of autonomous vehicle technology.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-54175359","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":7,"Time":"13 January 2016","Title":"Google's self-drive cars had to be stopped from crashing","Content":"Google's self-drive cars had to be stopped from crashing\nPublished\n13 January 2016\nShare\nIMAGE SOURCE,\nAP\nImage caption,\nGoogle acknowledges that some of its cars would have crashed if its drivers had not overridden the software\nGoogle drivers had to intervene to stop its self-driving cars from crashing on California's roads 13 times between September 2014 and November 2015.\nThe disclosure follows a local regulator's demand for the information.\nSix other car tech companies also revealed data about autonomous-driving safety incidents of their own.\nGoogle wants to build cars without manual controls, but California-based Consumer Watchdog now says the company's own data undermines its case.\nPrivacy project director John Simpson asked: \"How can Google propose a car with no steering wheel, brakes or driver?\n\"Release of the disengagement report was a positive step, but Google should also make public any video it has of the disengagement incidents, as well as any technical data it collected, so we can fully understand what went wrong as it uses our public roads as its private laboratory.\"\nIMAGE SOURCE,\nAP\nImage caption,\nGoogle has been testing pod-like cars, without steering wheels, on private roads\n'Getting better'\nThe 32-page report says during 15 months of tests of California's public roads:\nGoogle operated its cars in autonomous mode for 424,331 miles (682,895km)\nThere were 272 cases when the cars' own software detected a \"failure\" that caused it to alert the driver and hand over control\nThere were 69 further events when the drivers seized control without being prompted to do so because they perceived there was a safety threat\nComputer simulations carried out after the fact indicated that in 13 of the driver-initiated interventions, there would have been a crash if they had not taken control\nTwo of these cases would have involved hitting a traffic cone\nThe other 11 would have been more serious\nIt adds: \"These events are rare and our engineers carefully study these simulated contacts and refine the software to ensure the self-driving car performs safely.\n\"We are generally driving more autonomous miles between these events.\n\"From April 2015 to November 2015, our cars self-drove more than 230,000 miles without a single such event.\"\nSoftware faults\nMost of the other companies to file \"vehicle disengagement reports\" with the California Department of Motor Vehicles (DMV) provided less detail:\nMedia caption,\nElon Musk hopes we'll summon our cars in the future\nTesla drivers had never had to intervene\nNissan drivers had intervened 106 times in 1,485 miles of tests - to avoid being rear-ended after braking too fast or crashing after braking too slowly\nMercedes-Benz drivers had intervened 1,051 times in 1,739 miles -59 of these had been unprompted, often because they had been \"uncomfortable\" with the software's behaviour\nDelphi drivers had intervened 405 times in 16,662 miles - 28 of these cases had been precautionary, because of nearby pedestrians or cyclists, and 212 had been due to difficulties making out road markings or traffic lights\nVolkswagen drivers had intervened 260 times in 14,945 miles\nBosch drivers had intervened 625 times in 935 miles - but all of these had been \"planned tests\"\n'Safer than humans'\nTowards the end of last year, California's DMV published draft proposals that fully licensed drivers would have to be behind the wheels and pedals of autonomous cars sold to the public.\nBut John Krafcik, the newly appointed president of Google's self-driving car project, said earlier this week that allowing humans to intervene could actually make a crash more likely.\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nJohn Krafcik warned human intervention could make self-driving cars more dangerous\nThe car \"has to shoulder the whole burden\", he said at the Detroit Auto Show.\nBut Mr Krafcik said Google's plans would be influenced by car manufacturers.\n\"No-one goes this alone,\" he said.\n\"We are going to be partnering more and more and more.\"\nOne expert said it could be decades before regulators allowed vehicles be built without manual controls.\n\"For a long period, you will see autonomous vehicles and human-driven cars share the road,\" said Prof David Bailey, from the Aston Business School, in Birmingham.\n\"That makes the situation more complicated, which makes a strong argument for letting people be able to take back control.\n\"From the point of view of people's acceptance and confidence in the technology, that will be needed anyway.\"\nMore on this story\nTesla chief: Apple is making rival car\nPublished\n11 January 2016\nFord looks for self-driving success\nPublished\n5 January 2016\nSelf-driving cars 'must have driver'\nPublished\n16 December 2015\nTesla autopilot 'driving dangerously'\nPublished\n22 October 2015\nRelated Internet Links\nGoogle\nBosch\nNissan\nDelphi\nTesla\nMercedes-Benz\nConsumer Watchdog\nAston Business School\nVolkswagen\nThe BBC is not responsible for the content of external sites.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Google's self-driving cars had to be stopped from crashing on California's roads 13 times between September 2014 and November 2015. These incidents were disclosed following a local regulator's demand for information. During this period, Google operated its cars in autonomous mode for 424,331 miles. There were 272 cases when the cars' software detected a 'failure' that caused it to alert the driver and hand over control, and 69 further events when the drivers seized control without being prompted due to perceived safety threats. Computer simulations indicated that in 13 of these driver-initiated interventions, there would have been a crash if they had not taken control, including two cases involving hitting a traffic cone and 11 more serious incidents. Google aims to build cars without manual controls, but Consumer Watchdog raised concerns about the company's data undermining this goal and called for the release of video and technical data related to these incidents.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-35301279","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":2,"Time":"Updated 5:18 PM EST, Mon December 12, 2022","Title":"Tesla: Our ‘failure’ to make actual self-driving cars ‘is not fraud’","Content":"CNN\n — \nTesla CEO Elon Musk has said numerous times since 2015 that Tesla cars would be entirely self-driving in two years, or less. But years after his self-imposed deadlines have blown by, it still hasn’t happened. Even when equipped with a $15,000 technology package that is literally called “Full Self Driving Capability,” a Tesla car can’t actually drive by itself.\nNow, lawyers for Tesla are arguing that while the company may have failed to live up to these lofty goals, that doesn’t mean it perpetuated a fraud, as alleged in a class-action lawsuit filed in September.\n“Mere failure to realize a long-term, aspirational goal is not fraud,” Tesla’s lawyers wrote in a November 28 court filing, asking that the suit be dismissed.\nThe lawsuit cited numerous times when Musk and others at Tesla had stated that, within a year or two, the cars would be fully self-driving thanks to software updates. For instance, in a 2016 Tweet, Musk stated that a Tesla car would be able to drive itself across the United States “by next year,” the suit said.\nLater that year, Tesla published a video, also cited in the lawsuit, which the automaker said showed one of its cars driving itself. The video was misleading, the lawsuit said, because according to Tesla employees involved in the video, numerous attempts had to be made before the car could be shown to maneuver through the route without obvious problems.\nThe lawsuit, filed by the California firm of Cotchett, Pitre & McCarthy, also cited numerous cases of crashes involving the use of Tesla’s driver assist technology.\nEven among Advanced Driver Assistance Systems, a type of technology now widely available from many automakers, Tesla’s Autopilot system falls behind those now available in global markets from other companies including Mercedes-Benz and Honda, the lawsuit alleges.\nAutopilot is Tesla’s name for it’s driver assistance system that helps keep the car in its lane on a highway. It is supposed to allow it to automatically avoid other cars, though it has had significant issues seeing stopped emergency vehicles on the side of the road. Tesla has said the system is designed to respond to moving vehicles, since reacting to non-moving objects could be dangerous. it can also change lanes while avoiding other cars.\nMany other car companies offer similar systems. The name Autopilot has come under criticism from safety groups and even governments for implying too great a degree of independent control. Tesla and Musk have defended the name because, in airplanes, the autopilot system still requires an active and alert pilot at the controls. A German court ruled in 2020 that Tesla should not be allowed to advertise its driver-assist technology as “Autopilot,” although the complaint was later thrown out, allowing for use of the term.\nFull Self Driving Capability is Tesla’s name for an additional system that, in limited release beta versions, allowed the car to independently handle steering, braking and accelerating in more complex environments such as city streets. According to Tesla’s Website, the feature no longer handles steering on city streets but only the gas and brakes.\nTesla and Musk have said that, with future software improvements that will be delivered “over the air” like smartphone updates, cars with these systems will, one day soon, be able to drive themselves with no involvement from a person.\nThat is not the case yet, though, and Tesla has missed its own predictions about when this would be possible.\nJust failing to meet Musk’s own expectations isn’t evidence that anyone purposely tried to deceive consumers, which would constitute fraud, Tesla said in its filing. Moreover, according to Tesla, buyers should have been well aware of the limitations of these systems before purchasing them, based on Tesla’s disclaimers on its website and owners’ manuals. Also, Tesla said, the plaintiffs didn’t show that, even if Tesla’s systems were involved in crashes, they were involved in more crashes than an unaided human driver would have when operating under the same conditions.\nTesla also listed a number of other more technical reasons the suit should be dismissed including that most of the plaintiffs, who were Tesla owners, had agreed to an arbitration clause when ordering their cars so these claims should not be tried in public courts or in class-action lawsuits. Tesla also argued too much time had elapsed since the cars were purchased and that the plaintiffs hadn’t really been harmed by the fact the cars never drove themselves, despite the system being called “Full Self-Driving” and costing some owners $15,000.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous vehicle failures. Tesla has faced criticism and legal challenges for failing to deliver on its promises of fully self-driving cars. The article discusses Tesla CEO Elon Musk's repeated claims since 2015 that Tesla cars would be entirely self-driving in two years or less, and how these self-imposed deadlines have not been met. It also mentions a class-action lawsuit filed in September alleging fraud due to Tesla's failure to realize these goals. The lawsuit cites instances where Tesla cars were not fully autonomous as advertised and raised concerns about crashes involving Tesla's driver assist technology. Tesla's Autopilot system is criticized for falling behind competitors, and the company's use of the term 'Autopilot' has faced controversy. While Tesla has introduced features like 'Full Self Driving Capability' in limited release beta versions, it acknowledges that the technology is not fully self-driving yet and has missed its own predictions.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2022\/12\/12\/business\/tesla-fsd-autopilot-lawsuit\/index.html","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":6,"Time":"6:55 PM GMT+2•March 27, 2018","Title":"Nvidia suspends all autonomous vehicle testing","Content":"Nvidia is temporarily stopping testing of its autonomous vehicle platform in response to last week’s fatal collision of a self-driving Uber car with a pedestrian. TechCrunch confirmed this with the company, which offered the following statement:\nUltimately [autonomous vehicles] will be far safer than human drivers, so this important work needs to continue. We are temporarily suspending the testing of our self-driving cars on public roads to learn from the Uber incident. Our global fleet of manually driven data collection vehicles continue to operate.\nUpdate: Shortly afterwards, the statement was apparently improved on internally and the following appended (brackets mine, replacing acronyms):\nThe accident was tragic. It’s a reminder of how difficult [self-driving car] technology is and that it needs to be approached with extreme caution and the best safety technologies. This tragedy is exactly why we’ve committed ourselves to perfecting this life-saving technology.\nLikely someone pointed out that it wasn’t particularly charming to respond to a fatal system failure in an autonomous vehicle by saying that “ultimately” they’ll be safer, even if it’s true.\nReuters first reported the news.\nThe manually driven vehicles, to be clear, are not self-driving ones with safety drivers, but traditionally controlled vehicles with a full autonomous sensor suite on them to collect data.\nToyota also suspended its autonomous vehicle testing out of concern for its own drivers’ well-being. Uber of course ceased its testing operations at once.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Nvidia has temporarily suspended testing of its autonomous vehicle platform in response to a fatal collision involving a self-driving Uber car and a pedestrian. The company stated that they are suspending self-driving car testing on public roads to learn from the Uber incident. They acknowledged the tragic nature of the accident and emphasized the need for caution and the development of the best safety technologies in self-driving car technology. Toyota has also suspended its autonomous vehicle testing, and Uber ceased its testing operations as well.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2018\/03\/27\/nvidia-reportedly-suspending-all-autonomous-vehicle-testing\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":10,"Time":"September 14, 2022","Title":"Tesla is sued by drivers over alleged false Autopilot, Full Self-Driving claims","Content":"Sept 14 (Reuters) - Tesla Inc (TSLA.O) was sued on Wednesday in a proposed class action accusing Elon Musk's electric car company of misleading the public by falsely advertising its Autopilot and Full Self-Driving features.\nThe complaint accused Tesla and Musk of having since 2016 deceptively advertised the technology as fully functioning or \"just around the corner\" despite knowing that the technology did not work or was nonexistent, and made vehicles unsafe.\nBriggs Matsko, the named plaintiff, said Tesla did this to \"generate excitement\" about its vehicles, attract investments, boost sales, avoid bankruptcy, drive up its stock price and become a \"dominant player\" in electric vehicles.\n\"Tesla has yet to produce anything even remotely approaching a fully self-driving car,\" Matsko said.\nThe lawsuit filed in federal court in San Francisco seeks unspecified damages for people who since 2016 bought or leased Tesla vehicles with Autopilot, Enhanced Autopilot and Full Self-Driving features.\nAdvertisement · Scroll to continue\nTesla did not immediately respond to requests for comment. It disbanded its media relations department in 2020.\nThe lawsuit followed complaints filed on July 28 by California's Department of Motor Vehicles accusing Tesla of overstating how well its advanced driver assistance systems (ADAS) worked. read more\nRemedies there could include suspending Tesla's license in California, and requiring restitution to drivers.\nAdvertisement · Scroll to continue\nTesla has said Autopilot enables vehicles to steer, accelerate and brake within their lanes, while Full Self-Driving lets vehicles obey traffic signals and change lanes.\nIt has also said both technologies \"require active driver supervision,\" with a \"fully attentive\" driver whose hands are on the wheel, \"and do not make the vehicle autonomous.\"\nMatsko, of Rancho Murieta, California, said he paid a $5,000 premium for his 2018 Tesla Model X to obtain Enhanced Autopilot.\nAdvertisement · Scroll to continue\nHe also said Tesla drivers who receive software updates \"effectively act as untrained test engineers\" and have found \"myriad problems,\" including that vehicles steer into oncoming traffic, run red lights, and fail to make routine turns.\nThe National Highway Traffic Safety Administration has since 2016 opened 38 special investigations of Tesla crashes believed to involve ADAS. Nineteen deaths were reported in those crashes.\nAdvertisement · Scroll to continue\nThe case is Matsko v Tesla Inc et al, U.S. District Court, Northern District of California, No. 22-05240.\nReporting by Jonathan Stempel in New York; editing by Jonathan Oatis\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here is the response in JSON format based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Title\": \"Tesla is sued by drivers over alleged false Autopilot, Full Self-Driving claims\",\n    \"Summary\": \"Tesla Inc was sued in a proposed class action for allegedly misleading the public by falsely advertising its Autopilot and Full Self-Driving features since 2016. The lawsuit seeks unspecified damages for people who bought or leased Tesla vehicles with Autopilot, Enhanced Autopilot, and Full Self-Driving features. It also mentions complaints filed by California's Department of Motor Vehicles regarding Tesla overstating its advanced driver assistance systems' effectiveness. The news highlights that the National Highway Traffic Safety Administration has opened numerous investigations into Tesla crashes involving ADAS, with reported deaths in some of these crashes.\"\n  }\n}\nThe news is relevant to autonomous vehicle failures, specifically regarding allegations of Tesla misleading the public about its Autopilot and Full Self-Driving features, as well as complaints about the effectiveness of its advanced driver assistance systems.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/tesla-is-sued-by-drivers-over-alleged-false-autopilot-full-self-driving-claims-2022-09-14\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":4,"Time":"Thu 22 Mar 2018 20.32 CET","Title":"Uber crash shows 'catastrophic failure' of self-driving technology, experts say","Content":"Video of the first self-driving car crash that killed a pedestrian suggests a “catastrophic failure” by Uber’s technology, according to experts in the field, who said the footage showed the autonomous system erring on one of its most basic functions.\nDays after a self-driving Uber SUV struck a 49-year-old pedestrian while she was crossing the street with her bicycle in Tempe, Arizona, footage released by police revealed that the vehicle was moving in autonomous mode and did not appear to slow down or detect the woman even though she was visible in front of the car prior to the collision. Multiple experts have raised questions about Uber’s Lidar technology, which is the system of lasers that the autonomous cars uses to “see” the world around them.\n“This is exactly the type of situation that Lidar and radar are supposed to pick up,” said David King, an Arizona State University professor and transportation planning expert. “This is a catastrophic failure that happened with Uber’s technology.”\n'Uber should be shut down': friends of self-driving car crash victim seek justice\nRead more\nThe videos of the car hitting Elaine Herzberg also demonstrated that the “safety driver” inside the car did not seem to be monitoring the road, raising concerns about the testing systems Uber and other self-driving car companies have deployed in cities across the US.\n“This safety driver was not doing any safety monitoring,” said Missy Cummings, a Duke University engineering professor who has testified about the dangers of self-driving technology. Research has shown that humans monitoring an automated system are likely to become bored and disengaged, she said, which makes this current phase of semi-autonomous testing particularly dangerous.\n“The problem of complacent safety drivers is going to be a problem for every company.”\nThe footage “strongly suggests a failure by Uber’s automated driving system and a lack of due care by Uber’s driver”, Bryant Walker Smith, a University of South Carolina law school professor and autonomous vehicle expert, said in an email. He noted that the victim is visible about two seconds before the collision, saying: “This is similar to the average reaction time for a driver. That means an alert driver may have at least attempted to swerve or brake.”\nThe car was traveling at 38 miles per hour at 10pm on Sunday, according to the Tempe police chief, Sylvia Moir, who told a reporter that she thought the video showed Uber was not at fault. Experts who reviewed the footage, however, said the opposite appeared to be true.\n“I really don’t understand why Lidar didn’t pick this up,” said Ryan Calo, a University of Washington law professor and self-driving expert. “This video does not absolve Uber.”\nAn Uber self-driving Volvo fitted with ‘Lidar’ technology. Photograph: Uber Handout\/EPA\nEven though the video appeared dark, King said there was likely more visibility than the footage suggested and noted that the darkness should not affect the car’s detection abilities.\n“Shadows don’t matter to Lidar,” added Cummings. “There is no question it should have been able to see her.”\nPolice have emphasized that the victim was not in a crosswalk at the time of the crash, but experts said the technology still should have stopped the vehicle, a Volvo, and King noted that the exact section where Herzberg entered the street is a common area for pedestrians to cross near a local park.\nJohn Simpson, the privacy and technology project director with Consumer Watchdog, said the video revealed a “complete failure” of Uber’s technology and its safety protocols, and said all testing programs on public roads should be suspended while the case is under investigation.\n“Uber appears to be a company that has been rushing and taking shortcuts to get these things on the road,” said Simpson, noting that Arizona leaders lured the corporation to its state with promises of fewer regulations, after Uber fought with California over its vehicles running red lights. “It’s inexcusable.”\nUber, which temporarily suspended testing, declined to comment on the causes of the crash. A spokesperson said in a statement that the video was “disturbing and heartbreaking”, adding: “Our cars remain grounded, and we’re assisting local, state and federal authorities in any way we can.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about an Uber self-driving car crash that experts have described as a 'catastrophic failure' of self-driving technology. The incident occurred in Tempe, Arizona, where a self-driving Uber SUV struck and killed a 49-year-old pedestrian who was crossing the street with her bicycle. The video footage released by police showed that the autonomous vehicle was in autonomous mode and did not appear to slow down or detect the woman before the collision. Experts have raised questions about Uber's Lidar technology, which is designed to 'see' the surroundings. They also expressed concerns about the safety driver inside the car, who seemed to be not monitoring the road. The news includes comments from various experts criticizing Uber's technology and safety protocols, and there are calls to suspend all testing programs on public roads during the investigation.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/mar\/22\/self-driving-car-uber-death-woman-failure-fatal-crash-arizona","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":1,"Time":"Tue 12 Jan 2016 23.00 CET","Title":"Google reports self-driving car mistakes: 272 failures and 13 near misses","Content":"Google’s self-driving cars might not yet have caused a single accident on public roads, but it’s not for want of trying.\nBetween September 2014 and November 2015, Google’s autonomous vehicles in California experienced 272 failures and would have crashed at least 13 times if their human test drivers had not intervened, according to a document filed by Google with the California Department of Motor Vehicles (DMV).\nWhen California started handing out permits for the testing of self-driving cars on public roads, it had just a few conditions. One was that manufacturers record and report every “disengagement”: incidents when a human safety driver had to take control of a vehicle for safety reasons.\nGoogle lobbied hard against the rule. Ron Medford, director of safety for the company’s self-driving car project, wrote at the time: “This data does not provide an effective barometer of vehicle safety. During testing most disengages occur for benign reasons, not to avoid an accident.”\nCrash involving self-driving Google car injures three employees\nRead more\nThe first annual reports were due on 1 January, and Google is the first company to share its data publicly. The figures show that during the 14-month period, 49 Google self-driving cars racked up over 424,000 autonomous miles and suffered 341 disengagements, when either the cars unexpectedly handed control back to their test drivers, or the drivers intervened of their own accord. The reports include both Google’s own prototype “Koala” cars and its fleet of modified Lexus RX450h.\nA Lexus RX450h modified with Google’s autonomous car software and sensors. Photograph: Tony Avelar\/AP\nIn 272 of those disengagements, the car detected a technology failure such as a communications breakdown, a strange sensor reading or a problem in a safety-critical system such as steering or braking.\nGoogle calls these “immediate manual control” disengagements. As the name suggests, the test driver is given audio and visual signals to alert them that they should take over driving without delay. Google test drivers typically responded to these warnings in 0.8 seconds.\nDrivers took over ‘many thousands of times’\nIn the remaining 69 disengagements, the human driver took control of the car on their own initiative, simply by grabbing the steering wheel or pressing the accelerator or brake pedal. The car automatically cedes control when this happens. Drivers do this fairly regularly if they suspect the car is doing (or is about to do) something hazardous or in response to other road users.\nHowever, Google admits that its drivers actually took over from their vehicles “many thousands of times” during the period. The company is reporting only 69 incidents because Google thinks California’s regulations require it only to report disengagements where drivers were justified in taking over, and not those where the car would have coped on its own.\nThe company decides this by replaying each disengagement in its online simulator over and over again. Google says that its powerful software, which now drives over 3m virtual miles each day, can accurately predict the behaviour of other drivers, pedestrians and cyclists and can thus determine whether the test driver’s intervention was required for safety.\nThis isn’t representative of where the technology will be when we’re ready to release it\nChris Urmson, director of Google’s self-driving car program\nBryant Walker Smith, assistant professor in the School of Law at the University of South Carolina, says the DMV could reasonably ask for more information. “Google could be clearer on how it draws the line between those driver-initiated disengagements that it reports and those that it does not,” he says. “The DMV is entitled to interpret its own rule, and it may have questions on this point.”\nConsumer Watchdog, a California-based campaign group, said the report shows that self-driving cars still need a human driver behind the wheel. Privacy project director John M Simpson said: “It’s unfathomable that Google is pushing back against those sensible safety protecting regulations. How can Google propose a car with no steering wheel or brakes when its own tests show that in 15 months the robot technology failed and handed control to the driver 272 times and a driver decided to intervene 69 times?\n“Release of the disengagement report was a positive step, but Google should also release any video it has of the disengagement incidents, as well as any technical data it collected.”\nIn 56 of the 69 driver disengagements reported to the DMV, Google calculated that its car would probably not have come into contact with another object. But, admits Google in its report, “we identified some aspect of the [car]’s behavior that could be a potential cause of contacts in other environments or situations if not addressed. This includes proper perception of traffic lights, yielding properly to pedestrians and cyclists, and violations of traffic laws.”\nGoogle classified the final 13 disengagements as “simulated contacts”: situations that would have resulted in a crash had the human driver not taken over. “In these cases, we believe a human driver could have taken a reasonable action to avoid the contact but the simulation indicated the [car] would not have taken that action,” the company says.\nAll self-driving car firms have filed disengagement reports\nThe report could be seen as a blow to Google’s insistence that self-driving cars should be fully autonomous. It latest prototypes are designed to operate without any driving controls for their human occupants to take over in an emergency (although those currently on public roads do have backup controls fitted).\n“It demonstrates that it is valuable to have a safety driver in the vehicle while testing, which is something we’ve always believed,” said Chris Urmson, director of Google’s self-driving car program. “But if you look at [regular] drivers, they’re effectively untrained in America. Expecting them to vigilantly monitor a system that operates as well as this does is a really a very challenging problem.”\nThe latest software for the Tesla Model S gives it autopilot capability. Photograph: The Washington Post\/Getty Images\nGoogle is not the only company to have filed a disengagement report with the DMV. Volkswagen\/Audi, Mercedes-Benz, Delphi, Tesla, Bosch and Nissan have all filed reports, which are currently under review by the department to confirm that they contain all the required information. The DMV told the Guardian that it does not currently have an expected date to complete its analysis of the data or draw conclusions from it.\nWhile Google has been testing its self-driving cars since 2008, the company will not be releasing disengagement data from before 2014. “This is the period we’re required to share with the DMV. Any data we would have from before that is just outdated,” Urmson says.\nGoogle notes that disengagements have been getting less common over the period of the report. However, Urmson cautions against expecting disengagements to drop regularly, year on year. “We’re continually adding capabilities to our vehicles, pushing them into more challenging situations,” he says. “Over the long view, we’d expect disengagements to be improving, but as we test in more challenging weather or driving situations, you could expect locally this to not look as good. And it really isn’t representative of where the technology will be when we’re ready to release it.”\nGoogle’s parent company, Alphabet, is reported to be planning to spin out its self-driving car technology into its own business later this year.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Google reports self-driving car mistakes: Between September 2014 and November 2015, Google’s autonomous vehicles in California experienced 272 failures and would have crashed at least 13 times if their human test drivers had not intervened. These failures include incidents where the car detected technology failures, communication breakdowns, sensor issues, and problems in safety-critical systems. Additionally, the report mentions 341 disengagements when the cars unexpectedly handed control back to their test drivers or when drivers intervened on their own. Google also acknowledges that its drivers took over 'many thousands of times' during this period. The report highlights instances where the car's behavior could have led to potential accidents and simulations of situations that would have resulted in crashes if the human driver had not taken control. The report suggests that self-driving cars may still require human drivers behind the wheel, and Google's latest prototypes are designed to operate without any driving controls for humans to take over in emergencies.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/jan\/12\/google-self-driving-cars-mistakes-data-reports","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":13,"Time":"JANUARY 12, 2016","Title":"Google says fewer tech glitches in its self-driving cars","Content":"By Alexandria Sage\nDETROIT (Reuters) - Google said on Tuesday the rate of software failures in its self-driving cars was declining as the technology was learning from its mistakes, bringing the tech company closer to its goal of fully autonomous vehicles.\nIn 424,000 miles of autonomous driving, Google said its cars had experienced 272 episodes in which the human test driver had to assume control of the vehicle, an occurrence called “disengagement”, when the autonomous technology failed.\n“As we continue to develop and refine the self-driving software, we are seeing fewer disengagements” despite more miles driven, Google said in a 33-page report submitted by law to the California Department of Motor Vehicles on Dec. 31.\nCalifornia regulations call for test drivers and steering wheels in autonomous vehicles.\nGoogle’s development of self-driving cars has spurred outsized interest around the world, but the company best known for search has disclosed little about its strategy, business plans or ultimate goals.\nWithout the need to watch the road, people in cars will talk on phones, watch entertainment, purchase consumer goods, among many other options, providing valuable data to Google.\nThe report, covering the period Sept. 24, 2014, when Google began testing on roads of Palo Alto, California, to Nov. 30, 2015, found disengagements occurred about every 785 miles in the fourth quarter of 2014. A year later, that had expanded to 5,318 miles between episodes.\nEighty-nine percent of disengagements occurred on city streets, where more obstacles and stop-and-go traffic make autonomous driving more difficult.\nProject director Chris Urmson said Google deliberately tests cars in different weather and times of the day, which explained why some months saw more episodes than others.\nGoogle, a unit of Alphabet Inc, said it kept the threshold for measuring disengagements low to gather as much data as possible to refine the technology.\nThere were another 69 episodes in which the test driver chose to take control of the vehicle rather than the car signaling to the driver to take control.\nUsing a simulator to replay the situation, Google found in 13 of these instances its cars would have hit another object had the test driver not taken control. Google said two involved traffic cones, and three were due to “another driver’s reckless behavior.”\nUrmson said the California DMV had not seen the report when it issued draft rules in December restricting how autonomous vehicles could operate for the next three years.\nReporting By Alexandria Sage; Editing by Meredith Mazzilli\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Google has reported that the rate of software failures in its self-driving cars is declining as the technology learns from its mistakes. In 424,000 miles of autonomous driving, Google stated that its cars experienced 272 episodes of 'disengagement,' where the human test driver had to take control due to autonomous technology failures. The report covers the period from September 24, 2014, to November 30, 2015, and it found that disengagements occurred about every 785 miles in the fourth quarter of 2014 but expanded to 5,318 miles between episodes a year later. Most of these disengagements (89%) happened on city streets, where obstacles and stop-and-go traffic make autonomous driving more challenging. Google also mentioned instances where the test driver had to take control to prevent potential collisions with objects like traffic cones or due to 'another driver's reckless behavior.'\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autoshow-detroit-google-idINKCN0UQ2LQ20160112","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":9,"Time":"Thu 22 Dec 2022 15.06 CET","Title":"Tesla behind eight-vehicle crash was in ‘full self-driving’ mode, says driver","Content":"The driver of a 2021 Tesla Model S told California authorities the vehicle was in “full self-driving mode” when the technology malfunctioned, causing an eight-vehicle crash on the San Francisco Bay bridge last month.\nThe crash on Thanksgiving Day resulted in two juveniles being transported to hospital and led to lengthy delays on the bridge. The incident was made public in a police report on Wednesday.\n‘Rude drivers will swerve in my lane’: are Tesla owners paying the price for Musk hate?\nRead more\nIt is the latest in a series of accidents blamed on Tesla technology. The electric automaker’s chief executive, Elon Musk, has heavily promoted “Full Self-Driving” (FSD) software, sold as $15,000 add-on to Tesla vehicles, but it faces legal, regulatory and public scrutiny.\nAfter the San Francisco accident, the driver told police the FSD software malfunctioned.\nThe police report said the vehicle was traveling at 55mph when it shifted lane but braked abruptly, slowing the car to about 20mph. That led to another vehicle hitting the Tesla and a chain reaction of crashes, according to Reuters.\nHowever, police were unable to determine if the software was in operation or that the driver’s account was accurate. The report was made public after a records request.\nThe crash occurred hours after Musk said Tesla would make FSD software available to anyone in North America who requested it. It previously offered the system only to drivers with high safety scores.\nThe police report said that if FSD malfunctioned, the driver should have manually taken control. Tesla has repeatedly said its advanced self-driving technology requires “active driver supervision” and its vehicles “are not autonomous”.\nDrivers are also warned when they install FSD that it “may do the wrong thing at the worst time”.\nThe National Highway Traffic Safety Administration (NHTSA), which is investigating Tesla after reports of braking “without warning, at random, and often repeatedly in a single drive”, did not immediately comment on the San Francisco crash.\nLast summer, NHTSA upgraded the investigation to what it calls an engineering analysis. The chair of the National Transportation Safety Board, Jennifer Homendy, has questioned if “full self-driving” is an accurate description of the technology – and said Tesla must do more to prevent misuse.","Relevant":true,"Overview":"Here is the JSON response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is about an eight-vehicle crash on the San Francisco Bay bridge involving a 2021 Tesla Model S that was reportedly in 'full self-driving mode.' The driver stated that the technology malfunctioned, leading to the crash. The incident resulted in two juveniles being transported to the hospital and caused significant bridge delays. However, it's noted that police were unable to determine if the self-driving software was in operation or if the driver's account was accurate. The National Highway Traffic Safety Administration (NHTSA) is investigating Tesla for reports of braking issues, but they did not comment on this specific crash. The news also highlights concerns about the accuracy of the 'full self-driving' technology and the need for better prevention of misuse.\"\n}\nThe news is indeed about an autonomous vehicle-related incident, specifically a Tesla in 'full self-driving mode' malfunctioning and causing an eight-vehicle crash on the San Francisco Bay bridge.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2022\/dec\/22\/tesla-crash-full-self-driving-mode-san-francisco","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":14,"Time":"8:49 PM GMT+2•October 26, 2022","Title":"Ford, VW-backed Argo AI is shutting down","Content":"cameraImage Credits: Ford\nArgo AI, an autonomous vehicle startup that burst on the scene in 2017 stacked with a $1 billion investment, is shutting down — its parts being absorbed into its two main backers: Ford and VW, according to people familiar with the matter.\nDuring an all-hands meeting Wednesday, Argo AI employees were told that some people would receive offers from the two automakers, according to multiple sources who asked to not be named. It was unclear how many would be hired into Ford or VW and which companies will get Argo’s technology.\nEmployees were told they would receive a severance package that includes insurance and two separate bonuses — an annual award plus a transaction bonus upon the deal close with Ford and VW. All Argo employees will receive these. For those who are not retained by Ford or VW, they will additionally receive termination and severance pay, including health insurance. Several people told TechCrunch that it was a generous package and that the founders of the company spoke directly to its more than 2,000 employees.\n“In coordination with our shareholders, the decision has been made that Argo AI will not continue on its mission as a company. Many of the employees will receive an opportunity to continue work on automated driving technology with either Ford or Volkswagen, while employment for others will unfortunately come to an end,” Argo said in a statement.\nFord said in its third-quarter earnings report released Wednesday that it made a strategic decision to shift its resources to developing advanced driver assistance systems, and not autonomous vehicle technology that can be applied to robotaxis. The company said it recorded a $2.7 billion non-cash, pretax impairment on its investment in Argo AI, resulting in an $827 million net loss for the third quarter.\nThat decision appears to have been fueled by Argo’s inability to attract new investors. Ford CEO Jim Farley acknowledged that the company anticipated being able to bring autonomous vehicle technology broadly to market by 2021.\n“But things have changed, and there’s a huge opportunity right now for Ford to give time — the most valuable commodity in modern life — back to millions of customers while they’re in their vehicles,” said Farley. “It’s mission-critical for Ford to develop great and differentiated L2+ and L3 applications that at the same time make transportation even safer.”\nFarley also insinuated that Ford would be able to buy AV tech down the line, instead of developing it in house. “We’re optimistic about a future for L4 ADAS, but profitable, fully autonomous vehicles at scale are a long way off and we won’t necessarily have to create that technology ourselves,” he added.\nFord also stated that the “development and customer enthusiasm for benefits of L2+ and L3 ADAS warrant dialing up the company’s near-term aspirations and commitment in those areas.”\nVW, Argo’s other primary backer, has also indicated plans to shift resources and will no longer invest in Argo AI. The company said it will use its software unit Cariad to drive forward development of highly automated and autonomous driving together with Bosch and, in the future, in China with Horizon Robotics.\nWhile a lesser player, Lyft had also taken a 2.5% stake into Argo. Lyft announced earlier this year plans to launch at least 1,000 self-driving vehicles on its ride-hailing network in a number of cities over the next five years, starting with Miami and Austin.\nThe ride-hailing company said in a statement that Argo has been a great partner and that this development does not impact Lyft’s autonomous strategy.\n“We will continue working with our other partners to advance the safety and commercialization of AV technology,” a Lyft spokesperson said in an emailed statement. “Lyft is the current leader in paid AV rides in North America with over 100,000 rides completed. We’re well positioned to win the AV transition through the combination of our hybrid network, marketplace engine and fleet management capabilities.”\nLyft also has partnerships with AV companies Motional and Waymo.\nArgo was founded in 2016 by Bryan Salesky and Pete Rander. The company came out of stealth in February 2017 when Ford announced it would invest $1 billion over five years into Argo. Since then, the company has raised more than $2.6 billion, primarily from Ford and VW, in a pursuit to develop, test and eventually commercialize its automated driving system.\nThe initial Ford investment came at a particularly hype-y time for the nascent autonomous vehicle industry. Startups, many founded by early pioneers of Google’s self-driving project, were landing eye-popping venture capital deals. A string of acquisitions followed: GM bought Cruise for $1 billion in 2016; Delphi, which is now Aptiv, acquired nuTonomy for $450 million; and Amazon bought Zoox.\nThe promises around commercializing AV technology have proven more difficult than expected. A wave of consolidation washed over the industry with companies folding, being absorbed into other companies, including Apple. Others, turned to the public market either through a traditional IPO like TuSimple, or by merging with a special purpose acquisition company as Aurora did in hopes of gaining the capital it needs to continue its mission.\nArgo seemed to be gaining ground in the past year. The company’s self-driving Ford Fusion vehicles, and now Ford Escape Hybrids, were frequently seen testing on public roads in Austin, Detroit, Miami, Palo Alto and Pittsburgh, where it is headquartered. In the EU, Argo was using the all-electric Volkswagen ID. Buzz for its testing programs in Hamburg and Munich. Argo also has several pilot programs underway in Austin, Miami and Pittsburgh with Lyft, Walmart and 412 Food Rescue.\nJust last month the company revealed an ecosystem of products and services designed to support commercial delivery and robotaxi operations. The products — a list that includes fleet management software, data analytics, high-definition mapping and cloud-based communication tools — stretches far beyond the self-driving system that allows a vehicle to navigate city streets without a human driver behind the wheel. Argo appeared to be telling the world it was open for business.\n“We are incredibly grateful for the dedication of the Argo AI team, and so proud of our achievements together,” Salesky and Rander said in a statement. “The team consistently delivered above and beyond, and we expect to see success for everyone in whatever comes next, including the opportunities presented by Ford and VW to continue their work on automated driving technology.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Argo AI, an autonomous vehicle startup backed by Ford and VW, is shutting down, with its parts being absorbed by its main backers. Argo AI employees will receive offers from Ford and VW, and details about how many will be hired and which companies will get Argo's technology are unclear. Employees will receive a severance package that includes insurance and bonuses. Ford recorded a $2.7 billion impairment on its investment in Argo AI, resulting in a net loss for the third quarter. Ford is shifting its resources to advanced driver assistance systems. VW also plans to shift resources and will no longer invest in Argo AI.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/10\/26\/ford-vw-backed-argo-ai-is-shutting-down\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":12,"Time":"1:30 AM GMT+2•May 25, 2022","Title":"Pony.ai loses permit to test autonomous vehicles with driver in California","Content":"cameraImage Credits: Pony.ai\nThe California Department of Motor Vehicles revoked Pony.ai’s permit to test its autonomous vehicle technology with a driver on Tuesday for failing to monitor the driving records of the safety drivers on its testing permit.\n“While reviewing Pony.ai’s application to renew the testing permit, the DMV found numerous violations on the driving records of active Pony.ai safety drivers,” a spokesperson for the DMV told TechCrunch, noting that Pony had 41 autonomous test vehicles and 71 safety drivers on its permit.\nPony confirmed the revocation of its permit to TechCrunch, saying the DMV took issue with the driving records of three of its safety operators.\n“Because of the critical role of safety drivers to facilitate the safe testing of autonomous technology and the need for these drivers to have a clean driving record as established by the DMV’s autonomous vehicle regulations, the DMV is revoking the permit, effective immediately.”\nThe news follows the suspension of Pony’s driverless permit, which allows it to test its AV tech without a human safety operator in the front seat, in November 2021 after a reported collision in Fremont, California. At the time, the National Highway Traffic and Safety Administration suspected a software default, so in March, Pony recalled three vehicles with similar potential software issues.\nThe California DMV did not respond in time to requests for more information about the status of Pony’s suspended driverless testing permit, but if the arguably safer version of that permit — the one that requires a driver to be present while testing — has been fully revoked, it’s unlikely the DMV will reinstate the startup’s driverless permit any time soon.\n“When there is an unreasonable risk to public safety, the DMV can immediately suspend or revoke permits,” the agency told TechCrunch.\nPony responded to TechCrunch on Wednesday, saying it is proud of its safety record, and that in the over 6.8 million real-world autonomous miles, no injuries have occurred.\n“We are in the process of reviewing the DMV’s notice,” a spokesperson for Pony told TechCrunch.\nPony currently does not hold any active testing permits in California.\nMeanwhile, in China, where most of Pony’s operations reside, the company just received a permit to provide driverless ride-hailing services to the public on open roads in Beijing. The company also recently scored a taxi license to operate a commercial service in Guangzhou.\nThis article has been updated to reflect Pony’s statement, as well as information regarding the number of safety operators with questionable driving records. ","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Pony.ai has lost its permit to test autonomous vehicles with drivers in California. The California Department of Motor Vehicles (DMV) revoked the permit due to violations on the driving records of active Pony.ai safety drivers. The DMV noted that Pony.ai had 41 autonomous test vehicles and 71 safety drivers on its permit. The revocation was immediate and attributed to the critical role of safety drivers in ensuring the safe testing of autonomous technology and the requirement for clean driving records as per DMV's regulations. This follows the suspension of Pony's driverless permit in November 2021 after a reported collision, which raised concerns about software defaults. While the DMV has not provided information about the status of the suspended driverless testing permit, the revocation of the permit requiring a driver to be present while testing makes it unlikely for the driverless permit to be reinstated anytime soon. The DMV can suspend or revoke permits in cases of unreasonable risk to public safety. Pony.ai has stated that it is reviewing the DMV's notice, and currently, the company does not hold any active testing permits in California. In contrast, in China, where most of Pony's operations are located, the company received a permit for driverless ride-hailing services on open roads in Beijing and obtained a taxi license to operate a commercial service in Guangzhou.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/05\/24\/pony-ai-loses-permit-to-test-autonomous-vehicles-with-driver-in-california\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":11,"Time":"Fri 1 Jul 2016 01.14 CEST","Title":"Tesla driver dies in first fatal crash while using autopilot mode","Content":"The first known death caused by a self-driving car was disclosed by Tesla Motors on Thursday, a development that is sure to cause consumers to second-guess the trust they put in the booming autonomous vehicle industry.\nThe 7 May accident occurred in Williston, Florida, after the driver, Joshua Brown, 40, of Ohio put his Model S into Tesla’s autopilot mode, which is able to control the car during highway driving.\nWhat happens when Tesla’s AutoPilot goes wrong: owners post swerving videos\nRead more\nAgainst a bright spring sky, the car’s sensors system failed to distinguish a large white 18-wheel truck and trailer crossing the highway, Tesla said. The car attempted to drive full speed under the trailer, “with the bottom of the trailer impacting the windshield of the Model S”, Tesla said in a blogpost.\nA police report in the Levy County Journal said the top of the vehicle “was torn off by the force of the collision”. The truck driver, Frank Baressi, 62, of Tampa, Florida, was uninjured, the Journal reported.\nAmerica’s National Highway Traffic Safety Administration (NHTSA) has opened an inquiry into the accident.\nBrown owned a technology company called Nexu Innovations and was a Tesla enthusiast who posted videos of his car on autopilot on YouTube. One video showed his car avoiding a collision on a highway, racking up 1m views after it was tweeted by Tesla CEO Elon Musk.\nOwner video of Autopilot steering to avoid collision with a truckhttps:\/\/t.co\/FZUAXSjlR7\n— Elon Musk (@elonmusk) April 17, 2016\nIn its 537-word statement on the incident, the electric vehicle company repeatedly went out of its way to shift blame for the accident. The first paragraph notes that this was Tesla’s first known autopilot death in some 130 million miles driven by its customers. “Among all vehicles in the US, there is a fatality every 94 million miles,” the company then notes.\nIt goes on to say that the car’s autonomous software is designed to nudge consumers to keep their hands on the wheels to make sure they’re paying attention. “Autopilot is getting better all the time, but it is not perfect and still requires the driver to remain alert,” the company said.\nAt the end of its blogpost, Tesla stated: “The customer who died in this crash had a loving family and we are beyond saddened by their loss,” the company said. “He was a friend to Tesla and the broader [electric vehicle] community, a person who spent his life focused on innovation and the promise of technology and who believed strongly in Tesla’s mission.”\nOur condolences for the tragic loss https:\/\/t.co\/zI2100zEGL\n— Elon Musk (@elonmusk) June 30, 2016\n“Preliminary reports indicate the vehicle crash occurred when a tractor-trailer made a left turn in front of the Tesla at an intersection on a non-controlled access highway. The driver of the Tesla died due to injuries sustained in the crash,” the NHTSA said.\nThe car continued to travel after passing under the truck’s trailer, veered off the road, and then crashed through two fences and into a power pole, the local police report said. Brown died at the scene.\nThe accident comes at a time when Americans have just started to become more comfortable with letting machines take the wheel. Tesla has generated enormous fanfare with its autopilot mode and inspired consumers – despite the company’s warnings – to see just how much they can do while letting the car drive.\nA popular video posted online appeared to show a man taking a nap as his car navigated busy traffic. One member of a Tesla owner message board recently quipped that he uses his free hands to play air guitar to Billy Idol songs. Another claimed that during a road trip, he noticed Tesla’s autopilot camera struggling to pick out lines during bright sunlight in the morning or at dusk.\nOther carmakers have said they will be releasing autonomous vehicles in the near future. Within a year, General Motors plans to test self-driving taxis with the ride-hailing app Lyft. One startup backed by Google is testing completely autonomous trucks in Nevada that will let drivers rest as they complete cross-country shipping routes.\nMusk recently hinted that the company plans to include its autopilot feature in its upcoming Model 3, which at $35,000 is being marketed to middle-class drivers.\nStatistically, self-driving cars are about to kill someone. What happens next?\nRead more\nThe appeal of the technology is obvious. And Tesla isn’t wrong when it says that statistically, vehicles driving themselves have a much better safety record than ones driven by humans.\nBut the Tesla accident offers the first test of how consumers will react when reminded that they are putting their lives in the hands of computer code when they turn over control of the wheel.\nBrown, for his part, appeared to believe that he was safe when letting go of the wheel. In one of the YouTube videos he posted of his Model S driving itself in slow-moving traffic, he said, “It takes all the stress out of it.”\n“You get to your destination slightly slower but at least now you don’t have to worry about anything. Just let it go.”","Relevant":true,"Overview":"Here is the response in the requested JSON format:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"A fatal crash involving a Tesla Model S occurred on May 7th, in Williston, Florida. The driver, Joshua Brown, 40, of Ohio, was using Tesla's autopilot mode at the time of the accident. The car's sensors failed to detect a large white 18-wheel truck and trailer crossing the highway. As a result, the car attempted to drive full speed under the trailer, impacting the windshield and causing the top of the vehicle to be torn off. Joshua Brown died at the scene. The truck driver, Frank Baressi, 62, of Tampa, Florida, was uninjured. The National Highway Traffic Safety Administration (NHTSA) initiated an inquiry into the accident. Tesla's statement acknowledged this as its first known autopilot-related death among its customers, highlighting that the system is not perfect and still requires driver alertness. This incident raised concerns about the safety of autonomous vehicles.\"\n}\nThe news is indeed about an autonomous vehicle failure, specifically involving a Tesla Model S in autopilot mode, resulting in a fatal crash.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/jun\/30\/tesla-autopilot-death-self-driving-car-elon-musk","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":3,"Time":"Sun 27 Mar 2022 15.00 CEST","Title":"How self-driving cars got stuck in the slow lane","Content":"“I\nwould be shocked if we do not achieve full self-driving safer than a human this year,” said Tesla chief executive, Elon Musk, in January. For anyone who follows Musk’s commentary, this might sound familiar. In 2020, he promised autonomous cars the same year, saying: “There are no fundamental challenges.” In 2019, he promised Teslas would be able to drive themselves by 2020 – converting into a fleet of 1m “robotaxis”. He has made similar predictions every year going back to 2014.\nFrom late 2020, Tesla expanded beta trials of its “Full Self-Driving” software (FSD) to about 60,000 Tesla owners, who must pass a safety test and pay $12,000 for the privilege. The customers will pilot the automated driver assistance technology, helping to refine it before a general release.\nWith the beta rollout, Tesla is following the playbook of software companies, “where the idea is you get people to iron out the kinks”, says Andrew Maynard, director of the Arizona State University risk innovation lab. “The difficulty being that when software crashes, you just reboot the computer. When a car crashes, it’s a little bit more serious.”\nPlacing fledgling technology into untrained testers’ hands is an unorthodox approach for the autonomous vehicle (AV) industry. Other companies, such as Alphabet-owned Waymo, General Motors-backed Cruise and AV startup Aurora, use safety operators to test technology on predetermined routes. While the move has bolstered Tesla’s populist credentials with fans, it has proved reputationally risky. Since putting its tech into the hands of the people, a stream of videos documenting reckless-looking FSD behaviour has racked up numerous views online.\nThere’s the video of a car in FSD mode veering sharply into oncoming traffic, prompting the driver to swerve off the road into a field. The one that shows a car repeatedly attempting to turn on to train tracks and into pedestrians. Another that captures the driver struggling to regain control of the car after the system prompts him to take over. What would appear to be the first crash involving FSD was reported to the US National Highway Traffic Safety Administration (NHTSA) in November last year; no one was injured, but the vehicle was “severely damaged”.\nTesla boss Elon Musk has promised the arrival of self-driving cars several times over the years. Photograph: Stephen Lam\/Reuters\nFSD is proficient at driving on motorways, where it’s “straightforward, literally”, says Taylor Ogan, a Tesla FSD owner and chief executive of Snow Bull Capital. On more complex, inner-city streets, he says the system is more unpredictable. Continuous software updates are supposed to iron out glitches. For example, the NHTSA forced Tesla to prevent the system from executing illegal “rolling stops” (moving slowly through a stop sign without ever coming to a full stop, while an “unexpected braking” problem is the subject of a current inquiry. In Ogan’s experience of trialling FSD though, “I haven’t even seen it get better. It just does crazier things more confidently.”\nMaynard says the “learner driver” metaphor holds for some of FSD’s issues, but falls apart when the technology engages in indisputably non-human behaviour. For example, a lack of regard for getting dangerously close to pedestrians and the time a Tesla ploughed into a bollard that FSD failed to register. Similar problems have emerged with Tesla’s Autopilot software, which has been implicated in at least 12 accidents (with one death and 17 injuries) owing to the cars being unable to “see” parked emergency vehicles.\nThere’s reason to believe that the videos that make their way online are some of the more flattering ones. Not only are the testers Tesla customers, but an army of super-fans acts as an extra deterrent to sharing anything negative. Any reports of FSD behaving badly can trigger a wave of outrage; any critical posts on the Tesla Motors Club, a forum for Tesla drivers, are inevitably greeted by people blaming users for accidents or accusing them of wanting Tesla to fail. “People are terrified that Elon Musk will take away the FSD that they paid for and that people will attack them,” says Ogan.\nThis helps to shield Tesla from criticism, says Ed Niedermeyer, the author of Ludicrous: The Unvarnished Story of Tesla Motors, who was “bombarded by an online militia” when he started reporting on the company. “Throughout Tesla’s history, this faith and sense of community… has been absolutely critical to Tesla’s survival,” he says. The proof, he adds, is that Musk can claim again and again to be a year from reaching full autonomous driving without losing the trust of fans.\nB\nut it’s not just Tesla that has missed self-imposed autonomous driving deadlines. Cruise, Waymo, Toyota and Honda all said they would launch fully self-driving cars by 2020. Progress has been made, but not on the scale anticipated. What happened?\n“Number one is that this stuff is harder than manufacturers realised,” says Matthew Avery, director of research at Thatcham Research. While about 80% of self-driving is relatively simple – making the car follow the line of the road, stick to a certain side, avoid crashing – the next 10% involves more difficult situations such as roundabouts and complex junctions. “The last 10% is really difficult,” says Avery. “That’s when you’ve got, you know, a cow standing in the middle of the road that doesn’t want to move.”\nIt’s the last 20% that the AV industry is stuck on, especially the final 10%, which covers the devilish problem of “edge cases”. These are rare and unusual events that occur on the road such as a ball bouncing across the street followed by a running child; complicated roadworks that require the car to mount the kerb to get past; a group of protesters wielding signs. Or that obstinate cow.\nSelf-driving cars rely on a combination of basic coded rules such as “always stop at a red light” and machine-learning software. The machine-learning algorithms imbibe masses of data in order to “learn” to drive proficiently. Because edge cases only rarely appear in such data, the car doesn’t learn how to respond appropriately.\nAn Uber self-driving car at its Pittsburgh technology centre in 2016. Photograph: Angelo Merendino\/Getty\nThe thing about edge cases is that they are not all that rare. “They might be infrequent for an individual driver, [but] if you average out over all the drivers in the world, these kinds of edge cases are happening very frequently to somebody,” says Melanie Mitchell, computer scientist and professor of complexity at the Santa Fe Institute.\nWhile humans are able to generalise from one scenario to the next, if a self-driving system appears to “master” a certain situation, it doesn’t necessarily mean it will be able to replicate this under slightly different circumstances. It’s a problem that so far has no answer. “It’s a challenge to try to give AI systems common sense, because we don’t even know how it works in ourselves,” says Mitchell.\nMusk himself has alluded to this: “A major part of real-world AI has to be solved to make unsupervised, generalised full self-driving work,” he tweeted in 2019. Failing a breakthrough in AI, autonomous vehicles that function on a par with humans probably won’t be coming to market just yet. Other AV makers use high-definition maps – charting the lines of roads and pavements, placement of traffic signs and speed limits – to partly get around this problem. But these maps need to be constantly refreshed to keep up with ever-changing conditions on roads and, even then, unpredictability remains.\nThe edge-case problem is compounded by AV technology that acts “supremely confidently” when it’s wrong, says Philip Koopman, associate professor of electrical and computer engineering at Carnegie Mellon University. “It’s really bad at knowing when it doesn’t know.” The perils of this are evident in analysing the Uber crash in which a prototype AV killed Elaine Herzberg as she walked her bicycle across a road in Arizona, in 2018. An interview with the safety operator behind the wheel at the time describes the software flipping between different classifications of Herzberg’s form – “vehicle”, “bicycle”, “other” – until 0.2 seconds before the crash.\nT\nhe ultimate aim of AV makers is to create cars that are safer than human-driven vehicles. In the US, there is about one death for every 100m miles driven by a human (including drunk driving). Koopman says AV makers would have to beat this to prove their technology was safer than a human. But he also believes somewhat comparable metrics used by the industry, such as disengagement data (how often a human needs to take control to prevent an accident), elide the most important issues in AV safety.\n“Safety isn’t about working right most of the time. Safety is all about the rare case where it doesn’t work properly,” says Koopman. “It has to work 99.999999999% of the time. AV companies are still working on the first few nines, with a bunch more nines to go. For every nine, it’s 10 times harder to achieve.”\nSome experts believe AV makers won’t have to completely crack human-level intelligence to roll out self-driving vehicles. “I think if every car was a self-driving car, and the roads were all mapped perfectly, and there were no pedestrians around, then self-driving cars would be very reliable and trustworthy,” says Mitchell. “It’s just that there’s this whole ecosystem of humans and other cars driven by humans that AI just doesn’t have the intelligence yet to deal with.”\nCruise Origin founder Kyle Vogt at the company’s launch. Photograph: Stephen Lam\/Reuters\nUnder the right conditions, such as quiet roads and favourable weather, self-driving cars can mostly function well. This is how Waymo is able to run a limited robotaxi service in parts of Phoenix, Arizona. However, this fleet has still been involved in minor accidents and one vehicle was repeatedly stumped by a set of traffic cones despite a remote worker providing assistance. (A Waymo executive claimed they were not aware of these incidents happening more than with a human driver.)\nDespite the challenges, the AV industry is speeding ahead. The Uber crash had a temporarily sobering effect; manufacturers suspended trials afterwards owing to negative press and Arizona’s governor suspended Uber’s testing permit. Uber and another ride-hailing company, Lyft, both then sold their self-driving divisions.\nBut this year has marked a return to hubris – with more than $100bn invested in the past 10 years, the industry can hardly afford to shirk. Carmakers General Motors and Geely and AV company Mobileye have said people may be able to buy self-driving cars as early as 2024. Cruise and Waymo both aim to launch commercial robotaxi operations in San Francisco this year. Aurora also plans to deploy fully autonomous vehicles in the US within the next two to three years.\nS\nome safety experts are concerned by the lack of regulation governing this bold next step. At present, every company “basically gets one free crash”, says Koopman, adding that the regulatory system in the US is predicated on trust in the AV maker until a serious accident occurs. He points to Uber and AV startup Pony.ai, whose driverless test permit was recently suspended in California after a serious collision involving one of its vehicles.\nA side-effect of Tesla sharing its technology with customers is that regulators are taking notice. Tesla has so far avoided the more stringent requirements of other AV makers, such as reporting crashes and systems failures and using trained safety professionals as testers, because of the claim that its systems are more basic. But California’s Department of Motor Vehicles, the state’s autonomous driving regulator, is considering changing the system, in part because of the dangerous-looking videos of the technology in action, as well as investigations into Tesla by the NHTSA.\nThe dearth of regulation so far highlights the lack of global consensus in this space. The question, says Maynard, is “is the software going to mature fast enough that it gets to the point where it’s both trusted and regulators give it the green light, before something really bad happens and pulls the rug out from the whole enterprise?”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n\"The news discusses the challenges and failures in the development of self-driving cars, particularly focusing on Tesla's Full Self-Driving (FSD) software.\",\n\"Elon Musk's repeated promises of achieving full self-driving capability and the expansion of FSD beta trials to Tesla owners are mentioned.\",\n\"Several incidents involving FSD behavior are described, including a car veering into oncoming traffic, attempting to turn onto train tracks, and struggling for control after system prompts.\",\n\"A crash involving FSD was reported to the US National Highway Traffic Safety Administration (NHTSA) in November.\",\n\"The article highlights the difficulty of testing autonomous technology with untrained users and contrasts Tesla's approach with other companies like Waymo, Cruise, and Aurora, which use safety operators and predetermined routes.\",\n\"The article mentions that Tesla enthusiasts and super-fans often discourage negative reports about FSD behavior.\",\n\"The challenge of dealing with 'edge cases'—rare and unusual events on the road that self-driving systems find difficult to handle—is discussed. These edge cases include unpredictable scenarios like animals on the road, roadworks, and protests.\",\n\"The limitations of machine learning in handling edge cases are mentioned, as self-driving systems struggle to generalize from limited data.\",\n\"The need for autonomous vehicles to be significantly safer than human-driven vehicles, especially in rare cases, is emphasized.\",\n\"Despite the challenges, the autonomous vehicle industry continues to advance with significant investments and plans for commercial deployments.\",\n\"Concerns are raised about the lack of regulation governing the self-driving industry, and the article mentions that every company currently gets 'one free crash' before regulations may be imposed.\",\n\"Regulatory scrutiny is increasing, with California's Department of Motor Vehicles considering changes due to dangerous-looking videos of Tesla's technology in action and investigations by the NHTSA.\"\n]\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2022\/mar\/27\/how-self-driving-cars-got-stuck-in-the-slow-lane","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":15,"Time":"Thu 16 Feb 2023 20.41 CET","Title":"Tesla recalls 362,000 vehicles over self-driving software flaws that risk crashes","Content":"Tesla said it would recall 362,000 US vehicles to update its Full Self-Driving (FSD) Beta software after regulators said on Thursday the driver assistance system did not adequately adhere to traffic safety laws and could cause crashes.\nThe National Highway Traffic Safety Administration (NHTSA) said the Tesla software allows a vehicle to “exceed speed limits or travel through intersections in an unlawful or unpredictable manner increases the risk of a crash”.\nTesla will release an over-the-air (OTA) software update free of charge, and the electric vehicle maker said is not aware of any injuries or deaths that may be related to the recall issue. The automaker said it had 18 warranty claims.\nTesla shares were down 1.6% at $210.76 on Thursday afternoon.\nThe recall covers 2016-2023 Model S, Model X, 2017-2023 Model 3, and 2020-2023 Model Y vehicles equipped with FSD Beta software or pending installation.\nNHTSA asked Tesla to recall the vehicles, but the company said despite the recall it did not concur with NHTSA’s analysis. The move is a rare intervention by federal regulators in a real-world testing program that the company sees as crucial to the development of cars that can drive themselves. FSD Beta is used by hundreds of thousands of Tesla customers.\nThe setback for Tesla’s automated driving effort comes about two weeks before the company’s 1 March investor day, during which its chief executive Elon Musk is expected to promote the EV maker’s artificial intelligence capability and plans to expand its vehicle lineup.\nTesla could not immediately be reached for comment.\nNHTSA has an ongoing investigation it opened in 2021 into 830,000 Tesla vehicles with driver assistance system Autopilot over a string of crashes with parked emergency vehicles. NHTSA is reviewing whether Tesla vehicles adequately ensure drivers are paying attention. NHTSA said on Thursday despite the FSD recall its “investigation into Tesla’s Autopilot and associated vehicle systems remains open and active”.\nTesla said in “certain rare circumstances … the feature could potentially infringe upon local traffic laws or customs while executing certain driving maneuvers”.\nPossible situations where the problem could occur include traveling or turning through certain intersections during a yellow traffic light and making a lane change out of certain turn-only lanes to continue traveling straight, NHTSA said.\nNHTSA said “the system may respond insufficiently to changes in posted speed limits or not adequately account for the driver’s adjustment of the vehicle’s speed to exceed posted speed limits”.\nLast year, Tesla recalled nearly 54,000 US vehicles with FSD Beta software that may allow some models to conduct “rolling stops” and not come to a complete stop at some intersections, posing a safety risk, NHTSA said.\nTesla and NHTSA say FSD’s advanced driving features do not make the cars autonomous and require drivers to pay attention.","Relevant":true,"Overview":"Here is the JSON format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Tesla recalls 362,000 vehicles over self-driving software flaws that risk crashes\",\n        \"Summary\": \"Tesla is recalling 362,000 US vehicles due to issues with its Full Self-Driving (FSD) Beta software, as it did not adhere to traffic safety laws and could potentially cause crashes. The National Highway Traffic Safety Administration (NHTSA) raised concerns about the software allowing vehicles to exceed speed limits or travel through intersections unlawfully, increasing the risk of accidents. Tesla will release a free over-the-air (OTA) software update to address the problem. The recall covers various Tesla models equipped with FSD Beta software. Despite the recall, Tesla disagreed with NHTSA's analysis. This intervention by federal regulators is significant for Tesla's self-driving development. The recall comes shortly before an investor day, during which Elon Musk planned to promote Tesla's AI capabilities and vehicle lineup expansion. NHTSA also mentioned an ongoing investigation into Tesla's Autopilot system and its ability to ensure driver attention. Tesla acknowledged that, in certain rare circumstances, the software could infringe upon local traffic laws during certain driving maneuvers.\"\n    }\n}\nThe news is relevant to autonomous vehicle failures, specifically regarding Tesla's Full Self-Driving (FSD) Beta software issues and the recall of 362,000 vehicles. The specific information has been extracted and detailed in the JSON response.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2023\/feb\/16\/tesla-recall-full-self-driving-cars","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":21,"Time":null,"Title":"Australian miner Fortescue says two driverless trucks involved in low-speed incident","Content":"By Melanie Burton\nMELBOURNE (Reuters) - Australian iron ore miner Fortescue said on Friday that one of its driverless trucks, traveling at low speed, ran into another that was parked at its remote Western Australian operations in an incident earlier this week.\nAn autonomous truck readies to pick up a load of iron ore at Australia's Fortescue Metals Group (FMG) Chichester Hub, which includes the Christmas Creek iron ore mine, in the Pilbara region, located southeast of the coastal town of Port Hedland in Western Australia, November 29, 2018. REUTERS\/Melanie Burton\nThe miner said in a statement that no one was hurt or at risk of being injured in the Feb. 11 incident. Fortescue is retrofitting 100 huge mining trucks with autonomous haulage systems (AHS) at its remote Chichester hub, aiming to more than double its self-driving fleet.\n“This was not the result of any failure of the autonomous system,” Fortescue Chief Executive Elizabeth Gaines said in the statement. The miner is conducting a full investigation into the incident and expects that to conclude in the near future.\n“On Monday, 11 February an AHS truck made contact with a parked AHS truck at slow speed,” the statement said, without disclosing the speed at which the moving vehicle was traveling or details of any damage to the trucks. “No manned vehicles or people were involved.”\nAnalysts said that minor accidents with autonomous vehicles had been reported in the mining industry before. All of Australia’s iron ore miners have transitioned into using some autonomous vehicles to cut costs as they can operate without breaks and drive more efficiently.\n“These things happen (across the industry) from time to time,” said UBS analyst Glyn Lawcock in Sydney.\nEarlier on Friday the West Australian paper, without citing sources, reported the moving truck backed into the stationary vehicle.\nFortescue, which said its AHS trucks have safely traveled more than 24.7 million km (14.8 million miles) since 2012, declined to comment directly on the West Australian’s report.\nA spokesman for the firm, which reports earnings on Feb. 20, said, “We already have a good understanding of the incident and expect the investigation to conclude in the near term.”\nThe Western Australian Department of Mines was notified by Fortescue and has begun an investigation, Director of Mines Safety Andrew Chaplyn said in a statement.\n“The department will review the company’s internal investigation report to determine what further actions may be required,” he said.\nReporting by Melanie Burton; Editing by Kenneth Maxwell and Christian Schmollinger\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"Australian iron ore miner Fortescue reported that on February 11, one of its driverless trucks, traveling at low speed, collided with another parked driverless truck at its remote Western Australian operations. Fortunately, no one was injured in the incident. Fortescue stated that this incident was not the result of any failure of the autonomous system and is conducting a full investigation to determine the exact circumstances. The Western Australian Department of Mines is also investigating the incident.\"\n}\nThe news is relevant to autonomous vehicle incidents, and the specific information about the incident has been extracted as detailed as possible.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-australia-fortescue-autonomous-idUSKCN1Q408G","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":22,"Time":null,"Title":"California says self-driving cars must have driver for now","Content":"By Alexandria Sage\nSAN FRANCISCO (Reuters) - California, the largest car market in the United States, issued draft rules on Wednesday for self-driving cars, requiring a licensed driver inside the vehicle in case of failure in a plan that stresses safety.\nThe regulations by the Department of Motor Vehicles are intended to help nurture the state's nascent but fast-growing autonomous vehicle technology industry while allowing traditional car companies and new entrants like Alphabet Inc GOOGL.O and Apple Inc AAPL.O to safely deploy their self-driving cars already in development.\nThe rules, which will face a period of public comment before being finalised, set out a path to take the industry from the current stage of vehicle testing to actually rolling them out to consumers.\nCalifornia has been at the forefront of the fast-growing autonomous vehicle industry, fuelled by technology companies in Silicon Valley, and is one of a handful of states to have passed regulations enabling self-driving car testing on public roads.\nCurrently, 11 companies have permits to drive autonomous vehicles on public roads in the state, provided there is a licensed driver in the car, with Ford being the most-recent entrant.\nThe proposed regulations require certification and third-party testing for carmakers, as well as regular reports back to the DMV for a period of three years. Data from that testing will be used to inform future regulation, the DMV said.\nManufacturers are also required to disclose the data they collect, other than from safety systems, and obtain approval to collect it. Concerns that self-driving cars could be a way for major data collectors like Google to collect information on consumers have fuelled privacy concerns.\nGoogle - which is operating its self-driving cars on the streets of Palo Alto, California and Austin, Texas - and other carmakers and suppliers have said the technology to build self-driving cars should be ready by 2020.\nEditing by Stephen R. Trousdale and Alan Crosby\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"California has issued draft rules requiring a licensed driver to be inside self-driving cars in case of failure. The rules are intended to promote safety while allowing autonomous vehicle technology development. Currently, 11 companies have permits for autonomous vehicle testing on public roads in California, with a licensed driver required in the car. The regulations also mandate certification, third-party testing, and data disclosure by car manufacturers.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/california-regulations-self-driving-idINKBN0TZ2SI20151216","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":24,"Time":"NOVEMBER 5, 2015","Title":"For Libya U.N. deal, former rebel brigades mean success or failure","Content":"By Patrick Markey, Ahmed Elumami\n* U.N. national government deal hangs in balance\n* Powerful local ex-rebel brigades are power brokers\n* Former Gaddafi general is a red line for western forces\nMISRATA, Libya, Nov 5 (Reuters) - In Misrata, a statue of a fist crushing a U.S. fighter jet that rebels seized as war bounty from Muammar Gaddafi’s compound stands testimony to the city’s part in Libya’s 2011 revolution.\nFour years after they helped defeat Gaddafi, Misrata’s armed brigades are expected to play their part among the former rebel forces, tribal fighters and other factions who will determine whether a U.N. plan to end Libya’s crisis survives or fails.\nA conflict between two rival governments and parliaments -- a self-proclaimed body in Tripoli and the internationally recognized set-up in the east -- has left the North African nation effectively partitioned.\nAfter months of talks, the United Nations has offered a deal to create a power-sharing government to represent each faction and reflect Libya’s traditional balance among its regions and cities.\nBut even if political leaders agree -- that is still far from clear -- it will be the brigades of former rebels in Tripoli, Misrata, Zintan and those in the east who decide how a political settlement works out on the ground.\nThe importance of Misrata, a port city and regional economic power, was underlined last year when it backed the takeover of Tripoli from rival factions to establish the self-proclaimed government there.\nNow most of Misrata’s brigades -- including the powerful Al-Majoub and Halbous units -- support the U.N. deal.\n“This is the only solution. There is no a way through arms,” said Ismail AlMjaree, a brigade commander in Misrata. “There is a new awareness among revolutionaries, they are asking: ‘Why are we still fighting, why isn’t the country being built?’”\nLibya’s post-revolt history though is not encouraging.\nIn the past two years, armed brigades have invaded Tripoli’s parliament to oppose political decisions, taken over ministries and even kidnapped a prime minister.\nWith no national army after the fall of Gaddafi, the rebel brigades were incorporated into quasi-official military units, assigned to ministries and government offices. Tens of thousands of fighters are on the state payroll.\nBut as the post-Gaddafi transition fragmented along regional lines, those brigades often stayed more loyal to commanders, tribes or cities than to the idea of a Libyan state.\nLess than 18 months ago, Misrata brigades battled in Tripoli with rivals from the western city of Zintan, who had controlled the airport since 2011.\nThat conflict forced the Zintan fighters out and destroyed the international airport. Most diplomats left the capital.\nLibya’s internationally recognised government and elected parliament now operate out of the eastern cities of Beyda and Tobruk, backed by a loose coalition of forces including former Gaddafi ally Gen. Khalifa Haftar and his Libyan National Army.\nIn the capital, the self-declared National Salvation government is supported by Misrata forces and a patchwork of Tripoli brigades, who operate independently under the nominal command of the interior ministry.\nWestern governments have been pushing for the U.N. deal, worried that Libya’s chaos has allowed Islamist militants and people smugglers to expand in the security vacuum. But western officials acknowledge reactions on the ground will be vital.\n“We have to recognize there will be hardliners who will not accept this. But we have to get as many of the military commanders as possible to accept and to implement this,” said one Western diplomat.\nFLUID SCENARIOS\nSplits have emerged in both camps as hardliners dig in. For the east, Tripoli is controlled by Islamist extremists; For Tripoli, Haftar in the east embodies all they fought against to oust Gaddafi.\nThe U.N. agreement calls for a unity government led by a presidential council of a prime minister, five deputy prime ministers and three senior ministers -- an unwieldy arrangement meant to appease all parties.\nThe elected House of Representatives (HOR) will be the main legislature, alongside a second chamber, the State Council, that includes members of the reinstated General National Congress (GNC) in Tripoli. The deal also envisages demobilisation of armed factions.\nBoth houses are still to vote on the deal. But already critics in both camps dismiss it as U.N.-imposed violation of Libyan sovereignty or reject the proposed candidates. Others question the legitimacy of the elected parliament.\n“Many people have what I call the Gaddafi virus. People who have power, don’t want to give it up,” said Abu Qassem Egzait, a GNC member who leads a block supporting the agreement.\n“It’s impossible that the GNC will vote. We are dealing with a very difficult situation. Maybe we can get support of independents for the deal without the GNC,” he said.\nThat may mean the emergence of a HOR supported by moderates within the GNC that leaves hardliners out of any deal.\nFor those against the U.N. accord, though, other options exist. Direct meetings between HOR and GNC members already took place in Tripoli, lawmakers said.\n“Every sensible person supports dialogue,” said Mohamed Busidra, a GNC member opposing the U.N. deal. “But it’s a mess. We were supposed to accept what was imposed on us, we were not even part of this.”\nDIVISIONS ON GROUND\nEven if the U.N. agreement works, questions remain about what happens next, with problems likely in the disarming of armed groups and the formation of a national army.\nThe European Union and international leaders are promising aid and financing for a new government. But it will be hard to manage the disparate armed factions after a settlement.\nInstalling any new national government in Tripoli may mean negotiating with six or seven local commanders.\nThis week, clashes broke out Tripoli between one local faction, the Tripoli Revolutionary Brigade, and Misrata forces, officials said. Last week, clashes erupted in western Libya between armed groups backing each of the rival governments.\nMore worrying are divisions in the east, where one group in favour of regional autonomy controls major oil ports and this week banned some tankers from the Zueitina terminal.\nThe key question for both camps is Haftar and his Libyan National Army. Haftar has waged war on Islamist militants in the eastern city of Benghazi and this year he was named commander in chief by the elected parliament in Tobruk.\nFor many in the east he represents the possibility of national army away from the influence of city-based brigades.\nFor Misrata and the west, the former Gaddafi general is unacceptable. Misrata commander AlMjaree says Libya needs a unified military force that includes members of Misrata, Zintan and other brigades. But there is one red line: Haftar.\n“If he has any part, it will be the first reason for the new government to fail,” he said. (Reporting by Patrick Markey; Editing by Giles Elgood)\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to Libya's current political situation and the role of former rebel brigades in determining the success or failure of a U.N. plan to end Libya's crisis. The article discusses the ongoing conflict between rival governments in Tripoli and the east, the importance of Misrata's armed brigades in supporting the U.N. deal, the challenges of disarming armed groups and forming a national army, and the divisions regarding the role of former Gaddafi general Haftar in the new government.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/libya-security-idUSL8N12X0OI20151105","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":17,"Time":"Tue 1 Mar 2016 08.28 CET","Title":"Google self-driving car collides with bus in California, accident report says","Content":"One of Google’s self-driving cars has collided with a public bus in Mountain View, an accident report has revealed, in what appears to be the first example of one of the company’s prototype cars causing an accident.\nThe incident happened on 14 February and was reported to California’s department of motor vehicles in an accident report that the agency posted on 29 February.\nThe car was rolling at 2mph (3kph) and the bus at 15mph. No one was injured.\nThe report does not address fault. However, Google wrote that its car was trying to get around some sandbags on a street when its left front struck the right side of the bus.\nThe car’s test driver, who under state law must be in the front seat to grab the wheel when needed, thought the bus would yield and did not have control when the collision happened, according to Google’s report.\nIf it is determined the Google vehicle was at fault, it would be the first time one of its SUVs caused an accident while in autonomous mode.\nJessica Gonzalez, a spokeswoman from the DMV, said the agency hoped to speak with Google on Monday about what went wrong.\nIn a detailed statement from its monthly report on its self-driving car project, Google said that the incident happened on El Camino Real, a busy six-lane boulevard with hundreds on intersections. The car was following a recent change to its programming, following “the spirit of the road” as well as the traffic code by hugging the far side of the right-turn lane to allow other cars to pass on the left.\n“It then detected sandbags near a storm drain blocking its path, so it needed to come to a stop. After waiting for some other vehicles to pass, our vehicle, still in autonomous mode, began angling back toward the center of the lane at around 2mph – and made contact with the side of a passing bus traveling at 15mph. Our car had detected the approaching bus, but predicted that it would yield to us because we were ahead of it,” Google said.\n“Our test driver, who had been watching the bus in the mirror, also expected the bus to slow or stop. And we can imagine the bus driver assumed we were going to stay put. Unfortunately, all these assumptions led us to the same spot in the lane at the same time. This type of misunderstanding happens between human drivers on the road every day.”\nThis type of misunderstanding happens between human drivers on the road every day.\nGoogle's monthly self-driving car report\nGoogle says it has refined its software following the incident, acknowledging that buses and other large vehicles are less likely to yield. “In this case we clearly bear some responsibility because if our car hadn’t moved there would’t have been a collision.”\n“We hope to handle situations like this more gracefully in the future.”\nHilary Rowen, a partner at the insurance regulation practice Sedgwick LLP and an expert in the issue of self-driving cars and legal responsibility, said the case is a good example of a conundrum that will soon be common.\n“Here, the software didn’t avoid the accident, but the human could have taken over,” she said. “Who’s at fault – the driver, the bus driver, or the software?\nRowen said in real world situations, both the driver and injured party will actually be incentivized to blame the software which, if found to be guilty, will leave the driver’s record clear and likely have a higher payout for the injured party.\n“Everybody’s going to be blaming the software all the time,” Rowen said. “All the time.” Rowen still thinks autonomous car insurance will be cheaper than human-driven car insurance because humans aren’t very good drivers.\n“At a very visceral level, people will accept a higher chance of being maimed or killed by a human being than they will by being maimed or killed by software,” she said. “The self-driving car will likely be able to make better risk calculations.”\nGoogle has been testing two dozen Lexus SUVs outfitted with sensors and cameras near the tech firm’s Silicon Valley headquarters.\nGoogle cars have been involved in more than a dozen collisions. In most cases, Google’s cars were rear-ended. No one has been seriously injured.\nGoogle reports self-driving car mistakes: 272 failures and 13 near misses\nRead more","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"A Google self-driving car collided with a public bus in Mountain View, California, on February 14. The accident report, posted by California's department of motor vehicles on February 29, revealed that the self-driving car was traveling at 2mph and the bus at 15mph, and fortunately, no one was injured. The report did not address fault explicitly, but Google mentioned that its car was trying to navigate around sandbags on a street when its left front struck the right side of the bus. The car's test driver, who was in the front seat to take control if necessary, believed the bus would yield and was not in control during the collision. Google stated that it had refined its software after the incident, acknowledging some responsibility, and hoped to handle similar situations better in the future. This incident marked one of the first instances of a Google self-driving car causing an accident while in autonomous mode.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/feb\/29\/google-self-driving-car-accident-california","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":19,"Time":"9:36 PM GMT+1•March 17, 2015","Title":"Google[x] Head Astro Teller Says Moonshots Are All About Embracing Failure","Content":"Google’s ‘Captain of Moonshots’ Astro Teller admitted to a packed crowd at SXSW that a number of Google[x] projects have experienced a series of “bumps and scrapes” over the last five years. But he also highlighted how the Google lab attempts to overcome those issues and “embrace failure” in projects that aren’t going so well.\nThe original moonshot proposal for the secret Google lab was to get people to the moon. It has since spun out 10 different projects that involve everything from biotechnology, robots and cancer detection nanotechnology.\nBut it has not been without a lot of problems. One example is Google Glass, which Teller said he regrets launching as if it were a finished product.\n“We made one great decision and one not-so-great decision. The great decision was the Explorer program. The thing we did not do well is that we allowed and somewhat encouraged too much exposure to the program. We did things to encourage people to think that [Glass] was a finished product,” he said.\nIt turns out building autonomous vehicles is also pretty difficult. For Google’s self-driving car project, the lab gave a bunch of Lexus SUVs to non-Google employees. They drove fine on the highway but not on city streets. The mistake Google made was in assuming humans would pay attention the whole time and be a reliable backup. That didn’t work out so well all the time.\n“People do really stupid things while driving,” he said. Afterward, Google switched to making the cars in-house, and that project is still underway.\nWe did things to encourage people to think that [Glass] was a finished product\nAstro Teller\nHe also shared Google’s “maniacal interest” in seeing projects through, even those which don’t seem to be working. Project Wing, a self-flying delivery vehicle, fell under that category. The team believed the idea to be a failure 18 months after the fact.\n“Fifty percent of the team knew it was a bad project after a few months, 80 percent knew after a year-and-a-half but they didn’t kill it,” he said. The team is still at it on that one, too. “I look forward to telling you more about that later this year,” Teller said.\nGoogle’s Project Loon, a plan to deliver Internet around the world using giant 20-meter balloons, had problems because the balloons leaked when they hit a certain altitude. The team resorted to studying things like the fluffiness of researcher’s socks to figure out what the problem was. “It turned out that actually mattered,” Teller told the crowd.\nThese moonshots are all about embracing failure, according to the head of Google[x].\n“You make a ton of progress by making a ton of mistakes,” he said. “The longer you work on something, the more you don’t really want to know what the world is going to tell you. The longer you put off that learning you will unconsciously put off that news because it is disheartening to hear that what you have been working on is not working.”\nGoogle’s self-driving car project now travels several thousand miles of city streets every day to find ways to get stuck. “It’s not about ensuring you don’t have these bumps and scrapes. It’s about ensuring we learn from them,” Teller said.","Relevant":true,"Overview":"Here's the response in JSON format based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to autonomous vehicle failures. It mentions that Google's self-driving car project encountered difficulties when they gave Lexus SUVs to non-Google employees for testing. The cars drove fine on the highway but not on city streets due to the assumption that humans would pay attention and act as reliable backups. Google later switched to making the cars in-house, and the project is still ongoing.\"\n}\nThe news is indeed relevant to autonomous vehicle failures, specifically concerning Google's self-driving car project's initial challenges.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2015\/03\/17\/googlex-head-astro-teller-says-moonshots-are-all-about-embracing-failure\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":18,"Time":"Tue 9 Aug 2022 19.08 CEST","Title":"Tesla’s self-driving technology fails to detect children in the road, group claims","Content":"A safe-technology campaign group opposed to Tesla’s self-driving technology has claimed to have run tests that show the software represents a potentially lethal threat to child pedestrians, in the latest in a series of claims and investigations to hit the world’s leading electric carmaker.\nThe Dawn Project says its test track results revealed that the latest version of Tesla’s Full Self-Driving (FSD) Beta software failed to detect a stationary, child-sized mannequin at an average speed of 25mph. The claim is the central plank of an ad campaign urging the public to press Congress to ban Tesla’s auto-driving technology.\nThe Dawn Project was founded by the software entrepreneur Dan O’Dowd, an outspoken critic of Tesla boss Elon Musk and what he says is Tesla’s “reckless” deployment of unsafe self-driving vehicles.\nCalifornia tech titan’s Senate run has one target: Tesla’s self-driving software\nRead more\nO’Dowd described the test results – which have been published on the Dawn Project’s website – as “deeply disturbing” and that Tesla’s software represented “a lethal threat to all Americans.”\nHe added: “Over 100,000 Tesla drivers are already using the car’s Full Self-Driving mode on public roads, putting children at great risk in communities across the country.”\nO’Dowd argued that the results show the need to prohibit self-driving cars until Tesla proves the vehicles “will not mow down children in crosswalks”.\nTesla has repeatedly hit back at claims that its self-driving technology is too underdeveloped to guarantee the safety of either the car’s occupants or other road users.\nO’Dowd has drawn accusations that he is little more than a competitor to Tesla because his company bills itself as an expert in making particular software used in automated driving systems. O’Dowd insists his Green Hills software doesn’t compete with Tesla, saying it doesn’t make self-driving cars. But he has acknowledged some car companies use his company’s software for certain components.\nAfter a fiery crash in Texas in 2021 that killed two, Musk tweeted that the autopilot feature, a less sophisticated version of FSD, was not switched on at the moment of collision.\nAt the company’s shareholder meeting earlier this month Musk said that Full Self-Driving has greatly improved, and he expected to make the software available by the end of the year to all owners that request it. But questions about its safety continue to mount.\nIn June, the National Highway Traffic Safety Administration (NHTSA), said it was expanding an investigation into 830,000 Tesla cars across all four current model lines. The expansion came after analysis of a number of accidents revealed patterns in the car’s performance and driver behavior.\nThe NHTSA said the widened investigation would aim to examine the degree to which Tesla’s autopilot technology and associated systems “may exacerbate human factors or behavioral safety risks by undermining the effectiveness of the driver’s supervision”.\nA second NHTSA investigation is also under way to determine if the removal of the forward-looking radar sensor on some newer Teslas is causing the vehicles to apply their brakes for no reason, which is called “phantom braking” and can lead to wrecks.\nSince 2016, the agency has investigated 30 crashes involving Teslas equipped with automated driving systems, 19 of them fatal. NHTSA’s Office of Defects Investigation is also looking at the company’s autopilot technology in at least 11 crashes where Teslas hit emergency vehicles.\nMany such wrecks aren’t investigated by the NHTSA. And in nearly 400 crashes involving cars with driver-assist systems reported by automakers between July 2021 and this past May, more Teslas were involved than all other manufacturers combined.\nThis article was amended on 11 August 2022 to provide further context of, and give appropriate significance to, the Dawn Project’s stated political opposition to Tesla’s self-driving technology. The headline was changed to indicate that the Dawn Project’s test results are claimed by the group, and have not been independently verified.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about autonomous vehicle failures, specifically related to Tesla's self-driving technology. The Dawn Project claims that their tests revealed that Tesla's Full Self-Driving (FSD) Beta software failed to detect a stationary, child-sized mannequin at an average speed of 25mph. The claim is part of an ad campaign urging the public to press Congress to ban Tesla's auto-driving technology. The article also mentions concerns about the safety of Tesla's self-driving technology and ongoing investigations by regulatory authorities, including the National Highway Traffic Safety Administration (NHTSA), into Tesla's autopilot technology and associated systems.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2022\/aug\/09\/tesla-self-driving-technology-safety-children","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":26,"Time":null,"Title":"U.S. probes touchscreen failures in Tesla Model S cars","Content":"By David Shepardson\nWASHINGTON (Reuters) - The U.S. National Highway Traffic Safety Administration (NHTSA) said Tuesday it had opened an investigation into 63,000 Tesla Model S cars after reports of media-control unit failures that led to the loss of the use of touchscreens.\nA Tesla logo adorns a 'Model S' car in the dealership in Berlin, Germany, November 18, 2015. REUTERS\/Hannibal Hanschke\/File Photo\nThe auto safety agency said the preliminary evaluation, covering 2012-2015 model year vehicles, comes after it received 11 complaints alleging premature failure of the media-control unit due to memory wear-out.\nA complete unit failure results in the loss of audible and visual touchscreen features, such as infotainment, navigation, and web browsing, and loss of the rear-camera image display when in reverse gear, the agency said.\nTesla did not immediately respond to a request for comment.\nNHTSA said the failure does not affect vehicle-control systems.\nTesla used the same unit in 159,000 2012-2018 Model S and 2016-2018 Model X vehicles built by Tesla through early 2018.\nThe memory control unit uses an Nvidia Corp Tegra 3 processor with an integrated 8GB eMMC NAND flash memory device, NHTSA said. Nvidia did not immediately respond to a request for comment.\nThe flash devices have a finite lifespan based on the number of program or erase cycles, NHTSA said.\nFailures resulting from memory wear-out “are likely to occur after periods of progressively degraded performance (e.g., longer power-up times, more frequent touchscreen resets, intermittent loss of cellular connectivity, loss of navigation),” the agency said.\nSome complaints said the failure could result in a loss of charging ability and that other safety alerts could be impacted. One driver said he could not clear fogged windows because he could not change climate controls.\nAnother complaint said the failure disabled safety monitors associated with Tesla’s driver-assistance system Autopilot.\nThe complaints said Tesla requires owners to pay to replace the unit once the warranty expires.\nReporting by David Shepardson; Editing by Bernadette Baum\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about the U.S. National Highway Traffic Safety Administration (NHTSA) opening an investigation into 63,000 Tesla Model S cars due to reports of media-control unit failures. These failures result in the loss of audible and visual touchscreen features, such as infotainment, navigation, and web browsing, as well as the loss of the rear-camera image display when in reverse gear. The investigation covers 2012-2015 model year vehicles and was initiated after receiving 11 complaints alleging premature failure of the media-control unit due to memory wear-out. Some complaints suggest that the failures could impact charging ability, safety alerts, and even Tesla's driver-assistance system Autopilot. Tesla used the same unit in 159,000 2012-2018 Model S and 2016-2018 Model X vehicles built by Tesla through early 2018. The memory control unit uses an Nvidia Corp Tegra 3 processor with an integrated 8GB eMMC NAND flash memory device, which has a finite lifespan based on the number of program or erase cycles.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/tesla-safety-idINKBN23V1X4","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":28,"Time":"DECEMBER 20, 2018","Title":"Uber resuming self-driving car testing in Pennsylvania","Content":"By David Shepardson\nWASHINGTON (Reuters) - Uber Technologies Inc [UBER.UL] on Thursday is resuming limited self-driving car testing on public roads in Pittsburgh, nine months after it suspended the program following a deadly accident in Arizona.\nFILE PHOTO: Uber's logo is displayed on a mobile phone, September 14, 2018. REUTERS\/Hannah Mckay\/File Photo\nThe return of testing comes days after the state of Pennsylvania granted Uber permission to resume testing.\nThe relaunch is a significant reduction from Uber’s previous self-driving operations. The company had operated its fleet in autonomous mode on public roads at high speeds, after dark, in areas crowded with pedestrians and with a single backup driver in the front seat.\nThis time, the cars will not operate at night or in wet weather and will not exceed speeds of 25 miles per hour, the company has said.\nUber will resume testing with two employees in the front seat, enable an automatic braking system at all times, and more strictly monitor safety employees after it said it made improvements to the vehicles’ self-driving software. The company for the time being will not resume picking up passengers in the robot cars, a service Uber launched in 2016.\nReuters reported earlier this month that Uber planned to initially begin driving “a handful” of cars on a mile loop between two company offices in Pittsburgh, where Uber first debuted its autonomous vehicles in 2016.\nUber also said it is resuming manual driving of its self-driving cars in Toronto and San Francisco that will be controlled by a driver and monitored by a second person in the passenger seat. Manual driving helps the company gather data on new driving scenarios, Uber said.\nUber said it “will only pursue a return to road for self-driving in these cities in coordination with federal, state, and local authorities.”\nEric Meyhofer, who heads Uber’s Advanced Technologies Group, said in a blog post that Uber has “reviewed and improved our testing program to ensure that our vehicles are considerate and defensive drivers.” He added that vehicles “must pass a series of more than 70 scenarios without safety-related failures on our test track” before testing on public roads.\nIn March, authorities in Arizona suspended Uber’s ability to test its self-driving cars after one of its cars hit and killed a woman crossing the street at night in the Phoenix suburb of Tempe, Uber’s largest testing hub. Uber also voluntarily halted its entire autonomous car testing program.\nThe crash was the first death attributed to a self-driving vehicle. The death prompted significant safety concerns about the nascent self-driving car industry, which is racing to get vehicles into commercial use.\nThe National Transportation Safety Board (NTSB) and National Highway Traffic Safety Administration are investigating the fatal crash.\nThe NTSB said Uber had disabled an emergency braking system in the modified Volvo test vehicle. Uber said the Volvo automatic emergency braking system will be activated at all times.\nReporting by David Shepardson; Editing by Christopher Cushing\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here's the JSON format response based on the provided news article:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"Uber is resuming limited self-driving car testing in Pittsburgh, Pennsylvania, after suspending the program for nine months following a deadly accident in Arizona. The testing will have significant restrictions, including not operating at night or in wet weather, not exceeding speeds of 25 miles per hour, and having two employees in the front seat. An automatic braking system will be active at all times, and safety employees will be more strictly monitored after improvements to the self-driving software. Uber will not resume picking up passengers with the self-driving cars for now. The company also plans to begin driving a few cars on a mile loop between two company offices in Pittsburgh. Manual driving of self-driving cars in Toronto and San Francisco will also resume, controlled by a driver and monitored by a second person. Uber emphasizes coordination with authorities in its testing program, which now includes over 70 safety-related scenarios to pass before testing on public roads. The article mentions a fatal accident in Arizona involving an Uber self-driving car and the subsequent suspension of the program, as well as investigations by the National Transportation Safety Board and the National Highway Traffic Safety Administration.\"\n}\nThe news is indeed relevant to autonomous vehicle testing, specifically regarding Uber's decision to resume limited self-driving car testing in Pennsylvania and the safety measures they are implementing.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-uber-selfdriving-idUSKCN1OJ1OD","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":23,"Time":"9:33 PM GMT+1•March 19, 2018","Title":"Here’s how Uber’s self-driving cars are supposed to detect pedestrians","Content":"A self-driving vehicle made by Uber has struck and killed a pedestrian. It’s the first such incident and will certainly be scrutinized like no other autonomous vehicle interaction in the past. But on the face of it it’s hard to understand how, short of a total system failure, this could happen, when the entire car has essentially been designed around preventing exactly this situation from occurring.\nSomething unexpectedly entering the vehicle’s path is pretty much the first emergency event that autonomous car engineers look at. The situation could be many things — a stopped car, a deer, a pedestrian — and the systems are one and all designed to detect them as early as possible, identify them and take appropriate action. That could be slowing, stopping, swerving, anything.\nUber’s vehicles are equipped with several different imaging systems which work both ordinary duty (monitoring nearby cars, signs and lane markings) and extraordinary duty like that just described. No less than four different ones should have picked up the victim in this case.\nTop-mounted lidar. The bucket-shaped item on top of these cars is a lidar, or light detection and ranging, system that produces a 3D image of the car’s surroundings multiple times per second. Using infrared laser pulses that bounce off objects and return to the sensor, lidar can detect static and moving objects in considerable detail, day or night.\nThis is an example of a lidar-created imagery, though not specifically what the Uber vehicle would have seen.\nHeavy snow and fog can obscure a lidar’s lasers, and its accuracy decreases with range, but for anything from a few feet to a few hundred feet, it’s an invaluable imaging tool and one that is found on practically every self-driving car.\nThe lidar unit, if operating correctly, should have been able to make out the person in question, if they were not totally obscured, while they were still more than a hundred feet away, and passed on their presence to the “brain” that collates the imagery.\nFront-mounted radar. Radar, like lidar, sends out a signal and waits for it to bounce back, but it uses radio waves instead of light. This makes it more resistant to interference, since radio can pass through snow and fog, but also lowers its resolution and changes its range profile.\nTesla’s Autopilot relies mostly on radar.\nDepending on the radar unit Uber employed — likely multiple in both front and back to provide 360 degrees of coverage — the range could differ considerably. If it’s meant to complement the lidar, chances are it overlaps considerably, but is built more to identify other cars and larger obstacles.\nThe radar signature of a person is not nearly so recognizable, but it’s very likely they would have at least shown up, confirming what the lidar detected.\nShort and long-range optical cameras. Lidar and radar are great for locating shapes, but they’re no good for reading signs, figuring out what color something is and so on. That’s a job for visible-light cameras with sophisticated computer vision algorithms running in real time on their imagery.\nThe cameras on the Uber vehicle watch for telltale patterns that indicate braking vehicles (sudden red lights), traffic lights, crossing pedestrians and so on. Especially on the front end of the car, multiple angles and types of camera would be used, so as to get a complete picture of the scene into which the car is driving.\nDetecting people is one of the most commonly attempted computer vision problems, and the algorithms that do it have gotten quite good. “Segmenting” an image, as it’s often called, generally also involves identifying things like signs, trees, sidewalks and more.\nThat said, it can be hard at night. But that’s an obvious problem, the answer to which is the previous two systems, which work night and day. Even in pitch darkness, a person wearing all black would show up on lidar and radar, warning the car that it should perhaps slow and be ready to see that person in the headlights. That’s probably why a night-vision system isn’t commonly found in self-driving vehicles (I can’t be sure there isn’t one on the Uber car, but it seems unlikely).\nSafety driver. It may sound cynical to refer to a person as a system, but the safety drivers in these cars are very much acting in the capacity of an all-purpose failsafe. People are very good at detecting things, even though we don’t have lasers coming out of our eyes. And our reaction times aren’t the best, but if it’s clear that the car isn’t going to respond, or has responded wrongly, a trained safety driver will react correctly.\nWorth mentioning is that there is also a central computing unit that takes the input from these sources and creates its own more complete representation of the world around the car. A person may disappear behind a car in front of the system’s sensors, for instance, and no longer be visible for a second or two, but that doesn’t mean they ceased existing. This goes beyond simple object recognition and begins to bring in broader concepts of intelligence such as object permanence, predicting actions and the like.\nIt’s also arguably the most advanced and closely guarded part of any self-driving car system and so is kept well under wraps.\nIt isn’t clear what the circumstances were under which this tragedy played out, but the car was certainly equipped with technology that was intended to, and should have, detected the person and caused the car to react appropriately. Furthermore, if one system didn’t work, another should have sufficed — multiple failbacks are only practical in high-stakes matters like driving on public roads.\nWe’ll know more as Uber, local law enforcement, federal authorities and others investigate the accident.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"A self-driving vehicle made by Uber has struck and killed a pedestrian. It’s the first such incident and will certainly be scrutinized like no other autonomous vehicle interaction in the past. The article discusses the technology and systems that Uber's self-driving vehicles are equipped with, including lidar, radar, and optical cameras, all designed to detect objects and pedestrians in various conditions, day or night. It also mentions the role of safety drivers and a central computing unit in ensuring safety. However, it's noted that it's not clear what circumstances led to the tragedy, but the technology should have detected the person and reacted appropriately.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2018\/03\/19\/heres-how-ubers-self-driving-cars-are-supposed-to-detect-pedestrians\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":27,"Time":"9:46 PM GMT+2•April 24, 2023","Title":"How Aurora is navigating the bumpy road to commercial self-driving trucks","Content":"cameraImage Credits: Aurora\nAmid economic uncertainty, tight capital markets, wary investors and consolidation in the nascent autonomous vehicle industry, Aurora Innovation is still motoring on toward its target of commercializing self-driving trucks by 2024, ticking off waypoints and milestones along the way.\nIt’s even managed to rack up a few wins in the past year, including launching or expanding pilot programs with FedEx, Ryder, Schneider and Uber Freight. More recently, Aurora announced its self-driving system is “feature complete” — industry jargon that means the technology has all the capabilities required for hauling freight autonomously on public roads. And it deployed the first of its “commercial ready” autonomous shipping terminals in Palmer, Texas, infrastructure needed if the company’s trucks are going to be on the road 24 hours of every day of the year.\nThree years ago, it wasn’t so clear if Aurora would survive its ambitious and risky moves.\nIn late 2020, Uber sold its self-driving unit Uber ATG to Aurora in a complex deal that would bolster the startup’s valuation to $10 billion along with its autonomy efforts, while giving Uber a 26% stake in the company.\nThe deal liberated Uber from what had been a troubling development path, including a 2017 lawsuit from Waymo for trade secret theft and patent infringement (which Uber settled in 2018), as well as the 2018 fatal crash involving an Uber ATG autonomous test vehicle. But while Aurora gained talented engineers it also faced the tricky process of integrating about 1,200 former Uber ATG employees into its operations.\nSix months later, Aurora would make another leap: This time into the public markets through a merger with a special purpose acquisition company, an increasingly fraught path that has stymied many other automotive startups. Aurora’s stock has suffered along with other mobility SPACs; its stock price debuted November 4, 2021 at $11.25 a share. Aurora stock has fallen 85% since then and was trading Monday at $1.46 a share.\n“I think that there is a kind of best house in a bad neighborhood kind of sentiment. That there’s a lot of stuff that’s maybe not going as well in this space, right? Some very public failures and backstepping,” co-founder and CEO Chris Urmson said in a recent interview. “But, you know, I think that’s short term. A short-term issue because as we continue to demonstrate progress and execution and the fact that we made strategic bets years ago that are paying off, right? That we expect that that will be recognized and, you know, things will move forward.”\nFresh challenges\nAurora co-founder and CEO Chris Urmson. Image Credits: Aurora\nAurora is now encountering another batch of challenges as it drives toward commercialization, from assuaging shareholders and its growing list of partners to navigating regulatory hurdles in California and raising more money.\nAnd despite that “feature complete” achievement, Aurora still has plenty of testing to do, including ramping up to haul 100 loads per week between Dallas and Houston by the end of this year.\nThe company has more than 30 trucks on the road in Texas today hauling goods with more than 50 trips a week. To date, the company’s trucks have traveled more than 400,000 miles and hauled 20 million pounds of freight for FedEx, Uber Freight, Werner and Schneider.\nScreenshot of Aurora trucking route as of April 2023. Image Credits: Aurora\nIf all goes according to plan, Aurora will deploy its self-driving system called Driver commercially in Texas, without onboard human supervision, by the end of 2024.\nAccording to Urmson, his company’s autonomy tech will fix what he calls a “systemic problem” in the U.S. supply chain: “There’s not enough people willing to drive trucks. We’re short about 80,000. We expect to be short about 150,000 by the end of the decade.”\nThe trucking industry also suffers from a whopping 90% turnover rate, and its workers must abide by federal 11-hour daily limits behind the wheel.\nUrmson says that the Aurora driver should be able to keep a truck moving for about 20 hours on an average day and that, plus associated fuel savings and reduced insurance costs due to safer driving, means theoretically doubling the revenue per truck for a fleet operator.\n“Getting from Houston to LA, for example, takes about three days by truck today, because of that 11 hours of service limitation. The Aurora Driver should be able to make that trip in 24 hours,” Urmson said.\nRegulatory speed bumps\nAn Aurora Innovation truck drives over a bridge in Texas. Image Credits: Aurora\nThose increased efficiencies and subsequent reduced costs are crucial to the appeal of Aurora’s technology to the trucking industry. But, there’s one ugly fly threateningly buzzing around the soup: California’s AB 316. If passed, this bill would require human operators in trucks within the state of California, and could kick off similar legislation elsewhere.\nWhile Urmson hopes that California will “see the economic and safety benefits” of autonomous trucking and allow driverless operation, Aurora is prepared for an eventuality where it becomes law: “It’ll mean helping our customers haul freight between Arizona and the East Coast,” Urmson said.\nIn other words: no business in California. If policies like this gain momentum, that trend could pose a threat to Aurora’s business plan.\n“It would certainly be if we ended up with kind of a checkerboard across the United States,” Urmson said. “But, I think in practice that’s unlikely.”\nThe Uber ATG boost\nFor now, Aurora’s primary goals are validating the functionality of its Driver within the state of Texas and doing everything it can to cut costs. Urmson told TechCrunch that the ATG acquisition has helped on both fronts.\nWith ATG, Aurora went from 600 to 1,800 employees “overnight,” and Urmson says they’re still operating with about 1,700. While some Uber ATG employees chose to leave after the deal was closed, Urmson said none were made redundant: “The ones who want to be here are, and that’s really all we can hope for, right?”\nAt this point, the teams are well integrated, according to Urmson, but it was something of a painful process.\n“We were very deferential around the culture on both sides. And I think that just led to confusion,” Urmson said, with conflicting lines of reporting and duplicated efforts. “Like any organization, there’s pockets of challenges, but I think that, again, those folks that were united in the mission of let’s go get the vehicles on the road, let’s do something that’s going to change the world, they fit in really well.”\nThe Uber ATG deal also helped Aurora become more efficient and cut costs with the goal of extending the company’s runway in the lead-up to commercialization. For instance, Urmson said that ATG’s batch processing of cloud computing was far more efficient than how Aurora was previously handling distributed tasks. “As you can imagine, for all of our ML [machine learning] stuff, all of our testing and simulation, there’s a massive amount of cloud orchestration that happens,” Urmson said.\nUsing ATG’s batching process, Aurora’s distributed systems team was able to take disparate cloud requests and lump them together into fewer, larger requests, saving time and reducing costs. “That’s been really powerful and it’s the heart of what we use today,” Urmson said, estimating the cost savings there alone to be in the “tens of millions of dollars.”\nOn the functional side, ATG’s offline Simultaneous Localization and Mapping (SLAM) procedure has Aurora generating high-res maps more efficiently. ATG’s sampling-based motion planning has been integrated to help the Aurora Driver in emergency, near-collision situations. The Aurora Driver’s behavior in construction zones also derives from ATG algorithms.\nAll of this is included in the Aurora Driver Beta 6.0 release, which “introduced the final driving capabilities needed to commercially haul freight without vehicle operators,” according to Aurora. That commercial launch, Urmson says, is still on target for next year, and those cost-cutting measures have ensured Aurora will have enough cash to get the company through the middle of 2024.\nMore cash, more runway\nAurora is now focused on converting all of those pilot programs into long-term customers. The company will host its first customer summit at the end of April in Dallas — an event that will bring together its pilot customers and partners to discuss the next steps toward commercial launch, according to the company.\nHowever, another raise is still in the plans. On April 6, Aurora filed for a proposed mixed shelf offering to raise $350 million.\n“We have not been shy about the fact we would need to raise additional capital in the future,” an Aurora spokesperson told TechCrunch when asked about the filing. “This is a standard filing that provides Aurora the flexibility to raise at a future date but is not a signal we intend to imminently fundraise.”\nIn the meantime, Urmson said he is focused on de-risking the business.\n“We’re going to wait for the right time and the right partners. What we see is, as a company, we look at these milestones, and our execution against them, and we see them as major moments of de-risking our business,” Urmson said.\nReducing risk and inspiring confidence now is key, particularly in the wake of widespread consolidation throughout the industry that has left some investors wary.\nThe abrupt termination of Argo AI last year has left many questioning the future of autonomy as a business. Meanwhile, Alphabet’s self-driving unit Waymo laid off staff in January. The company says its trucking effort, Waymo Via, is still operational with testing occurring in Texas. (Urmson previously led the former Google self-driving project.)\n“I think one of the really important early decisions we made as a company was to be independent,” Urmson said, referencing Argo AI’s funding by Ford and Volkswagen. “As a company, we have a mission to deliver the benefits of self-driving technology safely, quickly and broadly,” he said. “We work with amazing partners, but for none of them is that their mission.”\nFor Urmson, that misalignment between Argo’s quest for autonomy and its corporate parents’ need for profitability ultimately sealed Argo’s fate during tough economic times.\nUntil things improve, Urmson continues to drive home the importance of frugality on his team: “At each of our all-hands, we have a section we call ‘Every Little Bit Matters’ and we highlight places where teams across the company have saved between thousands and millions of dollars by either not doing something or finding a less expensive way to do it. I think that is a muscle that will serve us well, even when economic times become better.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous vehicles, specifically self-driving trucks by Aurora Innovation. It mentions Aurora's progress toward commercializing self-driving trucks by 2024, including successful pilot programs with companies like FedEx, Ryder, Schneider, and Uber Freight. Aurora has also achieved 'feature complete' status for its self-driving system and deployed autonomous shipping terminals. The article discusses challenges the company faces, such as regulatory hurdles in California and raising more money. It also mentions Aurora's goal of hauling 100 loads per week between Dallas and Houston by the end of the year and its efforts to address supply chain issues in the trucking industry. Additionally, there are references to Aurora's integration of former Uber ATG employees, its stock performance, and potential legislative challenges in California.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2023\/04\/24\/how-aurora-is-navigating-the-bumpy-road-to-commercial-self-driving-trucks\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":31,"Time":"2:17 AM GMT+2•July 31, 2019","Title":"Drone crash near kids leads Swiss Post and Matternet to suspend autonomous deliveries","Content":"cameraImage Credits: Matternet Inc.\n(opens in a new window)\nunder a license.\n(Update: With the investigation concluded and recommended improvements underway, the Matternet-Swiss Post partnership will take flight again in January 2020.)\nA serious crash by a delivery drone in Switzerland has grounded the fleet and put a partnership on ice. Within a stone’s throw of some kids, the incident raised grim possibilities for the possibilities of catastrophic failure of payload-bearing autonomous aerial vehicles.\nThe drones were operated by Matternet as part of a partnership with the Swiss Post (i.e. the postal service), which was using the craft to dispatch lab samples from one medical center for priority cases. As far as potential applications of drone delivery, it’s a home run — but twice now the craft have crashed, first with a soft landing and the second time a very hard one.\nThe first incident, in January, was the result of a GPS hardware error; the drone entered a planned failback state and deployed its emergency parachute, falling slowly to the ground. Measures were taken to improve the GPS systems.\nThe second failure in May, however, led to the drone attempting to deploy its parachute again, only to sever the line somehow and plummet to earth, crashing into the ground some 150 feet from a bunch of kindergartners. No one was hit, but this narrowly avoided being a worst-case scenario for the service: not just a craft failing, but the emergency systems failing as well, and immediately over a bunch of children. The incident was documented last month but not widely reported.\nFalling from a few hundred feet, the 12-kilogram (about 26 pounds) drone and payload could easily have seriously injured or even killed someone — this is why there are very strict regulations about flying over populated areas and crowds. Matternet noted that the area the drone came down in is not populated or a school, but rather a forested area of Zurich more appropriate for flying over.\nObviously they grounded the fleet following this incident and will not spin up again until Matternet addresses the various issues involved. How was it even possible, for instance, that the parachute line was capable of being cut by something on the drone?\nIEEE Spectrum first noted the news stateside. The company issued the following statement on the matter:\nThis is the first time ever that our vehicle parachute system has failed. As stated in the report, the flight termination system was triggered nominally per the drone’s specification, but the parachute cord was severed during the parachute deployment.\nAt Matternet, we take the safety of our technology and operations extremely seriously. A failure of the parachute safety mechanism system is unacceptable and we are taking all the appropriate measures to address it.\nSwiss Post and Matternet reacted to the incident immediately by grounding all the operations involving this vehicle type. Our experts analyzed the incident and proposed the appropriate mitigations which are being evaluated by FOCA. We will restart operations once Matternet and Swiss Post, FOCA and our hospital customers in Switzerland are satisfied that the appropriate mitigations have been applied.\nDrone delivery is a promising field, but situations like this don’t do it any favors when regulators take a look. Despite sunny predictions from the industry, there is a huge amount of work yet to be done in terms of flight proving the technology, and although two failures out of some 3,000 may not sound like a lot, if one of those failures is an uncontrolled fall that nearly takes out some kids, that could set back the entire industry.\n(This story has been slightly updated to accommodate a new statement from Matternet.)","Relevant":true,"Overview":"Here is the response in JSON format:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Title\": \"Drone crash near kids leads Swiss Post and Matternet to suspend autonomous deliveries\",\n    \"Summary\": \"A serious crash by a delivery drone in Switzerland has grounded the fleet and put a partnership on ice. The incident involved a payload-bearing autonomous aerial vehicle operated by Matternet as part of a partnership with Swiss Post. The crash was the result of a parachute failure, and it occurred close to a group of kindergartners, raising concerns about the safety of such operations over populated areas. The incident led to the suspension of autonomous deliveries until safety issues are addressed.\"\n  }\n}\nThe news is indeed about autonomous vehicle failures, specifically, a drone crash involving a payload-bearing autonomous aerial vehicle. The specific information includes details about the incident, the cause of the crash (parachute failure), and the suspension of autonomous deliveries due to safety concerns.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2019\/07\/30\/drone-crash-near-kids-leads-swiss-post-and-matternet-to-suspend-autonomous-deliveries\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":29,"Time":"Wed 7 Jun 2023 22.17 CEST","Title":"Self-driving Waymo car kills dog amid increasing concern over robotaxis","Content":"A Waymo self-driving car killed a small dog in San Francisco while in “autonomous mode” last month, the latest in a string of incidents that have led city officials to protest a planned expansion of the company’s driverless taxi services.\nMy week navigating the awkward teenage years of self-driving cars\nRead more\nThe 21 May collision occurred just before 11am on Toland Street in San Francisco as the car was in “autonomous mode” with a test driver in the front seat. A Waymo spokesperson said that an initial review showed that the system correctly identified the dog when it ran out from behind a parked car, but “was not able to avoid contact” due to how fast the canine was moving. The company said it was investigating the matter.\n“We send our sincere condolences to the dog’s owner,” the Waymo spokesperson said. “The trust and safety of the communities we are in is the most important thing to us and we’re continuing to look into this on our end.”\nThe car, a Jaguar, sustained minor damage, according to a California DMV report.\nThe collision comes as Waymo, which is owned by the Google parent company, Alphabet, and its competitor, Cruise, seek to dramatically scale up their robotaxi operations throughout the city. Cruise, operated by General Motors, currently has permission to charge fares for driverless taxi rides between 10pm and 6am in some parts of the city, but its cars must avoid the densest downtown areas. Waymo only has authority to charge fares if a safety driver is present in the car.\nThe California public utilities commission, which regulates the companies’ robotaxi services, is on the cusp of approving the companies’ requests to operate their self-driving taxis at all hours and throughout the entire city. The commission is set to vote on 29 June on whether to approve draft resolutions that would allow Waymo and Cruise to offer round-the-clock robotaxi service.\nBut the resolutions have faced fierce opposition from local officials. The San Francisco municipal transportation agency, San Francisco county transportation authority and the mayor’s office of disability authored a letter pushing back on the commission’s plan.\nThe city has recorded “dozens – perhaps hundreds or thousands – of incidents” in which autonomous vehicles were reported to drive erratically, block traffic and interfere with emergency response operations, officials wrote.\nLast year, a viral video showed police stopping one of Cruise’s autonomous vehicles for driving without headlights. When police pulled the car over, the driverless vehicle sped away to the other side of the intersection. In another incident, nearly 20 of Cruise’s self-driving cars blocked traffic for more than two hours, forcing the company’s employees to physically move the cars off the street.\nSign up to Headlines US\nFree newsletter\nGet the most important US headlines and highlights emailed direct to you every morning\nPrivacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.\nOther incidents include autonomous vehicles trapping other cars at the curb, entering construction zones and crime scenes and failing to follow directions from traffic control officers. San Francisco officials also pointed to a lack of data transparency and adequate reporting and monitoring as reasons for the commission to deny the companies’ requests.\n“This approach is backwards and is inconsistent with the commission’s power and duty to protect not only passenger safety but the safety of the general public,” they wrote.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"A Waymo self-driving car killed a small dog in San Francisco while in 'autonomous mode' last month. The collision occurred just before 11 am on Toland Street in San Francisco with a test driver in the front seat. Waymo's initial review indicated that the system correctly identified the dog but 'was not able to avoid contact' due to the speed of the dog. The car, a Jaguar, sustained minor damage. The incident has raised concerns over autonomous vehicle safety and has led to protests against the expansion of Waymo's driverless taxi services in the city.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/us-news\/2023\/jun\/07\/waymo-car-kills-dog-self-driving-robotaxi-san-francisco","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":32,"Time":"Wed 16 Sep 2020 22.10 CEST","Title":"Safety driver charged in 2018 incident where self-driving Uber car killed a woman","Content":"Prosecutors in Arizona have charged the safety driver behind the wheel of a self-driving Uber test car that struck and killed a woman in 2018 with negligent homicide.\nCourt records show that Rafaela Vasquez, 46, on Tuesday pleaded not guilty in the death of Elaine Herzberg.\nVasquez is the only person facing criminal consequences in the first death of a pedestrian involving a self-driving vehicle, after prosecutors last year said Uber was not criminally liable in the crash.\nFederal regulators, however, have said that while the probable cause of the crash was Vasquez’s failure to monitor the driving environment, Uber’s technology flaws and “insufficient” state policies also played a role.\nHerzberg died in March 2018 after being struck by a self-driving SUV while walking a bicycle across a street at night in Tempe, Arizona.\nUber to bring back self-driving cars in California for first time since 2018 death\nRead more\nTempe police said the self-driving car was driving at about 40 miles an hour in autonomous mode at the time of the crash, and a police report said Vasquez was repeatedly looking down while the car was moving instead of keeping her eyes on the road. Vasquez was the vehicle’s “backup” driver, meaning she sat in the front seat and was responsible for monitoring the car’s movements.\nHerzberg’s death prompted significant safety concerns about the nascent autonomous vehicle industry.\nIn November, the National Transportation Safety Board (NTSB) said that Vasquez “was visually distracted throughout the trip by her personal cell phone”.\nBut the board also faulted Uber for inadequate attention to safety and development decisions that contributed to the crash’s cause, . The software in the modified Volvo XC90, for example, did not properly identify Herzberg as a pedestrian and did not address “operators’ automation complacency”.\nBefore the crash, Uber had also deactivated the automatic emergency braking systems to ensure a smoother ride, relying instead on the backup driver to intervene. In the Tempe crash, the vehicles’s sensors determined that braking was needed only 1.3 seconds before impact, the NTSB reported. Vasquez did not start steering until less than a second before the crash. In an interview with NTSB investigators, the backup driver said she had been monitoring the car’s “self-driving interface”.\nIn addition to raising concerns about the technology’s “ineffective” monitoring of the driver, regulators also determined that Arizona’s safety policies governing self-driving vehicles were “insufficient”. While some states had been cautious about allowing companies to experiment with the new technology on public roads, Arizona lured self-driving car operators to the state by arguing it had fewer regulations than other jurisdictions.\n“We’ve got regulatory failures within the state, and we’ve got, in important ways, real fumbles on the part of Uber, but it’s this individual who is at the center of the homicide case,” said Michael G Bennett, an Arizona State University associate research professor who studies autonomous cars and has closely followed the case. “It strikes me on its face as being profoundly unfair.”\nBennett, who is based in Tempe, said the fatal crash was the result of a systemic breakdown in the introduction of a technology that was sold as the future of safer roads, but that the prosecution at this stage was only focused on accountability for an individual.\n“This woman is taking the fall when the state and a large corporation who invented a large chunk of this technology is taking minimal responsibility, if any at all,” he said. “It’s quite striking that the corporation seems to be walking away without a scar.”\nFriends of Herzberg told the Guardian in 2018 they believed Uber, as well as the Arizona government, should face consequences for the crash. “This shouldn’t have ever happened,” said Carole Kimmerle, a Mesa resident who said she had been friends with Herzberg for more than 10 years and had previously lived with her.\nHerzberg had struggled with homelessness, but was in the process of turning her life around when she was killed, her friends said. They argued the government should have done more to prevent these kinds of crashes.\nUber declined to comment and a lawyer for Vasquez did not immediately respond to a request to comment.\nVasquez was released pending trial, which was set for February 2021.\nReuters contributed to this report","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"In 2018, a self-driving Uber car struck and killed a woman named Elaine Herzberg in Tempe, Arizona. Rafaela Vasquez, the safety driver behind the wheel, has been charged with negligent homicide. Vasquez pleaded not guilty in the case. Federal regulators identified Vasquez's failure to monitor the driving environment as the probable cause of the crash, but also mentioned Uber's technology flaws and 'insufficient' state policies as contributing factors. The National Transportation Safety Board (NTSB) found that Vasquez was visually distracted by her personal cell phone during the trip. Uber was faulted for inadequate attention to safety and development decisions. The software in the modified Volvo XC90 did not properly identify Herzberg as a pedestrian. Uber had deactivated the automatic emergency braking system before the crash, relying on the backup driver to intervene. The crash raised significant safety concerns about the autonomous vehicle industry. Arizona's safety policies governing self-driving vehicles were also deemed 'insufficient' by regulators. Despite these issues, Vasquez is the only person facing criminal consequences in this case. Friends of the victim and some experts have criticized the handling of the case, suggesting that both Uber and the Arizona government should share responsibility.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/us-news\/2020\/sep\/16\/uber-self-driving-car-death-safety-driver-charged","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":37,"Time":"August 28, 2023","Title":"Focus: Tesla braces for its first trial involving Autopilot fatality","Content":"SAN FRANCISCO, Aug 28 (Reuters) - Tesla Inc (TSLA.O) is set to defend itself for the first time at trial against allegations that failure of its Autopilot driver assistant feature led to death, in what will likely be a major test of Chief Executive Elon Musk's assertions about the technology.\nSelf-driving capability is central to Tesla’s financial future, according to Musk, whose own reputation as an engineering leader is being challenged with allegations by plaintiffs in one of two lawsuits that he personally leads the group behind technology that failed. Wins by Tesla could raise confidence and sales for the software, which costs up to $15,000 per vehicle.\nTesla faces two trials in quick succession, with more to follow.\nThe first, scheduled for mid-September in a California state court, is a civil lawsuit containing allegations that the Autopilot system caused owner Micah Lee’s Model 3 to suddenly veer off a highway east of Los Angeles at 65 miles per hour, strike a palm tree and burst into flames, all in the span of seconds.\nThe 2019 crash, which has not been previously reported, killed Lee and seriously injured his two passengers, including a then-8-year old boy who was disemboweled. The lawsuit, filed against Tesla by the passengers and Lee's estate, accuses Tesla of knowing that Autopilot and other safety systems were defective when it sold the car.\nAdvertisement · Scroll to continue\nMUSK 'DE FACTO LEADER' OF AUTOPILOT TEAM\nThe second trial, set for early October in a Florida state court, arose out of a 2019 crash north of Miami where owner Stephen Banner’s Model 3 drove under the trailer of an 18-wheeler big rig truck that had pulled into the road, shearing off the Tesla's roof and killing Banner. Autopilot failed to brake, steer or do anything to avoid the collision, according to the lawsuit filed by Banner's wife.\nAdvertisement · Scroll to continue\nTesla denied liability for both accidents, blamed driver error and said Autopilot is safe when monitored by humans. Tesla said in court documents that drivers must pay attention to the road and keep their hands on the steering wheel.\n\"There are no self-driving cars on the road today,\" the company said.\nThe civil proceedings will likely reveal new evidence about what Musk and other company officials knew about Autopilot’s capabilities – and any possible deficiencies. Banner’s attorneys, for instance, argue in a pretrial court filing that internal emails show Musk is the Autopilot team's \"de facto leader\".\nAdvertisement · Scroll to continue\nTesla and Musk did not respond to Reuters’ emailed questions for this article, but Musk has made no secret of his involvement in self-driving software engineering, often tweeting about his test-driving of a Tesla equipped with \"Full Self-Driving\" software. He has for years promised that Tesla would achieve self-driving capability only to miss his own targets.\nTesla won a bellwether trial in Los Angeles in April with a strategy of saying that it tells drivers that its technology requires human monitoring, despite the \"Autopilot\" and \"Full Self-Driving\" names. The case was about an accident where a Model S swerved into the curb and injured its driver, and jurors told Reuters after the verdict that they believed Tesla warned drivers about its system and driver distraction was to blame.\nAdvertisement · Scroll to continue\nSTAKES HIGHER FOR TESLA\nThe stakes for Tesla are much higher in the September and October trials, the first of a series related to Autopilot this year and next, because people died.\n\"If Tesla backs up a lot of wins in these cases, I think they're going to get more favorable settlements in other cases,\" said Matthew Wansley, a former General Counsel of nuTonomy, an automated driving startup and Associate Professor of Law at Cardozo School of Law.\nOn the other hand, \"a big loss for Tesla - especially with a big damages award\" could \"dramatically shape the narrative going forward,\" said Bryant Walker Smith, a law professor at the University of South Carolina.\nIn court filings, the company has argued that Lee consumed alcohol before getting behind the wheel and that it is not clear whether Autopilot was on at the time of crash.\nJonathan Michaels, an attorney for the plaintiffs, declined to comment on Tesla’s specific arguments, but said \"we're fully aware of Tesla's false claims including their shameful attempts to blame the victims for their known defective autopilot system.\"\nIn the Florida case, Banner's attorneys also filed a motion arguing punitive damages were warranted. The attorneys have deposed several Tesla executives and received internal documents from the company that they said show Musk and engineers were aware of, and did not fix, shortcomings.\nIn one deposition, former executive Christopher Moore testified there are limitations to Autopilot, saying it \"is not designed to detect every possible hazard or every possible obstacle or vehicle that could be on the road,\" according to a transcript reviewed by Reuters.\nIn 2016, a few months after a fatal accident where a Tesla crashed into a semi-trailer truck, Musk told reporters that the automaker was updating Autopilot with improved radar sensors that likely would have prevented the fatality.\nBut Adam (Nicklas) Gustafsson, a Tesla Autopilot systems engineer who investigated both accidents in Florida, said that in the almost three years between that 2016 crash and Banner’s accident, no changes were made to Autopilot’s systems to account for cross-traffic, according to court documents submitted by plaintiff lawyers.\nThe lawyers tried to blame the lack of change on Musk. \"Elon Musk has acknowledged problems with the Tesla autopilot system not working properly,\" according to plaintiffs' documents. Former Autopilot engineer Richard Baverstock, who was also deposed, stated that \"almost everything\" he did at Tesla was done at the request of \"Elon,\" according to the documents.\nTesla filed an emergency motion in court late on Wednesday seeking to keep deposition transcripts of its employees and other documents secret. Banner’s attorney, Lake \"Trey\" Lytal III, said he would oppose the motion.\n\"The great thing about our judicial system is Billion Dollar Corporations can only keep secrets for so long,\" he wrote in a text message.\nReporting by Dan Levine and Hyunjoo Jin in San Francisco, Editing by Peter Henderson and Grant McCool\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here is the response in JSON format:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": [\n        {\n            \"Case\": \"California State Court Trial\",\n            \"Date\": \"Mid-September\",\n            \"Incident\": \"Autopilot system caused owner Micah Lee’s Model 3 to suddenly veer off a highway east of Los Angeles at 65 miles per hour, strike a palm tree and burst into flames, killing Lee and seriously injuring passengers, including an 8-year-old boy who was disemboweled.\",\n            \"Allegations\": \"The lawsuit accuses Tesla of knowing that Autopilot and other safety systems were defective when it sold the car.\"\n        },\n        {\n            \"Case\": \"Florida State Court Trial\",\n            \"Date\": \"Early October\",\n            \"Incident\": \"Owner Stephen Banner’s Model 3 drove under the trailer of an 18-wheeler big rig truck, shearing off the Tesla's roof and killing Banner. Autopilot failed to brake, steer, or avoid the collision.\",\n            \"Allegations\": \"The lawsuit filed by Banner's wife claims that Autopilot failed to take action to avoid the collision.\"\n        },\n        {\n            \"General_Information\": \"Tesla has faced allegations of Autopilot failures in multiple accidents, with the company denying liability and blaming driver error. The trials are expected to reveal more about what Tesla officials knew about Autopilot's capabilities and any possible deficiencies.\"\n        }\n    ]\n}\nThe provided news is indeed relevant to autonomous vehicle failures, specifically related to Tesla's Autopilot system. It discusses two upcoming trials in California and Florida, where accidents involving Autopilot failures resulted in fatalities. The lawsuits allege that Tesla's Autopilot system was defective and that the company may have been aware of these issues. The trials are expected to provide more information about these allegations.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/tesla-braces-its-first-trial-involving-autopilot-fatality-2023-08-28\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":41,"Time":"Published 11:35 AM EDT, Thu September 16, 2021","Title":"Some Tesla owners are losing trust in Elon Musk’s promises of ‘full self-driving’","Content":"Washington, DC\nCNN\n — \nFrustrated Tesla owners continue to wait for “full self-driving,” an expensive and long-delayed software feature that isn’t even guaranteed to help their cars’ resale values. Some of the company’s earliest backers of the “full self-driving” option are even beginning to lose faith in the promise of ever enjoying a truly autonomous Tesla.\nYears-long delays, buggy beta software, and the risk of no return on their investment in the option package have left some Tesla owners disappointed. Tesla CEO Elon Musk’s prognostications, and Tesla’s actual reality have diverged so much that some owners describe to CNN Business that they’ve lost confidence in his predictions. Some otherwise satisfied Tesla owners describe feeling duped into buying “full self-driving” ahead of its polished release, because Musk warned that the price would increase.\nCNN Business interviewed eight Tesla owners to discuss “full self-driving.” The option has been offered since 2016, when Tesla claimed all its new vehicles had the hardware for “full self-driving.” Paying extra for the “full self-driving” software seemed like a worthy investment as Musk claimed it would be finalized in 2018. But while the software’s price has gone from $3,000 to $10,000 since then, most Tesla owners have never come close to experiencing a self-driving Tesla.\nTesla is following in the steps of an unlikely rival: Subaru\nOwners with “full self-driving” today don’t get the big autonomous vision Musk has long promoted, but instead a set of features that drivers with only Autopilot don’t have. They include automated lane changes on highways and “Navigate on Autopilot,” which guides a Tesla from highway on-ramp to off-ramp. There’s also a parking assist feature as well as “smart summon,” in which the car can slowly drive through a parking lot to pick up passengers, and a feature to identify stop signs and traffic lights.\nFor years, Musk made grandiose claims about how soon Tesla’s cars will drive themselves. Musk and Tesla have fallen short of these deadlines repeatedly, but he’s continued to make optimistic predictions.\n“It’s financially insane to buy anything other than a Tesla,” Musk said in April 2019. “If you buy a car that does not have the hardware necessary for full self-driving, it is like buying a horse.”\nElon Musk, at a press event on the grounds of the Tesla Gigafactory on August 13, 2021 in Gr'nheide, Brandenburg,Germany. The first vehicles are to roll off the production line in Gr'nheide near Berlin from the end of 2021. The US company plans to build around 500,000 units of the compact Model 3 and Model Y series here each year.\nRecently Musk himself has questioned the value of a “full self-driving” subscription, which Tesla began also offering this summer.\n“We need to make full self-driving work in order for it to be a compelling value proposition,” Musk said on Tesla’s most recent earnings call. “Right now does it make sense for somebody to do an FSD subscription? I think it’s debatable.”\nLast month Musk said that in late September Tesla owners would be able to access the latest “full self-driving” software, which 2,000 Tesla owners have been testing as of March of this year. But Tesla owners say they have heard similar stories before.\nThis March Musk said that in about 10 days, Tesla owners would have an option to download the latest beta version of the “full self-driving” software. Two weeks later Musk said the option would hopefully arrive in April. Then in April he said the software would hopefully be available in May. In May, Musk said Tesla was “maybe a month or two away … but these things are hard to predict.” Then in August, Musk said the best guess for the public to have access to the latest software would be in about four weeks.\nAlex Costos of Plainville, Connecticut, who bought a Tesla Model 3 in 2018, heard Musk’s predictions. He described himself to CNN Business as a “Tesla fanboy,” and said he used to wear its merchandise everywhere.\nCostos said he bought “full self-driving” software in 2019, thinking he would get good value out of it even though it wasn’t usable yet. But two years later, that money feels lost, he told CNN Business. Costos is not part of the group of Tesla owners testing the beta version of “full self-driving.”\nCostos said he’s close to trading in his Model 3 for a new car and hasn’t yet really been able to enjoy “full self-driving.”\n“It stops at every light whether it’s green or red. That’s not working as intended,” Costos said of Tesla’s publicly available features of “full self-driving,” which generally requires drivers to tap the accelerator to go through an intersection.\nOther Tesla owners have described weaknesses in the company’s autonomous driving technology, such as confusing the moon for a traffic signal, or thinking an advertisement of a person on the back of a bus is a pedestrian.\nOne Tesla owner filed a lawsuit last month in the Albuquerque, New Mexico district court, claiming that Tesla fraudulently concealed its engineering failures and made false and misleading representations about “full self-driving.”\nCostos is especially concerned about the trade-in value of his Model 3. He does not expect dealerships will pay him a premium because his car has “full self-driving.” The software isn’t guaranteed to carry over to a future buyer of the car, making dealers less likely to pay him extra for it, he said. Vroom, a used car website, told CNN Business that there’s no guarantee “full self-driving” will transfer to a new owner, so it doesn’t add any value to models that come in with the feature.\nFord, VW, Nissan, Toyota, Subaru, Honda and GM’s driver-assist packages that compete with Tesla will carry over to new owners of vehicles, company spokespersons told CNN Business.\nCars at charging stations at a Tesla car dealership in Maplewood, Minnesota on June 13, 2021.\n“The longer you’re an owner the more you realize they’re kind of like any other car company. They have all the issues other car companies have, and other ones those companies don’t have,” Costos said.\nTesla generally does not engage with the professional news media and did not respond to a request for comment.\nPushing owners’ buttons\nBrian Self, a Tesla owner in Bristol, Tennessee, told CNN Business that he loves his Model S, but has been unhappy with how Tesla has handled the upcoming broad release of “full self-driving.”\n“He’s put out random tweets for three years that say, ‘Oh, yeah, full self-driving is getting ready to come out,” Self said. “You can’t believe them anymore.”\nSelf said he didn’t think it’s fair that a select group of Tesla owners who have paid for “full self-driving” driving access have received early access to the software. He said he felt that Tesla owners who purchased “full self-driving” should all have the option of receiving the latest version of the software.\nSelf said it appeared Tesla is using the limited release version of “full self-driving” as a form of advertising. Many of the Tesla owners selected to use the beta software have large social media presences.\nTesla first released the beta version of the latest “full self-driving” in October 2020 to a select group. Videos from the beta testers show the technology has limitations and requires vigilant human supervision. But the beta testers have also described that the system is generally improving.\nEven so, it’s gotten to the point that Musk’s delayed timelines have become a joke in the Tesla community.\n“Elon might say, ‘Oh, yeah, we’ll fix this, it’ll be out next week,’” Self said. “And everyone says — ‘Elon time two weeks, actual time two months.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news discusses frustrated Tesla owners who have been waiting for 'full self-driving' software, which has faced years-long delays, buggy beta software, and the risk of no return on their investment. Some owners are losing trust in Elon Musk's promises, as his predictions about self-driving capabilities have not been realized. The article mentions the discrepancy between Musk's optimistic claims and Tesla's actual progress. Some owners feel that they were misled into buying 'full self-driving' ahead of its polished release, and they are disappointed with the current state of the software. The piece also highlights concerns about the trade-in value of Tesla vehicles with 'full self-driving' and how it may not transfer to new owners, unlike competitor brands. Additionally, it mentions that some Tesla owners have filed lawsuits related to 'full self-driving,' and there is frustration among owners regarding the software's release timeline.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2021\/09\/16\/cars\/tesla-fsd-delay\/index.html","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":36,"Time":"Mon 26 Sep 2016 21.36 CEST","Title":"Passengers in Uber's self-driving cars waived right to sue for injury or death","Content":"Anyone requesting an Uber ride in a 12-sq mile area in the center of Pittsburgh might now be randomly allocated a self-driving Ford Fusion rather than a human-operated vehicle.\nBut passengers riding in Uber’s computer-controlled cars today might be surprised at just how experimental the technology is. According to documents obtained by the Guardian under public records laws, until as recently as June anyone not employed by Uber riding in one of its autonomous vehicles (AVs) had to sign a legal document waiving the company of any liability for their injury or death.\nOne senior Pittsburgh police officer signed a waiver on 23 June: “I acknowledge that some or all of the AVs in which I ride are in a development phase, are experimental in nature, and that riding in an AV may involve the potential for death, serious injury, and\/or property loss.”\nThe document was required by Uber’s Advanced Technologies Center (ATC), a research hub that in early 2015 hired dozens of robotics experts from nearby Carnegie Mellon University. Since then the ATC has been working furiously to catch up with the likes of Google and Tesla in self-driving technology – a task Uber CEO Travis Kalanick has described as “basically existential for us”.\nHow driverless cars could change our whole future\nJohn Naughton\nRead more\nOver the summer, ATC ran a “Friends & Family” program that allowed contacts and family members to ride in the same self-driving cars that are now being used in the public trial.\nThe legal document obtained by the Guardian details what might have gone wrong on any ride. “Risks associated with riding in an AV may include, without limitation, those caused by equipment failure, development vehicle operators or other safety drivers, actions of other motorists, weather, temperature, road conditions, negligence or human error,” it says.\n“Nobody has set up software that can reliably drive a car safely without a human,” Kalanick told Bloomberg Businessweek in August. “We are focusing on that.” In the current self-driving Ubers, a human is always in the driver’s seat to cope with technical failures and unexpected situations – such a family of geese crossing the road.\nAt the moment, self-driving Ubers can be hailed between 7am and 10pm, and only in a downtown area that has previously been meticulously mapped to within centimeters. Uber would not confirm whether the pilot program would operate in challenging weather conditions like heavy rain, fog, snow or ice. All rides are currently free during the trial.\nWhile transporting passengers does not materially affect the way autonomous vehicles navigate or drive, it does allow Uber to see how riders react to being driven by a machine.\nUber admits it is filming passengers, inside and outside the car, to inform how it develops the technology. Picking up and dropping off customers may be trickier without a human driver to wave to, for example, or to point out a safe place to stop.\nDuring the Friends & Family program, users were obliged to assume complete responsibility for riding in Uber’s AVs, including “any risks that may arise from the negligence or carelessness of [Uber and the ATC], operation of the AVs and\/or dangerous or defective equipment”. They also had to release the company of liability from beyond the grave, with a clause that stated: “I hereby agree on behalf of myself, my executors, administrators, heirs [and] next of kin”.\nSelf-driving car design needs to involve regulators more, US government argues\nRead more\nPublic passengers today do not have to sign any such waiver, and Uber confirmed to the Guardian that all riders in self-driving Ubers are covered by $5m of accident insurance, in line with pending legislation in Pennsylvania.\n“It’s possible that, by not charging for rides, Uber’s pilot program avoids Pennsylvania’s rules for taxicabs,” says Bryant Walker-Smith, a professor at the University of South Carolina. “Courts generally hold that providers of transport for hire have a heightened responsibility for the safety of their passengers. However, I’d expect a court to hold Uber to a similarly high standard if its program were to result in injury.”\nThe blistering pace of Uber’s technology development has taken even self-driving experts by surprise. Anthony Levandowski, who built Google’s first self-driving car, went on to found an autonomous trucking start-up called Otto. In September, he joined Uber as a vice-president in charge of self-driving technology.\nSpeaking to the Guardian shortly before he was hired, Levandowski said: “Uber’s effort seems to be very logical. But it will take a long time for the tech to penetrate and be ubiquitous everywhere. It could take 25 years.”\nKalanick wants driverless self-driving Ubers much quicker than that. “We are going commercial,” he told Bloomberg Businessweek. “This can’t just be about science.”","Relevant":true,"Overview":"Here is the JSON format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is about passengers in Uber's self-driving cars waiving their right to sue for injury or death. Uber passengers in a 12-square-mile area in the center of Pittsburgh can now be randomly allocated a self-driving Ford Fusion. Passengers riding in Uber’s self-driving cars had to sign a legal document waiving the company of any liability for injury or death. The document acknowledges that the self-driving cars are in a development phase and experimental in nature, with potential risks associated with equipment failure, actions of other motorists, weather, negligence, or human error. Uber has a 'Friends & Family' program for self-driving car trials. Currently, a human driver is always in the driver’s seat to cope with technical failures and unexpected situations. The self-driving Ubers operate between 7 am and 10 pm in a downtown area, and it's unclear if they will operate in challenging weather conditions. Uber films passengers both inside and outside the car to inform technology development. Passengers previously had to assume complete responsibility for riding in Uber’s AVs and release the company of liability.\"\n}\nIn this case, the news is relevant to autonomous vehicle technology, specifically about Uber's self-driving cars and the legal documents passengers had to sign, acknowledging the experimental nature of the technology and potential risks.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/sep\/26\/uber-self-driving-passengers-pittsburgh-injury-death-waiver","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":44,"Time":null,"Title":"NHTSA upgrades probe on Ford's police interceptor vehicles","Content":"By Reuters Staff\n(Reuters) - The National Highway Traffic Safety Administration (NHTSA) is upgrading its investigation on 20,275 Ford Motor Co’s Explorer Police Interceptor vehicles over sudden brake failures.\nIn April, the NHTSA had first opened a preliminary investigation into reports of brake failures in the model year 2015 vehicles.\nThe U.S. vehicle safety regulator is now conducting an engineering analysis, a step required before it can formally demand Ford for a recall.\nThe Sacramento police fleet has reported seven front hose failures in five different Explorer Police Interceptor vehicles used for its emergency vehicle operation course training, according to a document filed with the NHTSA\nNone of the failures resulted in any crashes so far, the document said.\nReporting by Radhika Rukmangadhan in Bengaluru; Editing by Shounak Dasgupta\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The National Highway Traffic Safety Administration (NHTSA) is upgrading its investigation on 20,275 Ford Motor Co’s Explorer Police Interceptor vehicles over sudden brake failures. In April, the NHTSA had first opened a preliminary investigation into reports of brake failures in the model year 2015 vehicles. The U.S. vehicle safety regulator is now conducting an engineering analysis, a step required before it can formally demand Ford for a recall. The Sacramento police fleet has reported seven front hose failures in five different Explorer Police Interceptor vehicles used for its emergency vehicle operation course training, according to a document filed with the NHTSA. None of the failures resulted in any crashes so far, the document said.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-ford-probe-nhtsa-idUSKCN0RW1RJ20151002","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":43,"Time":"JUNE 5, 2014","Title":"GM top executives spared in internal report on safety failure","Content":"By Ben Klayman\nWARREN Mich. (Reuters) - General Motors Co on Thursday issued a report detailing how for 11 years it turned a blind eye to an ignition-switch problem linked to at least 13 deaths but largely pinned the blame on what the report described as incompetent lower-level employees, leaving top brass untouched.\nA General Motors logo is seen on a Denali vehicle for sale at the GM dealership in Carlsbad, California January 4, 2012. REUTERS\/Mike Blake \/Files\nThe report, which will be the subject of upcoming congressional hearings, describes shortcomings of GM engineers, including a failure to understand “how the car was built.” Meanwhile, according to the 325-page report, the highest levels of the company were not made aware.\nProviding a rare peek into the operations of one of the world’s biggest automotive companies, the internal investigation said GM had a long-running corporate culture in which nobody took responsibility for problems.\nThe “GM nod” was how CEO Mary Barra described that culture, “when everyone nods in agreement to a proposed plan of action, but then leaves the room and does nothing,” the document said.\nIn February, GM finally began recalling vehicles for repairs. So far, 2.6 million vehicles have been identified. This recall, coupled with others announced by GM this year, has cost the company about $1.7 billion so far.\nBy 2011, three years before the recalls began, outside lawyers were warning GM’s in-house counsel that they needed to act, the report said.\nBarra said 15 employees found to have “acted inappropriately” have been fired. She did not name all the individuals, but said more than half of them had been in senior or executive roles.\nDuring April congressional hearings, Barra was unable to answer many questions, saying the internal investigation would find answers. But at Thursday’s news conference, she still left some questions unanswered, including why GM redesigned the flawed ignition switch but failed to follow normal procedures of assigning a new part number. That has led some critics to believe someone was covering up the change.\nBarra, who has served as CEO for about five months, said disciplinary action was taken against five others.\nDemocratic Senator Edward Markey of Massachusetts, who is pushing legislation to clamp down on automaker defects, said of the GM report: “We need more than an accounting of past mistakes” and “an internal investigation alone is not nearly enough to ensure that a decade-long tragedy like this never happens again.”\nSince early this year, the Detroit automaker has been enveloped in a scandal over why it took more than a decade to begin recalling low-cost Chevrolet Cobalts, Saturn Ions and other cars with ignition-switch problems that were causing them to stall during operation. Because of the engine stalls, air bags failed to deploy during crashes - some of them fatal - and drivers had difficulty operating their vehicles because power steering and brake systems also malfunctioned.\n“As years passed and fatalities mounted, engineers investigating the non-deployments and attempting to understand their cause were neither diligent nor incisive,” the GM report said.\nAlso infused through the document is the notion that GM engineers misdiagnosed the safety problem by failing to connect dots that would have linked the cars’ system failures.\nBut at the same time, GM “heard over and over from various quarters, including customers, dealers, the press and their own employees that the car’s ignition switch led to moving stalls” but employees “failed to take action or acted too slowly.”\nRISING CRASH COUNT\nSome new details also emerged about fatalities related to GM’s cars.\nThe report said GM had identified 54 frontal-impact crashes, involving the deaths of more than a dozen people, in which air bags did not deploy as a possible result of the faulty ignition switch.\nOnly last week, GM raised the count to 47, from 35, and has now raised it again, leading to questions about whether the 13 deaths linked to the defect will grow, as consumer advocates have predicted.\nAs expected, Barra also confirmed that GM will soon set up a fund to compensate victims of crashes linked to the faulty ignition switches.\nGM officials told reporters that the number of fatalities related to the part defect may rise but added that Kenneth Feinberg, who is looking into victim compensation, will determine that number.\nReuters reported on Monday that at least 74 people have died in crashes similar to those GM has linked to the faulty switches, based on an analysis of government data.\nCentral to the GM investigation, as well as probes by the U.S. Congress, Department of Justice and other federal and state agencies, is why the automaker did not announce vehicle recalls to fix the problem until this past February.\nGM said its investigation, conducted over 70 days, reviewed 41 million documents and more than 230 people were interviewed.\nCONSUMERS NOT PUNISHING GM\nThe move to spare the highest executives from blame drew some sharp criticism.\n“How do you truly fix a culture of carelessness and cover-up without cutting the head off the snake?” said Robert Hilliard, a lawyer for a plaintiff in a lawsuit against GM related to the ignition-switch defect.\nBarra previewed the report at the company’s technical center in Warren, Michigan, where she received a standing ovation from an estimated crowd of more than 1,000 employees. She emphasized that GM already has taken steps to beef up its internal safety operations.\nHer remarks were broadcast to GM’s 220,000 workers globally. Despite all the negative publicity in recent months, GM sales have been robust.\nThe Detroit automaker reported this week that U.S. sales in May were up 13 percent compared with a year ago, making for the best total sales since August 2008.\nWhile Barra noted a pattern of “incompetence and neglect” that she blamed on individuals who failed to “disclose critical pieces of information,” she added that there was “no conspiracy by the corporation to cover up facts.”\nFurthermore, Barra said the internal investigation “found no evidence that any employee made a trade-off between safety and cost” in failing to deal with the safety problem.\nThe full report was made public on Thursday.\n‘DEEPLY DISTURBING’ REPORT\nWith the submission of GM’s internal investigation, Congress is expected to announce a new round of hearings soon. Further details on a compensation fund for victims and their families are expected by early August.\nU.S. Representative Fred Upton, who chairs the House of Representatives’ Energy and Commerce Committee, said the findings of GM’s investigation were “deeply disturbing.”\nUpton, of Michigan, added that his committee will continue its investigation with an eye on “what legislative remedies may be necessary.”\nDemocratic Senator Richard Blumenthal of Connecticut said the GM report “seems like the best report money can buy. It absolves upper management, denies deliberate wrongdoing and dismisses corporate culpability.”\nThe internal report concluded that Barra, executives who reported directly to her, the board of directors and former CEO Dan Akerson did not know about the defective switches before December.\nIt also found that GM’s general counsel, Michael Millikin, was not responsible for the mishandling of defects and the recall delay. Millikin, who led the internal probe with former U.S. prosecutor Anton Valukas, is still employed by GM, Barra said.\nMore than one lawyer in GM’s legal department was fired as part of the 15 employees let go, including senior safety lawyer William Kemp, according to a source familiar with the matter who asked not to be named. A company spokesman declined to identify anyone fired beyond former engineers Ray DeGiorgio, who in 2002 dubbed the flawed part “the switch from hell,” and Gary Altman.\nGM’s use of Valukas came under withering attack because his law firm, Jenner & Block, has had ties to the company since 2002.\nBlumenthal, a former prosecutor, said those ties “undermine the credibility of this report” and underscored the need for federal investigations to press ahead.\nOn May 16, GM was slapped with a $35 million fine for its delayed response to the defect, which is the maximum that can be imposed by the U.S. Department of Transportation. Also, the National Highway Traffic Safety Administration has been leveling a $7,000-per-day fine on GM for missing an April 3 deadline for fully responding to the agency’s request for information about the ignition switch problem. These fines could be dwarfed, however, by potential actions the Justice Department could take. Earlier this year, Toyota Motor Corp was fined $1.2 billion for concealing problems related to sudden acceleration of some of its vehicles.\nThe NHTSA said GM’s probe appeared to support the agency’s findings that company practices “stood in the way of safety at a time when airbags were failing to work properly in millions of GM products.”\nAdditional reporting by Paul Lienert in Detroit and Marilyn Thompson, Susan Heavey and Karey Van Hall in Washington; Writing by Richard Cowan; editing by Lisa Von Ahn and Matthew Lewis\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about General Motors (GM) issuing a report detailing an ignition-switch problem linked to at least 13 deaths. The report largely blames lower-level employees and describes shortcomings of GM engineers, including a failure to understand 'how the car was built.' The report also mentions the 'GM nod' culture, where everyone agrees to a proposed plan of action but does nothing. GM began recalling vehicles for repairs, costing the company about $1.7 billion so far. The report mentions disciplinary action taken against employees, including senior and executive roles. It also highlights the failure of engineers to connect dots related to the safety problem. The report identifies 54 frontal-impact crashes and deaths linked to the faulty ignition switch. GM plans to set up a fund to compensate victims. The report notes criticism for sparing top executives from blame and upcoming congressional hearings.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-gm-recall\/gm-top-executives-spared-in-internal-report-on-safety-failure-idINKBN0EG2JG20140605","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":45,"Time":"July 28, 2023","Title":"Backup driver in 2018 Uber self-driving crash pleads guilty","Content":null,"Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about an autonomous vehicle failure related to a 2018 Uber self-driving crash. The backup driver involved in the incident has pleaded guilty.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/backup-driver-2018-uber-self-driving-crash-pleads-guilty-2023-07-28\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":49,"Time":null,"Title":"Factbox: How United States, others regulate autonomous vehicle testing","Content":"By Reuters Staff\n(Reuters) - An Uber self-driving sport utility vehicle struck and killed a woman crossing a street in Arizona on Sunday, the first fatality involving an autonomous vehicle and a potential blow to the technology expected to transform transportation.\nThe ride services company said it was suspending North American tests of its self-driving vehicles. Safety advocates have called for a national moratorium on the testing of all so-called robot cars on public roads.\nHere is a brief look at how the United States and some other countries regulate testing of autonomous vehicles.\n- So far, 21 U.S. states plus the District of Columbia have enacted legislation related to autonomous vehicles, according to the National Conference of State Legislatures. (here)\n- Nevada was the first to authorize operation of autonomous vehicles in 2011.\n- Arizona has opened its arms to companies testing self-driving vehicles as a means to economic growth and jobs. Republican Governor Doug Ducey reached out to Uber in 2016 after California regulators cracked down on the company over its failure to obtain testing permits.\n- China has issued licenses to automakers that allow self-driving vehicles to be road tested, the state-owned Xinhua news agency has reported. The licenses allow operators to test drive the vehicles on a 5.6-km (3.5-mile) public road in Jiading District of Shanghai, Xinhua said. Shanghai has regulations on road tests for such smart cars and has said it would promote the application and commercialization of vehicles using artificial intelligence technology and internet-linked functions, Xinhua reported.\n- South Korea’s transport ministry began the provisional permit system in February 2016 and has approved provisional permits for 41 self-driving cars as of March including 14 for Hyundai Motor Co 005380.KS, two for Kia Motors 000270.KS , and one for Audi-Volkswagen VOWG_p.DE. Audi-Volkswagen is the only foreign company with a provisional permit in South Korea.\nSouth Korea said it was aware of the Uber incident and that it makes “frequent improvements to the permit system and could make changes in light of the incident going forward.”\n- In Japan, all vehicles on roads require a driver to be sitting in the driver’s seat, according to the Transport Ministry. As long as someone is in the driver’s seat during an automated driving test and has his or her hands close to the wheel at all times, no special permit is needed to conduct self-driving tests.\nA ministry spokesman said of the Uber incident, “We will be following it closely, but it’s too early to say how it will affect our approach to self-driving tests.”\n- Singapore requires that all autonomous test vehicles undergo a vehicle safety assessment before they are approved for on-road trials. Test vehicles can only be driven in autonomous mode within an approved test-site. All test vehicles are required to have a qualified safety driver who is ready to take control, until autonomous vehicle trials demonstrate that the technology is ready for fully autonomous operations. All trial participants are also required to have third-party insurance for test vehicles and must share data.\nCompiled by Matthew Lewis; Reporting by Joseph White in Detroit, Joyce Lee in Seoul, Twinnie Siu in Hong Kong\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the JSON response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"An Uber self-driving sport utility vehicle struck and killed a woman crossing a street in Arizona on Sunday, the first fatality involving an autonomous vehicle and a potential blow to the technology expected to transform transportation. The ride services company said it was suspending North American tests of its self-driving vehicles.\"\n}\nThe news is indeed about an autonomous vehicle incident involving Uber, where a self-driving vehicle struck and killed a woman in Arizona. This information is relevant to autonomous vehicle failures, and the specific details of the incident have been extracted.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-uber-factbox\/factbox-how-united-states-others-regulate-autonomous-vehicle-testing-idUSKBN1GW2PY","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":40,"Time":"Sun 25 Mar 2018 09.00 CEST","Title":"Death by robot: the new mechanised danger in our changing world","Content":"Elaine Herzberg\nWas killed last Sunday by an Uber autonomous car that hit the 49-year-old at approximately 40mph as she was crossing the road in Tempe, Arizona. Police confirmed there was an operator in the Volvo SUV at the time of the collision, and stated that it didn’t appear the car had slowed down.\nCars on the assembly line at Ford’s plant in Flat Rock, Michigan. Photograph: Bill Pugliano\/Getty Images\nRobert Williams\nCited as the first human killed by a robot, Williams was an assembly worker at Ford’s Flat Rock plant in Michigan. He was crushed to death by the arm of a one-tonne robot on the production line on 25 January 1979. His family was awarded $10m in damages. The court concluded that he was struck on the head due to inadequate safety measures.\nRobot-assisted surgery is becoming more common. Photograph: Alamy\nSurgery patients\nWhile robot-assisted surgery has allowed surgeons to perform operations remotely with minimal invasiveness, a 2016 US study found that it was linked to the deaths of 144 people between 2008 and 2013. Causes of death included parts falling into patients’ bodies, machines turning off\/on at the wrong time and loss of video feeds. “Despite widespread adoption of robotic systems… a non-negligible number of technical difficulties and complications are still being experienced during procedures,” the study said.\nA Remotec Andros F-6A bomb-disposal robot similar to the one used to kill Micah Johnson. Photograph: Damian Berg\/US Navy\/EPA\nMicah Johnson\nBelieved to be the first person killed by a police robot. In July 2016, Johnson was involved in a standoff with Dallas law enforcement after fatally shooting five officers and injuring seven others. Police attached a block of C-4 explosive to a bomb-disposal robot and detonated it when it was near the suspect. “We saw no other option,” said the Dallas police chief.\nAn Oerlikon GDF robot cannon. Photograph: Rheinmetall Air Defence\nNine South African soldiers\nIn October 2007, an Oerlikon GDF-005 semi-autonomous anti-aircraft gun jammed and exploded before firing 250 high-explosive rounds at soldiers who were training with it. The weapon is capable of tracking targets and reloading automatically. “I think it is bad luck more than anything else,” one expert told the New Scientist.","Relevant":true,"Overview":"Here's the JSON response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"Elaine Herzberg was killed last Sunday by an Uber autonomous car that hit the 49-year-old at approximately 40mph as she was crossing the road in Tempe, Arizona. Police confirmed there was an operator in the Volvo SUV at the time of the collision, and stated that it didn't appear the car had slowed down.\"\n}\nThe news is indeed relevant to autonomous vehicle failures, and the specific information about the case has been extracted as requested.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/mar\/25\/death-by-robot-mechanised-danger-in-our-changing-world","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":48,"Time":"May 11, 2023","Title":"TuSimple gets Nasdaq delisting notice for failing to file quarterly report","Content":"Companies\nTuSimple Holdings Inc\nMay 11 (Reuters) - TuSimple Holdings Inc (TSP.O) said on Thursday it had received a delisting notice from the Nasdaq for failing to file its quarterly report on time, sending its shares tumbling by about 20% in early trading.\nThe exchange intends to suspend trading in shares of the autonomous driving technology company on May 15 unless it files an appeal, the company said.\nTuSimple last reported quarterly results for the period ended Sept. 30 in late October. It said in March that it failed to file its 2022 annual report on time as it needed more time to hire new principal accountants.\nThe company said on Thursday it had appointed UHY as its new independent registered public accounting firm for 2022.\nThe San Diego-based company announced a restructuring in December and cut about 350 jobs. It removed CEO Xiaodi Hou in October following a board investigation.\nThe probe revealed that some employees spent paid hours last year working for Hydron Inc, a startup working on autonomous trucks mostly in China.\nReporting by Tanya Jain and Akash Sriram in Bengaluru\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here's the response in JSON format based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"TuSimple Holdings Inc received a delisting notice from the Nasdaq for failing to file its quarterly report on time. The exchange intends to suspend trading in shares of the autonomous driving technology company on May 15 unless it files an appeal. TuSimple last reported quarterly results for the period ended Sept. 30 in late October. It said in March that it failed to file its 2022 annual report on time as it needed more time to hire new principal accountants. The company has appointed UHY as its new independent registered public accounting firm for 2022. The San Diego-based company announced a restructuring in December and cut about 350 jobs. It removed CEO Xiaodi Hou in October following a board investigation, which revealed that some employees spent paid hours last year working for Hydron Inc, a startup working on autonomous trucks mostly in China.\"\n}\nThe news is relevant to TuSimple Holdings Inc and discusses various issues related to the company, including its failure to file quarterly reports on time and its restructuring. However, it does not specifically mention any autonomous vehicle failures.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/tusimple-gets-nasdaq-delisting-notice-failing-file-quarterly-report-2023-05-11\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":42,"Time":"10:40 PM GMT+1•January 17, 2022","Title":"New York Times ad warns against Tesla’s “Full Self-Driving”","Content":"cameraImage Credits: The Dawn Project\nA full page advertisement in Sunday’s New York Times took aim at Tesla’s “Full Self-Driving” software, calling it “the worst software ever sold by a Fortune 500 company” and offering $10,000, the same price as the software itself to the first person who could name “another commercial product from a Fortune 500 company that has a critical malfunction every 8 minutes.”\nThe ad was taken out by The Dawn Project, a recently founded organization aiming to ban unsafe software from safety critical systems that can be targeted by military-style hackers, as part of a campaign to remove Tesla Full Self-Driving (FSD)  from public roads until it has “1,000 times fewer critical malfunctions.”\nThe founder of the advocacy group, Dan O’Dowd, is also the CEO of Green Hill Software, a company that builds operating systems and programming tools for embedded safety and security systems. At CES, the company said BMW’s iX vehicle is using its real-time OS and other safety software, and it also announced the availability of its new over-the-air software product and data services for automotive electronic systems.\nDespite the potential competitive bias of The Dawn Project’s founder, Tesla’s FSD beta software, an advanced driver assistance system that Tesla owners can access to handle some driving function on city streets, has come under scrutiny in recent months after a series of YouTube videos that showed flaws in the system went viral.\nThe NYT ad comes just days after the California Department of Motor Vehicles told Tesla it would be “revisiting” its opinion that the company’s test program, which uses consumers and not professional safety operators, doesn’t fall under the department’s autonomous vehicle regulations. The California DMV regulates autonomous driving testing in the state and requires other companies like Waymo and Cruise that are developing, testing and planning to deploy robotaxis to report crashes and system failures called “disengagements. Tesla has never issued those reports.\nTesla CEO Elon Musk has since vaguely responded on Twitter, claiming Tesla’s FSD has not resulted in accident or injury since its launch. The U.S. National Highway Traffic Safety Administration (NHTSA) is investigating a report from the owner of a Tesla Model Y, who reported his vehicle went into the wrong lane while making a left turn in FSD mode, resulting in the vehicle being struck by another driver.\nEven if that was the first FSD crash, Tesla’s Autopilot, the automaker’s ADAS that comes standard on vehicles, has been involved in around a dozen crashes.\nAlongside the NYT ad, The Dawn Project published a fact check of its claims, referring to its own FSD safety analysis that studied data from 21 YouTube videos totaling seven hours of drive time.\nThe videos analyzed included beta versions 8 (released December 2020) and 10 (released September 2021), and the study avoided videos with significantly positive or negative titles to reduce bias. Each video was graded according to the California DMV’s Driver Performance Evaluation, which is what human drivers must pass in order to gain a driver’s license. To pass a driver’s test, drivers in California must have 15 or fewer scoring maneuver errors, like failing to use turn signals when changing lanes or maintaining a safe distance from other moving vehicles, and zero critical driving errors, like crashing or running a red light.\nThe study found that FSD v10 committed 16 scoring maneuver errors on average in under an hour and a critical driving error about every 8 minutes. There as an improvement in errors over the nine months between v8 and v10, the analysis found, but at the current rate of improvement, “it will take another 7.8 years (per AAA data) to 8.8 years (per Bureau of Transportation data) to achieve the accident rate of a human driver.”\nThe Dawn Project’s ad makes some bold claims that should be taken with a grain of salt, particularly because the sample size is far too small to be taken seriously from a statistical standpoint. If, however, the seven hours of footage is indeed representative of an average FSD drive, the findings could be indicative of a larger problem with Tesla’s FSD software and speak to the broader question of whether Tesla should be allowed to test this software on public roads with no regulation.\n“We did not sign up for our families to be crash test dummies for thousands of Tesla cars being driven on the public roads…” the ad reads.\nFederal regulators have started to take some action against Tesla and its Autopilot and FSD beta software systems.\nIn October, NHTSA sent two letters to the automaker targeting the its use of non-disclosure agreements for owners who gain early access to FSD beta, as well as the company’s decision to use over-the-air software updates to fix an issue in the standard Autopilot system that should have been a recall. In addition, Consumer Reports issued a statement over the summer saying the FSD version 9 software upgrade didn’t appear to be safe enough for public roads and that it would independently test the software. Last week, the organization published its test results, which revealed that “Tesla’s camera-based driver monitoring system fails to keep a driver’s attention on the road.” CR found that Ford’s BlueCruise, on the other hand, issues alerts when the driver’s eyes are diverted.\nSince then, Tesla has rolled out many different versions of its v10 software – 10.9 should be here any day now, and version 11 with “single city\/highway software stack” and “many other architectural upgrades” coming out in February,  according to CEO Elon Musk.\nReviews of the latest version 10.8 are skewed, with some online commenters saying it’s much smoother, and many others stating that they don’t feel confident in using the tech at all. A thread reviewing the newest FSD version on the Tesla Motors subreddit page shows owners sharing complaints about the software, with one even writing, “Definitely not ready for the general public yet…”\nAnother commenter said it took too long for the car to turn right onto “an entirely empty, straight road…Then it had to turn left and kept hesitating for no reason, blocking the oncoming lane, to then suddenly accelerate once it had made it onto the next street, followed by a just-as-sudden deceleration because it changed its mind about the speed and now thought a 45 mph road was 25 mph.”\nThe driver said it eventually had to disengage entirely because the system completely ignored an upcoming left turn, one that was to occur at a standard intersection “with lights and clear visibility in all directions and no other traffic.”\nThe Dawn Project’s campaign highlights a warning from Tesla that its FSD “may do the wrong thing at the worst time.”\n“How can anyone tolerate a safety-critical product on the market which may do the wrong thing at the worst time,” said the advocacy group. “Isn’t that the definition of defective? Full Self-Driving must be removed from our roads immediately.”\nNeither Tesla nor The Dawn Project could be reached for comment.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about concerns and criticisms related to Tesla's 'Full Self-Driving' (FSD) software. The Dawn Project, an advocacy organization, took out a full-page advertisement in The New York Times criticizing Tesla's FSD software. The ad offers a reward for finding another commercial product from a Fortune 500 company with as many critical malfunctions. It mentions flaws and issues with Tesla's FSD software, including viral YouTube videos showcasing problems. The California Department of Motor Vehicles expressed concerns about Tesla's test program for autonomous driving. The Dawn Project conducted a study of FSD safety, finding significant errors in FSD version 10, with a critical driving error occurring approximately every 8 minutes. Federal regulators, including NHTSA and Consumer Reports, have also raised concerns about Tesla's FSD software and driver monitoring. The news discusses various versions of Tesla's FSD software and user reviews, highlighting concerns about its readiness for the general public. The Dawn Project calls for the immediate removal of Tesla's FSD from public roads due to safety concerns.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/01\/17\/new-york-times-ad-warns-against-teslas-full-self-driving\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":50,"Time":"Tue 5 Jul 2016 23.44 CEST","Title":"Why self-driving cars aren't safe yet: rain, roadworks and other obstacles","Content":"Last week’s fatal crash involving a Tesla Model S offers a startling reminder that driverless technology is still a work in progress.\nAs Tesla’s own blogpost on the “tragic loss” points out, the autopilot technology that was controlling Joshua Brown’s car when it ploughed into a truck is in a “public beta phase”. That means the software has been released into the wild to be stress-tested by members of the public so that bugs can be flushed out. It’s the kind of approach we are used to seeing when we gain early access to new email applications or virtual reality headsets. As Apple co-founder Steve Wozniak told the New York Times: “Beta products shouldn’t have such life-and-death consequences”.\nUntil the investigation into the tragic incident concludes, we won’t know whether it was caused by a software glitch or human error – particularly with reports suggesting the driver may have been watching a Harry Potter DVD. All we know is that “neither autopilot nor the driver” noticed the white side of the tractor trailer against the brightly lit sky “so the brake was not applied”.\nTesla’s autopilot uses both cameras and radar to detect and avoid obstacles, so in this case we know there must have been a double failure. The cameras struggled with the glare from the sun, while the radar – according to Musk – “tunes out what looks like an overhead road sign to avoid false braking events”.\nElon Musk may have taken to aggressively dismissing coverage of the crash on his Twitter account, but there are still significant every day flaws that presents obstacles to wider adoption of self-driving car technology.\nSensor fusion\nWhen you have multiple sensors giving conflicting information, which one do you defer to? This seemed to be an issue at play in the fatal Tesla crash, where the one sensor that did spot the truck misinterpreted it as a road sign overhead.\n“The big question for driverless car makers is: how does the intelligence of the machine know that the radar sensor is the one to believe? That’s the secret sauce,” says Sridhar Lakshmanan, a self-driving car specialist and engineering professor at the University of Michigan-Dearborn.\nRoadworks\nWhen Delphi sent an autonomous car 3,400 miles across the US in April 2015, engineers had to take control of the car only for a 50-mile stretch. The reason? Unpredictable urban conditions with unmarked lanes and heavy roadworks. In other words, an average city commute.\nSandbags (and assumptions)\nOne of Google’s self-driving cars collided with a public bus in Mountain View in February as it tried to navigate some sandbags on the street. In attempting to move around the sandbags, the car’s left front struck the side of the bus that was trying to overtake. The car had detected the bus but predicted it would yield, and the test driver behind the wheel also made that assumption.\n“Unfortunately, all these assumptions led us to the same spot in the lane at the same time. This type of misunderstanding happens between human drivers on the road every day,” said Google of the incident.\nWeather\nAdverse weather conditions create visibility problems for both people and the sensors that power driverless technology. Rain can reduce the range and accuracy of laser-based Lidar sensors, obscure the vision of on-board cameras and create confusing reflections and glare. In a bid to improve the performance of driverless technology in soggy conditions, Google has started testing its cars on public roads near Seattle, where regular rain is guaranteed.\nHacking\nAs cars become more hi-tech they become more vulnerable to hacking. With driverless vehicles, the extra computers, internet connectivity and sensors increase the possible vulnerabilities. In a proof-of-concept attack, security researcher Jonathan Petit showed that lidar can be easily fooled into detecting a non-existent obstacle using a handheld laser pointer, which can force the car to slow down, stop or swerve.\nHumans\nJust as humans are at fault in more than 90% of car accidents, so too can they be the weakest link in semi-autonomous vehicles – particularly when a functionality labelled as “autopilot” encourages users to place their trust in the machine. “Maybe these intermediate levels [of automation] are not a viable consumer product,” says Richard Wallace, the director of the Transportation Systems Analysis group within the Center for Automotive Research. “They go a little too far in encouraging drivers to check out and yet they aren’t ready to take control.”\nAnd other humans\nIt’s not just the humans inside cars with self-driving technology, but those in other vehicles that need to be vigilant. Accident rates involving driverless cars are twice as high as for regular cars, according to a study by the University of Michigan’s Transportation Research Institute which looked at data from Google, Delphi and Audi.\nHowever the driverless cars weren’t at fault – they are typically hit from behind by inattentive or aggressive humans unaccustomed to self-driving motorists being such sticklers for the road rules. Google has started to programme its cars differently to behave in more familiar, human ways, such as inching forward at a four-way stop to indicate they will be moving next.\nBut it’s this collision where the biggest challenges for technology firms lie, encouraging adoption of rapidly developing new technology for a population that is quirky, unpredictable and, in turn, both sceptical and overtrusting.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news discusses the challenges and safety issues related to self-driving cars. It mentions a fatal crash involving a Tesla Model S, which highlights that autonomous technology is still in a 'public beta phase.' The crash involved the car's autopilot system failing to detect a truck, and it remains unclear whether the cause was a software glitch or human error. The article also discusses challenges such as sensor fusion, roadworks, adverse weather conditions, hacking vulnerabilities, and the role of human drivers in accidents involving self-driving cars.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/jul\/05\/tesla-crash-self-driving-car-software-flaws","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":51,"Time":"Mon 14 Nov 2022 12.00 CET","Title":"Landmark trial involving Tesla autopilot weighs if ‘man or machine’ at fault","Content":"Tesla will play a major role in a manslaughter trial this week over a fatal crash caused by a vehicle operating on autopilot, in what could be a defining case for the self-driving car industry.\nAt the trial’s heart is the question of who is legally responsible for a vehicle that can drive – or partially drive – itself.\nTesla under US criminal investigation over self-driving claims, sources say\nRead more\nKevin George Aziz Riad is on trial for his role in a 2019 crash. Police say Riad exited a freeway in southern California in a Tesla Model S, ran a red light and crashed into a Honda Civic, killing Gilberto Lopez and Maria Guadalupe Nieves-Lopez. Tesla’s autopilot system, which can control speed, braking and steering, was engaged at the time of the crash that killed the couple, who were on their first date.\nTesla does not face charges in the case, but trial could shape public perceptions of the company and act as a test case for whether the technology has advanced faster than legal standards, experts say.\n“Who’s at fault, man or machine?” Edward Walters, an adjunct professor at the Georgetown University law school who specializes in the law governing self-driving cars. “The state will have a hard time proving the guilt of the human driver because some parts of the task are being handled by Tesla.”\nRiad’s lawyer has said that his client should not have been charged with a crime while prosecutors have argued Riad’s speeding and failure to brake were reckless.\nThe trial comes as the electric carmaker faces growing scrutiny and criticism that its autopilot has made drivers inattentive and contributed to accidents and deaths. Elon Musk, the company cofounder, has said that Tesla is significantly more safe when used with its autopilot system, and has touted it as a step to fully autonomous driving.\nIn September, Musk said he believed the company had a “moral obligation” to roll out what he describes as “full self-driving” software, even if it was not perfect and Tesla faced lawsuits, because doing so could save lives.\nBut Tesla’s system has faced ongoing scrutiny and has been implicated in numerous collisions, some of them fatal. US federal regulators are currently investigating more than a dozen Tesla crashes into parked first responder vehicles over a period of four years, resulting in multiple injuries and one death.\nThe US justice department is investigating whether Tesla itself should face criminal charges over its self-driving claims, Reuters reported, which experts have said could pose a challenge to prosecutors in the California trial.\n“The DoJ probe helps [Riad] because his claim is going to be ‘I relied on their advertising. Therefore, I was not aware of the risk there,’” said Robert Blecker, a criminal law professor at New York Law School.\nIn addition to the criminal trial related to the crash, the family of Gilberto Lopez is suing Tesla in a trial scheduled for July.\nTesla’s self-driving technology fails to detect children in the road, group claims\nRead more\n“I can’t say that the driver was not at fault, but the Tesla system, autopilot and Tesla spokespeople encourage drivers to be less attentive,” Donald Slavik, an attorney whose firm is representing Lopez’s family in a lawsuit against Tesla, told Reuters.\nTesla understood the risks of its system but failed to manage those, Slavik said. “Tesla knows people are going to use autopilot and use it in dangerous situations,” he said.\nThe ongoing legal and regulatory scrutiny of Tesla could shape perception of the company, which poses a risk as it looks to defend itself in coming lawsuits, said Bryant Walker Smith, a law professor at the University of South Carolina, who is also an adviser on new transportation technology.\n“The narrative of Tesla potentially shifts from this innovative tech company doing cool things to this company just mired in legal trouble. That is the risk, and narrative is very important in civil litigation because both sides tell a jury a story,” he said.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a landmark trial involving a Tesla autopilot system in a fatal crash in 2019. Kevin George Aziz Riad is on trial for his role in the crash, where a Tesla Model S operating on autopilot ran a red light and crashed into a Honda Civic, killing two people. The trial is central to the question of legal responsibility when a vehicle can partially drive itself, and it could have significant implications for the self-driving car industry. Tesla itself does not face charges in the case, but the trial could shape public perceptions of the company and the advancement of self-driving technology. The trial also comes amid growing scrutiny of Tesla's autopilot system, with allegations that it has made drivers inattentive and contributed to accidents. The US justice department is investigating whether Tesla should face criminal charges over its self-driving claims. Additionally, the family of one of the crash victims is suing Tesla in a separate trial scheduled for July.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2022\/nov\/14\/tesla-autopilot-landmark-case-man-v-machine","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":55,"Time":null,"Title":"Safety driver in fatal Arizona Uber self-driving car crash charged with homicide","Content":"By David Shepardson\nWASHINGTON (Reuters) - The back-up safety driver behind the wheel of a self-driving Uber Technologies UBER.N test vehicle that struck and killed a woman in Tempe, Arizona, in 2018 was charged with negligent homicide, prosecutors said on Tuesday.\nSlideshow ( 2 images )\nRafael Vasquez, age 46, who is also known as Rafaela, pleaded not guilty on Tuesday after being charged in the death of Elaine Herzberg on Aug. 27, court records show. She was released pending trial set for February 2021.\nHerzberg died after she was struck while walking a bicycle across a street at night. The first recorded death involving a self-driving vehicle prompted significant safety concerns about the nascent autonomous vehicle industry.\nUber declined comment. A lawyer for Vasquez did not immediately respond to a request to comment.\nA Tempe police report said Vasquez was repeatedly looking down instead of keeping her eyes on the road. Prosecutors in March 2019 said Uber was not criminally liable in the crash.\n“Distracted driving is an issue of great importance in our community,” said Maricopa County Attorney Allister Adel. “When a driver gets behind the wheel of a car, they have a responsibility to control and operate that vehicle safely.”\nPolice said previously the crash was “entirely avoidable” and that Vasquez was streaming “The Voice” TV program at the time of the crash.\nIn November, the National Transportation Safety Board (NTSB) faulted Vasquez’s inactions and Uber for inadequate attention to safety and decisions in the company’s autonomous vehicle development.\nThe NTSB said the probable cause was Vasquez’s failure to monitor the driving environment “because she was visually distracted throughout the trip by her personal cell phone.” She was supposed to act in the event of an emergency.\nUber made a series of development decisions that contributed to the crash’s cause, the NTSB said. The software in the modified Volvo XC90 did not properly identify Herzberg as a pedestrian and did not address “operators’ automation complacency.”\nUber deactivated the automatic emergency braking systems in the Volvo XC90 vehicle and precluded the use of immediate emergency braking, relying instead on the back-up driver.\nReporting by David Shepardson; Editing by Aurora Ellis and Stephen Coates\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about the safety driver behind the wheel of a self-driving Uber test vehicle being charged with negligent homicide after the vehicle struck and killed a woman named Elaine Herzberg in Tempe, Arizona, in 2018. Rafael Vasquez, also known as Rafaela, pleaded not guilty and is awaiting trial set for February 2021. The incident raised significant safety concerns about the autonomous vehicle industry. The National Transportation Safety Board (NTSB) faulted Vasquez's inactions and Uber for inadequate attention to safety. Vasquez was found to be visually distracted by her personal cell phone during the trip, and Uber's development decisions, including deactivating the emergency braking system and relying on the backup driver, were identified as contributing factors to the crash.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/uber-selfdriving-idUSKBN26708P","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":52,"Time":"Fri 1 Jul 2016 19.43 CEST","Title":"Tesla driver killed while using autopilot was watching Harry Potter, witness says","Content":"The Tesla driver killed in the first known fatal crash involving a self-driving car may have been watching a Harry Potter movie at the time of the collision in Florida, according to a truck driver involved in the crash.\nThe truck driver, Frank Baressi, 62, told the Associated Press that the Tesla driver Joshua Brown, 40, was “playing Harry Potter on the TV screen” during the collision and was driving so fast that “he went so fast through my trailer I didn’t see him”.\nTesla driver dies in first fatal crash while using autopilot mode\nRead more\nThe disclosure raises further questions about the 7 May crash in Williston, Florida, which occurred after Brown put his Model S into Tesla’s autopilot mode, which is able to control a car while it’s driving on the highway.\nThe fatal crash, which federal highway safety regulators are now investigating, is a significant setback and a public relations disaster for the growing autonomous vehicle industry. Tesla Motors Inc’s shares, however, were down less than 1% on Friday in early trading.\nBaressi, who did not immediately respond to requests for comment, said the Harry Potter movie “was still playing when he died and snapped a telephone pole a quarter mile down the road”. He told the AP, however, that he heard the movie but didn’t see it.\nThe Florida highway patrol told Reuters that there was a portable DVD player in the vehicle.\nAccording to Tesla’s account of the crash, the car’s sensor system, against a bright spring sky, failed to distinguish a large white 18-wheel truck and trailer crossing the highway. In a blogpost, Tesla said the self-driving car attempted to drive full speed under the trailer “with the bottom of the trailer impacting the windshield of the Model S”.\nThe disclosure raises further questions about the May crash, which occurred after Joshua Brown put his Model S into Tesla’s autopilot mode, which is able to control a car while it’s driving on the highway. Photograph: Facebook\nThe top of the vehicle was “torn off by the force of the collision”, according to a police report in the local Levy County Journal.\nBaressi was uninjured.\nElon Musk, the CEO of Tesla, tweeted his condolences regarding the “tragic loss”, but the company’s statement deflected blame for the crash. His 537-word statement noted that this was Tesla’s first known autopilot death in roughly 130m miles driven by customers.\n“Among all vehicles in the US, there is a fatality every 94 million miles,” the statement said.\nIt goes on to say that the car’s autonomous software is designed to nudge consumers to keep their hands on the wheels to make sure they’re paying attention. “Autopilot is getting better all the time, but it is not perfect and still requires the driver to remain alert,” the company said.\nNews of the death came just as BMW announced that it is joining forces with US computer chip firm Intel and Mobileye, an Israeli tech company, to develop its own self-driving cars.\nGoogle, Uber and numerous automakers have also tested self-driving technology. BMW said the goal of the new collaboration was to develop cars that would eventually allow them to take their eyes off the road.\nBrown, who owned a technology company called Nexu Innovation, was a Tesla enthusiast who posted videos of his car on autopilot on YouTube. One of them showed his vehicle avoiding a crash on the highway. The footage racked up 1m views after Musk tweeted it.\nOne of his first videos appeared to show Brown temporarily driving with no hands in slow-moving traffic. The Associated Press also reported that records show he received eight speeding tickets in six years.\nSince Tesla introduced the autopilot mode last October, Model S drivers have recorded videos of themselves online pushing the technology to its limits.\nTesla drivers post viral, self-driving 'stunts' using autopilot technology\nRead more\nOn YouTube, there are multiple videos showing drivers with their hands off the wheel. Musk has advised against this. However, one of the videos was uploaded by actress Talulah Riley, Musk’s second wife.\nAt the end of Tesla’s blogpost announcing Brown’s death, the company described the victim, who they did not name, as someone with “a loving family and we are beyond saddened by their loss”.\n“He was a friend to Tesla and the broader [electric vehicle] community, a person who spent his life focused on innovation and the promise of technology and who believed strongly in Tesla’s mission.”\nThe Associated Press contributed reporting","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a Tesla driver who was killed while using autopilot. The driver, Joshua Brown, was reportedly watching a Harry Potter movie on the TV screen during the collision. The collision occurred in Florida, and the truck driver, Frank Baressi, mentioned that Brown was driving so fast that he didn't see him. The crash raised questions about the use of Tesla's autopilot mode, and federal highway safety regulators were investigating the incident. Tesla's sensor system failed to distinguish a large white 18-wheel truck and trailer crossing the highway, resulting in the collision. The top of the vehicle was torn off by the force of the collision. Elon Musk, the CEO of Tesla, expressed condolences, but the company's statement noted that it was Tesla's first known autopilot death in roughly 130 million miles driven by customers. The company emphasized that while autopilot was improving, it still required the driver to remain alert.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/jul\/01\/tesla-driver-killed-autopilot-self-driving-car-harry-potter","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":59,"Time":null,"Title":"Factbox - How United States, others regulate autonomous vehicle testing","Content":"By Reuters Staff\n(Reuters) - An Uber self-driving sport utility vehicle struck and killed a woman crossing a street in Arizona on Sunday, the first fatality involving an autonomous vehicle and a potential blow to the technology expected to transform transportation.\nThe ride services company said it was suspending North American tests of its self-driving vehicles. Safety advocates have called for a national moratorium on the testing of all so-called robot cars on public roads.\nHere is a brief look at how the United States and some other countries regulate testing of autonomous vehicles.\n- So far, 21 U.S. states plus the District of Columbia have enacted legislation related to autonomous vehicles, according to the National Conference of State Legislatures. (here)\n- Nevada was the first to authorize operation of autonomous vehicles in 2011.\n- Arizona has opened its arms to companies testing self-driving vehicles as a means to economic growth and jobs. Republican Governor Doug Ducey reached out to Uber in 2016 after California regulators cracked down on the company over its failure to obtain testing permits.\n- China has issued licenses to automakers that allow self-driving vehicles to be road tested, the state-owned Xinhua news agency has reported. The licenses allow operators to test drive the vehicles on a 5.6-km (3.5-mile) public road in Jiading District of Shanghai, Xinhua said. Shanghai has regulations on road tests for such smart cars and has said it would promote the application and commercialization of vehicles using artificial intelligence technology and internet-linked functions, Xinhua reported.\n- South Korea’s transport ministry began the provisional permit system in February 2016 and has approved provisional permits for 41 self-driving cars as of March including 14 for Hyundai Motor Co, two for Kia Motors , and one for Audi-Volkswagen. Audi-Volkswagen is the only foreign company with a provisional permit in South Korea.\nSouth Korea said it was aware of the Uber incident and that it makes “frequent improvements to the permit system and could make changes in light of the incident going forward.”\n- In Japan, all vehicles on roads require a driver to be sitting in the driver’s seat, according to the Transport Ministry. As long as someone is in the driver’s seat during an automated driving test and has his or her hands close to the wheel at all times, no special permit is needed to conduct self-driving tests.\nA ministry spokesman said of the Uber incident, “We will be following it closely, but it’s too early to say how it will affect our approach to self-driving tests.”\n- Singapore requires that all autonomous test vehicles undergo a vehicle safety assessment before they are approved for on-road trials. Test vehicles can only be driven in autonomous mode within an approved test-site. All test vehicles are required to have a qualified safety driver who is ready to take control, until autonomous vehicle trials demonstrate that the technology is ready for fully autonomous operations. All trial participants are also required to have third-party insurance for test vehicles and must share data.\nCompiled by Matthew Lewis; Reporting by Joseph White in Detroit, Joyce Lee in Seoul, Twinnie Siu in Hong Kong\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"An Uber self-driving sport utility vehicle struck and killed a woman crossing a street in Arizona on Sunday, the first fatality involving an autonomous vehicle and a potential blow to the technology expected to transform transportation. The ride services company said it was suspending North American tests of its self-driving vehicles.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/uk-autos-selfdriving-uber-factbox-idUKKBN1GW2RE","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":57,"Time":"Updated 4:49 PM EST, Thu February 16, 2023","Title":"Tesla recalling nearly 363,000 vehicles equipped with ‘Full Self-Driving’","Content":"New York\nCNN\n — \nTesla is recalling all 363,000 US vehicles with its so-called “Full Self Driving” driver assist software due to safety risks, another blow to the feature that is central to the automaker’s business model.\n“Full self-driving,” as it currently stands, navigates local roads with steering, braking and acceleration, but requires a human driver prepared to take control at any moment, as the system makes judgment errors. The National Highway Traffic Safety Administration said that, based on its analysis, Tesla’s FSD feature “led to an unreasonable risk to motor vehicle safety based on insufficient adherence to traffic safety laws.” And it warned FSD could violate traffic laws at some intersections “before some drivers may intervene.”\n“The FSD Beta system may allow the vehicle to act unsafe around intersections, such as traveling straight through an intersection while in a turn-only lane, entering a stop sign-controlled intersection without coming to a complete stop, or proceeding into an intersection during a steady yellow traffic signal without due caution,” said the recall notice, posted on NHTSA’s website.\nTesla will attempt to fix the the FSD feature, which costs $15,000, through an over-the-air software update, the notice added.\nWhile Tesla CEO Elon Musk has not yet commented on the nature or scope of the problem, he tweeted that “the word “recall” for an over-the-air software update is anachronistic and just flat wrong!”\nBut NHTSA said in a statement that “manufacturers must initiate a recall for any repair, including a software update, that remedies an unreasonable risk to safety.” The federal agency said it will “continue to monitor the recall remedies for effectiveness.”\nThe notice said that the problems are present with all cars with the current version of the FSD software, which is available on all four Tesla models, the Model S, Model X, Model 3 and Model Y.\nIt also said Tesla has identified18 reports of incidents received between May 8, 2019, and September 12, 2022, that may be related to the conditions described above. It said Tesla is not aware of any injuries or deaths caused in those incidents. NHTSA itself has identified at least 273 crashes that involved one of Tesla’s driver assist systems.\nA Troubled Development\nFSD is considered key to the company’s basic business plan, given the premiums that drivers pay for the features, and it’s ability to attract buyers to chose Tesla cars in the first place. Tesla and Musk have repeatedly claimed that FSD, even in its current “beta” form, is safer than cars driven solely by humans. He told investors last month that Tesla has collected data from about 100 million miles of drivers using FSD outside of highways.\n“Our published data shows that improvement in safety,” he said. “It’s very clear. So we would not have released the FSD Beta if the safety statistics were not excellent.”\nBut other safety experts have questioned the validity of Tesla’s safety claims. There have been high-profile accidents of Tesla cars using FSD or its more rudimentary predecessor known as “Autopilot.” Some of those accidents included fatalities.\nNHTSA is also investigating that predecessor, Autopilot. That technology combines lane-keeping assist with adaptive cruise control to keep a car in a lane on a highway, as opposed to the promise of “full self-driving,” which Tesla says aims to one day be able to operate a vehicle without human supervision on a city street.\nWhile “this recall seeks to address a specific set of concerns identified by the agency,” NHTSA’s statement said that this recall does not address its earlier investigations. “Accordingly, the agency’s investigation into Tesla’s Autopilot and associated vehicle systems remains open and active.”\nLast month, Tesla disclosed in the company’s annual financial report that it “has received requests from the US Justice Department for documents related to Tesla’s Autopilot and FSD features.”\nMoving Deadlines\nMusk has repeatedly predicted that the company would soon build a truly self-driving car. But it has also repeatedly pushed back its own self-imposed deadlines. Tesla owners have filed a class-action lawsuit over the predictions and missed deadlines, which is still pending.\n“Mere failure to realize a long-term, aspirational goal is not fraud,” Tesla’s lawyers wrote in a November 28 court filing, asking that the suit be dismissed.\nTesla 'full self-driving' triggered an eight-car crash, a driver tells police\nMusk has said for years that the price of “full self-driving” would increase periodically as it develops and moves closer to regulatory approval. He tweeted in May 2020 that when “full self-driving” had that approval, the feature would “probably” be worth more than “$100,000.” But as recently as a July 2021 call with investors, Musk said it was “debatable” that the feature was worth the $10,000 Tesla was charging at that time.\nIn September, when CNN Business spoke with 13 people who have cars with the “full self-driving” beta, the overwhelming majority, 11 people, said they felt it wasn’t worth $15,000. And it’s been the subject of controversy for years, including a recent ad that played during the Super Bowl in a few markets.\nTesla does not appear close to regulatory approval for “full self-driving.” In August of 2022, the California DMV said that the name “full self-driving” is “a deceptive practice” and grounds for suspending or revoking Tesla’s license to sell vehicles in the state.\nTesla, which has disbanded its public relations staff and has not responded to press inquiries for several years, could not be reached for comment.\nCNN’s Matt McFarland contributed to this report.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"Tesla recalling nearly 363,000 vehicles equipped with ‘Full Self-Driving’\",\n\"Summary\": \"Tesla is recalling all 363,000 US vehicles with its so-called “Full Self Driving” driver assist software due to safety risks. The National Highway Traffic Safety Administration (NHTSA) found that Tesla’s FSD feature “led to an unreasonable risk to motor vehicle safety based on insufficient adherence to traffic safety laws.” The recall notice mentions specific issues like unsafe behavior around intersections, including traveling straight through an intersection while in a turn-only lane, entering a stop sign-controlled intersection without coming to a complete stop, and proceeding into an intersection during a steady yellow traffic signal without due caution. Tesla plans to fix the FSD feature through an over-the-air software update. The recall affects all four Tesla models - Model S, Model X, Model 3, and Model Y. There have been 18 reports of incidents related to these conditions, but no injuries or deaths reported. NHTSA is also investigating Tesla’s Autopilot technology, and Tesla is facing requests for documents from the US Justice Department related to Autopilot and FSD features.\"\n}\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2023\/02\/16\/business\/tesla-fsd-recall\/index.html","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":60,"Time":"August 29, 2023","Title":"Tesla faces more questions from feds on Autopilot safety","Content":"Companies\nTesla Inc\nCarparts.Com Inc\nGeneral Motors Co\nShow more companies\nAug 29 (Reuters) - The U.S. auto safety regulator investigating Tesla's Autopilot driver assistance system is demanding an explanation for a software change that allows drivers to keep their hands off the wheel for longer, risking collisions, documents released on Tuesday showed.\nThe National Highway Traffic Safety Administration (NHTSA) ordered Tesla to answer questions about the Autopilot change and produce documents in a so-called special order dated July 26. The NHTSA order did not compel Tesla to recall vehicles.\n\"The resulting relaxation of controls...could lead to greater driver inattention and failure of the driver to properly supervise Autopilot,\" NHTSA said in its letter to Tesla.\nThe NHTSA order underscores the challenges regulators face keeping up with safety-critical vehicle technology that can be changed overnight with a software upgrade delivered over the air.\nNHTSA asked when the software update was introduced, the number of vehicles affected, Tesla's reason for installing, and collisions or near-misses involving vehicles with the software.\nAdvertisement · Scroll to continue\n\"NHTSA is concerned that this feature was introduced to consumer vehicles and, now that the existence of this feature is known to the public, more drivers may attempt to activate it,\" the agency wrote.\nIn April, Chief Executive Elon Musk tweeted that Tesla is gradually reducing the steering wheel nag - alerts aimed at making sure drivers using Tesla's \"Full Self-Driving\" (FSD) system keep their hands on the wheel. He also this week said \"Yeah\" in response to an X message that \"No steering wheel nag will be a *game changer* for FSD user satisfaction.\"\nAdvertisement · Scroll to continue\nloading\nloading\nMusk will soon face a major test of his assertions about the technology's safety when Tesla defends itself later this year in a trial over allegations that failure of its Autopilot driver assistant feature led to death.\nTesla shares were up 7% in late trading Tuesday, outpacing the broader market.\nThe agency's acting head Ann Carlson told Reuters last week that a resolution of the Autopilot investigation will come soon.\nAdvertisement · Scroll to continue\nSpeaking broadly about the problem of driver attention, Carlson said it is \"really important that driver monitoring systems take into account that humans over-trust technology.\"\nMajor automakers are racing each other to deploy technology that allows motorists to take their hands off the wheel under certain conditions.\nGeneral Motors (GM.N), Mercedes-Benz (MBGn.DE) and other automakers are challenging Tesla's Autopilot technology. But those automakers use more robust monitoring systems than Tesla's to assure drivers keep their eyes on the road and are ready to re-take control.\nAdvertisement · Scroll to continue\nThe agency is investigating the performance of Autopilot after identifying more than a dozen crashes in which Tesla vehicles hit stationary emergency vehicles. It is also investigating whether Tesla vehicles adequately ensure drivers are paying attention when using the driver assistance system.\nIn June 2022, NHTSA upgraded an earlier probe of 830,000 Tesla vehicles into an engineering analysis - a required step before it could potentially demand a recall.\nAutopilot is intended to enable cars to steer, accelerate and brake automatically within their lane, while \"Enhanced Autopilot\" and “Full Self-Driving” allow vehicles to do more sophisticated driving tasks. Tesla says those technologies “require active driver supervision and do not make the vehicle autonomous.”\nThe special order requests documents and explanations by Aug. 25 to help with NHTSA's inquiry. Late responses can incur a penalty of $26,315 per day.\nReporting by Hyunjoo Jin and Joseph White; Editing by Peter Henderson and Lisa Shumaker\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab\nJoseph White\nThomson Reuters\nJoe White is a global automotive correspondent for Reuters, based in Detroit. Joe covers a wide range of auto and transport industry subjects, writes The Auto File, a three-times weekly newsletter about the global auto industry. Joe joined Reuters in January 2015 as the transportation editor leading coverage of planes, trains and automobiles, and later became global automotive editor. Previously, he served as the global automotive editor of the Wall Street Journal, where he oversaw coverage of the auto industry and ran the Detroit bureau. Joe is co-author (with Paul Ingrassia) of Comeback: The Fall and Rise of the American Automobile Industry, and he and Paul shared the Pulitzer Prize for beat reporting in 1993.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about Tesla facing questions from the U.S. auto safety regulator regarding changes to its Autopilot driver assistance system. The National Highway Traffic Safety Administration (NHTSA) is seeking an explanation for a software change that allows drivers to keep their hands off the wheel for longer, potentially risking collisions. NHTSA expressed concerns about driver inattention and the failure of drivers to properly supervise Autopilot due to this change. The agency is also interested in details such as when the software update was introduced, the number of affected vehicles, Tesla's reasons for installing it, and any collisions or near-misses involving vehicles with the software. Additionally, the article mentions a trial over allegations that failure of Autopilot led to a death and the ongoing investigation into the performance of Autopilot, particularly in crashes involving stationary emergency vehicles. The news also highlights competition among automakers, including General Motors and Mercedes-Benz, challenging Tesla's Autopilot technology and using more robust monitoring systems to ensure driver attention. Lastly, the article notes that late responses to NHTSA's inquiries can incur penalties.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/nhtsa-issues-special-order-regarding-teslas-driver-monitoring-system-autopilot-2023-08-29\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":54,"Time":"8:55 PM GMT+2•April 11, 2022","Title":"Autonomous Cruise car encounter with police raises policy questions","Content":"cameraImage Credits: Cruise\nNo technology is perfect. Even self-driving cars trained to obey traffic laws are bound to run into issues that cause them to commit a citable offense. Such was the case with a Cruise-operated hatchback in San Francisco last weekend, which was pulled over by local law enforcement for failing to switch on its headlights. While the car came to a stop, as video of the incident shows, there’s policy to be established when it comes to interactions between autonomous vehicles and police.\nOriginally published on Instagram, the video shows the car — one of Cruise’s Chevy Bolts — in the city’s Richmond District pulling over to the side of the road when signaled to do so by an officer, ahead of an intersection. The policeperson walks toward the car and attempts unsuccessfully to open the driver-side door, at which point the Cruise vehicle begins to drive down the road — only to pull over again and activate its hazards. Police approach the car a second time in a presumed effort to figure out how to turn on the headlights.\nWelcome to the future. Cop pulls over driverless car (because no lights?) Then Cruise goes on the lamb. (via https:\/\/t.co\/mtmsIeOAUP) pic.twitter.com\/ecQ5xXuSnS\n— Seth Weintraub (@llsethj) April 10, 2022\nCruise, which a little over two months ago began letting San Francisco residents hail rides in its driverless vehicles, asserts that the pulled-over vehicle acted as intended. The headlights indeed malfunctioned — and have been fixed, according to the company — but the car yielded to police and then pulled over to the “nearest safe location.” One of the officers contacted Cruise after the traffic stop, and no citation was issued.\nBut the episode raises questions about procedure where self-driving vehicles are involved with the police.\nAs it turns out, the vehicle in the video wasn’t entirely autonomous. Cruise in 2019 began testing computer vision and sound detection AI to help its cars respond to emergency vehicles. However, a spokesperson told TechCrunch that Cruise personnel directed the car to pull over at the second location — across the intersection — when it became clear that the car was the subject of a traffic stop and the officer was clear of the car. When asked, the spokesperson declined to reveal whether Cruise vehicles would behave differently if the stop happened on a highway versus a city street, for example, saying only that the company has a team of people to feed Cruise vehicles with information in instances where the vehicles experience problems.\nWe work closely with the SFPD on how to interact with our vehicles, including a dedicated phone number for them to call in situations like this.\n— cruise (@Cruise) April 10, 2022\nThe most common contact people in the U.S. have with police takes the form of a traffic stop. But not all drivers are treated equally. Black drivers are almost twice as likely to be pulled over as white drivers, far too often with deadly consequences. Several experts argue that autonomous cars will de-escalate interactions with the police by, for instance, reducing investigations associated with hit-and-run offenses. But others, including Elizabeth E. Joh, a law professor at UC Davis, expect that autonomous vehicles will increase police powers if police are allowed to seize cars remotely.\n“An autonomous car would be a programmable car. Perhaps speed enforcement will be a thing of the past, either because cars will be programmed to stay within the legal speed limit, or because such violations will be automatically enforced with a ticket sent to drivers’ electronic dashboards,” Joh writes in an essay on the subject. “But even in this version of the future, police will still seize cars. A person inside the autonomous car may have an outstanding arrest warrant. The police may suspect that the car contains contraband or evidence of a crime. By no longer requiring human control, autonomous cars may even encourage more types of crime to take place within them.”\nWhile no company developing autonomous vehicles has indicated that they’d provide law enforcement this sort of access, police might make the case for it, citing the confusion that can arise from incidents like the Cruise traffic stop. Cruise’s isn’t the first autonomous car to get pulled over — that distinction goes to a Google prototype in 2015. But as autonomous vehicle developers increasingly deploy their fleets on public roads, the issue is more likely to come to the fore.\n“The police and the public will interact differently when there is little human involvement in driving,” Joh continues in the essay. “[B]ecause so much of the Fourth Amendment is premised upon human drivers and human police officers, that means that a future of automated car stops will pose novel and difficult questions of law and policy that we should begin to address now.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous vehicles as it discusses a Cruise-operated hatchback, one of Cruise's Chevy Bolts, being pulled over by local law enforcement in San Francisco for failing to switch on its headlights. The incident involved the car coming to a stop, police approaching the vehicle, and the car eventually pulling over again and activating its hazards. Cruise states that the headlights malfunctioned but the car yielded to the police and pulled over to the nearest safe location. The news also mentions that the vehicle in the video wasn't entirely autonomous, as Cruise personnel directed it to pull over at the second location. The incident raises questions about procedures when self-driving vehicles interact with the police and discusses potential future implications of autonomous vehicles in law enforcement.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/04\/11\/autonomous-cruise-car-encounter-with-police-raises-policy-questions\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":61,"Time":"Sat 1 Jul 2017 04.23 CEST","Title":"Volvo admits its self-driving cars are confused by kangaroos","Content":"Volvo’s self-driving car is unable to detect kangaroos because hopping confounds its systems, the Swedish carmaker says.\nThe company’s “Large Animal Detection system” can identify and avoid deer, elk and caribou, but early testing in Australia shows it cannot adjust to the kangaroo’s unique method of movement.\nThe managing director of Volvo Australia, Kevin McCann, said the discovery was part of the development and testing of driverless technology, and wouldn’t pose problems by the time Volvo’s driverless cars would be available in 2020.\nThe road ahead: self-driving cars on the brink of a revolution in California\nRead more\n“Any company that would be working on the autonomous car concept would be having to do the same developmental work,” he said. “We brought our engineers into Australia to begin the exercise of gathering the data of how the animals can move and behave so the computers can understand it more.”\nEarlier this month, Volvo’s Australian technical manager, David Pickett, told the ABC the troubles had arisen because their cars’ object detection systems used the ground as a reference point.\nThis meant a kangaroo’s hopping was making it difficult to judge how close they were.\n“When it’s in the air, it actually looks like it’s further away, then it lands and it looks closer,” he said.\nMcCann added: “Autonomous cars are a continuing development. A driverless car does not yet exist, and developing technology to recognise kangaroos is part of that development.\nVolvo’s Trent Victor runs through some of the features of the self-driving XC90 during a trial in Adelaide. Photograph: Rick Goodman\/AAP\n“We are developing a car that can recognise kangaroos,” he said.\nVolvo’s detection system was designed in Sweden, where it was tested in areas populated with moose, before trials at a nature reserve in Canberra revealed the problem with kangaroos.\nKangaroos cause more accidents than any other animal in Australia – the marsupials are responsible for about 90% of collisions between vehicles and animals – although most are not serious.\nA spokeswoman for Robert Bosch Australia, which develops component technology for driverless cars, said their system could theoretically recognise kangaroos.\n“Although it hasn’t been tested in a kangaroo-specific environment, there was an instance where black swans were interfering, and so they had to build into the car the ability to recognise animals,” Amy Kaa said.\nVolvo plans to release its first autonomous cars by 2020 and has pledged zero fatalities or serious injuries from all its cars by that time.\n“The whole development process has to take in as many variations of conditions as possible,” McCann said. “It’s a fairly drawn-out process. We don’t even refer to it specifically as kangaroo detection, it’s what we call small animal detection.”\nThe carmaker offers now semi-autonomous features in its S90 and XC90 models, which it says give “a taste of the future of autonomous driving”.\nThe cars can automatically maintain a safe distance from the vehicle in front, and spot potential collisions in urban environments. McCann said a feature called “run-off road assist” would keep passengers safe in near-collisions.","Relevant":true,"Overview":"Here is the JSON format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"Volvo admits its self-driving cars are confused by kangaroos. The company’s “Large Animal Detection system” can identify and avoid deer, elk, and caribou but cannot adjust to the kangaroo’s unique method of movement, particularly when it's in the air, which makes it difficult to judge their distance. Volvo is developing technology to recognize kangaroos, and it plans to release its first autonomous cars by 2020 with a goal of zero fatalities or serious injuries.\"\n}\nThe news is relevant to autonomous vehicle failures, specifically related to Volvo's self-driving cars having difficulty detecting and adapting to kangaroo movement patterns.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2017\/jul\/01\/volvo-admits-its-self-driving-cars-are-confused-by-kangaroos","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":67,"Time":null,"Title":"Exclusive: Apple in talks with potential suppliers of sensors for self-driving cars - sources","Content":"By Stephen Nellis\nSAN FRANCISCO (Reuters) - Apple Inc has held talks with at least four companies as possible suppliers for next-generation lidar sensors in self-driving cars, evaluating the companies’ technology while also still working on its own lidar unit, three people familiar with the discussions said.\nApple's headquarters in Cupertino, California in a 2017 aerial photo. REUTERS\/Noah Berger\nThe moves provide fresh evidence of Apple’s renewed ambitions to enter the autonomous vehicle derby, an effort it calls Project Titan. The talks are focused on next-generation lidar, a sensor that provides a three-dimensional look at the road.\nApple is seeking lidar units that would be smaller, cheaper and more easily mass produced than current technology, the three people said. The iPhone maker is setting a high bar with demands for a “revolutionary design,” one of the people familiar with the talks said. The people declined to name the companies Apple has approached.\nThe sensor effort means Apple wants to develop the entire chain of hardware to guide autonomous vehicles and has joined automakers and investors in the race to find winning technologies.\nCurrent lidar systems, including units from Velodyne Inc mounted on Apple’s fleet of self-driving test vehicles, use laser light pulses to render precise images of the environment around the car. But the systems can cost $100,000 and use mechanical parts to sweep the laser scanners across the road.\nThat makes them too bulky and prone to failure for use in mass-produced vehicles. The shortcomings have spurred $1 billion in investment at dozens of startups and mature companies alike to make lidar smaller, cheaper and more robust.\nApple’s interest in next-generation lidar sensors comes as it has sharply increased its road testing while bringing on key hires from Tesla Inc and Alphabet Inc’s Google.\nIt remains unclear whether the goal of Apple’s Project Titan is to build its own vehicle or supply the hardware and software elements of self-driving car while pairing with a partner for the entire vehicle.\nBut what is clear from Apple’s interest in cheaper lidar systems is that it wants to control the “perception stack” of sensors, computers and software to drive an autonomous vehicle, regardless of who makes the vehicle, another person familiar with the talks said. The three people familiar with the talks declined to be identified because the discussions are not public.\nIn addition to evaluating potential outside suppliers, Apple is believed to have its own internal lidar sensor under development, two of the people said.\nAlphabet-owned Waymo has taken a similar path, assembling a sensor and computer system while inking deals to buy vehicles from Fiat Chrysler Automobiles.\nApple gets “a lot of optionality by working on the perception stack,” said the second person familiar with the talks. “Bringing a passenger car to the market is really, really hard, and there’s no reason right now they need to jump into it.”\nREDUCING COSTS\nThe designs Apple is seeking could potentially be made with conventional semiconductor manufacturing techniques, all four people familiar with the talks said.\nThat has the potential to lower prices from the many thousands to the hundreds of dollars as the sensors are produced in larger numbers, similar to chips in phones and other devices. Apple also wants sensors that can see several hundred meters (yards) down the road.\nThe long-distance requirement shows Apple is interested in fully self-driving vehicles, versus the more limited features such as adaptive cruise control used today, two people familiar with the matter said.\n“They’re not happy with most of what they see,” the first person familiar with the matter said. “They’re looking for a revolutionary design.”\nA third person familiar with the matter said Apple is seeking a “design-oriented” sensor that would be sleek and unobtrusive enough to fit into the overall lines of a vehicle.\nApple declined to comment.\nApple once investigated building its own vehicle. The company had a team of more than a dozen engineers dedicated to detailed work such as ensuring doors closed quietly instead of slamming shut, a fourth person briefed on the matter said.\nApple last year re-hired Doug Field, an Apple veteran who was serving as Tesla’s engineering chief, to work on Project Titan. The project has about 1,200 people, according to a count in court documents.\nField has been putting his stamp on the effort, laying off about 190 workers but also bringing on key hires such as Michael Schwekutsch, who oversaw electric drive train technology at Telsa. Apple also ramped up its testing miles in California, driving nearly 80,000 last year compared to 800 the year before.\nReporting by Stephen Nellis in San Francisco; Editing by Greg Mitchell and Cynthia Osterman\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the JSON format response based on the provided news article:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"Apple Inc has held talks with at least four companies as possible suppliers for next-generation lidar sensors in self-driving cars. The talks are focused on next-generation lidar, a sensor that provides a three-dimensional look at the road. Apple is seeking lidar units that would be smaller, cheaper and more easily mass-produced than current technology. Apple's interest in next-generation lidar sensors comes as it has sharply increased its road testing while bringing on key hires from Tesla Inc and Alphabet Inc’s Google. In addition to evaluating potential outside suppliers, Apple is believed to have its own internal lidar sensor under development. The designs Apple is seeking could potentially be made with conventional semiconductor manufacturing techniques, potentially lowering prices from the many thousands to the hundreds of dollars as the sensors are produced in larger numbers. Apple also wants sensors that can see several hundred meters (yards) down the road.\"\n}\nThe news article discusses Apple's talks with potential suppliers for next-generation lidar sensors in self-driving cars. It provides specific information about Apple's interest in developing lidar technology that is smaller, cheaper, and more easily mass-produced. Additionally, it mentions Apple's increased road testing and key hires in the autonomous vehicle field.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-apple-autonomous-exclusive-idUSKCN1RT16V","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":69,"Time":"June 8, 2023","Title":"Focus: UK self-driving car dreams could break down at the starting line","Content":"Companies\nAxa Sa\nBp Plc\nMicrosoft Corp\nShow more companies\nLONDON, June 8 (Reuters) - Britain could lose out on investments in autonomous vehicles (AVs) and see startups shift testing elsewhere if promised laws to regulate the technology are not passed before the next general election, startups and insurance companies said.\nDespite the government's vision to be a world leader in AV technology, truly driverless cars are not currently permitted on Britain's roads, making it difficult for start-ups to commercialise their vehicles and insurers to assess their risks.\nThe UK government said last August it would advance a bill in the current parliamentary session, which is expected to end this autumn, providing detailed regulations by 2025.\nThat has not yet happened, with political turmoil forcing the government to water down ambitions for this session.\nWith a national election due by January 2025, industry wants the government to bring forward legislation next session, or risk lengthy delays.\nAdvertisement · Scroll to continue\n\"There is a window of opportunity for the UK to at least be leading on something,\" said Claudio Gienal, head of UK and Ireland operations for global insurer AXA (AXAF.PA), which believes the technology could prevent accidents.\n\"Whoever moves first will have the advantage of attracting investment, skills and expertise.\"\n\"But if you're second or third, why should people come here?\" added Gienal, who wrote to Britain's finance minister in April urging the government to pass a bill in the next parliamentary session.\nAdvertisement · Scroll to continue\nThe British government's vision is built on forecasts that by 2035 around 40% of new cars could have self-driving capabilities and leadership in AV technology could create up to 38,000 new skilled jobs in a market worth 42 billion pounds ($53 billion) to Britain by that date.\nTransport Minister Mark Harper in December said there would no longer be a Transport Bill this session, and did not mention a separate AV bill in an outline of the ministry's legislative agenda.\nAdvertisement ·","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news discusses the potential risk to the UK's autonomous vehicle industry due to delays in passing regulations for self-driving cars. It mentions that truly driverless cars are not currently permitted on Britain's roads, making it difficult for startups to commercialize their vehicles and insurers to assess their risks. The article highlights concerns from industry leaders and insurance companies about the need for legislation to attract investment and expertise to the UK in the autonomous vehicle sector. Additionally, it mentions the government's vision for self-driving cars by 2035, which could create new jobs and contribute significantly to the British economy.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/uk-self-driving-car-dreams-could-break-down-starting-line-2023-06-08\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":65,"Time":"Tue 20 Mar 2018 18.07 CET","Title":"UK self-driving car trials continue despite death of US pedestrian","Content":"Britain is pushing ahead with tests of self-driving cars on public roads despite mounting public concern over safety after a pedestrian was killed by one in the US.\nThe country’s biggest carmaker, Jaguar Land Rover, has been experimenting with autonomous cars on roads in the Midlands and is set to demonstrate more of the cars’ features, including an emergency braking warning system, on urban streets this week.\nGovernment-backed trials using small autonomous vehicles in south London are due to end on Friday, with organisers reporting widespread public unease about the implications for road safety and cybersecurity.\nA self-driving Uber car killed a woman in Tempe, Arizona on Sunday night – the first time a self-driving vehicle has killed someone that was not its occupant. Elaine Herzberg, 49, was wheeling her bicycle when she was struck by the Volvo, and later died of her injuries in hospital.\nUS government safety investigators were sent to examine the crash site and Uber has suspended its test fleets of self-driving cars across the US and Canada.\nPolice in Arizona said initial video footage suggested Herzberg walked out suddenly. One previous death involving autonomous cars, a Tesla Model S owner killed in Florida in 2016 when his car crashed on autopilot, was blamed on the driver’s inattention, but investigators highlighted design flaws in the vehicle.\nMany in the motor and insurance industries expect safety benefits from autonomous cars since more than 90% of accidents involve human error. In 2016, the latest full year for which data is available, 448 pedestrians were killed by vehicles on UK roads, and more than 6,000 in the US. But fears remain over how driverless cars will interact with humans on the roads.\nChristian Wolmar, the author of Driverless Cars: a Road to Nowhere, said the Arizona accident would have a big impact: “We don’t know precisely what happened, but it is clear Uber are worried by withdrawing all their cars. Driverless cars will not be accepted if there is a perception that they are not 100% safe. Of course new technology has blips, but this one, that no one has particularly asked for, is being sold on the basis that it’s so much safer.”\nNoel Sharkey, emeritus professor of artificial intelligence at the University of Sheffield, said: “Autonomous vehicles present us with a great future opportunity to make our roads safer. But the technology is just not ready yet and needs to mature before it goes on the road. Too many mistakes and the public may turn its back on the technology.”\nIn London, members of the public have been using low-speed autonomous pods on cycle paths and walkways around the Greenwich peninsula as the culmination of a three-year Gateway study into people’s responses to driverless technology.\nGateway said that under half of 1,300 public responses were positive about the new technology, with those uncertain or opposed citing cybersecurity and road safety fears. A spokesman said: “The lesson is you absolutely have to build in security and road safety from the get-go.”\nThe pods have had one reported collision, hitting a barrier with the roads minister, Jesse Norman, onboard. He has nonetheless pledged to keep the UK in the vanguard of developing autonomous technology, recently confirming an overhaul of road laws to include self-driving cars. Greenwich is expected to to allow Ford and Jaguar Land Rover autonomous cars on its streets in the next phase of testing.\nGatwick announced it would be testing autonomous vehicles to shuttle staff across the airfield, which it said could lead to “an Uber-like service” for ground staff to hail.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous vehicle failures. It mentions a self-driving Uber car killing a woman in Tempe, Arizona, and the suspension of Uber's test fleets of self-driving cars across the US and Canada. It also discusses concerns and public unease about the implications for road safety and cybersecurity related to autonomous vehicles.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/mar\/20\/uk-self-driving-car-trials-to-continue-despite-fatal-collision-in-us","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":72,"Time":"8:43 PM GMT+2•July 1, 2022","Title":"Behind the scenes of Waymo’s worst automated truck crash","Content":"cameraImage Credits: Waymo\nThe most serious crash to date involving a self-driving truck might have resulted in only moderate injuries, but it exposed how unprepared local government and law enforcement are to deal with the new technology.\nOn May 5, a Class 8 Waymo Via truck operating in autonomous mode with a human safety operator behind the wheel was hauling a trailer northbound on Interstate 45 toward Dallas, Texas. At 3:11 p.m., just outside Ennis, the modified Peterbilt was traveling in the far right lane when a passing truck and trailer combo entered its lane.\nThe driver of the Waymo Via truck told police that the other semi truck continued to maneuver into the lane, forcing Waymo’s truck and trailer off the roadway. She was later taken to a hospital for injuries that Waymo described in its report to the National Highway Traffic Safety Administration as “moderate.” The other truck drove off without stopping.\nWhile Waymo’s autonomous semi truck was not at fault in the hit and run, the incident highlights gaps in reporting mechanisms, and raises questions about how ready the public and law enforcement are to cope with heavy, fast-moving vehicles that have no human driver.\nThe stakes for the autonomous trucking industry, which is still in its infancy, couldn’t be any higher. One crash, even if the company is not at fault, could tarnish the public’s image of the technology. \nWaymo’s trucking origins\nWaymo started testing its driverless technology with semi trucks in 2017, beginning in California and Arizona. At the time, it was in the middle of an epic legal battle with Uber over technology allegedly taken from Waymo by engineer Anthony Levandowski, and subsequently purchased by Uber as part of self-driving truck startup Otto.\nWaymo’s self-driving trucks, which are part of a delivery and logistics division the company calls Waymo Via, rely on similar technologies to its robotaxis: a suite of sensors, including cameras, radars and lidars, and powerful on-board computers. All have qualified truck drivers — known as autonomous specialists — in the driver’s seat.  \nIn 2018, Waymo began hauling freight in Georgia, and it branded its delivery business Waymo Via in 2020. It then expanded into New Mexico and Texas, and inked deals with logistics companies like J.B. Hunt, UPS and C.H. Robinson. Earlier this month, it committed to a long-term strategic partnership with Uber and announced a pilot delivery program with home goods e-tailer Wayfair.\nThat pilot is due to start in July on the same stretch of I-45 highway where the May crash occurred. \nInside the crash\nUsing reports from local police and the Department of Transportation, and data supplied by Waymo to NHTSA, TechCrunch has attempted to reconstruct the worst self-driving truck crash on U.S. roads to date.\nAccording to Waymo, the Peterbilt 579 truck was not carrying freight for any customers or partners; it was conducting “standard” testing with a weighted load.\nBehind the wheel was a 40-year-old autonomous specialist with a decade of truck driving experience; there was also a software operator on board. Like many workers in Waymo vehicles, both were actually employed by Transdev, a multinational transit and mobility company.\nAlthough the ultimate aim of automated trucks is to eliminate, or at least greatly reduce, staffing costs, self-driving truck startups today operate with a safety driver and an engineer or technician on board.\nWaymo reported that its truck was driving in autonomous mode at 62 miles per hour, slightly below the speed limit, when the other truck entered its lane and forced it off the road. \nA Waymo Via self-driving truck was hit by another semi truck in May 2022. Image Credits: Ennis Police Department\nWaymo told TechCrunch that the safety operator did not take control of the truck from its autonomous system. \n“The technology was not a factor, as this collision was caused by a human driver of another vehicle when they crossed the lane line and collided with the cab of Waymo’s vehicle and continued driving,” spokesperson Katherine Barna wrote in an email.\nEnnis PD photos, obtained under public records laws, show the Waymo truck and trailer by the side of the highway. They appear to have been prevented from sliding onto a parallel suburban road by a crash barrier. An Ennis police officer noted the truck itself sustained only minor damage: one picture shows damage to the truck’s lidar laser-ranging sensor.\nPhoto of Waymo Via’s lidar sensor, which was damaged in a crash in Texas. Image Credits: Ennis Police Department\nThe driver, however, was taken to a nearby hospital with unspecified, moderate injuries. The attending officer classified the incident as a hit and run. Waymo told TechCrunch that it understands the driver is doing well, following their injury. The driver did not respond to a request from TechCrunch for comment.\nBecause the system was active during at least some of the 30 seconds preceding the collision, Waymo was required to report it to NHTSA, to comply with the agency’s Standing General Order on Crash Reporting for automated vehicles.\nGaps in the system\nThere are no checkboxes on a Texas Department of Transportation crash report to record whether the vehicles involved are operating with full or partial automation, and that information was not recorded in the narrative section of the Waymo crash report.\nEnnis PD Detective Paul Asby, who later investigated the incident, told TechCrunch that he did not know the truck was operating autonomously at the time of the collision.\nAt the hospital, the Waymo driver told police the hit-and-run vehicle belonged to Helwig Trucking, a local carrier with about 15 trucks. (Waymo also confirmed that the truck’s cameras captured enough details to identify the other vehicle.) Helwig did not respond to a request for comment.\nThe driver left her phone number with the police and was released from the hospital, and the Waymo truck was towed away. Waymo also provided a contact number to the police. Detective Asby was assigned to the case, and quickly established that the crash was the fault of the Helwig driver. He contacted the company to get its side of the story, and its insurance details. But when it came to Waymo, Asby met a wall of silence.\n“I was going to speak to the driver because she was taken to the hospital but I’ve tried to contact her cell phone and it says it’s not a valid number,” he said. “The same thing for the passenger who was in there with her.”\nSubsequent calls to Waymo itself went unanswered. “They never did return my calls. I inactivated the case, but the insurance information is in there if they want it,” he says. “Maybe they’re so rich they don’t care.” \nWaymo told TechCrunch that it is not aware of any attempt by Ennis PD to contact it for information, and that it did not have any need to contact the department itself. \nHow it’s going\nThe Ennis crash is not the only one to have involved a Waymo semi truck. In February, a similar Waymo Peterbilt 579 traveling southbound on Interstate 10 near Sacaton, Arizona, was struck by a box truck traveling in the adjacent lane, and which had just also hit a motor coach. The Waymo vehicle was traveling 50 mph in a 75 mph limit zone. TechCrunch was not immediately able to source a police report detailing the crash; there were no reported injuries.\nIf Waymo had not been required to report the crashes to NHTSA, there is a chance they might never have come to light. The official crash reports gathered by Texas, which has welcomed multiple self-driving truck operations to its highways, appear insufficient to fully record incidents involving driverless vehicles. Local law enforcement has historically been similarly ill-equipped to deal with driving systems instead of driving humans. \nWaymo is trying to close those gaps, says Barna. “Waymo has built the Waymo Driver to interact with First Responders; and has worked closely with public safety officials to ensure the safe introduction of our technology in every market that we operate in,” she told TechCrunch.  “We have a team with decades of law enforcement experience that has provided training to hundreds of officers and firefighters in California, Arizona and Texas detailing best practices for safe interactions with Waymo vehicles.”\n“We’ve got a mountain of work to do integrating these things into society,” said Steve Viscelli, a sociologist at the University of Pennsylvania who studies trucking, and acts as an advisor to Aurora’s self-driving truck effort. “We need to talk a lot more about what they mean for supply chains, for workers and for the highway. There are a lot of people who are going to do stupid and aggressive stuff around them because they don’t like self-driving vehicles.”\nWaymo has told the U.S. Department of Transportation that it has 47 trucks, which have driven more than 1.6 million miles. It would not disclose to TechCrunch how many of those miles were driven under some level of automated control.\nAutomated trucking companies have “got the basic driving stuff down,” says Viscelli. “It’s what happens with the family on vacation and the tire’s off, or when there’s construction that changes the shape of the road, or debris on the highway. It’s when you have confidence in those issues that’s going to determine when they’re on the road. But I would not be surprised to see trucks without drivers on lanes next year.”\nUpdated: TechCrunch updated the article to reflect that Waymo also provided a contact number to the police.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"On May 5, a Class 8 Waymo Via truck operating in autonomous mode with a human safety operator behind the wheel was involved in a crash just outside Ennis, Texas. The Waymo truck was traveling in the far right lane when a passing truck and trailer combo entered its lane, forcing Waymo’s truck and trailer off the roadway. The driver of the Waymo Via truck was taken to the hospital with 'moderate' injuries. The other truck drove off without stopping. Waymo reported that its truck was driving in autonomous mode at 62 miles per hour, slightly below the speed limit, when the collision occurred. The safety operator did not take control of the truck from its autonomous system. The incident highlights gaps in reporting mechanisms and raises questions about the readiness of the public and law enforcement to cope with autonomous vehicles.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/07\/01\/behind-the-scenes-of-waymos-worst-automated-truck-crash\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":76,"Time":"Tue 28 Feb 2023 02.59 CET","Title":"Elon Musk overstated Tesla’s autopilot and self-driving tech, new lawsuit says","Content":"Elon Musk is facing yet another lawsuit as shareholders of Tesla accuse the chief executive and his company of overstating the effectiveness and safety of their electric vehicles’ autopilot and full self-driving technologies.\nTesla recalls 362,000 vehicles over self-driving software flaws that risk crashes\nRead more\nShareholders have alleged in the proposed class-action lawsuit that Tesla defrauded them over four years with false and misleading statements that concealed how its technologies – suspected as a possible cause of multiple fatal crashes – “created a serious risk of accident and injury”. The case was filed on Monday in a San Francisco federal court.\nThe case centers on the financial fallout of Tesla’s failed autopilot features, citing when the company’s share price fell after reports that the National Highway Traffic Safety Administration and the Securities and Exchange Commission had begun investigating the technologies.\nThe share price also fell 5.7% on 16 February 2023 after NHTSA forced a recall of more than 362,000 Tesla vehicles equipped with full self-driving beta software because they could be unsafe around intersections.\n“As a result of defendants’ wrongful acts and omissions, and the precipitous decline in the market value of the company’s common stock, plaintiff and other class members have suffered significant losses and damages,” the complaint said.\nTesla, which does not have a media relations department, did not immediately respond to requests for comment.\nThe suit, which is led by shareholder Thomas Lamontagne seeks unspecified damages for Tesla shareholders from 19 February 2019 to 17 February 2023. Chief financial officer Zachary Kirkhorn and his predecessor Deepak Ahuja are also defendants.\nMusk has been sued in the past over how his decisions affect company shares, including a suit over tweets he sent about taking Tesla private – which shareholders claimed cost them millions of dollars. That case was decided in Musk’s favor earlier this month. In another lawsuit continuing this week, shareholders claim they received “misleading” information that led them to approve an exorbitant pay package for the billionaire.\nThe legal action Musk faces is not limited to Tesla. The executive, who also helms Twitter, SpaceX and Neuralink, is facing lawsuits spanning all of his companies, including a recent suit filed by Twitter shareholders who say the executive’s antics when purchasing the social media firm contributed to volatility in the company’s price.\nAs these suits continue, Musk is expected at Tesla’s 1 March investor day to promote the company’s artificial intelligence capability and plans to expand its vehicle lineup.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a lawsuit against Elon Musk and Tesla, where shareholders accuse them of overstating the effectiveness and safety of Tesla's autopilot and full self-driving technologies. Shareholders allege that Tesla defrauded them over four years with false and misleading statements that concealed how its technologies may have caused multiple fatal crashes, creating a serious risk of accident and injury. The lawsuit also mentions a recall of 362,000 Tesla vehicles equipped with full self-driving beta software due to safety concerns around intersections. The lawsuit seeks unspecified damages for Tesla shareholders from February 19, 2019, to February 17, 2023, and includes defendants like Tesla's CFO and former CFO. Elon Musk has faced previous lawsuits related to his impact on company shares, and this lawsuit is part of a series of legal actions against him across his various companies.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2023\/feb\/27\/elon-musk-tesla-lawsuit-autopilot","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":71,"Time":"Updated 11:23 PM GMT+2, July 28, 2023","Title":"The backup driver in the 1st death by a fully autonomous car pleads guilty to endangerment","Content":"PHOENIX (AP) — The backup Uber driver for a self-driving vehicle that killed a pedestrian in suburban Phoenix in 2018 pleaded guilty Friday to endangerment in the first fatal collision involving a fully autonomous car.\nMaricopa County Superior Court Judge David Garbarino, who accepted the plea agreement, sentenced Rafaela Vasquez, 49, to three years of supervised probation for the crash that killed 49-year-old Elaine Herzberg. Vasquez told police that Herzberg “came out of nowhere” and that she didn’t see Herzberg before the March 18, 2018, collision on a darkened Tempe street.\nVasquez had been charged with negligent homicide, a felony. She pleaded guilty to an undesignated felony, meaning it could be reclassified as a misdemeanor if she completes probation.\nAuthorities say Vasquez was streaming the television show “The Voice” on a phone and looking down in the moments before Uber’s Volvo XC-90 SUV struck Herzberg, who was crossing with her bicycle.\nRELATED COVERAGE\nCongress urged to revive long-stalled debate about regulating self-driving vehicles\nTesla shouldn’t call driving system Autopilot because humans are still in control, Buttigieg says\nCruise wants to test self-driving cars all over California\nVasquez’s attorneys said she was was looking at a messaging program used by Uber employees on a work cellphone that was on her right knee. They said the TV show was playing on her personal cellphone, which was on the passenger seat.\nDefense attorney Albert Jaynes Morrison told Garbarino that Uber should share some blame for the collision as he asked the judge to sentence Vasquez to six months of unsupervised probation.\n“There were steps that Uber failed to take,” he said. By putting Vasquez in the vehicle without a second employee, he said. “It was not a question of if but when it was going to happen.”\nProsecutors previously declined to file criminal charges against Uber, as a corporation. The National Transportation Safety Board concluded Vasquez’s failure to monitor the road was the main cause of the crash.\n“The defendant had one job and one job only,” prosecutor Tiffany Brady told the judge. “And that was to keep her eyes in the road.”\nMaricopa County Attorney Rachel Mitchell said in a statement after the hearing that her office believes the sentence was appropriate “based on the mitigating and aggravating factors.”\nThe contributing factors cited by the NTSB included Uber’s inadequate safety procedures and ineffective oversight of its drivers, Herzberg’s decision to cross the street outside of a crosswalk and the Arizona Department of Transportation’s insufficient oversight of autonomous vehicle testing.\nThe board also concluded Uber’s deactivation of its automatic emergency braking system increased the risks associated with testing automated vehicles on public roads. Instead of the system, Uber relied on the human backup driver to intervene.\nIt was not the first crash involving an Uber autonomous test vehicle. In March 2017, an Uber SUV flipped onto its side, also in Tempe when it collided with another vehicle. No serious injuries were reported, and the driver of the other car was cited for a violation.\nHerzberg’s death was the first involving an autonomous test vehicle but not the first in a car with some self-driving features. The driver of a Tesla Model S was killed in 2016 when his car, operating on its Autopilot system, crashed into a semitrailer in Florida.\nNine months after Herzberg’s death, in December 2019, two people were killed in California when a Tesla on Autopilot ran a red light, slammed into another car. That driver was charged in 2022 with vehicular manslaughter in what was believed to be the first felony case against a motorist who was using a partially automated driving system.\nIn Arizona, the Uber system detected Herzberg 5.6 seconds before the crash. But it failed to determine whether she was a bicyclist, pedestrian or unknown object, or that she was headed into the vehicle’s path, the board said.\nThe backup driver was there to take over the vehicle if systems failed.\nThe death reverberated throughout the auto industry and Silicon Valley and forced other companies to slow what had been a fast march toward autonomous ride-hailing services. Uber pulled its self-driving cars out of Arizona, and then-Gov. Doug Ducey prohibited the company from continuing its tests of self-driving cars.\nVasquez had previously spent more than four years in prison for two felony convictions — making false statements when obtaining unemployment benefits and attempted armed robbery — before starting work as an Uber driver, according to court records.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The backup Uber driver for a self-driving vehicle that killed a pedestrian in suburban Phoenix in 2018 pleaded guilty Friday to endangerment in the first fatal collision involving a fully autonomous car. The driver, Rafaela Vasquez, 49, was sentenced to three years of supervised probation for the crash that killed 49-year-old Elaine Herzberg. Vasquez had initially been charged with negligent homicide but pleaded guilty to an undesignated felony. The National Transportation Safety Board concluded Vasquez’s failure to monitor the road was the main cause of the crash. The board also cited Uber's inadequate safety procedures and ineffective oversight of its drivers as contributing factors.\"\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/autonomous-vehicle-death-uber-charge-backup-driver-1c711426a9cf020d3662c47c0dd64e35","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":83,"Time":"12:56 AM GMT+2•August 2, 2016","Title":"In-car cameras let autonomous vehicles track passengers as well as pedestrians","Content":"cameraImage Credits: Bloomberg \/ Contributor under a license.\nPowerful sensors and software have made autonomous cars impressively aware of their surroundings — but it isn’t just obstacles, pedestrians, and other cars that a vehicle’s AI needs to watch out for. It also needs to know what’s going on inside itself, and researchers from Fraunhofer are hard at work making that happen.\n“We are expanding sensor technology to the entire interior,” said Fraunhofer’s Michael Voit, who manages the research group, in a company blog post. “Using depth-perception cameras, we capture the vehicle’s interior, identify the number of people, their size and their posture. From this we can deduce their activities.”\nLots going on in this car that the AI would probably want to be aware of.\nThere are plenty of benefits in a car knowing how many passengers it has, where they are, and what they’re doing. In case of emergency — a sensor failure, for instance — the car will know how long it will take for a driver to reach the wheel. If a kid in the back seat slips out of their seatbelt while mom and dad are taking a nap (the privilege of owning a self-driving car), they can be warned. In a collision scenario, the car could steer so that it is struck in an unoccupied quarter. Airbag deployment, too, could be adjusted to the size or position of a passenger.\nOf course, some of these things can be accomplished with existing tech — weight sensors and the like — but more and finer awareness of the passengers is still a worthwhile goal in autonomous vehicles. Work is progressing on identifying various actions and common objects as well — understanding that a person is reaching for the sun visor, or installing a child seat, for example.\nSo far the cameras have only been installed in the R&D company’s driving simulator, but the plan is to put it into a smartened-up minivan for real-world testing.\nFraunhofer is working with Volkswagen Group Research, Bosch, Visteon, and others on this “Intelligent Car Interior” project, and the whole thing is funded by grants from the German government.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news discusses the development of in-car cameras for autonomous vehicles that can track passengers' activities and movements inside the vehicle. These cameras use depth-perception technology to identify the number of passengers, their size, posture, and activities. The information collected can be used for various purposes, including emergency situations, such as sensor failures, alerting passengers, optimizing airbag deployment based on passenger size and position, and improving overall passenger safety. The project involves collaboration with Volkswagen Group Research, Bosch, Visteon, and others and is funded by grants from the German government. The cameras have been installed in an R&D company's driving simulator and are planned to be tested in a real-world smartened-up minivan.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2016\/08\/01\/in-car-cameras-let-autonomous-vehicles-track-passengers-as-well-as-pedestrians\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":91,"Time":"MARCH 22, 2018","Title":"Daughter of Uber autonomous vehicle victim retains lawyer","Content":"By David Schwartz\nPHOENIX, Ariz (Reuters) - The daughter of the woman killed by an Uber [UBER.UL] self-driving vehicle in Arizona has retained a personal injury lawyer, underlying the potential high stakes of the first fatality caused by an autonomous vehicle.\nFILE PHOTO: U.S. National Transportation Safety Board (NTSB) investigators examine a self-driving Uber vehicle involved in a fatal accident in Tempe, Arizona, U.S., March 20, 2018. A women was struck and killed by the vehicle on March 18, 2018. National Transportation Safety Board\/Handout via REUTERS\nThe law firm of Bellah Perez in Glendale, Arizona, said in a statement on Thursday it was representing the daughter of Elaine Herzberg, who died on Sunday night after being hit by the Uber self-driving SUV in the Phoenix suburb of Tempe.\n“As the first pedestrian death involving an Uber autonomous vehicle, the incident has sparked a national debate about the safety of self-driving cars, exposing the technology’s blind spots and raising questions of liability,” the law firm said.\nThe firm did not immediately return phone calls seeking more information.\nFall-out from the accident could stall the development and testing of self-driving vehicles, which are designed to perform far better than human drivers and sharply reduce the number of motor vehicle fatalities that occur each year.\nThe fatality also presents an unprecedented liability challenge because self-driving vehicles, which are still in the development stage, involve a complex system of hardware and software often made by outside suppliers. The specifics of how Uber’s technology operates are not known.\nOn Thursday a group of 10 Democratic senators cited the fatality in Tempe in a letter sent to Uber and 59 other companies including Ford Motor Co F.N and Tesla Inc TSLA.O highlighting how the use of forced arbitration clauses to settle customer disputes would prevent victims of accidents involving self-driving vehicles from exercising their legal rights.\nThe senators, led by Richard Blumenthal, a member of the Senate Commerce, Science and Transportation Committee, noted that Uber includes forced arbitration in its standard terms of service with users.\n“Had the victim been a passenger ... the victim’s family could have been denied recourse through the courts,” the letter read. The senators asked the companies to commit to not using the arbitration clauses in contracts related to self-driving cars.\n“When injury or death does occur, a forced arbitration clause would prevent consumers from exercising their fundamental legal rights as Americans,” the senators wrote.\nMany companies include forced arbitration clauses in contracts with customers, requiring that any disputes be settled in binding arbitration and barring customers from suing in a court of law. Arbitration rulings, generally, cannot be appealed.\nACCIDENT LIABILITY?\nHerzberg, 49, who was homeless, was jay-walking across a divided four-lane road with her bicycle when she was struck while in the far right-hand lane. A video taken from a dash-mounted camera inside the vehicle that was released by Tempe police on Wednesday showed the SUV traveling along a dark street when suddenly the headlights illuminate Herzberg in front of the SUV.\nShe later died from her injuries.\nOther footage showed the human driver who was behind the wheel mostly looking down and not at the road in the seconds before the accident.\nUber, like many other companies testing self-driving vehicles, has a human driver in each vehicle as a monitor and to act as a backup if necessary.\nFew details of the incident have emerged amid the investigations by police and federal safety regulators. Police have said the vehicle, a Volvo XC90 which was operating in autonomous mode, was traveling at about 40 mile per hour (65 km per hour) at the time of the collision and did not appear to brake.\nPolice have said that following their probe, they will submit the case to the Maricopa County Attorney’s office, which will determine if there is any basis for a case for potential criminal prosecution.\nOne key question for investigators will be how the vehicle’s technology failed to notice the pedestrian crossing the street in front of it, despite the darkness. Self-driving cars typically use a combination of sensors, including radar and light-sensing Lidar, to identify objects, including potential obstacles coming into range.\nIn company presentations, Uber has stated its self-driving technology includes sensors that provide a 360-degree view around the vehicle.\nWriting by Alexandria Sage; Additional reporting by David Shepardson in Washington, D.C.; Editing by Leslie Adler\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the response in JSON format:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to autonomous vehicle failures. The article discusses the daughter of a woman who was killed by an Uber self-driving vehicle in Arizona. It highlights the potential high stakes of the first fatality caused by an autonomous vehicle. The incident has sparked a national debate about the safety of self-driving cars and raised questions of liability. The article also mentions that the accident could affect the development and testing of self-driving vehicles and presents an unprecedented liability challenge due to the complex system of hardware and software involved. It further discusses the use of forced arbitration clauses in contracts related to self-driving cars and potential legal challenges. The article provides details about the victim's circumstances, the vehicle's speed, the human driver's actions, and the investigation into how the technology failed to detect the pedestrian.\"\n}\nThe news is relevant to autonomous vehicle failures, and the specific information has been extracted as requested.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-uber-idUSKBN1GY30I","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":88,"Time":"12:33 AM GMT+2•May 8, 2021","Title":"Tesla refutes Elon Musk’s timeline on ‘full self-driving’","Content":"cameraImage Credits: Patrick T. Fallon \/ Bloomberg \/ Getty Images\nWhat Tesla CEO Elon Musk says publicly about the company’s progress on a fully autonomous driving system doesn’t match up with “engineering reality,” according to a memo that summarizes a meeting between California regulators and employees at the automaker.\nThe memo, which transparency site Plainsite obtained via a Freedom of Information Act request and subsequently released, shows that Musk has inflated the capabilities of the Autopilot advanced driver assistance system in Tesla vehicles, as well the company’s ability to deliver fully autonomous features by the end of the year. \nTesla vehicles come standard with a driver assistance system branded as Autopilot. For an additional $10,000, owners can buy “full self-driving,” or FSD — a feature that Musk promises will one day deliver full autonomous driving capabilities. FSD, which has steadily increased in price and capability, has been available as an option for years. However, Tesla vehicles are not self-driving. FSD includes the parking feature Summon as well as Navigate on Autopilot, an active guidance system that navigates a car from a highway on-ramp to off-ramp, including interchanges and making lane changes. Once drivers enter a destination into the navigation system, they can enable “Navigate on Autopilot” for that trip.\nTesla vehicles are far from reaching that level of autonomy, a fact confirmed by statements made by the company’s director of Autopilot software CJ Moore to California regulators, the memo shows.\n“Elon’s tweet does not match engineering reality per CJ,” according to the memo summarizing the conversation between regulators with the California Department of Motor Vehicles’ autonomous vehicles branch and four Tesla employees, including Moore.\nTesla’s decision to scrap its PR department could create a PR nightmare\nThe memo, which was written by California DMV’s Miguel Acosta, states that Moore described Autopilot — and the new features being tested — as a Level 2 system. That description matters in the world of automated driving.\nThere are five levels of automation under standards created by SAE International. Level 2 means two primary functions — like adaptive cruise and lane keeping — are automated and still have a human driver in the loop at all times. Level 2 is an advanced driver assistance system, and has become increasingly available in new vehicles, including those produced by Tesla, GM, Volvo and Mercedes. Tesla’s Autopilot and its more capable FSD were considered the most advanced systems available to consumers. However, other automakers have started to catch up.\nLevel 4 means the vehicle can handle all aspects of driving in certain conditions without human intervention and is what companies like Argo AI, Aurora, Cruise, Motional, Waymo and Zoox are working on. Level 5, which is widely viewed as a distant goal, would handle all driving in all environments and conditions.\nHere is an important bit via Acosta’s summarization:\nDMV asked CJ to address from an engineering perspective, Elon’s messaging about L5 capability by the end of the year. Elon’s tweet does not match engineering reality per CJ. Tesla is at Level 2 currently. The ratio of driver interaction would need to be in the magnitude of 1 or 2 million miles per driver interaction to move into higher levels of automation. Tesla indicated that Elon is extrapolating on the rates of improvement when speaking about L5 capabilities. Tesla couldn’t say if the rate of improvement would make it to L5 by end of calendar year.\nPortions of this commentary were redacted. However, Plainsite was able to copy and paste the redacted part, which shows up as white space on a PDF, into another document.\nThe comments in the memo are contrary to what Musk has said repeatedly in the public sphere.\nMusk is frequently asked on Twitter and in quarterly earnings calls for progress reports on FSD, including questions about when it will be rolled out via software updates to owners who have purchased the option. In a January earnings call, Musk said he was “highly confident the car will be able to drive itself with reliability in excess of a human this year.” In April 2021, during the company’s first quarter earnings call, Musk said “it’s really quite, quite tricky. But I am highly confident that we will get this done.”\nThe memo released this week provided other insights into Tesla’s push to test and eventually unlock greater levels of autonomy, including the number of vehicles testing a beta version of “Navigate on Autopilot on City Streets,” a feature that is meant to handle driving in urban areas and not just highways. Regulators also asked the Tesla employees if and how participants were being trained to test this feature, and how the sales team ensures that messaging about the vehicle capabilities and limitations are communicated.\nWhat is up with Tesla’s value?\nAs of the March meeting, there were 824 vehicles in a pilot program testing a beta version of “city streets.”  About 750 of those vehicles were being driven by employees and 71 by non-employees. Pilot participants are located across 37 states, with the majority of participants in California. As of March 2021, pilot participants have driven more than 153,000 miles using the City Streets feature, the memo states. The memo noted that Tesla planned to expand this pool of participants to approximately 1,600 later that month.\nTesla told the DMV that it is working on developing a video for the participants and that the next group of participants will include referrals from existing participants. “The new participants will be vetted by Tesla by looking at insurance telematics based on the VINs registered to that participant,” according to the memo.\nTesla also told the DMV that it is able to track when there are failures or when the feature is deactivated. Moore described these as “disengagements,” a term also used by companies testing and developing autonomous vehicle technology. The primary difference worth noting here is that these companies only use employees who are trained safety drivers, not the public.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to autonomous vehicle technology. It discusses how Tesla CEO Elon Musk's public statements about the progress of Tesla's autonomous driving systems, including Autopilot and 'full self-driving' (FSD), do not align with the engineering reality. The memo obtained via a Freedom of Information Act request highlights that Tesla's Autopilot is considered a Level 2 system, with two primary functions automated but still requiring human driver intervention. Elon Musk's claims of achieving Level 5 capability by the end of the year are contradicted in the memo, which indicates that Tesla is currently at Level 2 and that significant advancements would be needed to reach higher levels of automation. The memo also mentions the testing of a beta version of 'Navigate on Autopilot on City Streets' and the number of vehicles involved in the pilot program.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2021\/05\/07\/tesla-refutes-elon-musks-timeline-on-full-self-driving\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":93,"Time":"SEPTEMBER 26, 2019","Title":"British Airways owner says pilot strikes to hit profit","Content":"By Laurence Frost, Tanishaa Nadkar\n(Reuters) - British Airways parent IAG ICAG.L warned on Thursday that full-year profit will be hit by a pilots' strike at the UK carrier and weaker sales by budget stablemates Vueling and Level.\nShares in Europe’s third largest airline group tumbled after it said recurring operating profit will fall 215 million euros ($235 million) to 3.27 billion this year in the wake of a strike that grounded thousands of BA flights this month.\nThe Sept. 9-10 strike dealt a heavy blow to British Airways, still reeling from a third major computer failure in two years, which seriously disrupted operations in August.\nPilot unions called off a second walk-out this week but said industrial action was likely to resume unless progress was made on a pay dispute.\n“Any further industrial action will additionally impact IAG’s full-year 2019 operating profit,” the group cautioned.\nShares in the owner of Iberia, Aer Lingus and Vueling were down 3.6% at 462.7 pence as of 1333 GMT.\nIAG’s setbacks add to strains across the industry - including weaker demand, rising fuel costs and low-cost competition - which contributed to travel group Thomas Cook’s collapse and pose a threat to second-tier airlines.\nBut further bankruptcies among competitors will help to support IAG’s growth next year, group Chief Executive Willie Walsh told analysts.\n“A number of weaker airlines are either disappearing or significantly reducing their capacity,” he said, predicting a “slightly softer” environment in 2020.\nFILE PHOTO: British Airways aircraft are seen at Heathrow Airport in west London, Britain, February 23, 2018. REUTERS\/Hannah McKay\/File Photo\nIAG now expects passenger unit revenue to be slightly lower before currency effects, with full-year capacity growing by about 4%, less than the 5% previously planned.\nThe group nevertheless aims to expand into the gap left by Thomas Cook at London’s Gatwick airport by acquiring take-off and landing slots sold off under bankruptcy or reallocated by aviation authorities, Walsh said.\n“We clearly see Gatwick as an opportunity for us for growth purposes,” he said.\nThe BA strikes accounted for 137 million euros of the earnings hit, IAG said, with a further 33 million stemming from industrial action by Heathrow airport staff and 45 million from weaker bookings at Vueling and Level.\nVueling, which competes against European budget carriers easyJet EZJ.L and Ryanair RYA.I, has seen a \"very noticeable change in booking patterns\", with passenger numbers and yield down for October-November, Walsh said.\nLevel, launched two years ago as a low-cost long-haul carrier, has suffered along with other airlines with exposure to crisis-hit Argentina.\nBritish Airways, whose offer of a 11.5% pay rise over three years has been rejected by pilots, has yet to resume talks with their representatives.\nThe BALPA union reacted angrily on Thursday to what it saw as an attempt to blame pilots for the group’s worsening outlook.\n“The BA pilots believe their management is treating them with contempt and are now asking BALPA to consider more strike dates,” it said.\nReporting by Laurence Frost in Paris; Tanishaa Nadkar, Shashwat Awasthi and Aakash Jagadeesh Babu in Bengaluru; Editing by Jane Merriman and Jason Neely\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about British Airways parent company IAG warning of a hit to full-year profit due to a pilots' strike at British Airways and weaker sales by budget stablemates Vueling and Level. The strike in September grounded thousands of BA flights, and the company expects its recurring operating profit to fall by 215 million euros ($235 million) to 3.27 billion this year as a result. British Airways has also faced disruption from a third major computer failure in two years. Pilot unions have called off a second walk-out but said industrial action was likely to resume unless progress was made on a pay dispute. The strikes accounted for 137 million euros of the earnings hit, with further losses stemming from industrial action by Heathrow airport staff and weaker bookings at Vueling and Level.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-iag-outlook-idINKBN1WB0M8","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":77,"Time":"Published 3:42 AM GMT+2, August 17, 2021","Title":"US probing Autopilot problems on 765,000 Tesla vehicles","Content":"DETROIT (AP) — The U.S. government has opened a formal investigation into Tesla’s Autopilot partially automated driving system after a series of collisions with parked emergency vehicles.\nThe investigation covers 765,000 vehicles, almost everything that Tesla has sold in the U.S. since the start of the 2014 model year. Of the crashes identified by the National Highway Traffic Safety Administration as part of the probe, 17 people were injured and one was killed.\nNHTSA says it has identified 11 crashes since 2018 in which Teslas on Autopilot or Traffic Aware Cruise Control have hit vehicles at scenes where first responders have used flashing lights, flares, an illuminated arrow board or cones warning of hazards. The agency announced the action Monday in a posting on its website.\nThe probe is another sign that NHTSA under President Joe Biden is taking a tougher stance on automated vehicle safety than under previous administrations. Previously the agency was reluctant to regulate the new technology for fear of hampering adoption of the potentially life-saving systems.\nThe investigation covers Tesla’s entire current model lineup, the Models Y, X, S and 3 from the 2014 through 2021 model years.\nThe National Transportation Safety Board, which also has investigated some of the Tesla crashes dating to 2016, has recommended that NHTSA and Tesla limit Autopilot’s use to areas where it can safely operate. The NTSB also recommended that NHTSA require Tesla to have a better system to make sure drivers are paying attention. NHTSA has not taken action on any of the recommendations. The NTSB has no enforcement powers and can only make recommendations to other federal agencies.\n“Today’s action by NHTSA is a positive step forward for safety,” NTSB Chair Jennifer L. Homendy said in a statement Monday. “As we navigate the emerging world of advanced driving assistance systems, it’s important that NHTSA has insight into what these vehicles can, and cannot, do.”\nLast year the NTSB blamed Tesla, drivers and lax regulation by NHTSA for two collisions in which Teslas crashed beneath crossing tractor-trailers. The NTSB took the unusual step of accusing NHTSA of contributing to the crash for failing to make sure automakers put safeguards in place to limit use of electronic driving systems.\nThe agency made the determinations after investigating a 2019 crash in Delray Beach, Florida, in which the 50-year-old driver of a Tesla Model 3 was killed. The car was driving on Autopilot when neither the driver nor the Autopilot system braked or tried to avoid a tractor-trailer crossing in its path.\n“We are glad to see NHTSA finally acknowledge our long standing call to investigate Tesla for putting technology on the road that will be foreseeably misused in a way that is leading to crashes, injuries, and deaths,” said Jason Levine, executive director of the nonprofit Center for Auto Safety, an advocacy group. “If anything, this probe needs to go far beyond crashes involving first responder vehicles because the danger is to all drivers, passengers, and pedestrians when Autopilot is engaged.”\nAutopilot has frequently been misused by Tesla drivers, who have been caught driving drunk or even riding in the back seat while a car rolled down a California highway.\nA message was left seeking comment from Tesla, which has disbanded its media relations office. Shares of Tesla Inc., based in Palo Alto, California, fell 4.3% Monday.\nNHTSA has sent investigative teams to 31 crashes involving partially automated driver assist systems since June of 2016. Such systems can keep a vehicle centered in its lane and a safe distance from vehicles in front of it. Of those crashes, 25 involved Tesla Autopilot in which 10 deaths were reported, according to data released by the agency.\nTesla and other manufacturers warn that drivers using the systems must be ready to intervene at all times. In addition to crossing semis, Teslas using Autopilot have crashed into stopped emergency vehicles and a roadway barrier.\nThe probe by NHTSA is long overdue, said Raj Rajkumar, an electrical and computer engineering professor at Carnegie Mellon University who studies automated vehicles.\nTesla’s failure to effectively monitor drivers to make sure they’re paying attention should be the top priority in the probe, Rajkumar said. Teslas detect pressure on the steering wheel to make sure drivers are engaged, but drivers often fool the system.\n“It’s very easy to bypass the steering pressure thing,” Rajkumar said. “It’s been going on since 2014. We have been discussing this for a long time now.”\nThe crashes into emergency vehicles cited by NHTSA began on Jan. 22, 2018 in Culver City, California, near Los Angeles when a Tesla using Autopilot struck a parked firetruck that was partially in the travel lanes with its lights flashing. Crews were handling another crash at the time.\nSince then, the agency said there were crashes in Laguna Beach, California; Norwalk, Connecticut; Cloverdale, Indiana; West Bridgewater, Massachusetts; Cochise County, Arizona; Charlotte, North Carolina; Montgomery County, Texas; Lansing, Michigan; and Miami, Florida.\n“The investigation will assess the technologies and methods used to monitor, assist and enforce the driver’s engagement with the dynamic driving task during Autopilot operation,” NHTSA said in its investigation documents.\nIn addition, the probe will cover object and event detection by the system, as well as where it is allowed to operate. NHTSA says it will examine “contributing circumstances” to the crashes, as well as similar crashes.\nAn investigation could lead to a recall or other enforcement action by NHTSA.\n“NHTSA reminds the public that no commercially available motor vehicles today are capable of driving themselves,” the agency said in a statement. “Every available vehicle requires a human driver to be in control at all times, and all state laws hold human drivers responsible for operation of their vehicles.”\nThe agency said it has “robust enforcement tools” to protect the public and investigate potential safety issues, and it will act when it finds evidence “of noncompliance or an unreasonable risk to safety.”\nIn June, NHTSA ordered all automakers to report any crashes involving fully autonomous vehicles or partially automated driver assist systems.\nTesla uses a camera-based system, a lot of computing power, and sometimes radar to spot obstacles, determine what they are, and then decide what the vehicles should do. But Carnegie Mellon’s Rajkumar said the company’s radar was plagued by “false positive” signals and would stop cars after determining overpasses were obstacles.\nNow Tesla has eliminated radar in favor of cameras and thousands of images that the computer neural network uses to determine if there are objects in the way. The system, he said, does a very good job on most objects that would be seen in the real world. But it has had trouble with parked emergency vehicles and perpendicular trucks in its path.\n“It can only find patterns that it has been quote-unquote trained on,” Rajkumar said. “Clearly the inputs that the neural network was trained on just do not contain enough images. They’re only as good as the inputs and training. Almost by definition, the training will never be good enough.”\nTesla also is allowing selected owners to test what it calls a “full self-driving” system. Rajkumar said that should be investigated as well.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a formal investigation by the U.S. government into Tesla's Autopilot system. The investigation covers 765,000 Tesla vehicles sold in the U.S. since the 2014 model year. It was initiated due to a series of collisions with parked emergency vehicles, resulting in 17 injuries and one fatality. The investigation focuses on 11 crashes since 2018 in which Teslas on Autopilot or Traffic Aware Cruise Control hit vehicles at scenes where first responders were using flashing lights, flares, illuminated arrow boards, or cones warning of hazards. The National Transportation Safety Board (NTSB) has also recommended that NHTSA and Tesla limit Autopilot's use to safe areas and improve systems to ensure driver attention. The probe is seen as a sign of increased focus on automated vehicle safety by NHTSA.\"\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/technology-business-61557d668b646e7ef48c5543d3a1c66c","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":92,"Time":"Mon 19 Mar 2018 23.48 CET","Title":"Self-driving Uber kills Arizona woman in first fatal crash involving pedestrian","Content":"An autonomous Uber car killed a woman in the street in Arizona, police said, in what appears to be the first reported fatal crash involving a self-driving vehicle and a pedestrian in the US.\nTempe police said the self-driving car was in autonomous mode at the time of the crash and that the vehicle hit a woman, who was walking outside of the crosswalk and later died at a hospital. There was a vehicle operator inside the car at the time of the crash.\nUber said in a statement on Twitter: “Our hearts go out to the victim’s family. We are fully cooperating with local authorities in their investigation of this incident.” A spokesman declined to comment further on the crash.\nWho's driving? Autonomous cars may be entering the most dangerous phase\nRead more\nThe company said it was pausing its self-driving car operations in Phoenix, Pittsburgh, San Francisco and Toronto. Dara Khosrowshahi, Uber’s CEO, tweeted: “Some incredibly sad news out of Arizona. We’re thinking of the victim’s family as we work with local law enforcement to understand what happened.”\nUber has been testing its self-driving cars in numerous states and temporarily suspended its vehicles in Arizona last year after a crash involving one of its vehicles, a Volvo SUV. When the company first began testing its self-driving cars in California in 2016, the vehicles were caught running red lights, leading to a high-profile dispute between state regulators and the San Francisco-based corporation.\nPolice identified the victim as 49-year-old Elaine Herzberg and said she was walking outside of the crosswalk with a bicycle when she was hit at around 10pm on Sunday. Images from the scene showed a damaged bike. The 2017 Volvo SUV was traveling at roughly 40 miles an hour, and it did not appear that the car slowed down as it approached the woman, said Tempe sergeant Ronald Elcock.\nElcock said he had watched footage of the collision, which has not been released to the public. Police identified the operator of the car as Rafaela Vasquez, 44, and said she was cooperative and there were no signs of impairment.\nA still image taken from video provided by ABC-15 at the scene, where a pedestrian with a bicycle was hit. Photograph: AP\nThe self-driving technology is supposed to detect pedestrians, cyclists and others and prevent crashes.\nJohn M Simpson, privacy and technology project director with Consumer Watchdog, said the collision highlighted the need for tighter regulations of the nascent technology.\n“The robot cars cannot accurately predict human behavior, and the real problem comes in the interaction between humans and the robot vehicles,” said Simpson, whose advocacy group called for a national moratorium on autonomous car testing in the wake of the deadly collision.\nSimpson said he was unaware of any previous fatal crashes involving an autonomous vehicle and a pedestrian.\nTesla Motors was the first to disclose a death involving a self-driving car in 2016 when the sensors of a Model S driving in autopilot mode failed to detect a large white 18-wheel truck and trailer crossing the highway. The car drove full speed under the trailer, causing the collision that killed the 40-year-old behind the wheel in the Tesla.\nEarlier this year, California regulators approved the testing of self-driving cars on public roads without human drivers monitoring inside.\n“The technology is not ready for it yet, and this just sadly proves it,” said Simpson.\nIn one recent incident, California police officers found a Tesla that was stopped in the middle of a five-lane highway and found a driver asleep behind the wheel. The man said the vehicle was in “autopilot”, which is Tesla’s semi-autonomous driver assist system, and he was arrested on suspicion of drunk driving.\nAn Uber self-driving car travels in Pittsburgh, Pennsylvania. Photograph: Angelo Merendino\/AFP\/Getty Images\nIn another recent case, a Tesla car rear-ended a fire truck on a freeway, with the driver again telling the authorities the car was in autopilot mode at the time of the collision.\nMichael G Bennett, an Arizona State University associate research professor who studies autonomous cars, said the self-driving vehicles have become ubiquitous around campus and on the streets in Tempe. Often they have operators behind the wheels, but sometimes they are fully autonomous with no human inside.\nThe fatal collision could spark significant calls for reform and reflections within the industry, he said.\n“It may be problematic for the industry, because one of their central arguments for the value of the technology is that it is superior to human drivers,” said Bennett, adding that autonomous cars should be able to detect pedestrians and avoid hitting them, even if they aren’t in crosswalks: “Every day, pedestrians in cities around the world step outside of the crosswalk.”\nRage against the machine: self-driving cars attacked by angry Californians\nRead more\nThe governor of Arizona, Doug Ducey, has been a strong proponent of allowing corporations to test the technology in his state, publicly slamming other governments for “over regulation” and in 2016 urging Uber to “ditch California” and launch in his region. In March, he issued new rules and said that more than 600 automated vehicles have driven on public roads in the state.\n“Our prayers are with the victim, and our hearts go out to her family,” Patrick Ptak, Ducey’s spokesman, said in an email to the Guardian, adding, “Public safety is our top priority.”\nLinda Bailey, the executive director of the National Association of City Transportation Officials (Nacto), said in an interview that there has not been enough regulatory oversight of testing and that some governments are overwhelmed trying to understand autonomous technology and its limitations.\n“There’s an essential role for the public sector in regulating the safety of these vehicles, which has been largely left to private companies,” she said, adding that Nacto supports third-party testing of the vehicles.\nTempe’s mayor, Mark Mitchell, defended the city’s ongoing support of autonomous vehicles in a statement Monday, saying: “All indications we had in the past show that traffic laws are being obeyed by the companies testing here.”\nThis article was amended on 22 March to correct the name of the Uber safety driver, who police had initially misidentified.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a fatal crash involving an autonomous Uber car and a pedestrian in Arizona. The incident occurred when the self-driving Uber car was in autonomous mode and struck a woman, Elaine Herzberg, who was walking outside of the crosswalk. Tragically, Ms. Herzberg later died at the hospital. The Uber CEO expressed condolences to the victim's family, and the company cooperated with local authorities in the investigation. The vehicle operator at the time of the crash was identified as Rafaela Vasquez, and there were no signs of impairment. The self-driving technology was supposed to detect pedestrians and prevent crashes. This incident has raised concerns about the interaction between autonomous vehicles and pedestrians, and it may lead to calls for tighter regulations in the industry.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/mar\/19\/uber-self-driving-car-kills-woman-arizona-tempe","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":97,"Time":"12:00 PM GMT+1•February 9, 2022","Title":"Waabi’s new simulator could scale autonomous vehicle tech faster","Content":"cameraImage Credits: Waabi\nTesting autonomous vehicles on public roads is an expensive and time-consuming endeavor, and one that Raquel Urtasun, former chief scientist at Uber ATG, doesn’t think is the most expedient route to market. Waabi, Urtasun’s self-driving truck technology startup that launched last June, has come out with a key component of its strategy to scale its tech – Waabi World, a high-fidelity closed-loop simulator that doesn’t just virtually test Waabi’s self-driving software, but also teaches it in real time. \n“Our simulator is immersive as well as reactive,” Urtasun told TechCrunch. “That means it can really mimic the world in all its diversity, beauty and fidelity, as well as automatically create scenarios and stress test the Waabi Driver, and also teach the Waabi Driver how to learn just by experiencing the simulator.”\nSimulation as a form of accelerating the path to market is not new to the AV industry. Waymo, Cruise, Aurora, TuSimple, Tesla and others have all touted the benefits of using simulations made from real-world data to test their AV systems, particularly against made-up scenarios that the systems haven’t encountered and cataloged in the real world yet.\nUrtasun said that’s great and all, but the current simulators being used in the industry “don’t provide what is really required to significantly reduce the number of miles that you need to drive in the real world in order to test and develop and deploy this technology.”\nWaabi’s secret sauce? A simulator that can automatically build digital twins of the world from data, perform near-real-time sensor simulation, manufacture scenarios to stress test the Waabi Driver, and teach the driver to learn from its mistakes without human intervention, according to Waabi.\nMost AV tech developers do some version of this, but Urtasun reckons Waabi has advanced the tech significantly and in a way that’s truly AI-first and rooted in automation. Let’s dive into more detail.\nCreating a digital twin \nWaabi’s competitors are tackling simulation by getting artists to create three-dimensional CAD models of the world and assign material properties for every object, like trees and buildings, Urtasun said. Those objects are either manually composed together to create a scene or the artists use automation techniques like procedural generation that combine human-generated assets and algorithms to create a bigger artificial world out of the little pieces. \nCruise, Waymo and Aurora have all confirmed that they follow a similar process to the one Urtasun describes.  \n“Our approach is very different,” said Urtasun. “We utilize AI to recreate digital twins from everywhere that we have driven. Every time that you have a vehicle driving, collecting data, we can recreate that and we can recreate that with super high fidelity and we only need to observe it once. So this technique scales so much better.”\nIn other words, Waabi World includes an AI system that takes the raw sensor data and automatically creates a digital twin with it – no artists or procedural generation needed.\nRealistic sensor simulation\nAfter creating the virtual environment for testing, Waabi has to simulate how the sensors on the trucks will observe a scene in the real world. Waabi uses a combination of AI and simple physics to enable faster and more realistic sensor simulations that allow for a more immersive simulator for the Waabi Driver, Urtasun said.\n“The industry standard is to build a super sophisticated physics simulator, where in order for it to work, you need to know all the details about everything,” said Urtasun. “Where is the sun, where are the material properties of everything. They need a lot of computation to be able to create those simulations.” \nUrtasun said Waabi World doesn’t need all that labeling of the surrounding environment. She uses a combination of simple physics to provide a rough idea of what the software stack sees through the sensors and AI with her new generation of algorithms to complete the picture.\n“We don’t forget about physics, but we simply use it as a stepping stone towards the final simulation, and then you can scale and be much more realistic much faster,” said Urtasun.\nUsing AI to stress test the Waabi Driver\nMost AV companies understand that part of the beauty of simulation is that you can create scenarios for the software stack that haven’t been recorded during real-life driving, which helps test the system against edge cases. \nThe industry standard so far still revolves around simulation teams finding a small set of scenarios that they observed in the real world or thought up in their minds and creating variations of them by varying the speed, acceleration, geography and starting conditions. This process is often done manually. \nWaabi World uses AI to automatically generate those scenarios in a closed-loop simulation (meaning it’s reactive), and it chooses scenarios to test the Waabi Driver against by observing how the driver behaves in the world and understanding where the failures of the system are.\n“Waabi World can generate scenarios that have a high probability of making the Waabi Driver fail,” said Urtasun. “So when the autonomy system is very good, you’re going to need millions and billions of scenarios in order for you to see a problem or mistake that this self-driving system might make. It becomes like finding a needle in the haystack.”\nLearning in real time, like a human\nOften in the AV industry, when a system is being tested, either in simulation or on the road, it’s not learning at the same time. The brain, or software stack, doesn’t evolve until it’s been updated with a new version of software, which has been tweaked by engineers after recording what sorts of mistakes the driver made and figuring out how to avoid them.\nWaabi World has the ability to teach the Waabi Driver to drive automatically just by experiencing the simulations, Urtasun said. \n“As the Waabi Driver is experiencing the world, Waabi World can tell the driver what mistakes it’s making, and then that driver can take the information and instantaneously update its brain to actually better handle the situations. That way you can continuously and automatically improve your software stack.”\nThis is similar to how humans learn, Urtasun said. We don’t wait for data to be collected and sent back to the servers, and then for engineers to decide which pieces they want to use for learning and update our brains. Rather, we experience something and instantaneously our brain rewires to be able to handle the situation better. \nWhen it comes to testing on the roads, however, Urtasun said Waabi will not allow its driver to learn while driving, even though it has the capability to do so, for safety reasons. \n“Without having a full verification of those new changes, you might potentially be introducing something dangerous,” she said. “You need to know before you put software on the road if it passes all the safety tests first.”\nWill we get to see the Waabi Driver in action?\nPresumably. Urtasun would not admit to having tested self-driving trucks on public roads at all, although Waabi had to get its initial data sets from somewhere. Instead, she said there would be more news coming soon. \nOther competitors in the trucking space are already forging ahead a path to commercialization. TuSimple recently completed its first driver-out pilot on public roads. Waymo Via, Waymo’s trucking and freight unit, has signed on J.B. Hunt as its first self-driving freight customer when it hits the market within, it expects, the next few years.  \nWaabi may be a little late to the game, but if Waabi World is everything Urtasun says it is, the company is coming in hot.\n“I’m not worried about being late,” said Urtasun. “On the contrary, I think we have perfect timing. Our simulator is the next innovation the industry really needs, so we can go really fast, and then you will see that progress on the autonomy front, as well.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about Waabi, a self-driving truck technology startup, introducing a high-fidelity closed-loop simulator called Waabi World to scale its autonomous vehicle technology. The simulator aims to virtually test Waabi's self-driving software, create scenarios, stress test the Waabi Driver, and teach it to learn from its mistakes in real-time. The article provides detailed information about how Waabi's simulator works, including its ability to automatically build digital twins of the world from data, simulate sensors realistically, use AI to generate test scenarios, and enable the driver to learn from simulations.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/02\/09\/waabis-new-simulator-could-scale-autonomous-vehicle-tech-faster\/","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":99,"Time":"Thu 22 Mar 2018 02.02 CET","Title":"Video released of Uber self-driving crash that killed woman in Arizona","Content":"Video of the first self-driving car crash that killed a pedestrian showed how the autonomous Uber failed to slow down as it fatally hit a 49-year-old woman walking her bike across the street.\nThe newly released footage of the collision that killed Elaine Herzberg in Tempe, Arizona, on Sunday night has raised fresh questions about why the self-driving car did not stop when a human entered its path and has sparked scrutiny of regulations in the state, which has encouraged testing of the autonomous technology.\n“It’s just awful,” Tina Marie Herzberg White, a stepdaughter of the victim, told the Guardian on Wednesday. “There should be a criminal case.”\nPolice have released two videos of the case – one outside and one showing the interior of the Volvo SUV. The four-second exterior video showed the car driving down a somewhat dark and largely empty street as it collided into the woman walking directly in its path.\nTempe Police Vehicular Crimes Unit is actively investigating\nthe details of this incident that occurred on March 18th. We will provide updated information regarding the investigation once it is available. pic.twitter.com\/2dVP72TziQ\n— Tempe Police (@TempePolice) March 21, 2018\nThe 14-second video inside the car showed the operator, identified by police as Rafaela Vasquez, 44, appearing to look at something inside the vehicle and not at the road at the time of the collision. She alternated between looking down and looking forward and appeared shocked at the last minute just as the car failed to stop.\nUber typically refers to its operators in the front of the autonomous cars as “safety drivers” who can intervene and take control of the car, but generally let the vehicles run in autonomous mode. The cars rely on radar sensors that are meant to detect pedestrians, cyclists, cars and other obstacles.\nIt is unclear what went wrong in this case. It was around 10pm at the time of the crash, and the video showed the woman appearing in view a second or so before the collision. She was not walking in a crosswalk when the car hit her, though Herzberg’s loved ones and some autonomous driving experts have argued that the technology still should have detected her.\nLocal prosecutors will decide whether criminal charges are warranted. Some have argued that under new rules issued by Arizona’s governor, a strong proponent of the technology, a company like Uber could possibly be criminally liable if an autonomous car negligently killed someone. But the local police chief, Sylvia Moir, suggested in an interview that she believed Uber was not at fault.\n“It’s absolutely ridiculous,” said White, who said she was shocked to learn that an autonomous car had killed Herzberg, who she had known for more than 20 years. “I can’t believe that the [driver] that was in the car did not see her.”\nCompanies manufacturing the technology have argued that self-driving cars are safer than humans, but skeptics have pointed out that the industry is entering a dangerous phase while the cars are not yet fully autonomous, but human operators are not fully engaged.\nWhite said she did not know whether Herzberg’s immediate family might pursue a civil case: “Ain’t no amount of money in the world going to bring her back.”\nUber has temporarily suspended its self-driving program, but has not commented on what caused the crash. The company said in a statement on Wednesday night: “The video is disturbing and heartbreaking to watch, and our thoughts continue to be with Elaine’s loved ones. Our cars remain grounded, and we’re assisting local, state and federal authorities in any way we can.”\nWhite said she did not believe the cars should be allowed to return: “This might be the first, but I’ll tell you what, there will be more.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a self-driving Uber car crash in Arizona that resulted in the death of a pedestrian, Elaine Herzberg. The news includes details about the released video footage of the collision, questions about why the self-driving car did not stop when a human entered its path, and the investigation by the Tempe Police Vehicular Crimes Unit. It also mentions the operator inside the car, identified as Rafaela Vasquez, who appeared to be distracted, and the debate about whether criminal charges are warranted. Uber temporarily suspended its self-driving program following the incident.\"\n}\ncase_text_end=======","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/mar\/22\/video-released-of-uber-self-driving-crash-that-killed-woman-in-arizona","query_name":"updated_file_autonomous_vehicle_failures.xlsx"}
{"num":43,"Time":"March 29, 2022","Title":"U.S. agency to review if Pony.ai complied with crash reporting order","Content":"Companies\nToyota Motor Corp\nFollow\nHyundai Motor Co\nFollow\nWASHINGTON, March 29 (Reuters) - The National Highway Traffic Safety Administration (NHTSA) on Tuesday said it will review whether startup technology firm Pony.ai complied with government reporting requirements for driverless crashes.\nPony.ai earlier this month agreed to issue a recall for some versions of its autonomous driving system software after an October crash in California. The recall, which the NHTSA said was the \"first recall of an automated driving system,\" covered three vehicles.\nThe NHTSA said it will review whether Toyota-backed Pony.ai complied with reporting requirements \"with respect to both the timeliness and accuracy of its reports.\"\nOn Oct. 28, a Pony.ai vehicle that had been operating in autonomous mode hit a street sign on a median in Fremont, California, after turning right, prompting California in December to suspend the company's driverless testing permit. No one was injured in the incident.\nAdvertisement · Scroll to continue\nPony.ai on Tuesday said it reported the incident to the NHTSA in \"a good faith effort to comply with the relevant requirements\" and added that it \"has been fully cooperating with NHTSA throughout the process.\"\nPony.ai said earlier that the crash occurred less than 2.5 seconds after the automated driving system shut down. It said in very rare circumstances, a planning system diagnostic check \"could generate a 'false positive' indication of a geolocation mismatch.\"\nAdvertisement · Scroll to continue\nThe NHTSA told Pony.ai earlier it believed the software had a safety defect and requested that the company conduct a recall.\nThe company said it has updated the software code and the three affected vehicles have been repaired.\nEarlier this month, Pony.ai announced that it had completed a new round of financing that gives it a valuation of $8.5 billion.\nCalifornia last year said Pony.ai had 10 Hyundai Motor (005380.KS) Kona electric vehicles registered under its driverless testing permit.\nAdvertisement · Scroll to continue\nThe suspension does not impact Pony.ai's permit for testing with a safety driver.\nReporting by David Shepardson; Editing by Mark Porter\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about the National Highway Traffic Safety Administration (NHTSA) reviewing whether startup technology firm Pony.ai complied with government reporting requirements for driverless crashes. Pony.ai agreed to issue a recall for some versions of its autonomous driving system software after an October crash in California, which prompted the NHTSA to review the timeliness and accuracy of its reports. The incident involved a Pony.ai vehicle operating in autonomous mode hitting a street sign in Fremont, California. No one was injured in the incident. Pony.ai reported the incident to the NHTSA and cooperated with the agency throughout the process. The company also mentioned that the crash occurred less than 2.5 seconds after the automated driving system shut down and attributed it to a rare planning system diagnostic check 'false positive' indication of a geolocation mismatch. The NHTSA requested a recall, and Pony.ai updated the software code and repaired the affected vehicles. Additionally, Pony.ai recently completed a new round of financing with a valuation of $8.5 billion. California had previously registered 10 Hyundai Motor Kona electric vehicles under Pony.ai's driverless testing permit, and the suspension of Pony.ai's permit for driverless testing without a safety driver was mentioned.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/us-agency-review-if-ponyai-complied-with-crash-reporting-order-2022-03-29\/","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":42,"Time":"Fri 23 Jun 2023 16.00 CEST","Title":"Fire chief warns against ‘unleashing’ self-driving taxis in San Francisco","Content":"Amid a rush to get more robotaxis on the roads of California’s largest cities, the fire chief in San Francisco has warned the vehicles are “not ready for primetime”, citing multiple recent incidents of the vehicles obstructing fire engines and blocking roads during emergencies.\nMy week navigating the awkward teenage years of self-driving cars\nRead more\nDriverless cars have become a growing nuisance for San Francisco’s residents and officials alike. Of particular concern for police and firefighters is the unwelcome appearance of robotaxis at the scene of an emergency that hinders their work.\nSo far in 2023, the San Francisco police department has logged nearly 40 incidents where a robotaxi interfered with fire department work, according to the Los Angeles Times. The fire departments told the outlet that incidents have included cars blocking one-way streets and firehouse driveways, forcing fire trucks to find alternate routes to get to blazes.\nOther incidents that have attracted headlines included a Cruise robotaxi that appeared to idle in the midst of a mass shooting for several minutes in early June, while in January a firefighter was reportedly forced to break the window of a Cruise vehicle to stop it after the car nearly drove over hoses that were being used to put out a raging house fire.\nCruise has denied blocking emergency personnel “at any point” while they responded to the shooting that injured nine people. “Our car initially stopped as it was approaching an active emergency scene, then proceeded to perform a U-turn and pull over,” a spokesperson told the Guardian. “Throughout this time, all vehicles, including emergency response vehicles, were able to proceed around our car.”\nSuch incidents have become more common since June 2022 when state officials granted Cruise, a company controlled by the automaker General Motors, approval to launch its driverless ride-hailing service on certain streets in San Francisco. In April, the company got permission from city leaders to operate anywhere in the city 24 hours a day, seven days a week, the San Francisco Chronicle reported.\nAmid this expansion other companies, including Waymo, a Google spinoff and Zoox, which is owned by Amazon, have sought approval to operate in San Francisco as well as Los Angeles.\nThe California department of motor vehicles has logged nearly 70 autonomous vehicle collision reports this year, including a 21 May incident in San Francisco in which a dog was struck and killed by a Waymo vehicle operating in autonomous mode but with an employee in the front seat, according to the DMV report.\nThough no one has been killed or reported being seriously injured due to a robotaxi obstructing emergency crews, the San Francisco fire chief, Jeanine Nicholson, urged the state to move more slowly. “I don’t want something bad to happen because we can’t get to a scene. A fire can double in size in a minute. We are dealing with life and death and I’m not being dramatic in saying that,” she told the LA Times.\n“I’m not against the technology,” she continued. “But we need to fix what’s not working right now, before they are unleashed on the rest of the city.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about concerns raised by the San Francisco fire chief, Jeanine Nicholson, regarding the deployment of self-driving taxis (robotaxis) in San Francisco. She has warned that these vehicles are 'not ready for primetime' due to multiple recent incidents of robotaxis obstructing fire engines and blocking roads during emergencies. The incidents include cars blocking one-way streets and firehouse driveways, forcing fire trucks to find alternate routes, and instances where robotaxis interfered with fire department work. There have been approximately 40 such incidents in 2023. One notable incident involved a Cruise robotaxi idling at the scene of a mass shooting, and another incident where a firefighter had to break the window of a Cruise vehicle to stop it from driving over hoses during a house fire. While no serious injuries have been reported, the fire chief is urging caution and improvements in the technology before further deployment in the city.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/us-news\/2023\/jun\/23\/self-driving-taxis-fire-chief-san-francisco","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":29,"Time":"Sat 19 Aug 2023 19.55 CEST","Title":"Cruise agrees to cut fleet of San Francisco robotaxis in half after crashes","Content":"General Motors’ Cruise autonomous vehicle unit has agreed to cut its fleet of San Francisco robotaxis in half as authorities investigate two recent crashes in the city.\nThe state department of motor vehicles (DMV) asked for the reduction after a Cruise vehicle without a human driver collided with an unspecified emergency vehicle on Thursday.\nRobotaxi breakdowns cause mayhem in San Francisco days after expansion vote\nRead more\n“The DMV is investigating recent concerning incidents involving Cruise vehicles in San Francisco,” the department said on Saturday in a statement to the Associated Press. “Cruise has agreed to a 50% reduction and will have no more than 50 driverless vehicles in operation during the day and 150 driverless vehicles in operation at night.”\nThe development comes just over a week after California regulators allowed Cruise and the Google spinoff Waymo to operate autonomous robotaxis throughout San Francisco at all hours, despite safety worries spurred by recurring problems with unexpected stops and other erratic behavior.\nThe decision on 10 August by the California public utilities commission made San Francisco the first major US city with two fleets of driverless vehicles competing for passengers.\nOn Thursday around 10pm, the Cruise vehicle had a green light, entered an intersection, and was hit by the emergency vehicle responding to a call, the San Francisco Chronicle reported, based on tweets from Cruise.\nThe robotaxi was carrying a passenger, who was taken by ambulance to a hospital with injuries that were not severe, Cruise told the newspaper.\nAlso on Thursday night, a Cruise car without a passenger collided with another vehicle in San Francisco, the newspaper reported.\nThe San Francisco fire department did not immediately respond to a request for comment from the newspaper.\nThe robotaxi almost immediately identified the emergency response vehicle as it came into view, Greg Dieterich, Cruise’s general manager in San Francisco, said in a statement on the company website.\nAt the intersection, visibility is occluded by buildings, and it’s not possible to see objects around a corner until they are very close to the intersection, Dieterich’s statement said. The Cruise autonomous vehicle detected the siren as soon it was distinguishable from background noise, he wrote.\n“The AV’s ability to successfully chart the emergency vehicle’s path was complicated by the fact that the emergency vehicle was in the oncoming lane of traffic, which it had moved into to bypass the red light,” Dieterich wrote.\nThe Cruise vehicle identified the risk of a crash and braked, reducing its speed, but could not avoid the collision, he wrote.\nThe Cruise rollout has not been all smooth sailing. The vehicles have had several traffic-related problems since the company launched its driverless ride-hailing service in San Francisco in June 2022. Most recently amid increased traffic from the Outside Lands music festival on 11 August, a number of self-driving cars seemed to glitch, causing mayhem when as many as 10 Cruise cars blocked a main thoroughfare.\nOther incidents that have attracted headlines included a Cruise robotaxi that appeared to idle in the middle of a mass shooting for several minutes in early June, while in January a firefighter was reportedly forced to break the window of a Cruise vehicle to stop it after the car nearly drove over hoses that were being used to put out a raging house fire.\nCruise vehicles have driven more than 3m autonomous miles in the city and have interacted with emergency vehicles more than 168,000 times in the first seven months of this year alone, the statement said. “We realize that we’ll always encounter challenging situations, which is why continuous improvement is central to our work.”\nThe company will work with regulators and city departments to reduce the likelihood of a crash happening again, Dieterich wrote.\nThe DMV said the fleet reduction will remain until its investigation ends and Cruise takes corrective action to improve safety. “The DMV reserves the right, following investigation of the facts, to suspend or revoke testing and\/or deployment permits if there is determined to be an unreasonable risk to public safety.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"General Motors’ Cruise autonomous vehicle unit has agreed to cut its fleet of San Francisco robotaxis in half as authorities investigate two recent crashes in the city. The state department of motor vehicles (DMV) asked for the reduction after a Cruise vehicle without a human driver collided with an unspecified emergency vehicle on Thursday. On Thursday around 10pm, the Cruise vehicle had a green light, entered an intersection, and was hit by the emergency vehicle responding to a call. The robotaxi was carrying a passenger, who was taken by ambulance to a hospital with injuries that were not severe. Also on Thursday night, a Cruise car without a passenger collided with another vehicle in San Francisco. The Cruise rollout has not been all smooth sailing, with several traffic-related problems since the company launched its driverless ride-hailing service in San Francisco in June 2022. Other incidents have included a Cruise robotaxi idling in the middle of a mass shooting and a firefighter breaking the window of a Cruise vehicle to stop it. Cruise vehicles have driven more than 3 million autonomous miles in the city and have interacted with emergency vehicles more than 168,000 times in the first seven months of this year alone. The company will work with regulators and city departments to reduce the likelihood of a crash happening again.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/us-news\/2023\/aug\/19\/cruise-halves-fleet-san-francisco-robotaxis","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":71,"Time":"OCTOBER 30, 2020","Title":"Waymo releases Phoenix area self-driving car incident score card","Content":"By Jane Lanhee Lee\nFILE PHOTO: A Waymo Chrysler Pacifica Hybrid self-driving vehicle approaches during a demonstration in Chandler, Arizona, November 29, 2018. REUTERS\/Caitlin O’Hara\/File Photo\/File Photo\nOAKLAND, Calif (Reuters) - Self-driving tech company Waymo, which this month opened its driverless taxi service in the Phoenix suburbs to the public, on Friday said its autonomous vehicles there had been involved in 18 minor incidents since 2019 during tests and actual rides.\nWaymo, a unit of Google parent Alphabet Inc GOOGL.O, said it was releasing the data waymo.com\/safety to improve transparency and open a public dialogue. Some residents have complained about the hundreds of Waymo vans driving around town, telling Reuters in the past that their driving patterns were dangerous as they stopped too often, running the risk of being rear-ended by a human driver.\nWaymo also said it hopes its safety data will help companies and regulators devise industry-wide safety standards for self-driving cars.\nAccording to the data, the Waymo vehicles in Phoenix had minor incidents once for about every 339,000 miles traveled and another 29 incidents were avoided with the intervention of a safety driver. That amounted to once in every 210,000 miles. Still none of the events, including those that were prevented, would have led to serious injuries, it said.\nWaymo cars were rear ended 11 times, according to the data. Matthew Schwall, head of field safety at Waymo told reporters in a briefing that an analysis of the Phoenix operations show that its cars were not rear ended more than the average human-driven vehicle there.\nHe said that of the incidents recorded in Arizona, eight of the most significant ones involved human errors. Also he said Waymo self-driving technology was able to always avoid incidents like striking a fixed object or departing a roadway. These, he noted, are frequent incidents with human drivers that can lead to fatalities.\nSchwall declined to say if Waymo would release collision and safety data on a regular basis.\nReporting By Jane Lanhee Lee, Additional Reporting Paul Lienert; Editing by David Gregorio\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"Waymo, a self-driving tech company, has reported 18 minor incidents involving its autonomous vehicles in the Phoenix area since 2019. These incidents occurred during tests and actual rides. Waymo aims to improve transparency by releasing safety data on their website and hopes it will contribute to industry-wide safety standards for self-driving cars. The data indicates that Waymo vehicles in Phoenix experienced minor incidents once every 339,000 miles and avoided 29 other incidents with the intervention of a safety driver, occurring once every 210,000 miles. Fortunately, none of these incidents, including those prevented, resulted in serious injuries. Waymo cars were rear-ended 11 times, but the data suggests that they were not rear-ended more frequently than human-driven vehicles in the area. Eight of the most significant incidents were attributed to human errors, while Waymo's technology successfully avoided incidents involving striking fixed objects or departing from the roadway, which are common incidents with human drivers that can lead to fatalities.\"\n}\nThe news is relevant to driverless car incidents, and the specific information about the incidents has been extracted as detailed as possible in the JSON response.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/autonomous-waymo-safety-int-idUSKBN27F23R","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":74,"Time":"MARCH 28, 2018","Title":"Uber’s use of fewer safety sensors prompts questions after Arizona crash","Content":"By Heather Somerville, Paul Lienert, Alexandria Sage\nTEMPE, Ariz.\/PITTSBURGH (Reuters) - When Uber decided in 2016 to retire its fleet of self-driving Ford Fusion cars in favor of Volvo sport utility vehicles, it also chose to scale back on one notable piece of technology: the safety sensors used to detect objects in the road.\nThat decision resulted in a self-driving vehicle with more blind spots than its own earlier generation of autonomous cars, as well as those of its rivals, according to interviews with five former employees and four industry experts who spoke for the first time about Uber’s technology switch.\nDriverless cars are supposed to avoid accidents with lidar – which uses laser light pulses to detect hazards on the road - and other sensors such as radar and cameras. The new Uber driverless vehicle is armed with only one roof-mounted lidar sensor compared with seven lidar units on the older Ford Fusion models Uber employed, according to diagrams prepared by Uber.\nIn scaling back to a single lidar on the Volvo, Uber introduced a blind zone around the perimeter of the SUV that cannot fully detect pedestrians, according to interviews with former employees and Raj Rajkumar, the head of Carnegie Mellon University’s transportation center who has been working on self-driving technology for over a decade.\nThe lidar system made by Velodyne - one of the top suppliers of sensors for self-driving vehicles - sees objects in a 360-degree circle around the car, but has a narrow vertical range that prevents it from detecting obstacles low to the ground, according to information on Velodyne’s website as well as former employees who operated the Uber SUVs.\nAutonomous vehicles operated by rivals Waymo, Alphabet Inc’s self-driving vehicle unit, have six lidar sensors, while General Motors Co’s vehicle contains five, according to information from the companies.\nUber declined to comment on its decision to reduce its lidar count. In a statement late Tuesday, an Uber spokeswoman said, “We believe that technology has the power to make transportation safer than ever before and recognize our responsibility to contribute to safety in our communities. As we develop self-driving technology, safety is our primary concern every step of the way.”\nUber referred questions on the blind spot to Velodyne. Velodyne acknowledged that with the rooftop lidar there is a roughly three meter blind spot around a vehicle, saying that more sensors are necessary.\n“If you’re going to avoid pedestrians, you’re going to need to have a side lidar to see those pedestrians and avoid them, especially at night,” Marta Hall, president and chief business development officer at Velodyne, told Reuters.\nThe safety of Uber’s self-driving car program is under intense scrutiny since Elaine Herzberg, 49, was killed last week after an Uber Volvo XC90 SUV operating in autonomous mode struck and killed her while she was jaywalking with her bicycle in Tempe, Arizona.\nThe precise causes of the Arizona accident are not yet known, and it is unclear how the vehicle’s sensors functioned that night or whether the lidar’s blind spot played a role. The incident is under investigation by local police and federal safety officials who have offered few details, including whether Uber’s decision to scale back its sensors is under review.\nUber has said it is cooperating in the investigation and has pulled all of its autonomous cars off the road, but has provided no further details about the crash.\nFILE PHOTO: A self driving Volvo vehicle, purchased by Uber, moves through an intersection in Scottsdale, Arizona, U.S., December 1, 2017. REUTERS\/Natalie Behring\/File Photo\nLike the older Fusion model, Uber’s top competitors place multiple, smaller lidar units around the car to augment the central rooftop lidar, a practice experts in the field say provides more complete coverage of the road.\nThe earlier Fusion test cars used seven lidars, seven radars and 20 cameras. The newer Volvo test vehicles use a single lidar, 10 radars and seven cameras, Uber said.\nSince Uber launched a self-driving car program in early 2015, it has hustled to catch up with Waymo, which began working on the technology in 2009. Uber management moved swiftly and confidently even as some car engineers voiced caution, according to former employees, in a rush to get more cars driving more miles.\nSeven experts who have reviewed the crash agree that a self-driving system should have seen Herzberg and braked. She had crossed nearly the entire four-lane, empty road before being struck by the front right side of the vehicle. The night was clear and streetlights were lit.\n“Radar is supposed to compensate for (the lidar’s) blind spot,” said Rajkumar.\nUber declined to comment on its radar system. Volvo Car Group, owned by China’s Geely, declined to comment. A Ford spokesman said the company was not involved in Uber’s use of the Fusion or the self driving technology employed on the cars.\nTo be sure, there are many possible causes of the crash other than the lidar blind spot. There could have been a software failure in the Uber car, said Richard Murray, an engineering professor at California’s Institute of Technology and the former head of Caltech’s student self-driving team.\n“But this would be quite surprising since there was nothing else on the road,” he said.\nTHE BLIND ZONE\nAn Uber diagram of the Fusion model notes that “front, rear and wing-mounted lidar modules aid in the detection of obstacles in close proximity to the vehicle, as well as smaller ones that can get lost in blind spots.”\nA diagram of its Volvo version shows a single lidar system on the roof. In reducing its lidar units, Uber chose to rely more on radar to detect obstacles that may end up in those blind spots, according to company statements.\nAt Uber’s September 2016 unveiling of its Pittsburgh self-driving car operation, it was still using the Fusions, but had a Volvo on display. Uber staff pointed to the sleekness of the SUV and the relatively small roof mount with only one lidar system, a more attractive upgrade from the Fusion, which had a bulkier look with more sensors attached to the exterior.\nA former employee said Uber justified the decision to slim down to one lidar by saying they “overdid it” with the additional sensors on the Fusions, suggesting the multiple lidars were unnecessary as Uber continued to refine its self-driving system.\nUber’s decision to move from the Fusion to a much taller vehicle exacerbated the issue of a blind spot from a single lidar unit, said former employees, because the lidar now sits up higher on top of an SUV, further reducing its ability to see low-lying objects - from squirrels to the wheels of a bicycle or a person’s legs.\nOne former Uber employee involved in testing both the Fusions and Volvo SUVs said that during a test run in late 2016, the Volvo failed to see a delivery truck’s tailgate lift that extended into the street, and the car nearly hit it going 35 miles-per-hour.\nUber declined to comment on specific testing incidents, but said its technology is constantly being updated and improved, and every incident in the cars is logged and checked out by an engineer.\nAdditional reporting by Salvador Rodriguez in San Francisco and Eric Johnson in Seattle; editing by Joe White and Edward Tobin\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to driverless car incidents. The article discusses Uber's decision to reduce the number of safety sensors, specifically lidar sensors, on their self-driving vehicles, resulting in blind spots. It also mentions a recent incident in Tempe, Arizona, where an Uber self-driving vehicle struck and killed a pedestrian, raising questions about the role of the lidar's blind spot in the accident. The article provides information about the number of lidar sensors used by Uber and its competitors, as well as some criticism of Uber's sensor strategy. The investigation into the Arizona incident is ongoing.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-uber-selfdriving-sensors-insight-idUSKBN1H337Q","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":75,"Time":"6 March 2019","Title":"Uber 'not criminally liable' for self-driving death","Content":"Uber 'not criminally liable' for self-driving death\nPublished\n6 March 2019\nShare\nIMAGE SOURCE,\nREUTERS\nImage caption,\nUber ended its self-driving operations in Arizona following last year's crash\nUber will not face criminal charges for a fatal crash involving one of its self-driving cars.\nProsecutors have ruled that the company is not criminally liable for the death of Elaine Herzberg, 49, who was struck as she crossed a road in Tempe, Arizona.\nThe car's back-up driver could still face criminal charges.\nA police report has previously called the incident \"entirely avoidable\".\nArizona Uber crash driver was 'watching TV'\nUber ends Arizona driverless car programme\nUber car 'had six seconds to respond' in fatal crash\n\"After a very thorough review of all evidence presented, this office has determined that there is no basis for criminal liability for the Uber corporation,\" wrote Yavapai County Attorney Sheila Sullivan Polk in a letter.\nThe crash occurred in March 2018, and involved a Volvo XC90 that Uber had been using to test its self-driving technology.\nJust before the crash, Ms Herzberg had been walking with a bicycle across a poorly lit stretch of a multi-lane road.\nDash-cam footage released by police after the incident appeared to show the vehicle's back-up driver, Rafaela Vasquez, taking her eyes off the road moments before the crash.\nFurther records from the streaming service Hulu suggested that Ms Vasquez had been streaming the TV show, The Voice, on a phone at the time of the crash.\nThe Yavapai County Attorney's office recommended an expert analysis of the video, and that the Tempe police department collect further evidence on what the back-up driver would have seen on the road.\nThe office did not explain its reasoning for finding Uber to be not criminally liable.\nUber did not immediately respond to the BBC's request for comment.\nThe National Transportation Safety Board is also investigating the crash. It released a preliminary report last year that suggested the sensors on the Uber vehicle were working correctly, but that emergency braking manoeuvres may not have been enabled.\nFollowing the crash, authorities in Arizona suspended Uber's ability to test self-driving cars on the state's public roads. Uber subsequently pulled the plug on its autonomous car operation in Arizona, although the company has since resumed tests in Pennsylvania.\nMore on this story\nUber crash driver was 'watching TV'\nPublished\n22 June 2018\nUber car 'failed to identify' pedestrian\nPublished\n24 May 2018\nUber ends Arizona driverless car programme\nPublished\n23 May 2018","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"Uber will not face criminal charges for a fatal crash involving one of its self-driving cars. Prosecutors have ruled that the company is not criminally liable for the death of Elaine Herzberg, 49, who was struck as she crossed a road in Tempe, Arizona. The car's back-up driver could still face criminal charges. A police report has previously called the incident 'entirely avoidable.' The crash occurred in March 2018 and involved a Volvo XC90 that Uber had been using to test its self-driving technology. Just before the crash, Ms. Herzberg had been walking with a bicycle across a poorly lit stretch of a multi-lane road. Dash-cam footage released by police after the incident appeared to show the vehicle's back-up driver, Rafaela Vasquez, taking her eyes off the road moments before the crash. Further records from the streaming service Hulu suggested that Ms. Vasquez had been streaming the TV show, The Voice, on a phone at the time of the crash. The Yavapai County Attorney's office recommended an expert analysis of the video, and that the Tempe police department collect further evidence on what the back-up driver would have seen on the road. The office did not explain its reasoning for finding Uber to be not criminally liable. The National Transportation Safety Board is also investigating the crash.\"\n}\nThe news is relevant to a self-driving car incident involving Uber, and the specific information about the case has been extracted as detailed as possible.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-47468391","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":76,"Time":"20 February 2017","Title":"Driverless Roborace car crashes at speed in Buenos Aires","Content":"Driverless Roborace car crashes at speed in Buenos Aires\nPublished\n20 February 2017\nShare\nIMAGE SOURCE,\nROBORACE\nImage caption,\nThe red Devbot 1 completed the race, but the yellow Devbot 2 crashed\nBy Leo Kelion\nTechnology desk editor\nA landmark race between two driverless electric cars has ended badly for one of the contestants.\nThe unfortunate Devbot vehicle crashed out of the Roborace competition after misjudging a corner while travelling at high speed.\nThe incident occurred ahead of the start of the latest Formula E electric car race in Buenos Aires.\nThe other vehicle managed to complete the course after achieving a top speed of 186km\/h (116mph).\n\"One of the cars was trying to perform a manoeuvre, and it went really full-throttle and took the corner quite sharply and caught the edge of the barrier,\" Roborace's chief marketing officer Justin Cooke told the BBC.\n\"It's actually fantastic for us because the more we see these moments the more we are able to learn and understand what was the thinking behind the computer and its data.\n\"The car was damaged, for sure, but it can be repaired. And the beauty is no drivers get harmed because... there is no-one in them.\"\nPhotos of the resulting damage have been published by an Argentinian blog. Roborace also plans to upload footage from the event onto its YouTube channel this Friday.\nIMAGE SOURCE,\nROBORACE\nImage caption,\nThe cars communicate with each other to avoid contact\nThe Devbots are controlled by artificial intelligence software - rather than being remote-controlled by humans - and use a laser-based Lidar (light detection and ranging) system and other sensors to guide themselves. They also communicate to avoid collisions with each other.\nRoborace's organisers had previously showed off one of their Devbots speeding round the UK's Donington Park circuit last August, but this was the first time they had publicly displayed two vehicles competing against each other.\nIMAGE SOURCE,\nROBORACE\nImage caption,\nRoborace intends to replace the Devbots with sleeker models when the competition formally launches\nEven so, they billed the event as a test run ahead of future plans to pit 10 teams of robotic cars against each other, each powered by different AI software.\nMr Cooke stressed that crash barriers and a limit on the Devbot's top speed had meant spectators in the Argentine capital had not been put at risk.\nAnd he added that another incident involving the winning car illustrated built-in safety measures.\n\"A dog ran on to the track, and the car was able to slow down, avoid it and take another path,\" he said.\nIMAGE SOURCE,\nROBORACE\nImage caption,\nThe winning Devbot 1 managed to avoid running over a dog\nRoborace's chief executive Denis Sverdlov will reveal more details about his company's plans, at the Mobile World Congress trade show in Barcelona next week.\nThe company then intends to show off its tech again at the next Formula E race, in Mexico City on 1 April.\nMore on this story\nDriverless racing car makes track debut\nPublished\n29 August 2016\nRelated Internet Links\nRoborace\nThe BBC is not responsible for the content of external sites.","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"A driverless Roborace car crashed in Buenos Aires during a race. The Devbot vehicle crashed out of the Roborace competition after misjudging a corner while traveling at high speed. The incident occurred ahead of the start of the Formula E electric car race in Buenos Aires. Fortunately, no one was harmed, and the car can be repaired. The crash was attributed to the car's attempt to perform a maneuver and catch the edge of the barrier. Roborace's chief marketing officer mentioned that such incidents help them learn more about the computer's thinking and data. The Devbots are controlled by artificial intelligence software and use Lidar and other sensors for guidance and collision avoidance. Roborace plans to replace the Devbots with sleeker models in the future.\"\n}\nThe news is relevant to a driverless car incident, and the specific information about the case has been extracted and provided in the response.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-39027477","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":78,"Time":"MARCH 25, 2017","Title":"Uber suspends self-driving car program after Arizona crash","Content":"By Gina Cherelus\nMarch 25 (Reuters) - Uber Technologies Inc suspended its pilot program for driverless cars on Saturday after a vehicle equipped with the nascent technology crashed on an Arizona roadway, the ride-hailing company and local police said.\nThe accident, the latest involving a self-driving vehicle operated by one of several companies experimenting with autonomous vehicles, caused no serious injuries, Uber said.\nEven so, the company said it was grounding driverless cars involved in a pilot program in Arizona, Pittsburgh and San Francisco pending the outcome of investigation into the crash on Friday evening in Tempe.\n“We are continuing to look into this incident,” an Uber spokeswoman said in an email.\nThe accident occurred when the driver of a second vehicle “failed to yield” to the Uber vehicle while making a turn, said Josie Montenegro, a spokeswoman for the Tempe Police Department.\n“The vehicles collided, causing the autonomous vehicle to roll onto its side,” she said in an email. “There were no serious injuries.”\nTwo ‘safety’ drivers were in the front seats of the Uber car, which was in self-driving mode at the time of the crash, Uber said in an email, a standard requirement for its self-driving vehicles. The back seat was empty.\nPhotos and a video posted on Twitter by Fresco News, a service that sells content to news outlets, showed a Volvo SUV flipped on its side after an apparent collision involving two other, slightly damaged cars. Uber said the images appeared to be from the Tempe crash scene.\nWhen Uber launched the pilot program in Pittsburgh last year, it said that driverless cars “require human intervention in many conditions, including bad weather.” It also said the new technology had the potential to reduce the number of traffic accidents in the country.\nThe accident is not the first time a self-driving car has been involved in a collision. A driver of a Tesla Motors Inc Model S car operating in autopilot mode was killed in a collision with a truck in Williston, Florida in 2016.\nA self-driving vehicle operated by Alphabet Inc’s Google was involved in a crash last year in Mountain View, California, striking a bus while attempting to navigate around an obstacle.\nThe collision comes days after Uber’s former president Jeff Jones quit less than seven months after joining the San Francisco-based company, the latest in a string of high-level executives who have departed in recent months.\nIn February, Alphabet’s Waymo self-driving car unit sued Uber and its Otto autonomous trucking subsidiary, alleging theft of proprietary sensor technology. (Reporting by Gina Cherelus in New York; Editing by Frank McGurty and Bill Rigby)\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Uber Technologies Inc suspended its pilot program for driverless cars on Saturday after a vehicle equipped with the nascent technology crashed on an Arizona roadway. The accident occurred when the driver of a second vehicle 'failed to yield' to the Uber vehicle while making a turn, causing the autonomous vehicle to roll onto its side. Two 'safety' drivers were in the front seats of the Uber car, which was in self-driving mode at the time of the crash. There were no serious injuries. Uber said it was grounding driverless cars involved in a pilot program in Arizona, Pittsburgh, and San Francisco pending the outcome of the investigation into the crash.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/uber-tech-crash-idUSL2N1H20DE","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":87,"Time":"8:58 PM GMT+2•August 18, 2023","Title":"Cruise robotaxi involved in a crash with fire truck, one passenger injured","Content":"cameraImage Credits: Cruise\nA Cruise robotaxi and an emergency vehicle crashed late Thursday night and left a passenger injured. The crash is the latest in a string of incidents that occurred this week after winning approval from the California Public Utilities Commission to expand commercial operations in San Francisco.\nCruise posted a few details about the crash on X, the social media site formerly known as Twitter. Cruise said one of its self-driving Chevy Bolt EVs entered an intersection on a green traffic light at Polk and Turk streets when it was struck by an emergency vehicle that appeared to be en route to an emergency scene. A video posted by FriscoLive415 that includes police scanner information shows the airbags deployed in the vehicle and the passenger was complaining of a headache.\nA Cruise spokesperson said the company would provide an update as soon as it can.\nCruise, self-driving vehicle subsidiary under GM, said there was one passenger in the car, who was “treated on scene and transported via ambulance for what we believe are non-severe injuries.”\nThe company also added it would investigate to better understand the performance of its autonomous vehicles and will be in touch with the City of San Francisco about the event.\nThe crash, which occurred around 10 p.m. Thursday, comes a day after San Francisco City Attorney David Chiu filed motions with the CPUC to pause the firms’ plans to charge for robotaxi rides in the city at all hours. Chiu’s arguments parallel comments made by residents and other city officials during a public hearing ahead of the CPUC’s vote.\nDespite opposition, the CPUC voted to approve Cruise and Waymo’s final permit that allows both companies to operate 24 hours a day, seven days a week, charge for all driverless rides and expand their fleets. In the past week however, Cruise has had a series of snafus, including at least 10 of its driverless cars reportedly stalling and blocking traffic, which threatens to derail its commercial plans.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"A Cruise robotaxi was involved in a crash with an emergency vehicle late Thursday night at an intersection with a green traffic light at Polk and Turk streets in San Francisco. The collision left one passenger injured, who was treated on the scene and transported via ambulance for what is believed to be non-severe injuries. Cruise mentioned that the self-driving Chevy Bolt EV entered the intersection legally when it was struck by the emergency vehicle. The incident occurred after winning approval from the California Public Utilities Commission to expand commercial operations. Cruise has also experienced a series of issues with its driverless cars, including stalling and blocking traffic, in the past week, which has raised concerns about its commercial plans.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2023\/08\/18\/cruise-robotaxi-involved-in-a-crash-with-fire-truck-one-passenger-injured\/","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":46,"Time":"Updated 7:52 PM GMT+2, August 19, 2023","Title":"GM’s Cruise autonomous vehicle unit agrees to cut fleet in half after 2 crashes in San Francisco","Content":"General Motors’ Cruise autonomous vehicle unit has agreed to cut its fleet of San Francisco robotaxis in half as authorities investigate two recent crashes in the city.\nThe state Department of Motor Vehicles asked for the reduction after a Cruise vehicle without a human driver collided with an unspecified emergency vehicle on Thursday.\n“The DMV is investigating recent concerning incidents involving Cruise vehicles in San Francisco,” the DMV said Saturday in a statement to The Associated Press. “Cruise has agreed to a 50% reduction and will have no more than 50 driverless vehicles in operation during the day and 150 driverless vehicles in operation at night.”\nThe development comes just over a week after California regulators allowed Cruise and Google spinoff Waymo to operate autonomous robotaxis throughout San Francisco at all hours, despite safety worries spurred by recurring problems with unexpected stops and other erratic behavior.\nOTHER NEWS\nDianne Feinstein was at the center of a key LGBTQ+ moment. She’s being lauded as an evolving ally\nThe Dianne Feinstein they knew: Women of the Senate remember a tireless fighter and a true friend\nPhotographs documented US Sen. Dianne Feinstein’s groundbreaking career in politics\nThe decision Aug. 10 by the Public Utilities Commission made San Francisco the first major U.S. city with two fleets of driverless vehicles competing for passengers.\nOn Thursday around 10 p.m., the Cruise vehicle had a green light, entered an intersection, and was hit by the emergency vehicle responding to a call, the San Francisco Chronicle reported, based on tweets from Cruise.\nThe robotaxi was carrying a passenger, who was taken by ambulance to a hospital with injuries that were not severe, Cruise told the newspaper.\nAlso Thursday night, a Cruise car without a passenger collided with another vehicle in San Francisco, the newspaper reported.\nThe San Francisco Fire Department did not immediately respond to a request for comment from the newspaper.\nThe robotaxi almost immediately identified the emergency response vehicle as it came into view, Greg Dietrerich, Cruise’s general manager in San Francisco, said in a statement on the company website.\nAt the intersection, visibility is occluded by buildings, and it’s not possible to see objects around a corner until they are very close to the intersection, Dietrerich’s statement said. The Cruise autonomous vehicle detected the siren as soon it was distinguishable from background noise, he wrote.\n“The AV’s ability to successfully chart the emergency vehicle’s path was complicated by the fact that the emergency vehicle was in the oncoming lane of traffic, which it had moved into to bypass the red light,” Dietrerich wrote.\nThe Cruise vehicle identified the risk of a crash and braked, reducing its speed, but couldn’t avoid the collision, he wrote.\nCruise vehicles have driven more than 3 million autonomous miles in the city and have interacted with emergency vehicles more than 168,000 times in the first seven months of this year alone, the statement said. “We realize that we’ll always encounter challenging situations, which is why continuous improvement is central to our work.”\nThe company will work with regulators and city departments to reduce the likelihood of a crash happening again, Dietrerich wrote.\nThe DMV said the fleet reduction will remain until its investigation ends and Cruise takes corrective action to improve safety. “The DMV reserves the right, following investigation of the facts, to suspend or revoke testing and\/or deployment permits if there is determined to be an unreasonable risk to public safety.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Title\": \"GM’s Cruise autonomous vehicle unit agrees to cut fleet in half after 2 crashes in San Francisco\",\n\"Details\": [\n\"General Motors’ Cruise autonomous vehicle unit has agreed to cut its fleet of San Francisco robotaxis in half as authorities investigate two recent crashes in the city.\",\n\"The state Department of Motor Vehicles asked for the reduction after a Cruise vehicle without a human driver collided with an unspecified emergency vehicle on Thursday.\",\n\"The DMV is investigating recent concerning incidents involving Cruise vehicles in San Francisco, and Cruise has agreed to a 50% reduction in its fleet, limiting it to no more than 50 driverless vehicles in operation during the day and 150 driverless vehicles in operation at night.\",\n\"On Thursday around 10 p.m., a Cruise vehicle had a green light, entered an intersection, and was hit by an emergency vehicle responding to a call. The robotaxi was carrying a passenger who was taken to a hospital with non-severe injuries.\",\n\"Also on Thursday night, a Cruise car without a passenger collided with another vehicle in San Francisco.\",\n\"The Cruise vehicle detected the emergency response vehicle and attempted to reduce its speed but couldn't avoid the collision.\",\n\"Cruise vehicles have driven more than 3 million autonomous miles in the city and have interacted with emergency vehicles more than 168,000 times in the first seven months of this year alone.\",\n\"The DMV stated that the fleet reduction will remain in effect until its investigation concludes and Cruise takes corrective actions to improve safety.\"\n]\n}\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/san-francisco-cruise-robotaxi-crash-e721a81c1366c71a03c0aa50aa2e98f3","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":93,"Time":"19 May 2016","Title":"Google patents 'sticky car' to reduce crash injuries","Content":"Google patents 'sticky car' to reduce crash injuries\nPublished\n19 May 2016\nShare\nIMAGE SOURCE,\nGOOGLE\/UNITED STATES PATENT AND TRADEMARK OFFICE\nImage caption,\nStruck pedestrians would be glued to the car\nGoogle has patented a sticky coating for driverless cars that could reduce damage done to pedestrians in the event of a collision.\nPeople struck by the vehicle would become glued to its bonnet, rather than being thrown off and further injured.\nOne transport safety professor said the concept could reduce injuries sustained in a pedestrian collision.\nGoogle did not say whether it intended to implement the idea in its driverless vehicles.\n\"It does have some merit to it,\" said Andrew Morris, Professor of Human Factors in Transport Safety at Loughborough University.\nIMAGE SOURCE,\nAP\nImage caption,\nGoogle has been testing its autonomous cars in California\n\"When pedestrian accidents happen, often the person is thrown up onto the bonnet and there may be injuries from that contact, but sometimes there are not.\n\"But when a driver brakes in a collision, a totally natural reaction, the pedestrian is thrown onto the ground and you can get injuries from that contact.\"\nGoogle's patent explained that the adhesive layer would be hidden under a protective coating, to ensure it did not gather debris during journeys.\n\"Upon the initial impact between the colliding object and the vehicle, the coating is broken, exposing the adhesive layer,\" the patent document said.\nGoogle has been testing driverless cars since 2009, and the company says its vehicles have driven more than a million miles autonomously.\nIMAGE SOURCE,\nGOOGLE\/UNITED STATES PATENT AND TRADEMARK OFFICE\nImage caption,\nThe sticky coating would be hidden by a protective layer\nWhile there have been some minor collisions along the way, the most serious involving a bus, the company's monthly reports suggest a majority of incidents are the fault of human drivers in other vehicles.\nProf Morris said it remained to be seen whether the idea would work in practice.\n\"Whether they could realistically make a car that has the right material in it and works reliably, we can't categorically say that,\" he told the BBC.\nKevin Clinton, head of road safety at the Royal Society for the Prevention of Accidents, said: \"As well as developing new technology that prevents pedestrians from being hit in the first place, it's also crucial to develop ways of reducing the severity of injuries suffered if a pedestrian is knocked over.\"\n\"This idea is a fascinating example of just how far vehicle technology is changing,\" he told the BBC.\n\"It will obviously need to be developed and tested to ensure that it works reliably and doesn't cause any unintended consequences.\"\nNick Reed, academy director at the Transport Research Lab (TRL), agreed that the idea made sense in principle.\n\"The idea behind Google's patent is not new - others, including TRL, have discussed and presented on the idea of 'capturing' pedestrians after they are impacted,\" he told the BBC.\n\"In 1974, following initial research into the future of car safety technology by British Leyland, five prototypes were developed. This featured a spring-loaded pedestrian-catching cage which was activated in the event of an impact and raised to prevent the accident victim from sliding down or being thrown forward.\n\"The use of a sticky layer could be part of a number of methods used to help manage dynamic movements of pedestrians as a result of a collision.\"\nMore on this story\nGoogle self-driving car hits a bus\nPublished\n29 February 2016","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Google has patented a sticky coating for driverless cars that could reduce damage done to pedestrians in the event of a collision. People struck by the vehicle would become glued to its bonnet, rather than being thrown off and further injured. The adhesive layer would be hidden under a protective coating to ensure it did not gather debris during journeys. The patent explains that upon the initial impact between the colliding object and the vehicle, the coating is broken, exposing the adhesive layer. While there have been some minor collisions during Google's testing of driverless cars, the company's monthly reports suggest a majority of incidents are the fault of human drivers in other vehicles. Experts have differing opinions on whether the idea would work in practice, but it represents an innovative approach to pedestrian safety in driverless cars.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-36332146","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":88,"Time":"Wed 6 Sep 2023 00.28 CEST","Title":"Self-driving car blocking road ‘delayed patient care’, San Francisco officials say","Content":"San Francisco authorities and the company Cruise have offered conflicting accounts of an incident in which the fire department said two of the company’s robotaxis delayed an ambulance transporting a patient with critical injuries who later died at a hospital.\nThe rebel group stopping self-driving cars in San Francisco – one cone at a time\nRead more\nThe company denied the city’s claims and shared video with the Guardian that shows one of the vehicles quickly leaving the area. Reports of the incident have garnered outrage in San Francisco, which has been battling over the use of robotaxis vehicles in the city.\nConcerns over driverless taxis in the city have been growing for months amid reports that the vehicles have caused mayhem, blocking thoroughfares and getting in the way of first responders.\nProtesters, including taxi drivers and city transit workers, gathered outside the Cruise headquarters on Monday and mentioned the situation involving the robotaxis and the ambulance multiple times.\nIn an incident on 14 August, first responders were treating a pedestrian who had been struck by a vehicle and had life-threatening injuries with significant bleeding. Two autonomous Cruise vehicles had stopped in nearby lanes and were not moving, “blocking ingress and egress”, according to a San Francisco fire department report obtained by Forbes.\nAs the emergency crews loaded the patient into an ambulance, the vehicles remained stopped in the two lanes and police attempts to takeover the vehicles manually were unsuccessful, the report states. The fire department had to locate a police officer on scene and ask him to move to his vehicle in order to leave the scene, which the report states “further delayed patient care”.\n“This delay, no matter how minimal, contributed to a poor [patient] outcome,” an unidentified first responder wrote in the report. “In any significant traumatic event, time is of the essence to transport the [patient] to definitive care in order to give them the best possible chance at survival.”\nCruise disputes the fire department’s claims and offers a different account of what happened. The vehicles encountered the emergency scene, the company said in a statement, and one left the area when a stoplight turned green while the other stopped “to yield to first responders” who were directing traffic. During the entire period the vehicle was stopped, traffic was “unblocked and flowing” to the right of the robotaxi.\n“The ambulance behind the AV had a clear path to pass the AV as other vehicles, including another ambulance, proceeded to do. As soon as the victim was loaded into the ambulance, the ambulance left the scene immediately and was never impeded from doing so by the AV,” a spokesperson, Tiffany Testo, said in a statement.\nVideo of the incident provided by Cruise and viewed by the Guardian shows three Cruise vehicles, along with other cars, near the scene as first responders are arriving. Two autonomous vehicles leave the area – one continues up the street, away from first responders, where it briefly pauses before continuing on.\nA stalled Cruise vehicle remained on scene with cars – including an ambulance –passing in the lane to the right of the vehicle. The ambulance assisting the victim did not pass to the right of the vehicle and instead drove on its left side.\nThe patient died 20 to 30 minutes after reaching the hospital, according to the fire department’s report.\nSign up to First Thing\nFree daily newsletter\nOur US morning briefing breaks down the key stories of the day, telling you what’s happening and why it matters\nPrivacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.\nTesto noted that the patient’s death was caused by a human driver, and pointed to the company’s safety record.\n“During the course of more than 3 million miles of fully autonomous driving in San Francisco we’ve seen an enormous number of emergency vehicles – more than 168,000 interactions just in the first seven months of this year alone,” she said in a statement. “Our technology is always improving, and we maintain an open line of communication with first responders to receive feedback and discuss specific incidents to improve our response.”\nLast month, California regulators had allowed Cruise and the Google spinoff Waymo to expand and operate robotaxis throughout San Francisco at all hours. The companies had previously been operating a small fleet of vehicles without drivers during off-peak hours.\nJust a week later, Cruise agreed to cut its robotaxi fleet in the city in half as authorities investigate two crashes.\nWaymo and Cruise have disputed some accounts of the vehicles causing problems and argued the technology is safe and beneficial.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about an incident involving self-driving cars in San Francisco. According to the report, an incident occurred where two autonomous Cruise vehicles delayed an ambulance transporting a patient with critical injuries. The patient later died at a hospital, and there are conflicting accounts between San Francisco authorities and Cruise regarding the incident. San Francisco authorities claim that the vehicles blocked the ambulance and delayed patient care, while Cruise disputes this, stating that one vehicle left when a stoplight turned green, and the other yielded to first responders. The incident has caused outrage in San Francisco, with concerns growing over the use of robotaxis in the city, blocking thoroughfares, and getting in the way of first responders. There have also been protests outside Cruise headquarters related to this incident.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/us-news\/2023\/sep\/05\/san-francisco-cruise-robotaxi-death-ambulance","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":96,"Time":"5:00 PM GMT+1•February 8, 2015","Title":"Driverless Cars On The Road To Fender Benders","Content":"Iliya Rybchin\nContributor\nEditor’s note: Iliya Rybchin leads the media and entertainment business for Highnote Foundry.\nThis year, California will allow the testing of autonomous vehicles on public roads. Driverless cars will log thousands of miles this year – an Audi A7 already made a safe 550-mile trip from Silicon Valley to the 2015 Consumer Electronics Show in Las Vegas. However, it’s only a matter of time before a driverless car gets into a fender bender.\nA driverless car probably won’t cause the accident. It could easily be an inattentive driver rear ending the driverless car at an intersection. But as soon as that first incident occurs, the ensuing insurance claim and potential lawsuit will create ripples across multiple industries.\nWhen a human is not making driving decisions, who is really at fault? The manufacturer of the car? The manufacturer of the components in the car that process the data? The programmers who wrote the code? The driver who put their faith in the car and did not intervene in time? Should auto insurance policies become product liability policies?\nThis event will have broad implications, as drones, robots, sensor networks and other “human-less” technologies open up new areas of risk (and insurance) that today’s companies are not equipped to handle.\nSomeone Has to Take the Blame and Pay for It\nThe most obvious implication will be on auto-insurance policies. Individual policies will be modified to include riders and other waivers to avoid potential product liability claims. But the companies involved in developing the technology required for driverless cars will also be required to dramatically change their insurance policies.\nFor decades, automobile manufacturers have incorporated the costs and requirements associated with high levels of product liability into their operating procedures. But until now, this has not been something that chip manufacturers and technology developers have had – or been required to have – as a core competency.\nYou Can’t Call a Driverless Car to the Witness Stand\nCould we be facing the emergence of an Orwellian world? Already, dashboard cameras are standard in Russia, where fears of violence, corruption and misrepresentation combined with the high number of accidents have resulted in proactive consumer protections to obviate ambiguity as much as possible. And more and more frequently in cities like New York, taxis are outfitted with passenger cameras to protect both the driver and the passenger from untrue allegations.\nWhen lawsuits evolve around driverless cars, it will be difficult for courts to process and account for the class of data evidence that will evolve – reams of statistics about algorithms, traffic patterns, precipitation rates and traffic light patterns. It may well be easier, more practical and less open to interpretation to have a video record of every incident.\nHow will car and device manufacturers respond to demand as insurance companies give deep discounts for cars installed with automated cameras? What new markets will emerge to provide solutions, including not just the cameras themselves but also storage and parsing of the video, interpretation, and certification of authenticity?\nBeware of Unintended Consequences\nI hope that any incident involving driverless cars won’t squash the rapid evolution of this industry. While it’s impossible to forecast all of the widespread implications of emerging technologies, it will certainly be fascinating to see them evolve. For example:\nHumans are generally rational drivers and have route preferences. Driverless cars are always logical and have no preferences. Automated GPS systems choose (and constantly update) the quickest path from point A to point B given based on thousands of variables. When machines are in charge, it’s easy to imagine that quiet, upscale residential neighborhoods in congestion-laden cities like L.A. could become thoroughfares for driverless cars. Imagine the potential impact on congestion patterns and real estate values. Will depressed areas gentrify when thousands of Google-powered cars start driving through? Will strip malls, fast food restaurants, and gas stations that rely on congestion and drive-by traffic start to go out of business?\nWhat does the “driver” actually do when he\/she is not actually driving? When these cars go mainstream, new entertainment forms are sure to emerge to fill the time previously spent paying attention to the road. New forms of entertainment will need to be engaging but not too distracting. Is this a shot in the arm for a media industry desperate for new revenue streams? Undoubtedly our legislators and insurance providers will have some opinions about what constitutes as “safe in-car entertainment.”\nNobody can know all of the downstream implications of driverless cars. However, a simple fender bender will put the entire industry under a microscope. Clearly insurance companies – not generally known for their ability to innovate and pivot quickly – have both an opportunity and a risk. So, what should insurance companies be doing? As Alan Kay said, “the best way to predict the future is to invent it.”\nIn order to ensure a positive outcome from whatever changes are on the horizon, companies must take an active, participatory role in the conversation. Insurance companies can’t afford to be passive or to wait until driverless cars come to market. Rather, they must create their own future. This could mean being an investor in related technologies or entering into strategic partnerships.\nSo far, the discussions around driverless cars have involved Google, Audi, Mercedes, Toyota, and even Intel, Qualcomm, Navstar and Boeing. If driverless cars are to really become a mass-market reality, companies like Progressive, Aetna, Allstate and Geico better get into the conversation quickly.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news discusses the potential implications and challenges of driverless cars, including the possibility of accidents involving them and the resulting insurance and liability issues. It highlights the need to determine fault in accidents involving autonomous vehicles and discusses how the emergence of driverless cars could impact various industries, including insurance and entertainment. The article also touches on the need for companies, including insurance providers, to proactively engage with the evolving technology and take an active role in shaping its future.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2015\/02\/08\/driverless-cars-on-the-road-to-fender-benders\/","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":89,"Time":null,"Title":"Tesla fatal car crash prompts NTSB investigation","Content":"cameraImage Credits: Tesla\nThe United States National Transportation Safety Board is conducting an investigation into a fatal car crash involving a Tesla Model X car. On March 23, a Tesla car crashed into a freeway divider, killing the driver, causing a fire and shutting down two lanes of Highway 101 near Mountain View, Calif. It’s not clear if Tesla’s automated control system, Autopilot, was active at the time of the crash, the NTSB said in a tweet.\n“We have been deeply saddened by this accident, and we have offered our full cooperation to the authorities as we work to establish the facts of the incident,” a Tesla spokesperson said in a statement to TechCrunch.\nThis investigation comes shortly after a fatal accident involving one of Uber’s self-driving cars in Tempe, Ariz. prompted the NTSB to send over a field team. According to the NTSB’s most recent update, the team was meeting with representatives from Uber, the NHTSA and Tempe Police Department. The department also said it was gathering and collecting information about the test vehicle’s technology, the pedestrian and the safety driver.\nLast year, the NTSB looked into a 2016 accident involving Tesla’s Autopilot system in Florida. The NTSB partially faulted Tesla for the fatal crash, saying the system operated as intended but that the driver’s inattentiveness, due to over-reliance on the Autopilot system, resulted in the accident.\nMore TechCrunch\nTinder goes ultra-premium, Amazon invests in Anthropic and Apple explains its new AirPods\nWhy Solana, Polygon and Aptos expect the enterprise to drive mass adoption\nVenture capital is opening the gates for defense tech\nTech layoffs are all but a thing of the past","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a fatal car crash involving a Tesla Model X car on March 23 near Mountain View, Calif. The crash resulted in the death of the driver, a fire, and the shutdown of two lanes of Highway 101. The United States National Transportation Safety Board (NTSB) is conducting an investigation to determine the cause of the crash. It is not clear if Tesla's automated control system, Autopilot, was active at the time of the crash. This investigation comes after a previous NTSB investigation into a 2016 accident involving Tesla's Autopilot system in Florida, where the NTSB partially faulted Tesla for the fatal crash, citing the driver's inattentiveness due to over-reliance on the Autopilot system.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2018\/03\/27\/tesla-fatal-car-crash-prompts-ntsb-investigation\/","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":95,"Time":"Sat 8 Jul 2023 01.29 CEST","Title":"Protesters develop novel way to build cone-sensus against driverless cars","Content":"A group of San Francisco organizers are encouraging people to put traffic cones on the hoods of driverless vehicles as a form of protest against the cars’ expansion on city streets.\nA video of the group’s actions with step-by-step instructions on how to disable a robo-taxi with a cone has gone viral on Twitter and sparked intense debates about the pros and cons of autonomous vehicles and the value of protesting in this way.\nTechScape: Self-driving cars are here and they’re watching you\nRead more\nSafe Street Rebel, a group of organizers that advocate for pedestrian safety and reducing the number of cars on roads, is behind this stunt. It hopes it will raise the public’s awareness of the potential dangers driverless taxis pose before a pivotal vote by the California public utilities commission set to take place on 13 July.\nThe vote would allow Cruise, a company controlled by the automaker General Motors, and Waymo, a Google spinoff, to charge people for rides as a part of the state’s driverless autonomous vehicles passenger service deployment program, according to the meeting agenda.\nVideos of driverless cars with cones on their hoods have garnered thousand of likes on Twitter and led Safe Street Rebel to dub the days leading up to the meeting as “week of cone”.\n“It’s a great time,” one of the organizers said of putting cones on the hoods of robo-taxis. “We’re not damaging anyone’s property, it’s very fixable, but it is a funny and effective tactic that has really resonated.”\nWaymo however does not see the same frivolity the organizer describes. It says the cone scheme reflects a lack of understanding of how autonomous vehicles work and is “vandalism and encourages unsafe and disrespectful behavior on our roadways”, a spokesperson told the Guardian. It says it will call police on anyone caught interfering with their fleet.\nThe organizer, who asked to remain anonymous due to potential legal or punitive actions, said that this is just the latest of several actions Safe Streets Rebel has done in pursuit of getting cars off city streets, getting more money into public transportation and keeping pedestrians and bikers safe. Usually it focuses on human drivers, but it argues that driverless cars pose many risks to safety and the environment but are being sold as a strong alternative to vehicles driven by people.\n“They still require wide roads, tire wear, they have cameras everywhere,” the organizer said of robo-taxis. “It’s not just about ‘are they safer than a human driver?’ We want healthy cities that don’t require these high tech surveillance pods moving around.”\nSafe Streets Rebel is not alone in its skepticism of driverless taxis. In late July, the San Francisco fire chief, Jeanine Nicholson, urged the state to move slowly on allowing fleets of driverless cars to roam city streets 24x7. Driverless cars have been accused of blocking emergency vehicles and buses.\nIncidents that have attracted headlines included a Cruise robo-taxi that appeared to idle in the midst of a mass shooting for several minutes in early June. In January a firefighter was reportedly forced to break the window of a Cruise vehicle to stop it after the car nearly drove over hoses that were being used to put out a raging house fire.\nIn addition, the California department of motor vehicles has logged nearly 70 autonomous vehicle collision reports this year, including a 21 May incident in San Francisco in which a dog was struck and killed by a Waymo vehicle operating in autonomous mode but with an employee in the front seat, according to the DMV report.\nIn a statement to the Guardian, Cruise says that its cars are currently used to give free rides to people who work late into the night and do not have a reliable means of transportation. It adds that the cone protest has not had a meaningful impact on its fleet’s operation but it can block their rideshare and food delivery effort by adding to traffic and congestion.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"Protesters in San Francisco have developed a novel way to protest against driverless cars by placing traffic cones on the hoods of these vehicles. This protest, led by Safe Street Rebel, aims to raise awareness of potential dangers associated with driverless taxis before a pivotal vote by the California public utilities commission on 13 July. The vote concerns allowing companies like Cruise and Waymo to charge people for rides as part of the state's driverless autonomous vehicles passenger service deployment program. The protest has gained attention on social media, with videos of driverless cars with cones on their hoods going viral. Waymo has criticized the protest, calling it vandalism and encouraging unsafe behavior on the road. Safe Street Rebel has a history of advocating for pedestrian safety and reducing the number of cars on the road, and it sees driverless cars as posing risks to safety and the environment. There have been incidents involving driverless cars blocking emergency vehicles and buses, as well as reports of collisions, including one in which a dog was struck and killed by a Waymo vehicle.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2023\/jul\/07\/san-francisco-autonomous-cars-protest-cone","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":97,"Time":null,"Title":"Who is at fault if a driverless car crashes?","Content":null,"Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about determining fault in driverless car crashes. However, the specific details are not provided in the title.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/av\/technology-41055782","query_name":"updated_file_driverless_car_incidents.xlsx"}
{"num":4,"Time":"29 February 2016","Title":"Google self-driving car hits a bus","Content":"Google self-driving car hits a bus\nPublished\n29 February 2016\ncomments\nComments\nShare\nIMAGE SOURCE,\nAP\nImage caption,\nGoogle cars have clocked up over one million miles on public roads\nBy Dave Lee\nNorth America technology reporter\nOne of Google's self-driving cars crashed into a bus in California earlier this month. There were no injuries.\nIt is not the first time one of Google's famed self-driving cars has been involved in a crash, but it may be the first time it has caused one.\nGoogle is to meet with California's Department of Motor Vehicles (DMV) to discuss the incident, and determine where the blame lies.\nOn 14 February the car, travelling at 2mph (3km\/h), pulled out in front of a public bus going 15mph (24km\/h).\nThe human in the Google vehicle reported that he assumed the bus would slow down to let the car out, and so he did not override the car's self-driving computer.\nThe crash happened in Mountain View, near Google's headquarters.\nIn a statement, Google said: \"We clearly bear some responsibility, because if our car hadn't moved, there wouldn't have been a collision.\n\"That said, our test driver believed the bus was going to slow or stop to allow us to merge into the traffic, and that there would be sufficient space to do that.\"\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nIn all previous Google self-driving car collisions other road users were to blame\nThe company's self-driving cars have clocked up well over a million miles across various states in the US, and until now have only reported minor \"fender benders\" - the American slang for a small collision.\nIn all of those cases, other road users were to blame.\nGoogle releases a monthly report detailing the testing of its self-driving technology.\nAhead of the February report's publication, due Tuesday, a traffic incident filing was made public by the DMV.\nA setback\n\"The Google AV [autonomous vehicle] test driver saw the bus approaching in the left side mirror but believed the bus would stop or slow to allow the Google AV to continue,\" the report read.\n\"Approximately three seconds later, as the Google AV was re-entering the centre of the lane it made contact with the side of the bus.\n\"The Google AV was operating in autonomous mode and travelling at less than two mph, and the bus was travelling at about 15mph at the time of contact.\"\nThe car's movements were made more complex, the report said, by the presence of sandbags on the road.\nGoogle said it had now refined its self-driving algorithm.\n\"From now on, our cars will more deeply understand that buses (and other large vehicles) are less likely to yield to us than other types of vehicles, and we hope to handle situations like this more gracefully in the future.\"\nIf the DMV considers the Google car to be at fault for the collision, it could be seen as a setback for the company's ambitious autonomous vehicle plans.\nThe bus crash came just four days after a legal breakthrough for the self-driving project - the US National Highway Traffic Safety Administration told Google it would likely give the self-driving computer the same legal treatment as a human driver.\nThat decision would pave the way for self-driving cars without any typical controls, such as a steering wheel or pedals.\nFollow Dave Lee on Twitter @DaveLeeBBC and on Facebook\nMore on this story\nWhat is driving car makers to CES?\nPublished\n6 January 2016\nFord says it leads in self-driving tech\nPublished\n5 January 2016\nView comments","Relevant":true,"Overview":"Based on the provided news article, it appears to be about a self-driving car crash. Here's the JSON format response:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"One of Google's self-driving cars crashed into a bus in California earlier this month. There were no injuries. On 14 February, the car, traveling at 2mph (3km\/h), pulled out in front of a public bus going 15mph (24km\/h). The human in the Google vehicle reported that he assumed the bus would slow down to let the car out, and so he did not override the car's self-driving computer. The crash happened in Mountain View, near Google's headquarters. In a statement, Google said: 'We clearly bear some responsibility, because if our car hadn't moved, there wouldn't have been a collision.'\"\n}\nThis news is relevant to self-driving car crashes because it describes an incident where a Google self-driving car collided with a bus, providing specific details about the accident and the circumstances leading up to it.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-35692845","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":9,"Time":"19 August 2022","Title":"Driverless cars: Experts warn no easy answer to how safe they should be","Content":"Driverless cars: Experts warn no easy answer to how safe they should be\nPublished\n19 August 2022\ncomments\nComments\nShare\nIMAGE SOURCE,\nGETTY IMAGES\nBy Chris Vallance\nTechnology reporter\nThere is no easy answer as to how safe self-driving cars should be, an adviser to a new government-backed report has told the BBC.\nThe report, from the Centre for Data Ethics and Innovation, warns that it might not be enough for self-driving cars to be safer than normal cars.\nIt says the public may have little tolerance for driverless car crashes - even if they are safer on average.\nIt comes as the government sets out its plans for self-driving cars.\nNo self-driving cars are currently allowed on UK roads, but the government has now said the first such vehicles could be on UK roads by 2025.\nSome cars, coaches and lorries with self-driving features could even be operating on motorways in the next year, the Department for Transport said.\nThe government's plans include a \"safety ambition\" for vehicles: that they should be as safe as a competent human driver.\nIt says this will inform standards that need to be reached in order to be allowed to self-drive on the roads - and manufacturers could face sanctions if they are not met.\nBut the CDEI, a government expert body that leads its work on trustworthy innovation using data and AI, says the question of how safe autonomous vehicles should be is not one science alone can answer.\nIt says the public may have little tolerance for crashes that are seen as the fault of \"faceless technology companies or lax regulation\" even if, it adds, on average driverless cars are safer than humans.\nAnd if the public expect self-driving cars to be as safe as trains or planes, it would require a 100-fold increase in average safety over manually driven cars, it warns.\n\"What we wanted to do was say there's not an easy answer to this question\" said Professor Jack Stilgoe of University College London who advised the CDEI. He suggested that establishing how safe they should be was a democratic decision.\nThe CDEI says it is also important to consider how risk is distributed between different groups. Even if there are improvements in overall safety, \"some groups may see substantial safety improvements while others see none or even face new hazards\".\nBiased drivers\nThe report advises that other risks will need scrutiny as the technology is rolled out.\nOne is the potential for bias in algorithms controlling the cars.\nIt warns that some groups, such as wheelchair users, may be underrepresented in data used to train the software algorithms which control the cars - potentially causing bias.\nThe report also says that self-driving cars should be clearly identified, and that \"people have a right to know what sort of agents they are sharing the road with\".\nAn academic survey quoted by the CDEI in the report suggested that 86% of the public agreed with this.\nProfessor Stilgoe said there were also serious moral questions about how the testing of self-driving vehicles is conducted on public roads, as other road users could in effect become participants in these trials whether they liked it or not.\n\"There is something quite important about the ethical principle of informed consent,\" he said.\nThe technology might result in pressure to alter roads and the rules of the road to suit self-driving cars.\nProfessor Stilgoe said these needed to be debated and discussed transparently.\n\"The danger is sort of sleepwalking in to a world in which these changes happen in order to suit one mode of transport - and the benefits then don't get spread very widely,\" he said.\nSelf-driving road map\nOn Friday, the government is publishing a policy paper setting out planned laws ready for the introduction of self-driving vehicles on British roads.\nThe new laws will be brought forward when parliamentary time allows, the government has said.\nThe legislation is expected to state that manufacturers are responsible for the vehicle's actions when self-driving - something the Law Commission recommended earlier this year.\nIn April, proposed updates to the Highway Code were announced that would allow drivers to watch entertainment on a car's screen while the vehicle was self-driving.\nInitially this would likely be whilst the vehicle was driving at slow speed on a motorway, such as in congested traffic.\nVehicles that can drive themselves on motorways could be available to purchase within the next year, the government reiterated.\nThe government has also repeated its goal of a wider rollout of self-driving technologies by 2025 and £100m of funding for industry and research.\nThatcham Research, the motor insurers' automotive research centre, welcomed the government's ambition but warned \"complete clarity around the driver's legal responsibilities\" was needed, along with transparency on how the technology is marketed, \"how the dealer describes systems when handing over the keys and how the self-driving system itself communicates with the driver\".\nRelated Topics\nDriverless cars\nMore on this story\nWatching TV to be allowed in self-driving cars\nPublished\n20 April 2022\nMajor legal changes needed for driverless cars\nPublished\n26 January 2022\nView comments","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving car safety because it discusses the challenges and considerations surrounding the safety of self-driving cars. The article highlights that there is no easy answer to how safe self-driving cars should be and mentions concerns about public tolerance for self-driving car crashes, even if they are statistically safer than human-driven cars. It also mentions the government's plans for self-driving cars in the UK, including safety standards and sanctions for manufacturers if safety targets are not met. Additionally, the article discusses potential biases in algorithms controlling self-driving cars and the importance of transparency and identification of self-driving vehicles on the road. Overall, the news provides a detailed examination of safety-related issues and considerations in the development and deployment of self-driving cars.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-62598618","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":3,"Time":"Published 5:41 PM EST, Wed December 21, 2022","Title":"Tesla ‘full self-driving’ triggered an eight-car crash, a driver tells police","Content":"Washington\nCNN\n — \nA driver told authorities that their Tesla’s “full-self-driving” software braked unexpectedly and triggered an eight-car pileup in the San Francisco Bay Area last month that led to nine people being treated for minor injuries including one juvenile who was hospitalized, according to a California Highway Patrol traffic crash report.\nCNN Business obtained the report detailing the crash through a public records request Wednesday. California Highway Patrol reviewed videos that show the Tesla vehicle changing lanes and slowing to a stop.\nCalifornia Highway Patrol said in the Dec. 7 report that it could not confirm if “full self-driving” was active at the time of the crash. A highway patrol spokesperson told CNN Business on Wednesday that it would not determine if “full self-driving” was active, and Tesla would have that information.\nThe crash occurred about lunchtime on Thanksgiving, snarling traffic on Interstate 80 east of the Bay Bridge as two lanes of traffic were closed for about 90 minutes as many people traveled to holiday events. Four ambulances were called to the scene.\nThe pileup took place just hours after Tesla CEO Elon Musk had announced that Tesla’s driver-assist software “full self-driving” was available to anyone in North America who requested it. Tesla had previously restricted access to drivers with high safety scores on its rating system.\n“Full self-driving” is designed to keep up with traffic, steer in the lane and abide by traffic signals. It requires an attentive human driver prepared to take full control of the car at any moment. It’s delighted some drivers but also alarmed others with its limitations. Drivers are warned by Tesla when they install “full self-driving” that it “may do the wrong thing at the worst time.”\nThe report states that the Tesla Model S was traveling at about 55 mph and shifted into the far left-hand lane, but then braked abruptly, slowing the car to about 20 mph. That led to a chain reaction that ultimately involved eight vehicles to crash, all of which had been traveling at typical highway speeds.\nTesla’s driver-assist technologies, Autopilot and “full self-driving” are already being investigated by the National Highway Traffic Safety Administration following reports of unexpected braking that occurs “without warning, at random, and often repeatedly in a single drive.”\nThe agency has received hundreds of complaints from Tesla drivers. Some have described near crashes and concerns about their safety. This summer NHTSA upgraded the investigation to what it calls an engineering analysis, an indication that it’s seriously considering a recall.\nNHTSA told CNN Business a few days after the Thanksgiving Day crash that it was gathering addition information from Tesla and law enforcement about the crash.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a Tesla vehicle equipped with 'full self-driving' software that braked unexpectedly, triggering an eight-car pileup in the San Francisco Bay Area. The incident resulted in nine people being treated for minor injuries, including one juvenile who was hospitalized. The California Highway Patrol traffic crash report obtained by CNN Business details the crash, stating that the Tesla Model S was traveling at about 55 mph, shifted into the far left-hand lane, and then braked abruptly, causing a chain reaction involving eight vehicles. Additionally, the news mentions that Tesla's driver-assist technologies, including 'full self-driving,' are already being investigated by the National Highway Traffic Safety Administration (NHTSA) due to reports of unexpected braking incidents, and NHTSA is considering a recall. Therefore, this news is relevant to self-driving car crashes.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2022\/12\/21\/business\/tesla-fsd-8-car-crash\/index.html","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":7,"Time":"Published 1:48 AM GMT+2, May 25, 2018","Title":"Feds: Uber self-driving SUV saw pedestrian, did not brake","Content":"DETROIT (AP) — The autonomous Uber SUV that struck and killed an Arizona pedestrian in March spotted the woman about six seconds before hitting her, but did not stop because the system used to automatically apply brakes in potentially dangerous situations had been disabled, according to federal investigators.\nIn a preliminary report on the crash, the National Transportation Safety Board said Thursday that emergency braking is not enabled while Uber’s cars are under computer control, “to reduce the potential for erratic vehicle behavior.”\nInstead, Uber relies on a human backup driver to intervene. The system, however, is not designed to alert the driver.\nThe findings, which are not final, should be a warning to all companies testing autonomous vehicles to check their systems to make sure they automatically stop when necessary in the environment where they are being tested, said Alain Kornhauser, faculty chairman of autonomous vehicle engineering at Princeton University.\nUber, he said, likely determined in testing that its system braked in situations it shouldn’t have, possibly for overpasses, signs and trees. “It got spoofed too often,” Kornhauser said. “Instead of fixing the spoofing, they fixed the spoofing by turning it off.”\nIn the Tempe, Arizona, crash, the driver began steering less than a second before impact but didn’t brake until less than a second after impact, according to the NTSB, which has yet to determine fault.\nA video of the crash showed the driver looking down just before the vehicle struck and killed 49-year-old Elaine Herzberg in what is believed to be the first death involving a self-driving test vehicle.\nUber said in a company release that it has worked closely with the NTSB and is doing an internal review of its self-driving vehicle program. The company also has brought in former NTSB Chairman Christopher Hart as a safety advisor. “We look forward to sharing more on the changes we’ll make in the coming weeks,” the release said. The company declined further comment.\nThe report comes a day after Uber pulled its self-driving cars out of Arizona, eliminating the jobs of about 300 people who served as backup drivers and performed other jobs connected to the vehicles. Uber had suspended testing of its self-driving vehicles in Arizona, Pittsburgh, San Francisco and Toronto while regulators investigated the cause of the March 18 crash.\nSensors on the fully autonomous Volvo XC-90 SUV spotted Herzberg while the car was traveling 43 miles per hour and determined that braking was needed 1.3 seconds before impact, according to the report.\nA diagram in the NTSB report shows that the Uber system determined that the SUV needed to brake when it was at least 20 meters (65.6 feet) from Herzberg; it was traveling 39 mph (63 kilometers per hour) at impact. Kornhauser said that was enough distance for the SUV to stop, or slow considerably to mitigate damage from the crash.\nHerzberg was pushing a bicycle across a boulevard in the darkness when the crash occurred on a part of the road that had no crosswalk and was not lighted, the report said.\nShe was wearing dark clothing and did not look in the direction of the vehicle until just before impact. A toxicology report showed that she tested positive for methamphetamine and marijuana, according to the NTSB. Also, the bicycle had no side reflectors.\nUber also disabled the Volvo’s factory-equipped automatic emergency braking system when the vehicle is in autonomous mode, the report said.\nIn an interview with the NTSB, Uber’s backup driver said she had been monitoring the “self-driving interface.” While her personal and business telephones were in the vehicle, she said neither was in use at the time of the crash.\nThe NTSB said that all other aspects of the SUV’s self-driving system were running normally at the time, and there were no faults or diagnostic trouble messages.\nThe agency, which can make safety recommendations to other federal agencies, said information in the preliminary report can change as the investigation progresses and that no conclusions should be drawn from the report.\nThe report doesn’t provide “any decisive findings or conclusions,” said Daniel Scarpinato, spokesman for Arizona Gov. Doug Ducey. “We await the more thorough and final investigative report. Uber’s self-driving vehicle suspension remains in place.”\nTempe police turned their investigation over to prosecutors on Wednesday.\nAmanda Jacinto, a spokeswoman for the Maricopa County Attorney’s Office, said no decision has yet been made on charges against the driver or the ride-sharing company. She wouldn’t comment on the NTSB report.\nNo deadline has been set by the prosecutorial agency for deciding whether to bring charges, though typically those decisions are made within 30 days after getting cases from police, Jacinto said.\n_______\nAssociated Press writers Paul Davenport and Jacques Billeaud in Phoenix contributed to this report.","Relevant":true,"Overview":"Based on the provided news article, it is relevant to self-driving car crashes. Here's the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news article discusses a self-driving Uber SUV that struck and killed a pedestrian in Arizona. The autonomous vehicle detected the pedestrian about six seconds before the collision but did not apply the brakes because the system designed for emergency braking in potentially dangerous situations had been disabled. Instead, Uber relied on a human backup driver to intervene, but the system was not designed to alert the driver. The report highlights the absence of automatic braking during computer-controlled operation, which is a critical aspect of self-driving car safety. It also mentions the driver's delayed response in steering and braking, as well as the fact that Uber had suspended testing of its self-driving vehicles in multiple locations following the crash. The news provides insight into the circumstances of the crash and the safety concerns surrounding self-driving car technology.\"\n}\nSo, the news is indeed relevant to self-driving car crashes, as it discusses a fatal incident involving a self-driving Uber SUV and the disabling of the automatic emergency braking system, which is a critical safety feature in autonomous vehicles.","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/63ff0b97fe1c44f98e4ee02c70a6397e","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":11,"Time":"FEBRUARY 29, 2016","Title":"Google says it bears 'some responsibility' after self-driving car hit bus","Content":"By David Shepardson\n(Reuters) - Alphabet Inc's GOOGL.O Google said on Monday it bears \"some responsibility\" after one of its self-driving cars struck a municipal bus in a minor crash earlier this month.\nA prototype of Google's own self-driving vehicle is seen during a media preview of Google's current autonomous vehicles in Mountain View, California September 29, 2015. REUTERS\/Elijah Nouvelage\nThe crash may be the first case of one of its autonomous cars hitting another vehicle and the fault of the self-driving car. The Mountain View, California-based Internet search leader said it made changes to its software after the crash to avoid future incidents.\nIn a Feb. 23 report filed with California regulators, Google said the crash took place in Mountain View on Feb. 14 when a self-driving Lexus RX450h sought to get around some sandbags in a wide lane.\nGoogle said in the filing the autonomous vehicle was traveling at less than 2 miles per hour, while the bus was moving at about 15 miles per hour.\nThe vehicle and the test driver “believed the bus would slow or allow the Google (autonomous vehicle) to continue,” it said.\nBut three seconds later, as the Google car in autonomous mode re-entered the center of the lane, it struck the side of the bus, causing damage to the left front fender, front wheel and a driver side sensor. No one was injured in the car or on the bus.\nGoogle said in a statement on Monday that “we clearly bear some responsibility, because if our car hadn’t moved, there wouldn’t have been a collision. That said, our test driver believed the bus was going to slow or stop to allow us to merge into the traffic, and that there would be sufficient space to do that.”\nThe Santa Clara Valley Transportation Authority will investigate the circumstances of the accident, Stacey Hendler Ross, spokeswoman for the Santa Clara Valley Transportation Authority, said on Monday.\nShe said the Google car caused minor damage to the bus, striking the “pivoting joint,” or flexible area in the middle of the articulated bus. After the crash, 15 passengers on the bus were transferred to another bus.\nAn investigation to determine liability is pending, she said.\nJohn M. Simpson, privacy project director for advocacy group Consumer Watchdog, said the crash “is more proof that robot car technology is not ready for auto pilot.”\nRELATED COVERAGE\nUnlocking iPhone would leave millions exposed, Apple to tell Congress\nYahoo says may write down goodwill value of Tumblr\nSee more stories\nA spokesman for the California Department of Motor Vehicles said on Monday it will speak to Google to gather additional information, but added “the DMV is not responsible for determining fault.”\nSOFTWARE REFINED\nGoogle said it has reviewed this incident “and thousands of variations on it in our simulator in detail and made refinements to our software. From now on, our cars will more deeply understand that buses (and other large vehicles) are less likely to yield to us than other types of vehicles, and we hope to handle situations like this more gracefully in the future.”\nThere has been no official determination of fault in the crash. Google has previously said that its autonomous vehicles have never been at fault in any crashes.\nThe Mountain View Police Department said no police report was filed in the incident.\nA spokesman for the U.S. National Highway Traffic Safety Administration declined to comment.\nThe crash comes as Google has been making the case that it should be able to test vehicles without steering wheels and other controls.\nIn December, Google criticized California for proposing regulations that would require autonomous cars to have a steering wheel, throttle and brake pedals when operating on public roads. A licensed driver would need to be ready to take over if something went wrong.\nGoogle said in November that in six years of its self-driving project, it has been involved in 17 minor accidents during more than two million miles of autonomous and manual driving combined.\n“Not once was the self-driving car the cause of the accident,” Google said at the time.\nReporting by David Shepardson, additional reporting by Bernie Woodall; editing by Chris Reese, G Crosse\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": {\n\"Reason\": \"This news is about a self-driving car crash involving a Google self-driving vehicle and a municipal bus.\",\n\"Details\": \"In the news article, it is reported that a self-driving Lexus RX450h, owned by Alphabet Inc's Google, was involved in a minor crash with a municipal bus in Mountain View, California. The self-driving car attempted to get around some sandbags in a wide lane, traveling at less than 2 miles per hour, while the bus was moving at about 15 miles per hour. The self-driving car and its test driver believed that the bus would slow down or allow the autonomous vehicle to merge into traffic. However, three seconds later, as the self-driving car re-entered the center of the lane, it collided with the bus, causing damage to the left front fender, front wheel, and a driver-side sensor. Fortunately, there were no injuries reported. Google acknowledged responsibility for the collision, stating that if their car hadn't moved, the collision wouldn't have occurred. They also mentioned making software refinements to better handle situations involving buses and large vehicles.\"\n}\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-google-selfdrivingcar-idUSKCN0W22DG","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":12,"Time":"JUNE 4, 2020","Title":"Self-driving cars could only prevent a third of U.S. crashes: study","Content":"By Tina Bellon\n(Reuters) - Self-driving cars, long touted by developers as a way to eliminate road deaths, could likely only prevent a third of all U.S. road crashes, according to a study released on Thursday.\nThe Insurance Institute for Highway Safety (IIHS), a research group financed by U.S. insurers, found the remaining crashes were caused by mistakes that self-driving systems are not equipped to handle any better than human drivers.\nPartners for Automated Vehicle Education, a consortium of self-driving companies and researchers, said in a statement on Thursday the study wrongly assumed that automated cars could only prevent crashes caused by perception errors and incapacitation.\nSome 72% of crashes were avoidable, based on the study’s calculations, if accidents caused by speeding and violation of traffic laws were included, the consortium said.\nTraffic experts say roughly nine in 10 crashes result from human error and more than 36,000 people are estimated to have died in U.S. car crashes last year.\nSelf-driving vehicle developers, including traditional automakers and technology companies, have repeatedly positioned fully automated driving as a tool to drastically reduce road deaths.\nBut not all human mistakes can be eliminated by camera, radar and other sensor-based technology, according to the IIHS analysis of more than 5,000 representative police-reported crashes nationwide.\nOne-third of all crashes were the exclusive result of sensing and perception errors, or driver incapacitation, the study found.\nMost crashes were due to more complex errors, such as making wrong assumptions about other road users’ actions, driving too fast or too slow for road conditions, or making incorrect evasive maneuvers. Many crashes resulted from multiple mistakes.\n“Our goal was to show that if you don’t deal with those issues, self-driving cars won’t deliver massive safety benefits,” said Jessica Cicchino, IIHS vice president for research and a coauthor of the study.\nReporting by Tina Bellon; Editing by Richard Chang\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Based on the provided news article, here's the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to self-driving car crashes. According to the study mentioned in the article, self-driving cars are likely only capable of preventing a third of all U.S. road crashes. The study conducted by the Insurance Institute for Highway Safety (IIHS) found that self-driving systems cannot handle certain types of crashes any better than human drivers, particularly those caused by factors such as making wrong assumptions about other road users' actions, driving too fast or too slow for road conditions, or making incorrect evasive maneuvers. The study also found that one-third of all crashes were the exclusive result of sensing and perception errors or driver incapacitation. This information demonstrates that self-driving cars may not be as effective in preventing crashes as initially hoped.\"\n}\nThe news is relevant to self-driving car crashes because it discusses the limitations and challenges associated with self-driving technology in preventing various types of accidents, not just those caused by perception errors or incapacitation.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-self-driving-idUSKBN23B294","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":6,"Time":"10:40 PM GMT+2•June 6, 2023","Title":"A Waymo self-driving car killed a dog in ‘unavoidable’ accident","Content":"cameraImage Credits: Kirsten Korosec\nA Waymo robotaxi operating in autonomous mode struck and killed a small dog last month in San Francisco, according to an incident report filed with the California Department of Motor Vehicles. The incident appears to have been unavoidable, based on information provided in the report.\nAny collision involving an autonomous vehicle — even one in which the company is not at fault — can cause a backlash, particularly in a city like San Francisco, where there is already tension between city officials, AV tech companies and the public. If technological capability and a favorable regulatory environment are two legs of a solid AV commercialization stool, public perception is the crucial third. And a self-driving car killing a sweet pooch has the potential to kick out that third leg.\nIt also comes at a critical time for the industry and, specifically, for Waymo, which is expanding its robotaxi network in Phoenix and gearing up to charge fares for fully autonomous (meaning no human behind the wheel) rides in San Francisco.\nAccording to the report, one of Waymo’s self-driving Jaguar I-Pace cars was traveling on Toland Street, a low-speed street near Waymo’s depot, and the dog ran into the street. The vehicle was operating in autonomous mode, and a human safety operator was in the driver’s seat at the time of the accident.\nThe human operator didn’t see the dog, but the vehicle’s autonomous system did. However, a number of factors, including the speed and trajectory of the dog’s path, made the collision unavoidable, according to Waymo.\nIn response to our questions about the incident, Waymo sent TechCrunch the following statement:\nOn May 21 in San Francisco, a small dog ran in front of one of our vehicles with an autonomous specialist present in the driver’s seat, and, unfortunately, contact was made. The investigation is ongoing, however the initial review confirmed that the system correctly identified the dog which ran out from behind a parked vehicle but was not able to avoid contact. We send our sincere condolences to the dog’s owner. The trust and safety of the communities we are in is the most important thing to us and we’re continuing to look into this on our end.\nNeither the safety operator nor the autonomous system braked to avoid collision, according to Waymo. In both cases, that’s because of the “unusual path” the dog took at “a high rate of speed directly towards the side of the vehicle,” said a Waymo spokesperson.\nOne of the ways Waymo evaluates its autonomous driver’s collision avoidance performance is by comparing it to that of a model for a non-impaired, with eyes always on the conflict (NIEON) human driver. A Waymo spokesperson told TechCrunch that the company reconstructed last month’s event in simulation against the NIEON model, but the analysis showed a collision in this case was unavoidable.\nSagar Behere, VP of safety at AV verification and validation startup Foretellix, told TechCrunch that timing is a key factor in an AV’s ability to avoid collision. (Behere spoke to TechCrunch about AV technology generally, and not about Waymo specifically.)\n“If you saw the object, when did you see it? Did you see it in time to be able to act on it and make a good evasive maneuver?” said Behere. “Or maybe you saw it and predicted it would move in a way that required you to take no action? Or maybe you were about to take action, but then the object changed course.”\nDespite Waymo’s potential for plausible deniability here, the company and the industry at large are still at risk of a downturn in public perception, which could make AV expansion plans more difficult.\nRecent studies show that public perception around autonomous vehicles is improving, albeit slowly and mainly in regard to lower levels of automated driving like advanced driver assistance systems. People trust advanced driver assistance systems in today’s new vehicles (systems that require a human to be ready to take control) over fully autonomous vehicles when it comes to crash prevention, according to a study from the AAA’s Foundation for Traffic Safety, which collected responses from 2018 to 2020\nAnother study that collected nearly 6,000 responses between February and June 2022 found that trust in AVs decreases after a crash, while levels of concern increase. However, those with prior knowledge of AVs are more optimistic toward the technology, even after a crash, because they generally accept that AVs will not always make the right decisions at early stages of adoption.\nPublic perception aside, Waymo could face investigations from regulatory bodies like the National Highway Traffic Association. NHTSA requires manufacturers and operators of high-level autonomous vehicles to submit incident reports for crashes if the autonomous driving system was in use at any time within 30 seconds of the crash and the crash resulted in property damage or injury. The agency told TechCrunch it had reached out to Waymo for more information, but has no open investigations into the company at this time.\nIn 2018, when an autonomous vehicle from Uber’s now-shuttered AV unit hit and killed a pedestrian, the National Transportation Safety Board (NTSB) launched an investigation. Usually, the NTSB launches a highway investigation when there’s been a significant crash that highlights a potential national safety issue. A spokesperson from the agency told TechCrunch she doesn’t believe NTSB has any current investigations involving Waymo.\nThis article has been updated with NHTSA’s statement.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a Waymo self-driving car that was involved in an accident in San Francisco where it struck and killed a small dog. The incident report states that the collision was unavoidable, even though the vehicle's autonomous system correctly identified the dog. The human safety operator in the driver's seat did not see the dog, but the autonomous system did. However, due to the dog's high-speed and unusual path, neither the safety operator nor the autonomous system braked to avoid the collision. Waymo confirmed that they are investigating the incident, and the public perception of self-driving cars is a crucial concern in such cases, as it can impact the industry's expansion plans. Additionally, regulatory bodies like the National Highway Traffic Association (NHTSA) may launch investigations into such incidents, although NHTSA has not initiated an investigation at the time of this news.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2023\/06\/06\/a-waymo-self-driving-car-killed-a-dog-in-unavoidable-accident\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":5,"Time":"Mon 19 Mar 2018 23.48 CET","Title":"Self-driving Uber kills Arizona woman in first fatal crash involving pedestrian","Content":"An autonomous Uber car killed a woman in the street in Arizona, police said, in what appears to be the first reported fatal crash involving a self-driving vehicle and a pedestrian in the US.\nTempe police said the self-driving car was in autonomous mode at the time of the crash and that the vehicle hit a woman, who was walking outside of the crosswalk and later died at a hospital. There was a vehicle operator inside the car at the time of the crash.\nUber said in a statement on Twitter: “Our hearts go out to the victim’s family. We are fully cooperating with local authorities in their investigation of this incident.” A spokesman declined to comment further on the crash.\nWho's driving? Autonomous cars may be entering the most dangerous phase\nRead more\nThe company said it was pausing its self-driving car operations in Phoenix, Pittsburgh, San Francisco and Toronto. Dara Khosrowshahi, Uber’s CEO, tweeted: “Some incredibly sad news out of Arizona. We’re thinking of the victim’s family as we work with local law enforcement to understand what happened.”\nUber has been testing its self-driving cars in numerous states and temporarily suspended its vehicles in Arizona last year after a crash involving one of its vehicles, a Volvo SUV. When the company first began testing its self-driving cars in California in 2016, the vehicles were caught running red lights, leading to a high-profile dispute between state regulators and the San Francisco-based corporation.\nPolice identified the victim as 49-year-old Elaine Herzberg and said she was walking outside of the crosswalk with a bicycle when she was hit at around 10pm on Sunday. Images from the scene showed a damaged bike. The 2017 Volvo SUV was traveling at roughly 40 miles an hour, and it did not appear that the car slowed down as it approached the woman, said Tempe sergeant Ronald Elcock.\nElcock said he had watched footage of the collision, which has not been released to the public. Police identified the operator of the car as Rafaela Vasquez, 44, and said she was cooperative and there were no signs of impairment.\nA still image taken from video provided by ABC-15 at the scene, where a pedestrian with a bicycle was hit. Photograph: AP\nThe self-driving technology is supposed to detect pedestrians, cyclists and others and prevent crashes.\nJohn M Simpson, privacy and technology project director with Consumer Watchdog, said the collision highlighted the need for tighter regulations of the nascent technology.\n“The robot cars cannot accurately predict human behavior, and the real problem comes in the interaction between humans and the robot vehicles,” said Simpson, whose advocacy group called for a national moratorium on autonomous car testing in the wake of the deadly collision.\nSimpson said he was unaware of any previous fatal crashes involving an autonomous vehicle and a pedestrian.\nTesla Motors was the first to disclose a death involving a self-driving car in 2016 when the sensors of a Model S driving in autopilot mode failed to detect a large white 18-wheel truck and trailer crossing the highway. The car drove full speed under the trailer, causing the collision that killed the 40-year-old behind the wheel in the Tesla.\nEarlier this year, California regulators approved the testing of self-driving cars on public roads without human drivers monitoring inside.\n“The technology is not ready for it yet, and this just sadly proves it,” said Simpson.\nIn one recent incident, California police officers found a Tesla that was stopped in the middle of a five-lane highway and found a driver asleep behind the wheel. The man said the vehicle was in “autopilot”, which is Tesla’s semi-autonomous driver assist system, and he was arrested on suspicion of drunk driving.\nAn Uber self-driving car travels in Pittsburgh, Pennsylvania. Photograph: Angelo Merendino\/AFP\/Getty Images\nIn another recent case, a Tesla car rear-ended a fire truck on a freeway, with the driver again telling the authorities the car was in autopilot mode at the time of the collision.\nMichael G Bennett, an Arizona State University associate research professor who studies autonomous cars, said the self-driving vehicles have become ubiquitous around campus and on the streets in Tempe. Often they have operators behind the wheels, but sometimes they are fully autonomous with no human inside.\nThe fatal collision could spark significant calls for reform and reflections within the industry, he said.\n“It may be problematic for the industry, because one of their central arguments for the value of the technology is that it is superior to human drivers,” said Bennett, adding that autonomous cars should be able to detect pedestrians and avoid hitting them, even if they aren’t in crosswalks: “Every day, pedestrians in cities around the world step outside of the crosswalk.”\nRage against the machine: self-driving cars attacked by angry Californians\nRead more\nThe governor of Arizona, Doug Ducey, has been a strong proponent of allowing corporations to test the technology in his state, publicly slamming other governments for “over regulation” and in 2016 urging Uber to “ditch California” and launch in his region. In March, he issued new rules and said that more than 600 automated vehicles have driven on public roads in the state.\n“Our prayers are with the victim, and our hearts go out to her family,” Patrick Ptak, Ducey’s spokesman, said in an email to the Guardian, adding, “Public safety is our top priority.”\nLinda Bailey, the executive director of the National Association of City Transportation Officials (Nacto), said in an interview that there has not been enough regulatory oversight of testing and that some governments are overwhelmed trying to understand autonomous technology and its limitations.\n“There’s an essential role for the public sector in regulating the safety of these vehicles, which has been largely left to private companies,” she said, adding that Nacto supports third-party testing of the vehicles.\nTempe’s mayor, Mark Mitchell, defended the city’s ongoing support of autonomous vehicles in a statement Monday, saying: “All indications we had in the past show that traffic laws are being obeyed by the companies testing here.”\nThis article was amended on 22 March to correct the name of the Uber safety driver, who police had initially misidentified.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving car crashes. The article reports a fatal crash involving an autonomous Uber car in Arizona, where a woman was killed by the self-driving vehicle while walking outside of the crosswalk. The incident occurred while the self-driving car was in autonomous mode, making it one of the first reported fatal crashes involving a self-driving vehicle and a pedestrian in the US. The vehicle operator was inside the car at the time of the crash. Uber has paused its self-driving car operations in response to the incident. The victim was identified as 49-year-old Elaine Herzberg, who was hit by a 2017 Volvo SUV traveling at roughly 40 miles an hour. The collision raised concerns about the technology's ability to detect pedestrians and the need for tighter regulations in the autonomous vehicle industry. Additionally, the news mentions a previous crash involving one of Uber's self-driving vehicles and incidents involving Tesla's autopilot mode.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/mar\/19\/uber-self-driving-car-kills-woman-arizona-tempe","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":1,"Time":"Thu 22 Dec 2022 15.06 CET","Title":"Tesla behind eight-vehicle crash was in ‘full self-driving’ mode, says driver","Content":"The driver of a 2021 Tesla Model S told California authorities the vehicle was in “full self-driving mode” when the technology malfunctioned, causing an eight-vehicle crash on the San Francisco Bay bridge last month.\nThe crash on Thanksgiving Day resulted in two juveniles being transported to hospital and led to lengthy delays on the bridge. The incident was made public in a police report on Wednesday.\n‘Rude drivers will swerve in my lane’: are Tesla owners paying the price for Musk hate?\nRead more\nIt is the latest in a series of accidents blamed on Tesla technology. The electric automaker’s chief executive, Elon Musk, has heavily promoted “Full Self-Driving” (FSD) software, sold as $15,000 add-on to Tesla vehicles, but it faces legal, regulatory and public scrutiny.\nAfter the San Francisco accident, the driver told police the FSD software malfunctioned.\nThe police report said the vehicle was traveling at 55mph when it shifted lane but braked abruptly, slowing the car to about 20mph. That led to another vehicle hitting the Tesla and a chain reaction of crashes, according to Reuters.\nHowever, police were unable to determine if the software was in operation or that the driver’s account was accurate. The report was made public after a records request.\nThe crash occurred hours after Musk said Tesla would make FSD software available to anyone in North America who requested it. It previously offered the system only to drivers with high safety scores.\nThe police report said that if FSD malfunctioned, the driver should have manually taken control. Tesla has repeatedly said its advanced self-driving technology requires “active driver supervision” and its vehicles “are not autonomous”.\nDrivers are also warned when they install FSD that it “may do the wrong thing at the worst time”.\nThe National Highway Traffic Safety Administration (NHTSA), which is investigating Tesla after reports of braking “without warning, at random, and often repeatedly in a single drive”, did not immediately comment on the San Francisco crash.\nLast summer, NHTSA upgraded the investigation to what it calls an engineering analysis. The chair of the National Transportation Safety Board, Jennifer Homendy, has questioned if “full self-driving” is an accurate description of the technology – and said Tesla must do more to prevent misuse.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a Tesla Model S in 'full self-driving mode' experiencing a malfunction, resulting in an eight-vehicle crash on the San Francisco Bay bridge. The driver reported that the Full Self-Driving (FSD) software malfunctioned, causing the Tesla to shift lanes abruptly and slow down, leading to a chain reaction of crashes. This incident is the latest in a series of accidents attributed to Tesla technology, particularly the FSD software. The news also mentions that the National Highway Traffic Safety Administration (NHTSA) is investigating Tesla for reports of braking issues, and there are concerns about whether 'full self-driving' accurately describes the technology.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2022\/dec\/22\/tesla-crash-full-self-driving-mode-san-francisco","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":15,"Time":"20 November 2019","Title":"Uber self-driving crash 'mostly caused by human error'","Content":"Uber self-driving crash 'mostly caused by human error'\nPublished\n20 November 2019\nShare\nMedia caption,\nUber dashcam footage shows moment before fatal impact\nBy Dave Lee\nNorth America technology reporter\nA distracted safety operator in an Uber self-driving car was primarily to blame for a fatal crash in 2018, a US regulator has ruled.\nThe National Transportation Safety Board (NTSB) said an “inadequate safety culture” at Uber was also a major contributing factor.\nAs too were poor, or in some cases non-existent, rules governing firms testing self-driving technology.\nThe findings follow a near two-year investigation.\nIn a hearing to discuss the results, the NTSB urged national regulators to create a more thorough review process before the technology can be used on public roads.\nElaine Herzberg, 49, was killed when an Uber-owned self-driving car - operating in autonomous mode - struck her as she crossed a road in Tempe, Arizona, on 18 March 2018.\nIn the car was safety driver Rafaela Vasquez who, according to investigators, had been streaming a TV show on her mobile phone while behind the wheel.\nDashcam footage showed Ms Vasquez spent 36% of the journey that evening looking at the device.\nIn its experiments with driverless cars, Uber has mandated that a human operator pays attention at all times so they can take over in difficult situations or when the vehicle encounters a situation it does not know how to handle.\nUber crash driver was 'watching TV'\nSelf-driving Uber in fatal crash had 'safety flaws'\n\"Had the vehicle operator been attentive, she would likely have had sufficient time to detect and react to the crossing pedestrian to avoid the crash or mitigate the impact,\" the NTSB ruled.\nUber’s computers detected Ms Herzberg 5.6 seconds before impact, the NTSB said, but did not correctly identify her as a person.\nIMAGE SOURCE,\nNTSB\nImage caption,\nThe NTSB mapped the incident - yellow rings show metres ahead of the vehicle, orange lines centre of mapped travel and purple area the car's path\nThe report said Ms Herzberg was acting unsafely in attempting to cross the road where she did - investigators said toxicology reports suggested she had taken drugs that may have impaired her judgement.\nNevertheless, the NTSB said Uber had an \"inadequate safety culture, exhibited by a lack of risk assessment mechanisms, of oversight of vehicle operators, and of personnel with backgrounds in safety management”.\nIt acknowledged on Tuesday that the company had made significant changes since the accident.\nUber said it welcomed the recommendations.\n\"We deeply regret the March 2018 crash that resulted in the loss of Elaine Herzberg’s life, and we remain committed to improving the safety of our self-driving program,” said Nat Beuse, head of safety at Uber’s advanced technologies group.\n“Over the last 20 months, we have provided the NTSB with complete access to information about our technology and the developments we have made since the crash. While we are proud of our progress, we will never lose sight of what brought us here or our responsibility to continue raising the bar on safety.”\n_____\nFollow Dave Lee on Twitter @DaveLeeBBC\nDo you have more information about this or any other technology story? You can reach Dave directly and securely through encrypted messaging app Signal on: +1 (628) 400-7370\nMore on this story\nUber crash driver was 'watching TV'\nPublished\n22 June 2018\nSelf-driving Uber in fatal crash had 'safety flaws'\nPublished\n6 November 2019","Relevant":true,"Overview":"Here is the JSON format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Uber self-driving crash 'mostly caused by human error'\",\n        \"Published_date\": \"20 November 2019\",\n        \"Summary\": \"A distracted safety operator in an Uber self-driving car was primarily to blame for a fatal crash in 2018, a US regulator has ruled. The National Transportation Safety Board (NTSB) said an 'inadequate safety culture' at Uber was also a major contributing factor. As too were poor, or in some cases non-existent, rules governing firms testing self-driving technology.\",\n        \"Details\": [\n            \"The news is about a fatal crash involving an Uber self-driving car in 2018.\",\n            \"The crash was primarily caused by a distracted safety operator in the self-driving car.\",\n            \"The National Transportation Safety Board (NTSB) ruled that an 'inadequate safety culture' at Uber was a significant contributing factor.\",\n            \"The report mentions poor or non-existent rules governing firms testing self-driving technology.\",\n            \"The crash resulted in the death of Elaine Herzberg when she was struck by the Uber self-driving car.\",\n            \"The safety driver in the car had been distracted, streaming a TV show on her mobile phone while behind the wheel.\",\n            \"The NTSB stated that if the vehicle operator had been attentive, there might have been sufficient time to avoid the crash.\",\n            \"Uber's computers detected Ms. Herzberg before impact but did not correctly identify her as a person.\",\n            \"The NTSB also mentioned that Ms. Herzberg was acting unsafely, possibly impaired by drugs.\",\n            \"Uber was criticized for having an 'inadequate safety culture,' lacking risk assessment mechanisms, oversight of vehicle operators, and personnel with safety management backgrounds.\",\n            \"Uber acknowledged making significant changes since the accident and welcomed the recommendations for improving safety.\"\n        ]\n    }\n}\nThis news is relevant to self-driving car crashes because it discusses a fatal crash involving an Uber self-driving car in 2018, primarily caused by a distracted safety operator. The National Transportation Safety Board (NTSB) attributed the crash to an \"inadequate safety culture\" at Uber and mentioned issues with rules governing firms testing self-driving technology. It provides detailed information about the incident and the NTSB's findings.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-50484172","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":16,"Time":"11 May 2015","Title":"Self-driving car accidents revealed in California","Content":"Self-driving car accidents revealed in California\nPublished\n11 May 2015\nShare\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nDriverless cars are allowed on the roads in California subject to permits.\nFour out of the 48 self-driving cars on public roads in California have been involved in accidents in the last eight months, according to the state's Department of Motor Vehicles.\nThe agency began issuing permits for the testing of autonomous vehicles in September 2014.\nThree of the four cars belonged to Google, the fourth to parts supplier Delphi.\nBoth firms denied their vehicles had been at fault.\nUnder Californian law, details of car accidents remain confidential.\nHowever, Google said its driverless cars had never been the cause of an accident, and that the majority of \"minor fender-benders\" had been in the form of rear-end collisions from other drivers.\n\"Safety is our highest priority. Since the start of our programme six years ago, we've driven nearly a million miles autonomously, on both freeways and city streets, without causing a single accident,\" said a spokesperson.\nDelphi told the BBC its vehicle was hit while stationary at a crossroads and was in manual driving mode at the time.\n\"A police report indicates the fault of the accident is with the second vehicle, not Delphi. No-one was hurt in the incident,\" said a spokesperson.\nAn anonymous source told the Associated Press that two of the accidents occurred while the vehicles were occupied by human drivers, and all four vehicles were going very slowly at the time of the collisions.\nChris Urmson, director of Google's self driving car programme, wrote in a blog post that there have been 11 accidents involving Google cars since the project began six years ago but not one has been caused by one of its vehicles.\n\"Rear-end crashes are the most frequent accidents in America, and often there's little the driver in front can do to avoid getting hit,\" he said.\nMore on this story\nSelf-driving truck allowed on roads\nPublished\n8 May 2015\nCould driverless cars own themselves?\nPublished\n16 February 2015\nUK kickstarts driverless car changes\nPublished\n11 February 2015\nCar hackers 'drive' car with laptop\nPublished\n26 July 2013","Relevant":true,"Overview":"Based on the provided news article, it does contain information related to self-driving car accidents. Here is the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": [\n        \"Four out of the 48 self-driving cars on public roads in California have been involved in accidents in the last eight months.\",\n        \"Three of the four cars belonged to Google, the fourth to parts supplier Delphi.\",\n        \"Both firms denied their vehicles had been at fault.\",\n        \"Google said its driverless cars had never been the cause of an accident, and the majority of accidents were rear-end collisions from other drivers.\",\n        \"An anonymous source mentioned that two of the accidents occurred while the vehicles were occupied by human drivers, and all four vehicles were going very slowly at the time of the collisions.\",\n        \"Chris Urmson, director of Google's self-driving car program, stated that there have been 11 accidents involving Google cars since the project began six years ago, but none were caused by their vehicles.\"\n    ]\n}\nThe news article discusses self-driving car accidents in California, including details about the number of accidents, the companies involved (Google and Delphi), denials of fault by these companies, and information about the nature of the accidents. Therefore, the news is relevant to self-driving car crashes.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-32691887","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":14,"Time":"July 8, 2022","Title":"U.S. agency probing self-driving Cruise car crash in California","Content":"Companies\nLaw Firms\nGeneral Motors Co\nFollow\nWASHINGTON, July 7 (Reuters) - The National Highway Traffic Safety Administration has opened a special investigation into a recent crash of a Cruise self-driving vehicle in California that resulted in minor injuries, the agency said on Thursday.\nThe auto safety agency did not identify the specific crash, but a Cruise vehicle operating in driverless autonomous mode was involved in a crash involving minor injuries on June 3 in San Francisco, according to a report filed with the California Department of Motor Vehicles.\nThe state agency told Reuters it \"has had conversions with Cruise officials regarding the incident.\"\nSelf-driving car maker Cruise, which is majority-owned by General Motors (GM.N), said it was not issued a citation by police in the incident. Cruise said it had provided NHTSA with \"routine information\" the agency had sought in its special crash investigation.\nNHTSA's special crash investigations are separate from defect investigations used to determine if vehicles should be recalled. The agency has not opened a defect probe into Cruise.\nAdvertisement · Scroll to continue\nNHTSA has opened 45 special crash investigations into crashes since 2016 involving suspected automated and advanced driver systems; this is the first involving a Cruise vehicle.\nIn the report filed on the June 3 crash, Cruise said its vehicle entered a left-hand-turn lane and signaled for a turn, and then initiated a left turn on the green light.\nAt the same time, a 2016 Toyota Prius approached the intersection in the right-turn lane from the opposite direction traveling about 40 miles per hour in a 25 mph speed zone.\nAdvertisement · Scroll to continue\nThe Cruise autonomous vehicle stopped before completing the turn due to the oncoming Prius, which entered the intersection and made contact with the rear passenger side of the Cruise, which was later towed from the scene, the report said. Occupants of both vehicles received medical treatment for allegedly minor injuries, Cruise said.\nOn June 23, Cruise said it had started charging fares for driverless rides in San Francisco. read more Cruise earlier in June became the first company to secure a permit to charge for self-driving rides there, after it overcame objections by local officials. read more\nAdvertisement · Scroll to continue\nSelf-driving test cars with human safety drivers have become a constant sight in San Francisco, and completely driverless ones are increasingly common, too. Turning them into a fledgling business in a major U.S. city marks a milestone in the long- delayed journey toward driverless taxi service.\nReporting by David Shepardson; Editing by Franklin Paul and Leslie Adler\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Based on the provided news article, it appears to be relevant to self-driving car crashes. Here is the JSON format response as requested:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Title\": \"U.S. agency probing self-driving Cruise car crash in California\",\n    \"Description\": \"The National Highway Traffic Safety Administration (NHTSA) has opened a special investigation into a recent crash of a Cruise self-driving vehicle in California that resulted in minor injuries. The crash involved a Cruise vehicle operating in driverless autonomous mode on June 3 in San Francisco. The report mentions that the autonomous vehicle attempted a left turn but stopped due to an oncoming Toyota Prius, resulting in a collision. Both vehicles' occupants received medical treatment for minor injuries. Cruise was not issued a citation by the police. This marks the first special crash investigation involving a Cruise vehicle by NHTSA.\",\n    \"Source\": \"Reuters\",\n    \"Date\": \"July 7\",\n    \"Relevance\": \"The news is relevant to self-driving car crashes as it reports a special investigation by NHTSA into a crash involving a self-driving Cruise vehicle in California. The crash occurred in autonomous mode, resulting in minor injuries to occupants.\"\n  }\n}\nThe news article discusses a specific incident involving a self-driving Cruise vehicle in California that was involved in a crash while operating in autonomous mode. This information supports the relevance to self-driving car crashes.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/us-agency-probing-cruise-crash-california-2022-07-07\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":18,"Time":"14 March 2016","Title":"Google car crash 'not a surprise' - US transport secretary","Content":"Google car crash 'not a surprise' - US transport secretary\nPublished\n14 March 2016\ncomments\nComments\nShare\nMedia caption,\nAnthony Foxx is leading efforts to bring self-driving cars to US roads\nBy Dave Lee\nNorth America technology reporter\nThe recent crash involving a Google self-driving car and a bus was \"not a surprise\", the US transport secretary has said.\nAnthony Foxx told the BBC that accidents were inevitable, but that the emerging technology should not be compared \"against perfection\".\nNobody was hurt in the crash, but it was the first time Google's on-board computer has been blamed for causing a collision.\nSecretary Foxx was attending the South by Southwest Interactive festival in Austin, Texas.\nHe announced that seven US cities - Austin, Columbus, Denver, Kansas City, Pittsburgh, Portland and San Francisco - had reached the final stage of a competition to receive $40m in government funding for \"smart\" technologies.\nSecretary Foxx agreed that smart technologies could put some people out of work.\n\"Driverless technology presents a lot of potential for disruption on a number of fronts,\" he said.\n\"It's unclear to me now exactly how that future unfolds.\"\nRelative comparisons\nIMAGE SOURCE,\nAP\nImage caption,\nOne of Google's self-driving cars crashed into a bus in California earlier this year\nSecretary Foxx is leading efforts to bring self-driving cars to US roads. The Obama administration has committed $4bn to that goal - which includes attempts to develop standardised regulations for autonomous cars across the entire country.\nOn the public perception of self-driving cars following the February Google crash, he said: \"It's not a surprise that at some point there would be a crash of any technology that's on the road.\n\"But I would challenge one to look at the number of crashes that occurred on the same day that were the result of human behaviour.\"\n\"I think the question here isn't comparing the automated car against perfection, I think it's a relative comparison to what we have now on the roads which is you and I, and our eyeballs, and our brains.\"\nOne challenge would be to tackle the legal issue of responsibility when crashes occur - and whether the passenger should be liable, or, given that the computer was driving, the companies behind the software that failed.\n\"That's precisely the type of question that we're hoping over the next several months to provide feedback to both the states and the industry on.\"\nFollow Dave Lee on Twitter @DaveLeeBBC and on Facebook\nMore on this story\nGoogle self-driving car hits a bus\nPublished\n29 February 2016\nUS plans $4bn for self-driving rules\nPublished\n14 January 2016\nView comments","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving car crashes. In the article, it mentions a recent crash involving a Google self-driving car and a bus. The U.S. transport secretary, Anthony Foxx, stated that this crash was 'not a surprise' and that it was the first time Google's on-board computer has been blamed for causing a collision. While nobody was hurt in the crash, the fact that a self-driving car was involved in a collision is a clear indication of a self-driving car crash incident. Therefore, the news is relevant to self-driving car crashes.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-35800285","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":17,"Time":"Published 3:44 PM EST, Tue January 17, 2023","Title":"Tesla-induced pileup involved driver-assist tech, government data reveals","Content":"Washington, DC\nCNN\n — \nThe Tesla Model S that braked sharply and triggered an eight-car crash in San Francisco in November had the automaker’s controversial driver-assist software engaged within 30 seconds of the crash, according to data the federal government released Tuesday.\nThe Tesla Model S slowed to 7 mph on the highway at the time of the crash, according to the data. Publicly released video also showed the car moving into the far-left lane and braking abruptly.\nThe Tesla’s driver told authorities that the vehicle’s “full self-driving” software braked unexpectedly and triggered the pileup on Thanksgiving day. CNN Business was first to report last month the driver’s claim that “full self-driving” was active.\nThe National Highway Traffic Safety Administration then announced that it was sending a special crash investigation team to examine the incident. The agency typically conducts special investigations into about 100 crashes a year.\nThe pileup took place hours after Tesla CEO Elon Musk announced that its “full self-driving” driver-assist system was available to anyone in North America who requested it and had paid for the option. Tesla had previously restricted access to drivers with high scores on its safety rating system.\n“Full self-driving” is designed to keep up with traffic, steer in the lane and abide by traffic signals, but despite Tesla’s name for it, it requires an attentive human driver prepared to take full control of the car at any moment. It’s delighted some Tesla drivers but also alarmed others with its flaws. Drivers are warned on an in-car screen by Tesla when they install “full self-driving” that it “may do the wrong thing at the worst time.”\nTesla generally does not engage with the professional news media and did not respond to CNN’s request for comment.\n“We are proud of Autopilot’s performance and its impact on reducing traffic collisions. The benefit and promise of Autopilot is clear from the Vehicle Safety Report data that we have been sharing for 4 years,” Tesla said this month in an update to its vehicle safety data.\nTraffic safety experts have long questioned the merits of Tesla’s findings, which show fewer crashes when the driver-assist technologies are active, because among other things they’re generally used on highways where crashes are already rarer.\nTesla’s driver-assist technologies, Autopilot and “full self-driving” are already being investigated by the National Highway Traffic Safety Administration following reports of unexpected braking that occurs “without warning, at random, and often repeatedly in a single drive,” the agency has said in a statement.\nThe agency has received hundreds of complaints from Tesla users. Some have described near crashes and concerns for their safety.\nBryan Reimer, an autonomous vehicle researcher with the Massachusetts Institute of Technology’s AgeLab, told CNN Business the revelation that driver-assist technology was engaged raises questions about when NHTSA will act on its investigation, and what the future holds for Tesla’s driver-assist features.\n“How many more crashes will there be before NHTSA releases findings?” Reimer said.\nReimer said it remains to be seen if there’s a recall of any Tesla driver-assist features, and what it means for the automaker’s future. Musk has said before the company would be “worth basically zero” if it doesn’t provide “full self-driving.”\nThis story has been updated to reflect that a driver-assist system was active within 30 seconds of the crash.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a Tesla Model S that was involved in an eight-car crash in San Francisco in November. The federal government data revealed that the Tesla Model S had the automaker's controversial driver-assist software engaged within 30 seconds of the crash. The Tesla Model S slowed to 7 mph on the highway at the time of the crash, and publicly released video showed the car moving into the far-left lane and braking abruptly. The Tesla's driver claimed that the 'full self-driving' software braked unexpectedly and triggered the pileup. The National Highway Traffic Safety Administration sent a special crash investigation team to examine the incident. Tesla's driver-assist technologies, Autopilot and 'full self-driving,' are being investigated by the agency following reports of unexpected braking. Traffic safety experts are questioning the merits of Tesla's findings regarding the safety of their driver-assist technologies. This news contains specific information related to self-driving car crashes and is relevant to the topic.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2023\/01\/17\/business\/tesla-8-car-crash-autopilot\/index.html","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":20,"Time":"OCTOBER 4, 2017","Title":"GM's self-driving cars involved in six accidents in September","Content":"By David Shepardson\nWASHINGTON, Oct 4 (Reuters) - General Motors Co’s self-driving unit, Cruise Automation, told California regulators its vehicles were involved in six crashes in the state in September, but said none of automated vehicles were responsible.\nThe accidents did not result in any injuries or serious damage, according to the GM reports, but did demonstrate the challenges for developers of self-driving cars confronted by crowded urban streets.\nA U.S. Senate panel on Wednesday approved legislation that would allow auto makers to greatly expand testing of self-driving cars. Some safety groups have objected to the proposal, saying it gives too much latitude to automakers.\nThe incidents also demonstrate the difficulties of self-driving cars that obey all traffic laws coexisting with less careful human drivers. In total, GM Cruise vehicles have been involved in 13 crashes reported to California regulators in 2017, while Alphabet Inc’s Waymo vehicles have been involved in three crashes.\nCalifornia state law requires that all crashes involving self-driving vehicles be reported, regardless of severity.\nMost of the crashes involved drivers of other vehicles striking the GM cars that were slowing for stop signs, pedestrian or other issues. In one crash, a driver of a Ford Ranger was on his cell phone when he rear-ended a Chevrolet Bolt that was stopped at a red light.\nIn another instance, the driver of a Chevrolet Bolt noticed an intoxicated cyclist in San Francisco going the wrong direction toward the Bolt. The human driver stopped the Bolt and the cyclist hit the bumper and fell over. The bicyclist pulled on a sensor attached to the vehicle causing minor damage.\nIn another incident on Sept. 15 in San Francisco, a Dodge Charger in the left-turn lane attempted to illegally pass a Bolt in driverless mode. The GM Cruise employee took control of the vehicle and the Dodge scraped the front sensor and fled the scene without stopping.\n“All our incidents this year were caused by the other vehicle,” said a Rebecca Mark, spokeswoman for GM Cruise.\nOn Tuesday, Kyle Vogt, the head of Cruise Automation, highlighted his operation’s decision to test vehicles in San Francisco to speed development of the software that will pilot fully self-driving vehicles in the future.\nIn an August Waymo crash in Palo Alto, a driver took control of the autonomous vehicle after a vehicle ahead of it swerved to avoid an object on the road. The Waymo vehicle’s rear bumper scraped the left front bumper of a van in the adjacent lane. (Reporting by David Shepardson; Editing by Sandra Maler)\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here's the analysis of the provided news in JSON format:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"General Motors Co’s self-driving unit, Cruise Automation, told California regulators its vehicles were involved in six crashes in the state in September, but said none of automated vehicles were responsible. The accidents did not result in any injuries or serious damage, according to the GM reports, but did demonstrate the challenges for developers of self-driving cars confronted by crowded urban streets. Most of the crashes involved drivers of other vehicles striking the GM cars that were slowing for stop signs, pedestrian or other issues. In one crash, a driver of a Ford Ranger was on his cell phone when he rear-ended a Chevrolet Bolt that was stopped at a red light. In another instance, the driver of a Chevrolet Bolt noticed an intoxicated cyclist in San Francisco going the wrong direction toward the Bolt. The human driver stopped the Bolt and the cyclist hit the bumper and fell over. The bicyclist pulled on a sensor attached to the vehicle causing minor damage. In another incident on Sept. 15 in San Francisco, a Dodge Charger in the left-turn lane attempted to illegally pass a Bolt in driverless mode. The GM Cruise employee took control of the vehicle and the Dodge scraped the front sensor and fled the scene without stopping.\"\n}\nThis news is about self-driving car-related incidents involving General Motors' self-driving unit, Cruise Automation. It provides specific information about the accidents, the fact that none of the automated vehicles were responsible, and the circumstances surrounding each incident.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/autos-selfdriving-crashes\/gms-self-driving-cars-involved-in-six-accidents-in-september-idUSL2N1MF1RO","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":21,"Time":"SEPTEMBER 16, 2020","Title":"Safety driver in fatal Arizona Uber self-driving car crash charged with homicide","Content":"By David Shepardson\nWASHINGTON (Reuters) - The back-up safety driver behind the wheel of a self-driving Uber Technologies UBER.N test vehicle that struck and killed a woman in Tempe, Arizona, in 2018 was charged with negligent homicide, prosecutors said on Tuesday.\nSlideshow ( 2 images )\nRafael Vasquez, age 46, who is also known as Rafaela, pleaded not guilty on Tuesday after being charged in the death of Elaine Herzberg on Aug. 27, court records show. She was released pending trial set for February 2021.\nHerzberg died after she was struck while walking a bicycle across a street at night. The first recorded death involving a self-driving vehicle prompted significant safety concerns about the nascent autonomous vehicle industry.\nUber declined comment. A lawyer for Vasquez did not immediately respond to a request to comment.\nA Tempe police report said Vasquez was repeatedly looking down instead of keeping her eyes on the road. Prosecutors in March 2019 said Uber was not criminally liable in the crash.\n“Distracted driving is an issue of great importance in our community,” said Maricopa County Attorney Allister Adel. “When a driver gets behind the wheel of a car, they have a responsibility to control and operate that vehicle safely.”\nPolice said previously the crash was “entirely avoidable” and that Vasquez was streaming “The Voice” TV program at the time of the crash.\nIn November, the National Transportation Safety Board (NTSB) faulted Vasquez’s inactions and Uber for inadequate attention to safety and decisions in the company’s autonomous vehicle development.\nThe NTSB said the probable cause was Vasquez’s failure to monitor the driving environment “because she was visually distracted throughout the trip by her personal cell phone.” She was supposed to act in the event of an emergency.\nUber made a series of development decisions that contributed to the crash’s cause, the NTSB said. The software in the modified Volvo XC90 did not properly identify Herzberg as a pedestrian and did not address “operators’ automation complacency.”\nUber deactivated the automatic emergency braking systems in the Volvo XC90 vehicle and precluded the use of immediate emergency braking, relying instead on the back-up driver.\nReporting by David Shepardson; Editing by Aurora Ellis and Stephen Coates\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Based on the provided news article, it is relevant to self-driving car crashes for the following reasons:\nThe news discusses a fatal incident involving a self-driving Uber vehicle in Tempe, Arizona, in 2018, where a woman named Elaine Herzberg was struck and killed.\nThe backup safety driver, Rafael Vasquez, who was behind the wheel of the self-driving Uber test vehicle, has been charged with negligent homicide in connection with the crash.\nThe news highlights that Vasquez was reportedly distracted and repeatedly looking down instead of keeping her eyes on the road, which suggests a critical issue related to the operation of self-driving cars and the responsibilities of safety drivers.\nThe National Transportation Safety Board (NTSB) faulted Vasquez's inactions and Uber for inadequate attention to safety in their autonomous vehicle development, further emphasizing the connection to self-driving car crashes.\nGiven these points, the news article is relevant to self-driving car crashes.\nHere is the JSON format response as requested:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"The news is about a fatal self-driving car crash involving an Uber vehicle in Tempe, Arizona, in 2018. The backup safety driver, Rafael Vasquez, has been charged with negligent homicide for her role in the crash. The incident raised concerns about distracted driving and safety issues in the autonomous vehicle industry. The National Transportation Safety Board (NTSB) criticized Vasquez's actions and Uber's safety practices, further highlighting the relevance to self-driving car crashes.\"\n}\nIf you have any more questions or need further information, please let me know!","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/uber-selfdriving-idUSKBN26708P","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":13,"Time":"Tue 5 Jul 2016 23.44 CEST","Title":"Why self-driving cars aren't safe yet: rain, roadworks and other obstacles","Content":"Last week’s fatal crash involving a Tesla Model S offers a startling reminder that driverless technology is still a work in progress.\nAs Tesla’s own blogpost on the “tragic loss” points out, the autopilot technology that was controlling Joshua Brown’s car when it ploughed into a truck is in a “public beta phase”. That means the software has been released into the wild to be stress-tested by members of the public so that bugs can be flushed out. It’s the kind of approach we are used to seeing when we gain early access to new email applications or virtual reality headsets. As Apple co-founder Steve Wozniak told the New York Times: “Beta products shouldn’t have such life-and-death consequences”.\nUntil the investigation into the tragic incident concludes, we won’t know whether it was caused by a software glitch or human error – particularly with reports suggesting the driver may have been watching a Harry Potter DVD. All we know is that “neither autopilot nor the driver” noticed the white side of the tractor trailer against the brightly lit sky “so the brake was not applied”.\nTesla’s autopilot uses both cameras and radar to detect and avoid obstacles, so in this case we know there must have been a double failure. The cameras struggled with the glare from the sun, while the radar – according to Musk – “tunes out what looks like an overhead road sign to avoid false braking events”.\nElon Musk may have taken to aggressively dismissing coverage of the crash on his Twitter account, but there are still significant every day flaws that presents obstacles to wider adoption of self-driving car technology.\nSensor fusion\nWhen you have multiple sensors giving conflicting information, which one do you defer to? This seemed to be an issue at play in the fatal Tesla crash, where the one sensor that did spot the truck misinterpreted it as a road sign overhead.\n“The big question for driverless car makers is: how does the intelligence of the machine know that the radar sensor is the one to believe? That’s the secret sauce,” says Sridhar Lakshmanan, a self-driving car specialist and engineering professor at the University of Michigan-Dearborn.\nRoadworks\nWhen Delphi sent an autonomous car 3,400 miles across the US in April 2015, engineers had to take control of the car only for a 50-mile stretch. The reason? Unpredictable urban conditions with unmarked lanes and heavy roadworks. In other words, an average city commute.\nSandbags (and assumptions)\nOne of Google’s self-driving cars collided with a public bus in Mountain View in February as it tried to navigate some sandbags on the street. In attempting to move around the sandbags, the car’s left front struck the side of the bus that was trying to overtake. The car had detected the bus but predicted it would yield, and the test driver behind the wheel also made that assumption.\n“Unfortunately, all these assumptions led us to the same spot in the lane at the same time. This type of misunderstanding happens between human drivers on the road every day,” said Google of the incident.\nWeather\nAdverse weather conditions create visibility problems for both people and the sensors that power driverless technology. Rain can reduce the range and accuracy of laser-based Lidar sensors, obscure the vision of on-board cameras and create confusing reflections and glare. In a bid to improve the performance of driverless technology in soggy conditions, Google has started testing its cars on public roads near Seattle, where regular rain is guaranteed.\nHacking\nAs cars become more hi-tech they become more vulnerable to hacking. With driverless vehicles, the extra computers, internet connectivity and sensors increase the possible vulnerabilities. In a proof-of-concept attack, security researcher Jonathan Petit showed that lidar can be easily fooled into detecting a non-existent obstacle using a handheld laser pointer, which can force the car to slow down, stop or swerve.\nHumans\nJust as humans are at fault in more than 90% of car accidents, so too can they be the weakest link in semi-autonomous vehicles – particularly when a functionality labelled as “autopilot” encourages users to place their trust in the machine. “Maybe these intermediate levels [of automation] are not a viable consumer product,” says Richard Wallace, the director of the Transportation Systems Analysis group within the Center for Automotive Research. “They go a little too far in encouraging drivers to check out and yet they aren’t ready to take control.”\nAnd other humans\nIt’s not just the humans inside cars with self-driving technology, but those in other vehicles that need to be vigilant. Accident rates involving driverless cars are twice as high as for regular cars, according to a study by the University of Michigan’s Transportation Research Institute which looked at data from Google, Delphi and Audi.\nHowever the driverless cars weren’t at fault – they are typically hit from behind by inattentive or aggressive humans unaccustomed to self-driving motorists being such sticklers for the road rules. Google has started to programme its cars differently to behave in more familiar, human ways, such as inching forward at a four-way stop to indicate they will be moving next.\nBut it’s this collision where the biggest challenges for technology firms lie, encouraging adoption of rapidly developing new technology for a population that is quirky, unpredictable and, in turn, both sceptical and overtrusting.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving car crashes because it discusses a fatal crash involving a Tesla Model S, which was controlled by autopilot technology. The article highlights that the autopilot technology was in a 'public beta phase,' indicating that self-driving technology was a factor in the crash. It also mentions that the car's cameras struggled with glare from the sun, and the radar misinterpreted an overhead road sign, leading to a double failure. Additionally, the article discusses challenges related to sensor fusion, roadworks, weather conditions, hacking vulnerabilities, and human factors in self-driving car accidents. All of these factors are relevant to self-driving car crashes and safety concerns.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/jul\/05\/tesla-crash-self-driving-car-software-flaws","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":22,"Time":"September 1, 2022","Title":"GM startup Cruise recalls and revises self-driving software after crash","Content":"Companies\nLaw Firms\nGeneral Motors Co\nFollow\nCruise, LLC\nFollow\nWASHINGTON, Sept 1 (Reuters) - General Motors startup Cruise LLC (GM.N) said Thursday it had recalled and updated software in 80 self-driving vehicles after a June crash in San Francisco that left two people injured.\nFederal regulators said the recalled software could \"incorrectly predict\" an oncoming vehicle's path. Cruise said it had determined this unusual scenario would not recur after the update.\nThe National Highway Traffic Safety Administration (NHTSA) has stepped up its scrutiny of advanced driver assistance systems and autonomous vehicle systems in recent months. Last year, it directed all automakers and tech companies to promptly report crashes involving self-driving vehicles.\nNHTSA said Thursday that Cruise's recall filing \"to address a safety defect in its automated driving systems software\" was required by law.\nNHTSA added it \"expects all manufacturers, including those developing automated driving systems, to continuously ensure that they are meeting their requirements to initiate a recall for any safety issue that poses an unreasonable risk to safety.\"\nNHTSA said the recalled Cruise software could \"in certain circumstances when making an unprotected left, cause the (autonomous driving system) to incorrectly predict another vehicle’s path or be insufficiently reactive to the sudden path change of a road user.\"\nAdvertisement · Scroll to continue\nCruise disclosed Thursday that after the June 3 crash in San Francisco, it temporarily prevented its vehicles from making unprotected left turns and reduced the area in which its vehicles could operate.\nAfter the software update on July 6, Cruise said it had gradually reintroduced unprotected left turns, which refers to turning left at an intersection with a solid green light that directs all traffic, rather than a designated green arrow just for turning vehicles.\nAdvertisement · Scroll to continue\nCruise emphasized in a statement Thursday all vehicles had software updates and the recall \"does not impact or change our current on-road operations.\"\nThe company added \"Cruise AVs are even better equipped to prevent this singular, exceptional event.\"\nNHTSA said \"an improper (Automated Driving Systems) response can increase the risk of a crash.\"\nThe agency said last month it had opened a special investigation into the Cruise crash.\nAdvertisement · Scroll to continue\nIn rare circumstances, Cruise said the software caused the autonomous vehicle to hard brake while performing an unprotected left turn that it deemed necessary to avoid a severe front-end collision.\nThe self-driving vehicle \"had to decide between two different risk scenarios and chose the one with the least potential for a serious collision at the time, before the oncoming vehicle’s sudden change of direction,\" Cruise said.\nAdvertisement · Scroll to continue\nCruise also noted a police report found the party most at fault for the June crash was the other vehicle, which was traveling at 40 miles per hour in a 25-mile zone.\nIn March, startup technology firm Pony.ai agreed to recall some versions of its autonomous driving system software, after an October crash in California, that had been used in three vehicles.\nGM has lost nearly $5 billion since 2018 trying to build a robotaxi business in San Francisco and disclosed in July it lost $500 million on Cruise during the second quarter as it began charging for rides in a limited area of San Francisco.\nGM and Cruise in February disclosed they petitioned NHTSA for permission to deploy self-driving vehicles without steering wheels, mirrors, turn signals or windshield wipers.\nLast month, NHTSA said it would extend a public comment period on the request.\nReporting by David Shepardson; Editing by Jason Neely, Bernadette Baum and Alexander Smith\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here is the JSON format response based on the provided news article:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": [\n    \"General Motors startup Cruise LLC (GM.N) recalled and updated software in 80 self-driving vehicles after a June crash in San Francisco that left two people injured.\",\n    \"Federal regulators stated that the recalled software could 'incorrectly predict' an oncoming vehicle's path.\",\n    \"NHTSA (National Highway Traffic Safety Administration) required Cruise's recall filing 'to address a safety defect in its automated driving systems software.'\",\n    \"The recalled Cruise software could 'in certain circumstances when making an unprotected left, cause the (autonomous driving system) to incorrectly predict another vehicle’s path or be insufficiently reactive to the sudden path change of a road user.'\",\n    \"Cruise temporarily prevented its vehicles from making unprotected left turns after the June 3 crash and gradually reintroduced them after a software update on July 6.\",\n    \"NHTSA noted that 'an improper (Automated Driving Systems) response can increase the risk of a crash' and opened a special investigation into the Cruise crash.\",\n    \"Cruise mentioned that the other vehicle involved in the crash was found to be most at fault by a police report, traveling at 40 miles per hour in a 25-mile zone.\",\n    \"GM and Cruise petitioned NHTSA for permission to deploy self-driving vehicles without traditional controls like steering wheels and mirrors.\"\n  ]\n}\nThe news article is indeed relevant to self-driving car crashes. It discusses a recall and software update by General Motors' self-driving subsidiary, Cruise, following a crash in San Francisco. The article provides specific information about the reasons for the recall, including issues with the software's ability to predict oncoming vehicles' paths and its response during unprotected left turns, which are critical aspects related to self-driving car crashes. The involvement of NHTSA and the investigation into the crash further emphasize its relevance to self-driving car incidents.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/gms-cruise-recalls-revises-self-driving-software-after-crash-2022-09-01\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":25,"Time":"December 22, 2022","Title":"Tesla driver in multi-car crash told police self-driving software malfunctioned","Content":"WASHINGTON, Dec 21 (Reuters) - The driver of a 2021 Tesla (TSLA.O) Model S involved in an eight-vehicle crash last month on San Francisco's Bay Bridge told police he was in Full-Self Driving (FSD) mode which had malfunctioned, according to a police report made public Wednesday.\nThe Thanksgiving Day crash on Interstate-80 near Treasure Island resulted in two juveniles being transported to a local hospital for treatment of minor injuries and led to lengthy delays on the bridge.\nChief Executive Elon Musk has touted Tesla \"Full Self-Driving\" software as a potential cash cow for the world's biggest electric carmaker. But Tesla's advanced driver assistance systems - and Musk's claims about them - face growing legal, regulatory and public scrutiny.\nTesla sells the $15,000 FSD software as an ad-on which enables its vehicles to change lanes and park autonomously. That complements its standard \"Autopilot\" feature, which enables cars to steer, accelerate and brake within their lanes without driver intervention.\nAdvertisement · Scroll to continue\nThe Tesla driver told police the FSD malfunctioned but police were unable to determine if the software was in operation or if his statement was accurate, according to the report which was made public after a Reuters request.\nThe police report said the vehicle made an unsafe lane change and was slowing to a stop, which led to another vehicle hitting the Tesla and a chain reaction of additional crashes.\nAdvertisement · Scroll to continue\nThe police report said if FSD malfunctioned, the driver should have manually taken control of the vehicle.\nTesla did not respond to a request for comment.\nThe National Highway Traffic Safety Administration (NHTSA), which has been investigating the automaker's advanced driver assistance systems, did not comment.\nTesla's says \"Full Self-Driving\" gives access to more advanced driver assistance features but emphasizes \"all Tesla vehicles require active driver supervision and are not autonomous.\"\nAdvertisement · Scroll to continue\nNational Transportation Safety Board chair Jennifer Homendy has questioned Tesla's marketing the feature as \"full self-driving,\" when it is incapable of that and said Tesla must do more to ensure people do not misuse the feature.\nReporting by David Shepardson and Hyunjoo Jin; Editing by Lincoln Feast.\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Based on the provided news article, it appears to be relevant to self-driving car crashes. Here is the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is about a Tesla driver involved in an eight-vehicle crash on San Francisco's Bay Bridge. The driver claimed to be in Full-Self Driving (FSD) mode, which had malfunctioned, according to a police report. The crash resulted in two juveniles being transported to a hospital for minor injuries and caused delays on the bridge. The report mentions that if the FSD malfunctioned, the driver should have manually taken control of the vehicle. This incident highlights the potential issues and safety concerns associated with self-driving technology, particularly Tesla's Full Self-Driving software.\"\n}\nThis response confirms that the news is relevant to self-driving car crashes and provides specific information about the incident and the malfunction of the Full-Self Driving (FSD) mode in a Tesla vehicle.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/tesla-driver-multi-car-crash-told-police-self-driving-software-malfunctioned-2022-12-22\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":26,"Time":"August 21, 2023","Title":"California regulator probes crashes involving GM's Cruise robotaxis","Content":"Companies\nAlphabet Inc\nFollow\nGeneral Motors Co\nFollow\nSAN FRANCISCO, Aug 18 (Reuters) - California's autos regulator said on Friday it is investigating \"recent concerning incidents\" involving autonomous vehicles operated by General Motors (GM.N) unit Cruise in San Francisco and asked the company to take half its robotaxis off the roads.\nThe statement from California Department of Motor Vehicles (DMV) came after a Cruise robotaxi was involved in a crash with an emergency vehicle in San Francisco late on Thursday, the latest accident involving the self-driving cars.\nThe regulator also said it has asked Cruise to immediately reduce its active fleet of vehicles by 50% until the investigation is complete and Cruise takes actions to improve road safety. Cruise has agreed to a 50% reduction, it added.\n\"The DMV reserves the right, following investigation of the facts, to suspend or revoke testing and\/or deployment permits\" if it is determined to be an unreasonable risk to public safety, the regulator said in a statement.\nAdvertisement · Scroll to continue\nCruise said one of its cars \"entered the intersection on a green light and was struck by an emergency vehicle that appeared to be en route to an emergency scene\" after 10 p.m. on Thursday (0500 GMT Friday).\nThe car \"did identify the risk of a collision and initiated a braking maneuver, reducing its speed, but was ultimately unable to avoid the collision,\" the company, which is investigating the incident, said in a statement on Friday.\nAdvertisement · Scroll to continue\nInitial investigation shows the collision occurred when a fire truck was operating in an emergency with its forward facing red lights and siren on, the San Francisco Police Department said in a statement to Reuters.\nThe police said the sole passenger in the autonomous vehicle (AV) was transported to a local hospital with non-life-threatening injuries.\nThe California Public Utilities Commission (CPUC) last week voted to allow robotaxis from Cruise and Alphabet's (GOOGL.O) Waymo to operate at all hours of the day throughout San Francisco and charge passengers for rides despite strong opposition from residents and city agencies.\nAdvertisement · Scroll to continue\nThe two have been running robotaxi tests limited by times and geographic areas within San Francisco.\nCity Attorney David Chiu asked the CPUC on Thursday to halt its decision while the city files for a re-hearing. \"We have seen that this technology is not yet ready, and poor AV performance has interfered with the life-saving operations of first responders. San Francisco will suffer serious harms from this unfettered expansion,\" he said in a statement.\nReporting by Abhirup Roy and Hyunjoo Jin in San Francisco; editing by Diane Craft and Sonali Paul\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here's the JSON format response based on the provided news article:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"California regulator probes crashes involving GM's Cruise robotaxis\",\n        \"Description\": \"The California Department of Motor Vehicles (DMV) is investigating recent concerning incidents involving autonomous vehicles operated by General Motors (GM.N) unit Cruise in San Francisco. This investigation comes after a Cruise robotaxi was involved in a crash with an emergency vehicle in San Francisco. The regulator has asked Cruise to reduce its active fleet of vehicles by 50% until the investigation is complete and safety measures are taken. Cruise has agreed to this reduction. The DMV also reserves the right to suspend or revoke testing permits if it deems it an unreasonable risk to public safety.\",\n        \"Details\": [\n            \"The news article reports an investigation by the California DMV into recent incidents involving autonomous vehicles operated by GM's Cruise in San Francisco.\",\n            \"A specific incident mentioned in the article involves a Cruise robotaxi being involved in a crash with an emergency vehicle.\",\n            \"The regulator has requested Cruise to reduce its active fleet by 50% during the investigation and until safety improvements are made, and Cruise has agreed to this reduction.\",\n            \"The DMV has the authority to suspend or revoke testing permits if public safety is deemed at risk.\",\n            \"The article provides details about the collision, including the fact that the autonomous vehicle initiated a braking maneuver but was unable to avoid the collision when an emergency vehicle with lights and sirens on was en route to an emergency scene.\",\n            \"The news highlights concerns about the safety of self-driving cars and the need for further investigation.\"\n        ]\n    }\n}\nThis news article is relevant to self-driving car crashes because it discusses an investigation by the California DMV into incidents involving autonomous vehicles operated by GM's Cruise, specifically a crash between a Cruise robotaxi and an emergency vehicle. The article provides detailed information about the incident and the actions taken by regulators and Cruise in response to it.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/gms-cruise-robotaxi-collides-with-fire-truck-san-francisco-2023-08-19\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":28,"Time":"April 7, 2023","Title":"GM's Cruise recalls 300 self-driving vehicles to update software after bus crash","Content":"WASHINGTON, April 7 (Reuters) - General Motors' (GM.N) robotaxi unit Cruise LLC is recalling the automated driving software in 300 vehicles after one of its driverless vehicles crashed into the back of a San Francisco bus.\nThe March 23 collision was the fault of a software error in a Cruise automated vehicle (AV) that inaccurately predicted the movement of an articulated San Francisco Municipal Transit Authority bus, Cruise said on Friday. The crash caused moderate damage to the Cruise but did not result in any injuries.\nCruise in a National Highway Traffic Safety Administration (NHTSA) filing on Friday said the software was updated on March 25 to address concerns that the system \"may inaccurately predict the movement of articulated vehicles such as buses and tractor trailers.\"\n\"Fender benders like this rarely happen to our AVs, but this incident was unique,\" Cruise CEO Kyle Vogt said in a blog post. \"We do not expect our vehicles to run into the back of a city bus under any conditions, so even a single incident like this was worthy of immediate and careful study.\"\nAdvertisement · Scroll to continue\nCruise in a separate filing with California said the vehicle was traveling on Haight Street when a bus stopped ahead of it and the Cruise struck the rear bumper.\nVogt said, \"The bus’s behavior was reasonable and predictable. It pulled out into a lane of traffic from a bus stop and then came to a stop. Although our car did brake in response, it applied the brakes too late and rear-ended the bus at about 10 mph.\"\nAdvertisement · Scroll to continue\nThe driverless vehicle's view of the bus’s front section became fully blocked as the bus pulled out in front of the AV.\n\"Since the AV had previously seen the front section and recognized that the bus could bend, it predicted that the bus would move as connected sections with the rear section following the predicted path of the front section,\" Vogt said, adding that it was the only crash of its kind that the company has experienced.\nAdvertisement · Scroll to continue\nCruise said after the update it determined the crash would not recur.\nCruise in September disclosed that it recalled and updated software in 80 self-driving vehicles after a June crash in San Francisco that left two people injured.\nNHTSA last year said the software could \"incorrectly predict\" an oncoming vehicle's path.\nNHTSA in December opened a formal safety probe into the Cruise autonomous driving system after it received reports of incidents in which self-driving Cruise vehicles \"may engage in inappropriately hard braking or become immobilized.\"\nReporting by David Shepardson; Editing by David Goodman, Chizu Nomiyama and Mark Porter\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Based on the provided news article, it is relevant to self-driving car crashes. Here is the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"GM's Cruise recalls 300 self-driving vehicles to update software after bus crash\",\n        \"Description\": \"General Motors' (GM.N) robotaxi unit Cruise LLC is recalling the automated driving software in 300 vehicles after one of its driverless vehicles crashed into the back of a San Francisco bus. The March 23 collision was the fault of a software error in a Cruise automated vehicle (AV) that inaccurately predicted the movement of an articulated San Francisco Municipal Transit Authority bus.\",\n        \"Reason_for_relevance\": \"The news discusses a self-driving car crash involving a Cruise automated vehicle that rear-ended a San Francisco bus due to a software error. This incident provides specific information about the cause of the crash, which is related to self-driving technology and software inaccurately predicting the bus's movement.\",\n        \"Additional_context\": \"Cruise CEO Kyle Vogt mentioned that such incidents are rare but noteworthy, emphasizing that they do not expect their vehicles to run into the back of a city bus under any conditions. The article also mentions the software update that was implemented to address concerns related to the prediction of articulated vehicles' movements.\"\n    }\n}\nThe news article is relevant to self-driving car crashes because it describes an incident where a self-driving vehicle (Cruise automated vehicle) crashed into the back of a bus due to a software error that inaccurately predicted the bus's movement. The specific information about the crash and the software update to prevent future occurrences is provided.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/technology\/gm-self-driving-unit-cruise-recalls-300-vehicles-after-crash-2023-04-07\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":24,"Time":"Wed 16 Sep 2020 22.10 CEST","Title":"Safety driver charged in 2018 incident where self-driving Uber car killed a woman","Content":"Prosecutors in Arizona have charged the safety driver behind the wheel of a self-driving Uber test car that struck and killed a woman in 2018 with negligent homicide.\nCourt records show that Rafaela Vasquez, 46, on Tuesday pleaded not guilty in the death of Elaine Herzberg.\nVasquez is the only person facing criminal consequences in the first death of a pedestrian involving a self-driving vehicle, after prosecutors last year said Uber was not criminally liable in the crash.\nFederal regulators, however, have said that while the probable cause of the crash was Vasquez’s failure to monitor the driving environment, Uber’s technology flaws and “insufficient” state policies also played a role.\nHerzberg died in March 2018 after being struck by a self-driving SUV while walking a bicycle across a street at night in Tempe, Arizona.\nUber to bring back self-driving cars in California for first time since 2018 death\nRead more\nTempe police said the self-driving car was driving at about 40 miles an hour in autonomous mode at the time of the crash, and a police report said Vasquez was repeatedly looking down while the car was moving instead of keeping her eyes on the road. Vasquez was the vehicle’s “backup” driver, meaning she sat in the front seat and was responsible for monitoring the car’s movements.\nHerzberg’s death prompted significant safety concerns about the nascent autonomous vehicle industry.\nIn November, the National Transportation Safety Board (NTSB) said that Vasquez “was visually distracted throughout the trip by her personal cell phone”.\nBut the board also faulted Uber for inadequate attention to safety and development decisions that contributed to the crash’s cause, . The software in the modified Volvo XC90, for example, did not properly identify Herzberg as a pedestrian and did not address “operators’ automation complacency”.\nBefore the crash, Uber had also deactivated the automatic emergency braking systems to ensure a smoother ride, relying instead on the backup driver to intervene. In the Tempe crash, the vehicles’s sensors determined that braking was needed only 1.3 seconds before impact, the NTSB reported. Vasquez did not start steering until less than a second before the crash. In an interview with NTSB investigators, the backup driver said she had been monitoring the car’s “self-driving interface”.\nIn addition to raising concerns about the technology’s “ineffective” monitoring of the driver, regulators also determined that Arizona’s safety policies governing self-driving vehicles were “insufficient”. While some states had been cautious about allowing companies to experiment with the new technology on public roads, Arizona lured self-driving car operators to the state by arguing it had fewer regulations than other jurisdictions.\n“We’ve got regulatory failures within the state, and we’ve got, in important ways, real fumbles on the part of Uber, but it’s this individual who is at the center of the homicide case,” said Michael G Bennett, an Arizona State University associate research professor who studies autonomous cars and has closely followed the case. “It strikes me on its face as being profoundly unfair.”\nBennett, who is based in Tempe, said the fatal crash was the result of a systemic breakdown in the introduction of a technology that was sold as the future of safer roads, but that the prosecution at this stage was only focused on accountability for an individual.\n“This woman is taking the fall when the state and a large corporation who invented a large chunk of this technology is taking minimal responsibility, if any at all,” he said. “It’s quite striking that the corporation seems to be walking away without a scar.”\nFriends of Herzberg told the Guardian in 2018 they believed Uber, as well as the Arizona government, should face consequences for the crash. “This shouldn’t have ever happened,” said Carole Kimmerle, a Mesa resident who said she had been friends with Herzberg for more than 10 years and had previously lived with her.\nHerzberg had struggled with homelessness, but was in the process of turning her life around when she was killed, her friends said. They argued the government should have done more to prevent these kinds of crashes.\nUber declined to comment and a lawyer for Vasquez did not immediately respond to a request to comment.\nVasquez was released pending trial, which was set for February 2021.\nReuters contributed to this report","Relevant":true,"Overview":"Based on the provided news article, it is relevant to self-driving car crashes. Here's the JSON format response as requested:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Safety driver charged in 2018 incident where self-driving Uber car killed a woman\",\n        \"Description\": \"Prosecutors in Arizona have charged the safety driver behind the wheel of a self-driving Uber test car that struck and killed a woman in 2018 with negligent homicide. Court records show that Rafaela Vasquez, 46, on Tuesday pleaded not guilty in the death of Elaine Herzberg. Vasquez is the only person facing criminal consequences in the first death of a pedestrian involving a self-driving vehicle.\",\n        \"Details\": [\n            \"The news article reports on a self-driving Uber car that struck and killed a woman, Elaine Herzberg, in 2018.\",\n            \"The safety driver, Rafaela Vasquez, has been charged with negligent homicide in connection with the incident.\",\n            \"It mentions that federal regulators attributed the probable cause of the crash to Vasquez's failure to monitor the driving environment, as she was repeatedly looking down instead of keeping her eyes on the road.\",\n            \"The National Transportation Safety Board (NTSB) also noted that Vasquez was visually distracted by her personal cell phone during the trip, raising concerns about driver monitoring.\",\n            \"The article highlights issues with Uber's technology, stating that the software did not properly identify Herzberg as a pedestrian, and the emergency braking system was deactivated before the crash.\",\n            \"Regulators criticized Arizona's safety policies governing self-driving vehicles as 'insufficient.'\",\n            \"The news discusses the accountability and responsibility of both the safety driver and Uber in the fatal crash.\",\n            \"It includes reactions and opinions from experts and friends of the victim regarding the case and its implications.\"\n        ]\n    }\n}\nThe news article is relevant to self-driving car crashes as it discusses the incident involving a self-driving Uber car that resulted in a pedestrian's death, legal charges against the safety driver, and various factors contributing to the crash.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/us-news\/2020\/sep\/16\/uber-self-driving-car-death-safety-driver-charged","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":23,"Time":"Fri 1 Jul 2016 19.43 CEST","Title":"Tesla driver killed while using autopilot was watching Harry Potter, witness says","Content":"The Tesla driver killed in the first known fatal crash involving a self-driving car may have been watching a Harry Potter movie at the time of the collision in Florida, according to a truck driver involved in the crash.\nThe truck driver, Frank Baressi, 62, told the Associated Press that the Tesla driver Joshua Brown, 40, was “playing Harry Potter on the TV screen” during the collision and was driving so fast that “he went so fast through my trailer I didn’t see him”.\nTesla driver dies in first fatal crash while using autopilot mode\nRead more\nThe disclosure raises further questions about the 7 May crash in Williston, Florida, which occurred after Brown put his Model S into Tesla’s autopilot mode, which is able to control a car while it’s driving on the highway.\nThe fatal crash, which federal highway safety regulators are now investigating, is a significant setback and a public relations disaster for the growing autonomous vehicle industry. Tesla Motors Inc’s shares, however, were down less than 1% on Friday in early trading.\nBaressi, who did not immediately respond to requests for comment, said the Harry Potter movie “was still playing when he died and snapped a telephone pole a quarter mile down the road”. He told the AP, however, that he heard the movie but didn’t see it.\nThe Florida highway patrol told Reuters that there was a portable DVD player in the vehicle.\nAccording to Tesla’s account of the crash, the car’s sensor system, against a bright spring sky, failed to distinguish a large white 18-wheel truck and trailer crossing the highway. In a blogpost, Tesla said the self-driving car attempted to drive full speed under the trailer “with the bottom of the trailer impacting the windshield of the Model S”.\nThe disclosure raises further questions about the May crash, which occurred after Joshua Brown put his Model S into Tesla’s autopilot mode, which is able to control a car while it’s driving on the highway. Photograph: Facebook\nThe top of the vehicle was “torn off by the force of the collision”, according to a police report in the local Levy County Journal.\nBaressi was uninjured.\nElon Musk, the CEO of Tesla, tweeted his condolences regarding the “tragic loss”, but the company’s statement deflected blame for the crash. His 537-word statement noted that this was Tesla’s first known autopilot death in roughly 130m miles driven by customers.\n“Among all vehicles in the US, there is a fatality every 94 million miles,” the statement said.\nIt goes on to say that the car’s autonomous software is designed to nudge consumers to keep their hands on the wheels to make sure they’re paying attention. “Autopilot is getting better all the time, but it is not perfect and still requires the driver to remain alert,” the company said.\nNews of the death came just as BMW announced that it is joining forces with US computer chip firm Intel and Mobileye, an Israeli tech company, to develop its own self-driving cars.\nGoogle, Uber and numerous automakers have also tested self-driving technology. BMW said the goal of the new collaboration was to develop cars that would eventually allow them to take their eyes off the road.\nBrown, who owned a technology company called Nexu Innovation, was a Tesla enthusiast who posted videos of his car on autopilot on YouTube. One of them showed his vehicle avoiding a crash on the highway. The footage racked up 1m views after Musk tweeted it.\nOne of his first videos appeared to show Brown temporarily driving with no hands in slow-moving traffic. The Associated Press also reported that records show he received eight speeding tickets in six years.\nSince Tesla introduced the autopilot mode last October, Model S drivers have recorded videos of themselves online pushing the technology to its limits.\nTesla drivers post viral, self-driving 'stunts' using autopilot technology\nRead more\nOn YouTube, there are multiple videos showing drivers with their hands off the wheel. Musk has advised against this. However, one of the videos was uploaded by actress Talulah Riley, Musk’s second wife.\nAt the end of Tesla’s blogpost announcing Brown’s death, the company described the victim, who they did not name, as someone with “a loving family and we are beyond saddened by their loss”.\n“He was a friend to Tesla and the broader [electric vehicle] community, a person who spent his life focused on innovation and the promise of technology and who believed strongly in Tesla’s mission.”\nThe Associated Press contributed reporting","Relevant":true,"Overview":"Based on the provided news article, it appears to be about a fatal crash involving a Tesla driver using the autopilot feature. Here's the JSON format response as requested:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"The news is about a fatal crash involving a Tesla driver, Joshua Brown, who was using the autopilot feature. According to a witness, the Tesla driver may have been watching a Harry Potter movie on the car's TV screen at the time of the collision. The collision occurred in Florida, and the Tesla was driving so fast that it collided with a truck and trailer, resulting in the driver's death. This incident has raised questions about the safety of self-driving cars, and federal highway safety regulators are investigating the crash. The news also mentions that Tesla's sensor system failed to detect the truck and trailer, leading to the collision. Additionally, the article discusses Tesla's autopilot mode, the CEO's statement, and the broader context of self-driving technology in the industry.\"\n}\nSo, the news is indeed relevant to self-driving car crashes, and the specific information provided includes details about the crash, the involvement of Tesla's autopilot mode, the potential distraction of the driver with a movie, and the investigation by highway safety regulators.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/jul\/01\/tesla-driver-killed-autopilot-self-driving-car-harry-potter","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":10,"Time":"Published 11:47 PM GMT+2, June 15, 2022","Title":"US report: Nearly 400 crashes of automated tech vehicles","Content":"DETROIT (AP) — Automakers reported nearly 400 crashes over a 10-month period involving vehicles with partially automated driver-assist systems, including 273 with Teslas, according to statistics released Wednesday by U.S. safety regulators.\nThe National Highway Traffic Safety Administration cautioned against using the numbers to compare automakers, saying it didn’t weight them by the number of vehicles from each manufacturer that use the systems, or how many miles those vehicles traveled.\nAutomakers reported crashes from July of last year through May 15 under an order from the agency, which is examining such crashes broadly for the first time.\n“As we gather more data, NHTSA will be able to better identify any emerging risks or trends and learn more about how these technologies are performing in the real world,” said Steven Cliff, the agency’s administrator.\nTesla’s crashes happened while vehicles were using Autopilot, “Full Self-Driving,” Traffic Aware Cruise Control, or other driver-assist systems that have some control over speed and steering. The company has about 830,000 vehicles with the systems on the road.\nThe next closest of a dozen automakers that reported crashes was Honda, with 90. Honda says it has about six million vehicles on U.S. roads with such systems. Subaru was next with 10, and all other automakers reported five or fewer.\nIn a June 2021 order, NHTSA told more than 100 automakers and automated vehicle tech companies to report serious crashes within one day of learning about them and to disclose less-serious crashes by the 15th day of the following month. The agency is assessing how the systems perform and whether new regulations may be needed.\nSix people were killed in the crashes involving driver-assist systems, and five were seriously hurt, NHTSA said. Of the deaths, five occurred in Teslas and one was reported by Ford. Three of the serious injuries were in Teslas, while Honda and Ford each reported one.\nTesla’s crash number may appear elevated somewhat because it uses telematics to monitor its vehicles and get real-time crash reports. Other automakers don’t have such capability, so their reports may come slower or crashes may not be reported at all, NHTSA said. A message was left seeking comment from Tesla.\nTesla’s crashes accounted for nearly 70% of the 392 reported by the dozen automakers. Although the Austin, Texas, automaker calls its systems Autopilot and “Full Self-Driving,” it says the vehicles cannot drive themselves and the drivers must be ready to intervene at all times.\nAuto safety advocates said driver-assist and self-driving systems have potential to save lives, but not until NHTSA sets minimum performance standards and requires safety improvements to protect all road users.\n“It’s clear that U.S. road users are unwitting participants in beta testing of automated driving technology,” said Cathy Chase, president of Advocates for Highway and Auto Safety.\nSen. Edward Markey, D-Mass., said that although NHTSA’s data has limitations, it’s not isolated evidence that Tesla has ignored regulations, putting the public in danger. There has been “a never ending parade of reports” of Teslas on automated systems rolling through stop signs or braking for no reason, he said. NHTSA also is investigating Teslas that crash into parked emergency vehicles.\n“As today’s data suggests, this contempt for auto safety laws has real-world consequences,” Markey said while urging NHTSA to take enforcement action.\nBut many Tesla owners love the automation. Craig Coombs of Alameda, California, said he uses the systems in stop-and-go traffic and on frequent highway trips. “They really reduce driver fatigue overall,” he said.\nHe gives himself a “moderate” grade for paying attention while using the system but says he never takes his mind off the road entirely. He knows the technology isn’t perfect, and said he has had to take over driving at times.\nManufacturers were not required to report how many vehicles they have on the road that have the systems, nor did they have to report how far those vehicles traveled, or when the systems are in use, NHTSA said. At present, those numbers aren’t quantifiable, an agency official said.\nHowever, NHTSA has used the data to seek a recall, open investigations and provide information for existing inquiries, officials said.\n“This will help our investigators quickly identify potential defect trends that can emerge,” Cliff said. “These data will also help us identify crashes that we want to investigate and provide more information about how people in other vehicles interact with the vehicles.”\nHonda said it has packaged the systems to sell more of them, which could influence its numbers. “The population of vehicles that theoretically could be involved in a reportable event is much greater than the population of vehicles built by automakers with a less-aggressive deployment strategy,” the company said.\nAlso, reports to NHTSA are based on unverified customer statements about whether automated systems were running at the time of a crash. Those crashes may not qualify for reporting to NHTSA after more data is gathered, Honda said.\nNHTSA’s order also covered companies that are running fully autonomous vehicles, and 25 reported a total of 130 crashes. Google spinoff Waymo led with 62, followed by Transdev Alternative Services with 34 and General Motors-controlled Cruise LLC with 23.\nWaymo, the autonomous vehicle unit of Alphabet Inc., said it has more than 700 autonomous vehicles in its fleet. The company is running a fully autonomous ride-hailing service in Arizona and testing one in California. The company said all the crashes happened at low speeds, with air bags inflating in only two of them.\nIn 108 of the crashes involving fully autonomous vehicles, no injuries were reported, and there was only one serious injury. In most of the crashes, vehicles were struck from the rear.\n____\nAP Data Journalist Larry Fenn in New York and Video Journalist Terry Chea in Alameda, California, contributed. This story has been corrected to show that six deaths and five serious injuries were reported in the crashes.","Relevant":true,"Overview":"Based on the provided news article, here is the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to self-driving car crashes. The article reports nearly 400 crashes involving vehicles with partially automated driver-assist systems over a 10-month period. Specifically, 273 of these crashes involved Teslas. These crashes occurred while vehicles were using Autopilot, 'Full Self-Driving,' Traffic Aware Cruise Control, or other driver-assist systems that have some control over speed and steering. Additionally, six people were killed, and five were seriously injured in these crashes involving driver-assist systems, with five of the deaths occurring in Teslas. The article also mentions that NHTSA is assessing how these systems perform and whether new regulations may be needed for self-driving technology. Overall, this news is directly related to self-driving car crashes and provides detailed information about the crashes and their impact.\"\n}\nThe news is indeed relevant to self-driving car crashes, as it discusses a significant number of crashes involving vehicles with partially automated driver-assist systems, with specific details about the number of crashes and their connection to Tesla's driver-assist systems.","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/self-driving-car-crash-data-ae87cadec79966a9ba56e99b4110b8d6","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":31,"Time":"Published 1:03 AM GMT+2, September 16, 2020","Title":"Backup driver in fatal Arizona Uber autonomous crash charged","Content":"PHOENIX (AP) — The backup Uber driver involved in the first self-driving vehicle fatality has been charged with negligent homicide for being distracted in the moments before fatally striking a woman in suburban Phoenix.\nMaricopa County Attorney Allister Adel’s office said on Tuesday that Rafaela Vasquez was charged on Aug. 27 in the 2018 crash in Tempe that killed 49-year-old Elaine Herzberg. She pleaded not guilty during a hearing on Tuesday. Her attorney did not immediately respond to an inquiry from The Associated Press.\nProsecutors declined in March 2019 to file criminal charges against Uber, as a corporation, in Herzberg’s death.\nVasquez, 46, told investigators that she didn’t use her cell phones before the crash.\nBut the National Transportation Safety Board concluded Vasquez’s failure to monitor the road as she watched the television show “The Voice” on her phone was the main cause of the crash.\nThe contributing factors cited by the board included Uber’s inadequate safety procedures and ineffective oversight of its drivers, Herzberg’s decision to cross the street outside of a crosswalk, and the Arizona Department of Transportation’s insufficient oversight of autonomous vehicle testing.\nThe board also concluded Uber’s de-activation of its automatic emergency braking system increased the risks associated with testing automated vehicles on public roads. Instead of the system, Uber relied on the human backup driver to intervene.\nThe Uber system detected Herzberg 5.6 seconds before the crash. But it but failed to determine whether she was a bicyclist, pedestrian or unknown object, or that she was headed into the vehicle’s path, the board said.\nThe death reverberated throughout the auto industry and Silicon Valley and forced other companies to slow what had been a fast march toward autonomous ride-hailing services on public roads.\nUber pulled its self-driving cars out of Arizona the day before the NTSB issued a preliminary report on the crash, eliminating the jobs of about 300 people who served as backup drivers and performed other jobs connected to the vehicles.\nGov. Doug Ducey prohibited Uber from continuing its tests of self-driving cars after Herzberg was run over.\nA toxicology report showed that Herzberg tested positive for methamphetamine.\nVasquez had previously spent more than four years in prison for two felony convictions — making false statements when obtaining unemployment benefits and attempted armed robbery — before starting work as an Uber driver, according to court records.\nVasquez’s first name was listed on a driver’s license as Rafael, but police say Vasquez identifies as a woman and goes by the first name of Rafaela.\nThe decision not to criminally charge Uber in Herzberg’s death was made by Yavapai County Attorney Sheila Polk, whose officer handled the case after the prosecutor’s office in metro Phoenix cited a potential conflict of interest for having previously participated in a public-safety campaign with Uber.\nThe case was returned to prosecutors in metro Phoenix after the decision not to charge Uber had eliminated the conflict of interest.\nA trial for Vasquez is scheduled for Feb. 11, 2021.\n___\nThis story has been corrected to show Vasquez’s age is 46.\n___\nKrisher reported from Detroit.","Relevant":true,"Overview":"Here's the JSON format response based on the provided news article:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is about a fatal self-driving vehicle crash involving an Uber autonomous vehicle. The backup Uber driver, Rafaela Vasquez, has been charged with negligent homicide for being distracted before fatally striking a woman in suburban Phoenix in 2018. The National Transportation Safety Board (NTSB) concluded that Vasquez's failure to monitor the road while watching a TV show on her phone was the main cause of the crash. The NTSB also mentioned Uber's inadequate safety procedures, the de-activation of its automatic emergency braking system, and other contributing factors. This news is directly related to self-driving car crashes and provides detailed information about the incident.\"\n}\nThe news is relevant to self-driving car crashes because it discusses a fatal incident involving a self-driving Uber vehicle and the actions of the backup driver, Rafaela Vasquez, which were found to be a significant factor in the crash.","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/technology-business-arizona-phoenix-homicide-fdd1574ac6a3c418d4f2b569b797dc16","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":27,"Time":"Sun 28 Jun 2015 16.58 CEST","Title":"Google's self-driving cars are accident-prone – but it may not be their fault","Content":"Google’s self-driving cars are having a rough time on the streets of Mountain View, California. But a look at the evidence suggests it’s human error and not robots that are to blame.\nIn recent months, Google’s fleet of experimental self-driving cars have suffered five minor accidents while driving 200,000 miles around this sleepy Silicon Valley suburb. That is nearly ten times the national average for ‘property only’ fender benders, according to the National Highway Traffic Safety Administration.\nUsing a public records act request, the Guardian has obtained a report of the most recent incident, filed by Google in early June with the California Department of Motor Vehicles. In the report, Google notes that a self-driving Lexus was struck from behind 17 seconds after stopping at a traffic light. The other vehicle, a Honda Accord, simply drove into the back of it.\nSince April, Google’s Lexus SUVs have also been rear-ended by a BMW S3, a Toyota Camry and a Ford Expedition. In each case, the Google vehicle was either stationary or travelling at less than five miles per hour, giving its robotic driver no chance to avoid an impact. In fact, when robot cars meet in unfavourable circumstances, it seems they don’t collide.\nLast week it was reported that a self-driving Audi owned by Delphi Automotive took “appropriate action” to avoid one of Google’s self-driving Lexus cars after the Google car cut the Audi off on a Californian road.\nBut if Google’s self-driving algorithms are not to blame, why are its cars experiencing so many accidents?\nOne explanation could be the spinning laser scanners on their roofs, says Raj Rajkumar, designer of several autonomous cars at Carnegie Mellon University, including the winner of a 2007 self-driving vehicle competition run by Darpa, a US military research agency. “It is a distraction, and when people get distracted, I can imagine behaviours changing,” he says.\n“Another reason could be that Google cars have the Google logo splashed on them, saying they are self-driving cars. People looking at that could be distracted from their normal mode of operations,” he adds. Rajkumar is now CEO of Ottomatika, a company that helped develop technology for the first vehicle to complete a transcontinental self-driving road trip, from San Francisco to New York, in March. He noticed that passing drivers would often whip out a phone to take photos or videos of his car.\nOf course, the promise of self-driving cars is that they will reduce – or even eliminate – road traffic fatalities. “About 33,000 people die on America’s roads every year. That’s why so much of the enthusiasm for self-driving cars has focused on their potential to reduce accident rates,” says Chris Urmson, director of Google’s self-driving car program. He also points out that minor fender-benders like the ones in Mountain View often go unreported.\nThe few dozen experimental self-driving cars currently operating on public streets are packed with laser, radar, sonar and video sensors. This gives them a 360-degree view of the road ahead (and behind) that a human driver could never match. After travelling over 1.8 million miles in California, they have managed to avoid any serious accidents – and may have even prevented some from happening.\nHowever, there has been virtually no research on how human motorists respond to robotic vehicles, says Anuj Pradhan, a behavioural scientist at the University of Michigan Transportation Research Institute (UMTRI). “We do not fully understand the human reaction where self-driving cars are involved,” he says. “It’s an important question that we haven’t started looking at yet.”\nTwo of his colleagues at UMTRI, Michael Sivak and Brandon Schoettle, believe that driving is far more of a human interaction than you might expect. They found that in several types of car crashes, male-to-male accidents are underrepresented and female-to-female crashes are overrepresented, suggesting that our perceptions of fellow motorists are critical. “Furthermore, in many situations, drivers make eye contact and proceed according to the feedback received from other drivers,” they say. “Such feedback would be absent in interactions with self-driving vehicles.”\nWhen self-driving cars do become available to buy, they will be sharing the road with humans for decades to come. “Self-driving cars may have a ‘better’ driving style but it may not be a human driving style,” says Pradhan, “And that could affect how we predict or react to them.” He says that many self-driving car companies are now actively trying to humanise their algorithms to match the way people drive, slowing right down for curves, for instance, or hesitating at traffic lights.\nOne phenomenon that may help to reduce accidents in the short term: the distinctive and potentially distracting lidar sensors on top of vehicles are disappearing. In Google’s latest generation of self-driving cars, which received their permits to operate on California’s roads last week, the laser scanner has shrunk to a barely noticeable dome. Many other autonomous vehicles, including a Cadillac SRX built by Raj Rajkumar, hide them altogether. “We specifically made sure there was nothing on the car that makes it stand out,” he says.\nAnuj Pradhan thinks a better approach might be to identify autonomous vehicles so that motorists can give them leeway. “Should self-driving cars have a special marking so we can react accordingly?” he wonders. “If I see a learner driver, I give it a little more following distance. Perhaps that’s how regular drivers would react to a self-driving car.”\nUltimately, say Michael Sivak and Brandon Schoettle, we should be realistic about just how safe self-driving cars will make our highways. “It is not a foregone conclusion that a self-driving vehicle would ever perform more safely than an experienced, middle-aged driver,” they say. “And during the transition period when conventional and self-driving vehicles share the road, safety might actually worsen, at least for conventional vehicles.”\nGoogle’s rash of rear-ends might just be a coincidence, then, but we shouldn’t expect people to stop driving into robots anytime soon.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving car crashes because it discusses a series of accidents involving Google's self-driving cars. In recent months, Google's experimental self-driving cars have been involved in five minor accidents while driving in Mountain View, California. These accidents occurred even though the self-driving cars were either stationary or traveling at low speeds, and the accidents involved human-driven vehicles colliding with the self-driving cars. The article also explores potential reasons for these accidents, including distractions caused by the spinning laser scanners on the self-driving cars' roofs and the visibility of the Google logo on the vehicles. Additionally, it mentions that there has been virtually no research on how human motorists respond to self-driving cars, highlighting the complexities of human-robot interactions on the road. Overall, this news is directly related to self-driving car crashes and provides detailed information about the accidents and potential contributing factors.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2015\/jun\/28\/google-self-driving-cars-accidents","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":30,"Time":"Thu 22 Mar 2018 20.32 CET","Title":"Uber crash shows 'catastrophic failure' of self-driving technology, experts say","Content":"Video of the first self-driving car crash that killed a pedestrian suggests a “catastrophic failure” by Uber’s technology, according to experts in the field, who said the footage showed the autonomous system erring on one of its most basic functions.\nDays after a self-driving Uber SUV struck a 49-year-old pedestrian while she was crossing the street with her bicycle in Tempe, Arizona, footage released by police revealed that the vehicle was moving in autonomous mode and did not appear to slow down or detect the woman even though she was visible in front of the car prior to the collision. Multiple experts have raised questions about Uber’s Lidar technology, which is the system of lasers that the autonomous cars uses to “see” the world around them.\n“This is exactly the type of situation that Lidar and radar are supposed to pick up,” said David King, an Arizona State University professor and transportation planning expert. “This is a catastrophic failure that happened with Uber’s technology.”\n'Uber should be shut down': friends of self-driving car crash victim seek justice\nRead more\nThe videos of the car hitting Elaine Herzberg also demonstrated that the “safety driver” inside the car did not seem to be monitoring the road, raising concerns about the testing systems Uber and other self-driving car companies have deployed in cities across the US.\n“This safety driver was not doing any safety monitoring,” said Missy Cummings, a Duke University engineering professor who has testified about the dangers of self-driving technology. Research has shown that humans monitoring an automated system are likely to become bored and disengaged, she said, which makes this current phase of semi-autonomous testing particularly dangerous.\n“The problem of complacent safety drivers is going to be a problem for every company.”\nThe footage “strongly suggests a failure by Uber’s automated driving system and a lack of due care by Uber’s driver”, Bryant Walker Smith, a University of South Carolina law school professor and autonomous vehicle expert, said in an email. He noted that the victim is visible about two seconds before the collision, saying: “This is similar to the average reaction time for a driver. That means an alert driver may have at least attempted to swerve or brake.”\nThe car was traveling at 38 miles per hour at 10pm on Sunday, according to the Tempe police chief, Sylvia Moir, who told a reporter that she thought the video showed Uber was not at fault. Experts who reviewed the footage, however, said the opposite appeared to be true.\n“I really don’t understand why Lidar didn’t pick this up,” said Ryan Calo, a University of Washington law professor and self-driving expert. “This video does not absolve Uber.”\nAn Uber self-driving Volvo fitted with ‘Lidar’ technology. Photograph: Uber Handout\/EPA\nEven though the video appeared dark, King said there was likely more visibility than the footage suggested and noted that the darkness should not affect the car’s detection abilities.\n“Shadows don’t matter to Lidar,” added Cummings. “There is no question it should have been able to see her.”\nPolice have emphasized that the victim was not in a crosswalk at the time of the crash, but experts said the technology still should have stopped the vehicle, a Volvo, and King noted that the exact section where Herzberg entered the street is a common area for pedestrians to cross near a local park.\nJohn Simpson, the privacy and technology project director with Consumer Watchdog, said the video revealed a “complete failure” of Uber’s technology and its safety protocols, and said all testing programs on public roads should be suspended while the case is under investigation.\n“Uber appears to be a company that has been rushing and taking shortcuts to get these things on the road,” said Simpson, noting that Arizona leaders lured the corporation to its state with promises of fewer regulations, after Uber fought with California over its vehicles running red lights. “It’s inexcusable.”\nUber, which temporarily suspended testing, declined to comment on the causes of the crash. A spokesperson said in a statement that the video was “disturbing and heartbreaking”, adding: “Our cars remain grounded, and we’re assisting local, state and federal authorities in any way we can.”","Relevant":true,"Overview":"The provided news article is relevant to self-driving car crashes. Here is the JSON format response as per your request:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Uber crash shows 'catastrophic failure' of self-driving technology, experts say\",\n        \"Description\": \"Video of the first self-driving car crash that killed a pedestrian suggests a “catastrophic failure” by Uber’s technology, according to experts in the field, who said the footage showed the autonomous system erring on one of its most basic functions. Days after a self-driving Uber SUV struck a 49-year-old pedestrian while she was crossing the street with her bicycle in Tempe, Arizona, footage released by police revealed that the vehicle was moving in autonomous mode and did not appear to slow down or detect the woman even though she was visible in front of the car prior to the collision.\",\n        \"Reason_for_relevance\": \"The news is relevant to self-driving car crashes because it describes a self-driving Uber car involved in a fatal crash with a pedestrian. The article highlights a 'catastrophic failure' in Uber's self-driving technology, as it failed to detect and respond to a pedestrian in its path, resulting in the tragic accident. Experts have raised concerns about Uber's Lidar technology and the lack of safety monitoring by the 'safety driver' inside the car. The article also discusses the failure of the automated driving system and questions Uber's safety protocols. All these details are specific to self-driving car crashes.\"\n    }\n}\nThe news clearly discusses a self-driving car crash involving an Uber vehicle and provides specific information regarding the failure of the self-driving technology and safety measures, making it relevant to self-driving car crashes.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/mar\/22\/self-driving-car-uber-death-woman-failure-fatal-crash-arizona","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":32,"Time":"Tue 1 Aug 2023 14.28 CEST","Title":"Backup driver for self-driving Uber that killed Arizona pedestrian pleads guilty","Content":"The backup Uber driver for a self-driving vehicle that killed a pedestrian in suburban Phoenix in 2018 pleaded guilty on Friday to endangerment in the first deadly crash involving a fully autonomous car.\nArizona state judge David Garbarino, who accepted the plea agreement, sentenced Rafaela Vasquez to three years of supervised probation for the crash that killed 49-year-old Elaine Herzberg. Vasquez, 49, told police that Herzberg “came out of nowhere” and that she didn’t see Herzberg before hitting her on a darkened Tempe street on 18 March 2018.\nVasquez had been charged with felony negligent homicide. The charge to which she pleaded could be reclassified as a misdemeanor if she completes probation.\nAuthorities say Vasquez was streaming the television show The Voice on a phone and looking down in the moments before Uber’s Volvo XC-90 SUV struck Herzberg, who was crossing with her bicycle.\nVasquez’s attorneys said she was looking at a messaging program used by Uber employees on a work cellphone that was on her right knee. They said the TV show was playing on her personal cellphone, which was on the passenger seat.\nDefense attorney Albert Jaynes Morrison told Garbarino that Uber should share some blame for the crash as he asked the judge to sentence Vasquez to six months of unsupervised probation.\n“There were steps that Uber failed to take,” he said. By putting Vasquez in the vehicle without a second employee, he said, “it was not a question of if – but when – it was going to happen.”\nProsecutors previously declined to file criminal charges against Uber as a corporation. Federal transportation safety officials concluded Vasquez’s failure to monitor the road was the main cause of the crash.\n“The defendant had one job and one job only,” prosecutor Tiffany Brady told the judge. “And that was to keep her eyes in the road.”\nContributing factors cited by investigators included Uber’s inadequate safety procedures and ineffective oversight of its drivers, Herzberg’s decision to cross the street outside of a crosswalk, and the Arizona transportation department’s insufficient oversight of autonomous vehicle testing.\nSafety inspectors also concluded Uber’s deactivation of its automatic emergency braking system increased the risks associated with testing automated vehicles on public roads. Instead of the system, Uber relied on the human backup driver to intervene.\nHerzberg’s death was not the first crash involving an Uber autonomous test vehicle. In March 2017, an Uber SUV flipped on to its side, also in Tempe, when it collided with another vehicle. No serious injuries were reported, and the driver of the other car was cited for a violation.\nHerzberg’s death was the first involving an autonomous test vehicle but not the first in a car with some self-driving features. The driver of a Tesla Model S was killed in 2016 when his car, operating on its autopilot system, crashed into a semitrailer in Florida.\nSign up to First Thing\nFree daily newsletter\nOur US morning briefing breaks down the key stories of the day, telling you what’s happening and why it matters\nPrivacy Notice: Newsletters may contain info about charities, online ads, and content funded by outside parties. For more information see our Privacy Policy. We use Google reCaptcha to protect our website and the Google Privacy Policy and Terms of Service apply.\nNine months after Herzberg’s death, in December 2019, two people were killed in California when a Tesla on autopilot ran a red light and slammed into another car. That driver was charged in 2022 with vehicular manslaughter in what was believed to be the first felony case against a motorist who was using a partially automated driving system.\nIn Arizona, the Uber system detected Herzberg 5.6 seconds before the crash. But it failed to determine whether she was a bicyclist, pedestrian or unknown object, or that she was headed into the vehicle’s path, US transportation safety officials said.\nThe backup driver was there to take over the vehicle if systems failed.\nThe death reverberated throughout the auto industry and Silicon Valley and forced other companies to slow what had been a fast march toward autonomous ride-hailing services. Uber pulled its self-driving cars out of Arizona, and then Governor Doug Ducey prohibited the company from continuing its tests of self-driving cars.\nVasquez had previously spent more than four years in prison for two felony convictions – making false statements when obtaining unemployment benefits and attempted armed robbery – before starting work as an Uber driver, according to court records.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving car crashes as it discusses the case of a backup driver for a self-driving Uber vehicle that killed a pedestrian in Arizona in 2018. The backup driver, Rafaela Vasquez, pleaded guilty to endangerment in the first deadly crash involving a fully autonomous car. The news provides details about the circumstances of the crash, including Vasquez's actions before the accident, such as streaming a TV show on her phone and looking down, which resulted in her failing to monitor the road. The news also mentions the investigation findings, which attributed the main cause of the crash to Vasquez's failure to keep her eyes on the road, as well as factors like Uber's safety procedures, Herzberg's decision to cross outside of a crosswalk, and Uber's reliance on the human backup driver to intervene instead of an automatic emergency braking system. This news is directly related to self-driving car crashes and provides specific information about the incident.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2023\/aug\/01\/uber-self-driving-arizona-deadly-crash","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":36,"Time":"JUNE 5, 2020","Title":"Self-driving sector contends its cars can prevent many more crashes than insurance study says","Content":"By Tina Bellon\n(Reuters) - Companies working on self-driving vehicles have criticized an insurance industry study suggesting that only a third of all U.S. road crashes could be prevented by driverless cars, arguing that the study has underestimated the technology’s capabilities.\nThe study by the Insurance Institute for Highway Safety (IIHS), released on Thursday analyzed 5,000 U.S. crashes and concluded that likely only those caused by driver perception errors and incapacitation could be prevented by self-driving cars.\nThe autonomous vehicle industry quickly responded that its cars were programmed to prevent a vastly higher number of potential crash causes, including more complex errors caused by drivers making inadequate or incorrect evasive maneuvers.\nTaking those design choices into account, autonomous vehicles could avoid some 72% of crashes, said Partners for Automated Vehicle Education, a consortium of self-driving technology companies.\nThe group in a blog post (here here) on Thursday said it was \"fundamentally speculative\" to determine crash avoidance rates.\nNevertheless, companies developing self-driving cars for years have touted their ability to vastly reduce crashes, with some, like General Motors Co, calling them a key part in achieving “zero crashes.”\nThe Alliance for Automotive Innovation, an auto industry group, in a statement said that even reducing traffic fatalities by a third would be something to be proud of, but that its members aim to do more.\nJack Weast, vice president of autonomous vehicle standards at Intel Corp’s Mobileye, in an interview on Friday said the auto industry was assembling a vast list of likely road scenarios and human behavior that every driverless car should be able to navigate safely. Government agencies and insurance companies are part of that process, Weast said.\n“Crashes will never be zero until we have no more human drivers on the road,” he said. “But (self-driving cars) can combine physical laws with behavioral studies and do much more than a human driver.”\nReporting by Tina Bellon in New York; Editing by David Gregorio\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving car technology. It discusses a study by the Insurance Institute for Highway Safety (IIHS) that analyzed 5,000 U.S. crashes and concluded that only crashes caused by driver perception errors and incapacitation could be prevented by self-driving cars. However, companies in the self-driving vehicle industry criticized the study, arguing that their cars are programmed to prevent a much higher number of potential crash causes, including more complex errors caused by drivers making inadequate or incorrect evasive maneuvers. According to Partners for Automated Vehicle Education, autonomous vehicles could potentially avoid around 72% of crashes. The news also mentions that companies like General Motors Co. have touted self-driving cars as a key part of achieving 'zero crashes' and that the auto industry is working to navigate various road scenarios and human behaviors to make self-driving cars safer. Jack Weast, vice president of autonomous vehicle standards at Intel Corp’s Mobileye, stated that self-driving cars can combine physical laws with behavioral studies to achieve more than human drivers. Overall, the news is relevant to self-driving car technology and the debate over their potential to prevent crashes.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-idUSKBN23C2T7","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":33,"Time":"Sat 31 Mar 2018 20.40 CEST","Title":"Tesla car that crashed and killed driver was running on Autopilot, firm says","Content":"Tesla has said a car that crashed in California last week, killing its driver, was operating on Autopilot.\nExclusive: Arizona governor and Uber kept self-driving program secret, emails reveal\nRead more\nThe 23 March crash on highway 101 in Mountain View is the latest accident to involve self-driving technology. Earlier this month, a self-driving Volvo SUV that was being tested by the ride-hailing service Uber struck and killed a pedestrian in Arizona.\nFederal investigators are looking into the California crash, as well a crash in January of a Tesla Model S that may have been operating under the Autopilot system.\nIn a blogpost, Tesla said the driver of the sport-utility Model X that crashed in Mountain View, 38-year-old Apple software engineer Wei Huang, “had received several visual and one audible hands-on warning earlier in the drive and the driver’s hands were not detected on the wheel for six seconds prior to the collision.\n“The driver had about five seconds and 150 meters of unobstructed view of the concrete divider … but the vehicle logs show that no action was taken.”\nTesla also said the concrete highway divider had previously been damaged, increasing its impact on the car. The vehicle also caught fire, though Tesla said no one was in the vehicle when that happened.\nThe company said its Autopilot feature can keep speed, change lanes and self-park but requires drivers to keep their eyes on the road and hands on the wheel, in order to be able to take control and avoid accidents.\nAutopilot does not prevent all accidents, Tesla said, but it does make them less likely.\n“No one knows about the accidents that didn’t happen,” Tesla said, “only the ones that did. The consequences of the public not using Autopilot, because of an inaccurate belief that it is less safe, would be extremely severe.\n“There are about 1.25 million automotive deaths worldwide. If the current safety level of a Tesla vehicle were to be applied, it would mean about 900,000 lives saved per year.”\nThe company added that it “care[s] deeply for and feel[s] indebted to those who chose to put their trust in us. However, we must also care about people now and in the future whose lives may be saved if they know that Autopilot improves safety.\n“None of this changes how devastating an event like this is or how much we feel for our customer’s family and friends. We are incredibly sorry for their loss.”","Relevant":true,"Overview":"Based on the provided news article, it appears to be relevant to self-driving car crashes. Here is the JSON format response as requested:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"News_summary\": \"The news discusses a Tesla car crash in California that resulted in the death of its driver. Tesla stated that the car was operating on Autopilot.\",\n        \"Relevance_to_self-driving_car_crashes\": \"The news is relevant to self-driving car crashes because it involves a fatal accident where a Tesla vehicle was reportedly using the Autopilot feature. This raises concerns about the safety of self-driving technology and its involvement in the accident. Additionally, the article mentions previous incidents involving self-driving technology, including an Uber self-driving vehicle that struck and killed a pedestrian.\",\n        \"Tesla's Autopilot system details\": \"The article provides information about Tesla's Autopilot system, stating that it can control speed, change lanes, and self-park, but it requires drivers to keep their eyes on the road and hands on the wheel. It also mentions that the driver in this particular incident had received warnings but did not have their hands on the wheel prior to the collision. Tesla emphasizes that Autopilot does not prevent all accidents but makes them less likely. The company also discusses the potential life-saving benefits of the technology.\",\n        \"Investigation status\": \"Federal investigators are looking into the California crash, indicating the seriousness of the incident and its implications for self-driving technology.\",\n        \"Tesla's response and apology\": \"The article includes Tesla's response, where the company expresses condolences to the victim's family and friends and emphasizes its commitment to improving safety. Tesla also addresses concerns about public perception and safety related to Autopilot.\"\n    }\n}\nThis response summarizes the specific information from the news article that makes it relevant to self-driving car crashes, including details about the Tesla crash, the Autopilot system, and the ongoing investigation.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/mar\/31\/tesla-car-crash-autopilot-mountain-view","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":38,"Time":"4 June 2018","Title":"GM settles self-driving legal action with California biker","Content":"GM settles self-driving legal action with California biker\nPublished\n4 June 2018\nShare\nIMAGE SOURCE,\nREUTERS\nImage caption,\nGM has done extensive testing of its Cruise robot cars in cities and on busy roads\nGeneral Motors has settled a legal action brought by a motorcyclist knocked over by one of the carmaker's self-driving vehicles.\nOscar Nilsson sued soon after the collision with the GM Cruise car in December 2017.\nMr Nilsson sustained neck and shoulder injuries that needed lengthy treatment and caused him to take time off work.\nGM said the settlement had resulted from a mutual desire by both parties to resolve the legal action.\nAmerica's self-driving car race speeds up\nTwo self-drive car crashes reported\nUber car 'failed to identify' pedestrian\nLawyers for both sides told Reuters that final details of the settlement were still being worked out but the case should be concluded by the end of June.\nThe car moved to change lanes in heavy traffic on a San Francisco motorway last year, according to GM's report, but then aborted its manoeuvre when traffic in the lane it was leaving started moving.\nIt then collided with the motorbike, which was moving into the space the car had been moving out of.\nGM alleged the motorcyclist was partly at fault for the crash. And a police report did cite him for trying to pass the Cruise car. However, he was not given a ticket.\nAbout 40 crashes involving self-driving cars have taken place in California since January 2017. Records show 33 of the cars involved were GM Cruise vehicles, but none has been found to have been responsible for any of the crashes.\nMore on this story\nTwo self-drive car crashes reported\nPublished\n24 January 2018\nUber car 'failed to identify' pedestrian\nPublished\n24 May 2018\nTesla rebuked by death crash investigators\nPublished\n2 April 2018\nTesla hit police car 'while using Autopilot'\nPublished\n30 May 2018","Relevant":true,"Overview":"Based on the provided news article, it does pertain to self-driving car incidents. Here's the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"GM settles self-driving legal action with California biker\",\n        \"Publication_date\": \"4 June 2018\",\n        \"Summary\": \"General Motors has settled a legal action brought by a motorcyclist knocked over by one of the carmaker's self-driving vehicles. Oscar Nilsson sued soon after the collision with the GM Cruise car in December 2017. Mr. Nilsson sustained neck and shoulder injuries that needed lengthy treatment and caused him to take time off work. GM said the settlement had resulted from a mutual desire by both parties to resolve the legal action.\",\n        \"Key_details\": [\n            \"The news is about a legal action involving General Motors (GM) and a motorcyclist who was knocked over by one of GM's self-driving vehicles.\",\n            \"The incident occurred in December 2017 when a GM Cruise car was involved in a collision with the motorcyclist, Oscar Nilsson, resulting in injuries to Mr. Nilsson.\",\n            \"GM and Mr. Nilsson have reached a settlement to resolve the legal action.\",\n            \"The article also mentions that the GM Cruise car was changing lanes in heavy traffic on a San Francisco motorway when the collision happened.\",\n            \"This incident is part of the broader context of self-driving car incidents, with around 40 crashes involving self-driving cars in California since January 2017.\",\n            \"Although GM alleged that the motorcyclist was partly at fault for the crash, the article does not state that GM's self-driving vehicle was entirely without blame.\"\n        ]\n    }\n}\nThe provided news article discusses a legal action involving a self-driving car from General Motors (GM) and a motorcyclist who was injured in a collision with the self-driving vehicle. This information is relevant to self-driving car incidents, and the key details provide specific information about the incident, including the date, the injuries sustained, and the settlement between the parties involved. Additionally, it mentions the context of self-driving car incidents in California.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-44355150","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":34,"Time":"Mon 14 Nov 2022 12.00 CET","Title":"Landmark trial involving Tesla autopilot weighs if ‘man or machine’ at fault","Content":"Tesla will play a major role in a manslaughter trial this week over a fatal crash caused by a vehicle operating on autopilot, in what could be a defining case for the self-driving car industry.\nAt the trial’s heart is the question of who is legally responsible for a vehicle that can drive – or partially drive – itself.\nTesla under US criminal investigation over self-driving claims, sources say\nRead more\nKevin George Aziz Riad is on trial for his role in a 2019 crash. Police say Riad exited a freeway in southern California in a Tesla Model S, ran a red light and crashed into a Honda Civic, killing Gilberto Lopez and Maria Guadalupe Nieves-Lopez. Tesla’s autopilot system, which can control speed, braking and steering, was engaged at the time of the crash that killed the couple, who were on their first date.\nTesla does not face charges in the case, but trial could shape public perceptions of the company and act as a test case for whether the technology has advanced faster than legal standards, experts say.\n“Who’s at fault, man or machine?” Edward Walters, an adjunct professor at the Georgetown University law school who specializes in the law governing self-driving cars. “The state will have a hard time proving the guilt of the human driver because some parts of the task are being handled by Tesla.”\nRiad’s lawyer has said that his client should not have been charged with a crime while prosecutors have argued Riad’s speeding and failure to brake were reckless.\nThe trial comes as the electric carmaker faces growing scrutiny and criticism that its autopilot has made drivers inattentive and contributed to accidents and deaths. Elon Musk, the company cofounder, has said that Tesla is significantly more safe when used with its autopilot system, and has touted it as a step to fully autonomous driving.\nIn September, Musk said he believed the company had a “moral obligation” to roll out what he describes as “full self-driving” software, even if it was not perfect and Tesla faced lawsuits, because doing so could save lives.\nBut Tesla’s system has faced ongoing scrutiny and has been implicated in numerous collisions, some of them fatal. US federal regulators are currently investigating more than a dozen Tesla crashes into parked first responder vehicles over a period of four years, resulting in multiple injuries and one death.\nThe US justice department is investigating whether Tesla itself should face criminal charges over its self-driving claims, Reuters reported, which experts have said could pose a challenge to prosecutors in the California trial.\n“The DoJ probe helps [Riad] because his claim is going to be ‘I relied on their advertising. Therefore, I was not aware of the risk there,’” said Robert Blecker, a criminal law professor at New York Law School.\nIn addition to the criminal trial related to the crash, the family of Gilberto Lopez is suing Tesla in a trial scheduled for July.\nTesla’s self-driving technology fails to detect children in the road, group claims\nRead more\n“I can’t say that the driver was not at fault, but the Tesla system, autopilot and Tesla spokespeople encourage drivers to be less attentive,” Donald Slavik, an attorney whose firm is representing Lopez’s family in a lawsuit against Tesla, told Reuters.\nTesla understood the risks of its system but failed to manage those, Slavik said. “Tesla knows people are going to use autopilot and use it in dangerous situations,” he said.\nThe ongoing legal and regulatory scrutiny of Tesla could shape perception of the company, which poses a risk as it looks to defend itself in coming lawsuits, said Bryant Walker Smith, a law professor at the University of South Carolina, who is also an adviser on new transportation technology.\n“The narrative of Tesla potentially shifts from this innovative tech company doing cool things to this company just mired in legal trouble. That is the risk, and narrative is very important in civil litigation because both sides tell a jury a story,” he said.","Relevant":true,"Overview":"Based on the provided news article, here is the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": [\n        \"The news article discusses a manslaughter trial involving a fatal crash caused by a Tesla vehicle operating on autopilot.\",\n        \"The trial raises the question of who is legally responsible for a vehicle that can drive itself or partially drive itself, highlighting the issues surrounding self-driving cars.\",\n        \"The crash occurred when a Tesla Model S, with autopilot engaged, ran a red light and collided with a Honda Civic, resulting in fatalities.\",\n        \"The trial is expected to shape public perceptions of Tesla and could set a precedent for self-driving car technology's legal standards.\",\n        \"There is an ongoing investigation into Tesla by the US Department of Justice regarding its self-driving claims, which adds to the relevance of the news in the context of self-driving car issues.\"\n    ]\n}\nIn summary, this news is relevant to self-driving car crashes because it involves a fatal crash caused by a Tesla vehicle operating on autopilot, raises questions about legal responsibility in self-driving car incidents, and is part of a broader investigation into Tesla's self-driving technology.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2022\/nov\/14\/tesla-autopilot-landmark-case-man-v-machine","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":35,"Time":"3:21 AM GMT+2•August 19, 2023","Title":"Cruise told by regulators to ‘immediately’ reduce robotaxi fleet 50% following crash","Content":"cameraImage Credits: Roberto Baldwin\nCruise, the self-driving car subsidiary of GM, has been asked to reduce its robotaxi fleet by 50% in San Francisco following a crash Thursday night with a fire truck.\nThe California Department of Motor Vehicles, the agency that regulates the testing and deployment of autonomous vehicles in the state, requested the reduction in operations. The state agency said it is investigating “recent concerning incidents” involving Cruise vehicles in San Francisco. It called for Cruise to reduce its fleet by 50% and have no more than 50 driverless vehicles in operation during the day and 150 driverless vehicles in operation at night until the investigation is complete.\n“Safety of the traveling public is the California DMV’s top priority,” the DMV said in a statement issued Friday evening, adding that it has the right, following the investigation, to suspend or revoke testing and\/or deployment permits if it determines there’s an unreasonable risk to public safety. “The primary focus of the DMV’s regulations is the safe operation of autonomous vehicles and safety of the public who share the road with these vehicles.”\nCruise told TechCrunch it is complying with the request. Cruise also issued a blog post giving the company’s perspective of how and why the crash occurred.\n“Over one hundred people lose their lives every day on American roadways, and countless others are badly injured, Cruise said in a statement sent via email. “We believe it’s clear that Cruise positively impacts overall road safety, and look forward to working with the CA DMV to make any improvements and provide any data they need to reinforce the safety and efficiency of our fleet.”\nCruise has had a series of snafus, including at least 10 of its driverless cars reportedly stalling and blocking traffic, which threatens to derail its commercial plans. The string of glitches come just a week after winning approval from the California Public Utilities Commission to expand commercial operations in San Francisco.\nThe CPUC, the agency that regulates ride-hailing operations including those involving robotaxis, approved Cruise and Waymo on August 10 for final permits that allow the companies to operate 24 hours a day, seven days a week, expand their fleets and charge for rides throughout the city.\nThe latest Cruise incident occurred Thursday night when a Cruise robotaxi and an emergency vehicle crashed and left a passenger injured. Cruise said in a social media post that one of its self-driving Chevy Bolt EVs entered an intersection on a green traffic light at Polk and Turk streets when it was struck by an emergency vehicle that appeared to be en route to an emergency scene.\nEarlier this week, San Francisco City Attorney David Chiu filed motions with the CPUC to pause Cruise and Waymo’s plans to charge for robotaxi rides in the city at all hours. Chiu’s arguments parallel comments made by residents and other city officials during a public hearing ahead of the CPUC’s vote.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about self-driving car crashes. Cruise, the self-driving car subsidiary of GM, has been asked to reduce its robotaxi fleet by 50% in San Francisco following a crash with a fire truck. The California Department of Motor Vehicles (DMV) has requested this reduction and is investigating 'recent concerning incidents' involving Cruise vehicles in San Francisco. They have called for Cruise to have no more than 50 driverless vehicles in operation during the day and 150 driverless vehicles in operation at night until the investigation is complete. The DMV also mentioned the possibility of suspending or revoking testing and deployment permits if there's an unreasonable risk to public safety. Cruise has responded, acknowledging the crash and stating their commitment to safety while working with the DMV to improve their fleet's safety and efficiency. This news provides specific details about a self-driving car crash involving a Cruise robotaxi and an emergency vehicle, which left a passenger injured.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2023\/08\/18\/cruise-told-by-regulators-to-immediately-reduce-robotaxi-fleet-50-following-crash\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":40,"Time":"Updated 4:49 PM EST, Thu February 16, 2023","Title":"Tesla recalling nearly 363,000 vehicles equipped with ‘Full Self-Driving’","Content":"New York\nCNN\n — \nTesla is recalling all 363,000 US vehicles with its so-called “Full Self Driving” driver assist software due to safety risks, another blow to the feature that is central to the automaker’s business model.\n“Full self-driving,” as it currently stands, navigates local roads with steering, braking and acceleration, but requires a human driver prepared to take control at any moment, as the system makes judgment errors. The National Highway Traffic Safety Administration said that, based on its analysis, Tesla’s FSD feature “led to an unreasonable risk to motor vehicle safety based on insufficient adherence to traffic safety laws.” And it warned FSD could violate traffic laws at some intersections “before some drivers may intervene.”\n“The FSD Beta system may allow the vehicle to act unsafe around intersections, such as traveling straight through an intersection while in a turn-only lane, entering a stop sign-controlled intersection without coming to a complete stop, or proceeding into an intersection during a steady yellow traffic signal without due caution,” said the recall notice, posted on NHTSA’s website.\nTesla will attempt to fix the the FSD feature, which costs $15,000, through an over-the-air software update, the notice added.\nWhile Tesla CEO Elon Musk has not yet commented on the nature or scope of the problem, he tweeted that “the word “recall” for an over-the-air software update is anachronistic and just flat wrong!”\nBut NHTSA said in a statement that “manufacturers must initiate a recall for any repair, including a software update, that remedies an unreasonable risk to safety.” The federal agency said it will “continue to monitor the recall remedies for effectiveness.”\nThe notice said that the problems are present with all cars with the current version of the FSD software, which is available on all four Tesla models, the Model S, Model X, Model 3 and Model Y.\nIt also said Tesla has identified18 reports of incidents received between May 8, 2019, and September 12, 2022, that may be related to the conditions described above. It said Tesla is not aware of any injuries or deaths caused in those incidents. NHTSA itself has identified at least 273 crashes that involved one of Tesla’s driver assist systems.\nA Troubled Development\nFSD is considered key to the company’s basic business plan, given the premiums that drivers pay for the features, and it’s ability to attract buyers to chose Tesla cars in the first place. Tesla and Musk have repeatedly claimed that FSD, even in its current “beta” form, is safer than cars driven solely by humans. He told investors last month that Tesla has collected data from about 100 million miles of drivers using FSD outside of highways.\n“Our published data shows that improvement in safety,” he said. “It’s very clear. So we would not have released the FSD Beta if the safety statistics were not excellent.”\nBut other safety experts have questioned the validity of Tesla’s safety claims. There have been high-profile accidents of Tesla cars using FSD or its more rudimentary predecessor known as “Autopilot.” Some of those accidents included fatalities.\nNHTSA is also investigating that predecessor, Autopilot. That technology combines lane-keeping assist with adaptive cruise control to keep a car in a lane on a highway, as opposed to the promise of “full self-driving,” which Tesla says aims to one day be able to operate a vehicle without human supervision on a city street.\nWhile “this recall seeks to address a specific set of concerns identified by the agency,” NHTSA’s statement said that this recall does not address its earlier investigations. “Accordingly, the agency’s investigation into Tesla’s Autopilot and associated vehicle systems remains open and active.”\nLast month, Tesla disclosed in the company’s annual financial report that it “has received requests from the US Justice Department for documents related to Tesla’s Autopilot and FSD features.”\nMoving Deadlines\nMusk has repeatedly predicted that the company would soon build a truly self-driving car. But it has also repeatedly pushed back its own self-imposed deadlines. Tesla owners have filed a class-action lawsuit over the predictions and missed deadlines, which is still pending.\n“Mere failure to realize a long-term, aspirational goal is not fraud,” Tesla’s lawyers wrote in a November 28 court filing, asking that the suit be dismissed.\nTesla 'full self-driving' triggered an eight-car crash, a driver tells police\nMusk has said for years that the price of “full self-driving” would increase periodically as it develops and moves closer to regulatory approval. He tweeted in May 2020 that when “full self-driving” had that approval, the feature would “probably” be worth more than “$100,000.” But as recently as a July 2021 call with investors, Musk said it was “debatable” that the feature was worth the $10,000 Tesla was charging at that time.\nIn September, when CNN Business spoke with 13 people who have cars with the “full self-driving” beta, the overwhelming majority, 11 people, said they felt it wasn’t worth $15,000. And it’s been the subject of controversy for years, including a recent ad that played during the Super Bowl in a few markets.\nTesla does not appear close to regulatory approval for “full self-driving.” In August of 2022, the California DMV said that the name “full self-driving” is “a deceptive practice” and grounds for suspending or revoking Tesla’s license to sell vehicles in the state.\nTesla, which has disbanded its public relations staff and has not responded to press inquiries for several years, could not be reached for comment.\nCNN’s Matt McFarland contributed to this report.","Relevant":true,"Overview":"The provided news article is relevant to self-driving car technology, specifically regarding Tesla's \"Full Self-Driving\" (FSD) feature. Here is the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": [\n        \"Tesla is recalling all 363,000 US vehicles with its so-called 'Full Self Driving' driver assist software due to safety risks.\",\n        \"The National Highway Traffic Safety Administration (NHTSA) states that Tesla’s FSD feature 'led to an unreasonable risk to motor vehicle safety based on insufficient adherence to traffic safety laws.'\",\n        \"The recall notice mentions that the FSD Beta system may allow the vehicle to act unsafely around intersections, entering stop sign-controlled intersections without coming to a complete stop, and proceeding into an intersection during a steady yellow traffic signal without due caution.\",\n        \"Tesla will attempt to fix the FSD feature through an over-the-air software update.\",\n        \"The article mentions that NHTSA has identified at least 273 crashes that involved one of Tesla’s driver assist systems.\",\n        \"There have been high-profile accidents of Tesla cars using FSD, including some with fatalities.\"\n    ]\n}\nIn summary, the news is relevant to self-driving car crashes because it discusses Tesla's recall of vehicles equipped with the \"Full Self Driving\" feature due to safety risks, including issues related to intersections and potential traffic violations, as well as the NHTSA's concerns about the technology. It also mentions accidents involving Tesla cars using FSD, which are critical aspects related to self-driving car safety.","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2023\/02\/16\/business\/tesla-fsd-recall\/index.html","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":43,"Time":"1 September 2018","Title":"Apple self-driving car in minor crash","Content":"Apple self-driving car in minor crash\nPublished\n1 September 2018\nShare\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nApple is believed to have a secretive driving division known as Project Titan\nBy Dave Lee\nNorth America technology reporter\nA self-driving car owned by Apple was involved in an accident, California’s road authority has confirmed.\nThe car, a modified Lexus RX450h with autonomous sensors, was rear-ended by a human driver in a Nissan Leaf.\nHumans were unhurt, but the machines suffered moderate damage.\nApple’s car is understood to be part of an ambitious but secretive programme - Project Titan. Apple has not commented on the 24 August collision, understood to be the company's first.\nSpeculation as to what the project seeks to achieve ranges from a fully-fledged Apple car - or just working with existing car makers to provide autonomous technology.\nApple’s self-driving programme had been public knowledge, It was revealed that the company now has 66 such cars on the roads, with 111 drivers registered to operate them.\nLike every firm experimenting with autonomy in California, Apple must provide regular reports to the state’s Department of Motor Vehicles (DMV), including when a crash occurs.\n'Waiting for a safe gap'\nAccording to documents released by the DMV on Friday, Apple’s car was on the roads in Sunnyvale, a Silicon Valley city not far from Apple’s headquarters in Cupertino.\nThe crash happened just before 15:00 - it was dry, clear and there were no unusual conditions, the DMV said.\n“An Apple test vehicle in autonomous mode was rear-ended while preparing to merge onto Lawrence Expressway South from Kifer Road,” the incident description reads.\n“The Apple test vehicle was travelling less than 1mph waiting for a safe gap to complete the merge when a 2016 Nissan Leaf contacted the Apple test vehicle at approximately 15mph.\n\"Both vehicles sustained damage and no injuries were reported by either party.”\nThe DMV does not attribute blame in its reports. Self-driving cars being rear-ended, however, might be considered a trend.\nA recent report by investigative technology news site The Information revealed teething problems at Waymo, the self-driving car company spun out of Google, where there have been headaches caused by what humans might consider over-cautious driving.\nThe self-driving cars would stop abruptly in scenarios where humans might zip through, such as turning across a line of traffic.\n\"As a result, human drivers from time to time have rear-ended the Waymo vans,” the report noted.\n_____\nFollow Dave Lee on Twitter @DaveLeeBBC\nDo you have more information about this or any other technology story? You can reach Dave directly and securely through encrypted messaging app Signal on: +1 (628) 400-7370\nMore on this story\nWaymo retires Firefly driverless car\nPublished\n13 June 2017\nApple car clues emerge from letter\nPublished\n15 August 2015\nWould you get into a taxi that drove itself?\nPublished\n7 November 2017","Relevant":true,"Overview":"Based on the provided news article, it appears to be about a self-driving car owned by Apple being involved in a minor crash. Here's the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Apple self-driving car in minor crash\",\n        \"Date_published\": \"1 September 2018\",\n        \"Description\": \"A self-driving car owned by Apple was involved in an accident, California’s road authority has confirmed. The car, a modified Lexus RX450h with autonomous sensors, was rear-ended by a human driver in a Nissan Leaf. Humans were unhurt, but the machines suffered moderate damage.\"\n    }\n}\nThe news is relevant to self-driving car crashes because it describes an incident where a self-driving car owned by Apple was rear-ended by a human-driven car. This incident involved a self-driving car, the type of vehicles that are commonly associated with self-driving car technology and accidents.\nPlease let me know if you need any more specific information or further details.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-45380373","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":44,"Time":null,"Title":"Self-driving car industry confronts trust issues after Uber crash","Content":"By Alexandria Sage, Tina Bellon, Nick Carey\n(Reuters) - The fatal accident involving an Uber self-driving car cranks up pressure on the self-driving vehicle industry to prove its software and sensors are safe in the absence of strong government standards, experts in the field said.\nFILE PHOTO: U.S. National Transportation Safety Board (NTSB) investigators examine a self-driving Uber vehicle involved in a fatal accident in Tempe, Arizona, U.S., March 20, 2018. National Transportation Safety Board\/Handout via REUTERS\nAutomakers including General Motors Co GM.N, technology companies such as Alphabet Inc GOOGL.O and ride services providers like Uber Technologies Inc [UBER.UL] have all urged policy makers at the federal and state level not to put a heavy regulatory hand on an industry still in development. They have said their extensive testing demonstrates commitment to safety.\nUber is currently looking for a head of global safety operations who would “drive key strategic programs including Safety Experience and Building Trust,” according to a job posting on the company’s website. The search was posted before the Arizona fatality.\nThe Uber accident in Tempe, Arizona this week was the first death attributed to a self-driving car operating in autonomous mode. It has given ammunition to critics of the industry concerned that the lack of clear standards allows manufacturers to test faulty or partially developed technology on public streets.\nWell before Sunday’s fatal accident, industry executives had begun to confront questions about whether self-driving cars can be trusted. They have opened up about their testing methods without revealing secrets of system designs.\nPublic disclosure of self-driving car testing data is inconsistent and varies by state. California requires manufacturers to report instances when an autonomous vehicle system disengages. Arizona does not.\n“There is no question whatsoever that regulations are coming,” said Doug Mehl, a partner at A.T. Kearney’s automotive practice, based in Detroit. “But right now (automakers), software developers and service providers have an opportunity to shape what those regulations are going to look like.”\nAlphabet’s Waymo self-driving car unit has underscored in a report that its autonomous vehicles have now logged 5 million miles in real-world testing, and billions more in computer simulations. GM’s Cruise Automation unit has highlighted its decision to teach its driving system to navigate San Francisco’s congested streets.\nStill, Amnon Shashua, head of Intel Corp's INTC.O Mobileye vision systems unit, said the industry must do more. He has called for the self-driving vehicle industry to develop \"provable safety assurances\".\n“We need to prove that these vehicles are much, much safer than humans,” Shashua told Reuters. “How do you go and guarantee that you have a technology that the probability of a fatality per one hour of driving is 1,000 times better than a human? Nobody talks about that because nobody knows what to do.”\nNO FEDERAL STANDARDS\nMost self-driving vehicles are equipped with radar sensors and lidar sensors, which use lasers to detect obstacles around the vehicle. There are no federal standards yet specifying how such systems should work. Congress and federal regulators are still debating how tightly to regulate such systems.\n“There should be vision tests for the sensors they are using, both static and dynamic to see how well they work,” said Missy Cummings, a Duke University mechanical engineering professor.\nThe short video recorded by cameras in the Uber vehicle that struck pedestrian Elaine Herzberg while crossing a street in Tempe, Arizona late Sunday raises questions about whether the Uber system responded better than a human driver, experts said on Wednesday.\nUber has hired human operators to sit in driver’s seats of its autonomous vehicles to intervene if necessary. The video released by Tempe police shows a human operator behind the wheel of the Uber vehicle before the impact.\nThe operator is seen looking down, away from the street, in the seconds before the vehicle struck Herzberg. She was pushing a bicycle across the street from left lane into the right lane where the Uber vehicle was driving.\n“It seems it should have detected her,” Daniel Sperling, director of the Institute for Transportation Studies at University of California Davis told Reuters in an email after viewing the video. “It seems unlikely that a human driver would have done better. We do want AVs to do better than us and the potential exists.”\nAmericans were wary of autonomous vehicle technology even before Sunday’s fatality.\nAccording to a Reuters\/Ipsos opinion poll released in late January, two-thirds of Americans are uncomfortable about the idea of riding in self-driving cars.\n“The greater risk for the industry is that if people feel it is unsafe, or the testing is unsafe, you’ll see a real backlash against this technology,” said Matthew Johnson-Roberson, co-director of the University of Michigan Ford Center for Autonomous Vehicles.\nReporting by Alexandria Sage in San Francisco, Nick Carey and Paul Lienert in Detroit, Tina Bellon in New York. Editing by Joseph White, Kenneth Maxwell and David Gregorio\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Based on the provided news article, it appears to be related to self-driving cars, but it does not specifically report a self-driving car crash. Here's the JSON format response:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"The news discusses the aftermath of a fatal accident involving an Uber self-driving car in Tempe, Arizona. While the article talks about the pressure on the self-driving vehicle industry to prove the safety of its software and sensors in the absence of strong government standards, it does not provide details of the actual crash itself. Instead, it focuses on the challenges and trust issues facing the self-driving car industry, questions about testing methods, the need for regulations, and the public's perception of autonomous vehicles.\"\n}\nSo, while the news is relevant to self-driving cars and their safety concerns, it does not contain specific information about a self-driving car crash.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-uber-trust-idUSKBN1GY15F","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":39,"Time":null,"Title":"Uber’s fatal self-driving car crash prompts NTSB investigation","Content":"The National Transportation Safety Board is opening an investigation into the fatal accident involving one of Uber’s self-driving cars in Tempe, Arizona.\nUber’s self-driving car accident that resulted in a woman’s death raises a number of questions about insurance and liability. Although the car was in self-driving mode, there was a safety driver behind the wheel who theoretically should have been able to intervene.\nUber has since halted its self-driving car tests in Arizona, Pittsburgh and California. Last year, the NTSB looked into a 2016 accident involving Tesla’s Autopilot system in Florida. The NTSB partially faulted Tesla for the fatal crash, saying the system operated as intended but that the driver’s inattentiveness, due to over-reliance on the Autopilot system, resulted in the accident.\nIn an earlier statement to TechCrunch, an Uber spokesperson said, “Our hearts go out to the victim’s family. We are fully cooperating with local authorities in their investigation of this incident.”\nMore TechCrunch\nSecurity researcher warns of chilling effect after feds search phone at airport\nRussian zero-day seller offers $20M for hacking Android and iPhones\nIt's not clear X CEO Linda Yaccarino knew about Elon Musk's plan to charge for X\nElon Musk is probably right about one thing","Relevant":true,"Overview":"Based on the provided news, it appears to be related to self-driving car crashes. Here's the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is about the fatal self-driving car crash involving one of Uber's self-driving cars in Tempe, Arizona. The National Transportation Safety Board (NTSB) is opening an investigation into the accident. The incident raises questions about insurance and liability, as the car was in self-driving mode with a safety driver behind the wheel who theoretically should have been able to intervene. Uber has halted its self-driving car tests in multiple locations following this accident. The NTSB's previous investigation into a 2016 Tesla Autopilot system accident in Florida is also mentioned in relation to self-driving technology.\"\n}\nThe provided information shows that the news is relevant to self-driving car crashes and includes details about the Uber self-driving car accident, the NTSB investigation, and the implications for self-driving technology and safety.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2018\/03\/19\/ubers-fatal-self-driving-car-crash-prompts-ntsb-investigation\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":45,"Time":"Thu 16 Feb 2023 20.41 CET","Title":"Tesla recalls 362,000 vehicles over self-driving software flaws that risk crashes","Content":"Tesla said it would recall 362,000 US vehicles to update its Full Self-Driving (FSD) Beta software after regulators said on Thursday the driver assistance system did not adequately adhere to traffic safety laws and could cause crashes.\nThe National Highway Traffic Safety Administration (NHTSA) said the Tesla software allows a vehicle to “exceed speed limits or travel through intersections in an unlawful or unpredictable manner increases the risk of a crash”.\nTesla will release an over-the-air (OTA) software update free of charge, and the electric vehicle maker said is not aware of any injuries or deaths that may be related to the recall issue. The automaker said it had 18 warranty claims.\nTesla shares were down 1.6% at $210.76 on Thursday afternoon.\nThe recall covers 2016-2023 Model S, Model X, 2017-2023 Model 3, and 2020-2023 Model Y vehicles equipped with FSD Beta software or pending installation.\nNHTSA asked Tesla to recall the vehicles, but the company said despite the recall it did not concur with NHTSA’s analysis. The move is a rare intervention by federal regulators in a real-world testing program that the company sees as crucial to the development of cars that can drive themselves. FSD Beta is used by hundreds of thousands of Tesla customers.\nThe setback for Tesla’s automated driving effort comes about two weeks before the company’s 1 March investor day, during which its chief executive Elon Musk is expected to promote the EV maker’s artificial intelligence capability and plans to expand its vehicle lineup.\nTesla could not immediately be reached for comment.\nNHTSA has an ongoing investigation it opened in 2021 into 830,000 Tesla vehicles with driver assistance system Autopilot over a string of crashes with parked emergency vehicles. NHTSA is reviewing whether Tesla vehicles adequately ensure drivers are paying attention. NHTSA said on Thursday despite the FSD recall its “investigation into Tesla’s Autopilot and associated vehicle systems remains open and active”.\nTesla said in “certain rare circumstances … the feature could potentially infringe upon local traffic laws or customs while executing certain driving maneuvers”.\nPossible situations where the problem could occur include traveling or turning through certain intersections during a yellow traffic light and making a lane change out of certain turn-only lanes to continue traveling straight, NHTSA said.\nNHTSA said “the system may respond insufficiently to changes in posted speed limits or not adequately account for the driver’s adjustment of the vehicle’s speed to exceed posted speed limits”.\nLast year, Tesla recalled nearly 54,000 US vehicles with FSD Beta software that may allow some models to conduct “rolling stops” and not come to a complete stop at some intersections, posing a safety risk, NHTSA said.\nTesla and NHTSA say FSD’s advanced driving features do not make the cars autonomous and require drivers to pay attention.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n\"Tesla has recalled 362,000 US vehicles due to self-driving software flaws that risk crashes.\",\n\"The Full Self-Driving (FSD) Beta software did not adhere adequately to traffic safety laws and could cause crashes, as per regulators.\",\n\"The National Highway Traffic Safety Administration (NHTSA) pointed out that the Tesla software allowed vehicles to exceed speed limits and travel through intersections unlawfully or unpredictably, increasing the crash risk.\",\n\"The recall covers various Tesla models equipped with FSD Beta software.\",\n\"NHTSA has an ongoing investigation into Tesla vehicles with Autopilot, which remains open and active despite the FSD recall.\",\n\"Tesla had recalled vehicles with FSD Beta software in the past due to safety concerns.\",\n\"Tesla and NHTSA emphasize that FSD's advanced driving features do not make the cars autonomous and require drivers to pay attention.\"\n]\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2023\/feb\/16\/tesla-recall-full-self-driving-cars","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":46,"Time":"Fri 17 Jul 2015 16.56 CEST","Title":"Crash involving self-driving Google car injures three employees","Content":"Three Google employees have been injured in a crash involving one of the company’s self-driving cars.\nGoogle revealed the accident happened on 1 July when its car was rear-ended while stationary on a public road in Mountain View, California.\nIt is the first accident involving one of Google’s fleet of self-driving vehicles to have resulted in injury.\nThe modified Lexus sports utility vehicle, 20 of which are operating on public roads, was hit by another driver. Three Google employees within the self-driving car were taken to hospital complaining of whiplash and later released. The driver of the other car also complained of neck and back pain.\nThe Google car was stationary in a queue of traffic when it was hit by the vehicle travelling at 17mph, according to an accident report filed by Google.\nAccident prone\nThis is the 14th accident in six years and about 1.9 million miles of testing, according to the company. In 11 of the 14 accidents, Google said its car was rear-ended.\nTwo self-driving cars avoid each other on Californian roads\nRead more\nGoogle has also said that its cars have not caused any of the collisions while driving themselves. In 2011, a Google employee took one of the company’s cars to run an errand and rear-ended another vehicle, but the car was under human control at the time.\nChris Urmson, head of Google’s self-driving car programme, wrote that his automated cars “are being hit surprisingly often” by distracted drivers.\n“The clear theme is human error and inattention. We’ll take all this as a signal that we’re starting to compare favourably with human drivers.”\nUrmson said his team was exploring whether its cars could do something to alert distracted drivers before a collision. Honking would be one possibility, but Urmson said the noise could annoy residents of Mountain View where the cars are tested.\nGoogle acknowledges its self-driving cars had 11 minor accidents","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is about a crash involving a self-driving Google car that injured three employees. The accident occurred on 1 July when the Google car was rear-ended while stationary on a public road in Mountain View, California. It is the first accident involving one of Google’s fleet of self-driving vehicles to have resulted in injury. The modified Lexus sports utility vehicle, which is part of Google's self-driving fleet, was hit by another driver, and three Google employees inside the self-driving car were taken to the hospital with complaints of whiplash. The Google car was stationary in a queue of traffic when it was rear-ended, according to an accident report filed by Google. This incident is part of a series of accidents involving Google's self-driving cars, with 11 of the 14 accidents being rear-end collisions caused by other drivers. The clear theme in these accidents is human error and inattention, according to Chris Urmson, head of Google’s self-driving car program. Google has stated that its self-driving cars have not caused any of the collisions while driving themselves.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2015\/jul\/17\/crash-self-driving-google-car-injures-three","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":47,"Time":"Sun 4 Dec 2016 18.22 CET","Title":"Apple confirms it is working on self-driving cars","Content":"Apple has said for the first time that it is working on technology to develop self-driving cars.\nThe company, which has been rumoured to be interested in the automated car market for the past two years, confirmed its previously secret initiative in a statement to the US highways regulator.\n“The company is investing heavily in the study of machine learning and automation, and is excited about the potential of automated systems in many areas, including transportation,” said the letter from Steve Kenner, Apple’s director of product integrity, to the head of the National Highway Traffic Safety Administration (NHTSA).\nThe letter offered no details of the project, instead highlighting the “significant societal benefits of automated vehicles”, which it described as a life-saving technology, potentially preventing millions of car crashes and thousands of fatalities each year.\nWatch Google take its self-driving car for a spin Guardian\nAlthough fully automated cars are not expected to be in use in the UK for several years, trials have taken place in Britain and the US.\nThe British government is keen to ensure that the country is at the forefront of a fast developing market that could be worth £900bn by 2025, according to a KPMG study.\nEarlier this year, ministers launched a consultation on changes to motor insurance rules and the highway code aimed at allowing self-driving cars to be on the roads by 2020.\nThe business secretary, Greg Clark, said at the time: “The global market for autonomous vehicles presents huge opportunities for our automotive and technology firms. The research that underpins the technology and software will have applications way beyond autonomous vehicles.”\nRumours of Apple’s interest in the car market surfaced early last year. Known as Project Titan, the internal initiative was reportedly staffed by more than 1,000 engineers and other personnel working at top secret labs in Sunnyvale, California, some of whom were pulled from important consumer product teams.\nThe letter from Kenner was dated 22 November, but has only just emerged. It leaves open the possibility that Apple will go on to design and produce a car of its own, rather than merely provide technology to an existing manufacturer. Earlier this year, the company was rumoured to be in talks to take over the British car manufacturer McLaren.\n“To maximise the safety benefits of automated vehicles, encourage innovation and promote fair competition, established manufacturers and new entrants should be treated equally,” Apple wrote.\nKenner argued that manufacturers should pool their data as they develop automated systems, to help everyone identify unusual situations or “edge cases” that cars may encounter on the roads.\n“Companies should share de-identified scenario and dynamics data from crashes and near misses,” Kenner wrote. “By sharing data, the industry will build a more comprehensive data set than any one company could create alone.”\nBut the letter added: “Data sharing should not come at the cost of privacy.”\nApple urged the regulator to continue “thoughtful exploration of the ethical issues” of self-driving cars. “Because automated vehicles promise such a broad and deep human impact, companies should consider the ethical dimensions of them in comparably broad and deep terms,” it said.\nIf the age of self-driving cars is upon us, what's keeping them off the roads?\nRead more\nThe letter said these considerations include privacy, how the cars’ software systems make decisions and the impact on employment and public spaces.\nKenner ended the letter by saying: “Apple looks forward to collaborating with NHTSA and other stakeholders so that the significant societal benefits of automated vehicles can be realised safely, responsibly and expeditiously.”\nSeveral large technology companies and traditional car manufacturers are working on automated vehicle technology. Uber has raced ahead of its competitors, deploying Ford Fusions in Pittsburgh, Pennsylvania, which do not require hands on the wheel, as part of the company’s regular taxi service.\nIt is also developing a driverless car in partnership with Swedish carmaker Volvo.\nElectric car company Tesla is also involved, although it raised concerns earlier this year about the implications of driverless vehicles after one of its cars using autopilot mode was involved in a fatal crash.\nIn September, a Google self-driving car collided with another vehicle after the latter ran through a red light.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving cars as it discusses Apple's confirmation of working on self-driving car technology and its potential benefits. Apple's letter highlights the societal benefits, including preventing car crashes and fatalities through automated systems. Additionally, the news mentions trials and government interest in autonomous vehicles, as well as other companies like Uber, Tesla, and Google working on self-driving car technology.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/dec\/04\/apple-confirms-it-is-working-on-self-driving-cars","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":50,"Time":"8 October 2015","Title":"Who is responsible for a driverless car accident?","Content":"Who is responsible for a driverless car accident?\nPublished\n8 October 2015\nShare\nIMAGE SOURCE,\nVOLVO\nImage caption,\nLiability for accidents involving driverless cars is becoming a big issue worldwide\nVolvo says it will accept full liability for accidents involving its driverless cars, making it \"one of the first\" car companies to do so.\nIt joins Google, which has made similar claims.\nVolvo says it is trying to expedite regulation in the US, where \"a patchwork\" of rules is holding back the industry.\nUncertainty over liability for a driverless car crash is seen as one of the biggest barriers to adoption.\nWhy is Volvo doing this?\nIn a speech in Washington DC on Thursday, the president of Volvo Cars, Hakan Samuelsson, said that the US is currently \"the most progressive country in the world in autonomous driving\".\nHowever, he believes it \"risks losing its leading position\" because of the lack of Federal guidelines for the \"testing and certification\" of autonomous vehicles.\nInstead, car makers face inconsistent rules from state to state, which makes it harder to roll out their technology.\nFor instance, only a handful of US states such as California and Nevada allow the testing of autonomous vehicles on public roads - and even then rules around certification vary.\nRegulation is also slowed by unanswered ethical and legal concerns, particularly when it comes to liability for driverless car accidents.\nAnd the situation is \"even worse\" in Europe, according to Volvo's chief technical officer, Erik Coelingh.\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nGoogle's self-driving cars have been involved in several accidents in California, although the autonomous technology was not found to be at fault\nWhy is the issue of liability so important?\nAs with any new form of technology, it is important to get the regulation governing its implementation right.\nMr Coelingh told the BBC: \"Everybody is aware of the fact that driverless technology will never be perfect - one day there will be an accident.\n\"So the question becomes who is responsible and we think it's unrealistic to put that responsibility on our customers.\"\nBut Ben Gardener, a solicitor at Pinsent Masons, believes Volvo's guarantee is aimed at reducing uncertainty in the minds of governments and regulators.\n\"Volvo wants to remove the uncertainty of who would be responsible in the event of a crash. At the moment it could be the manufacturer of the technology, the driver, a maker of a component in a car.\"\nVolvo also told the BBC it would only accept liability for an accident if it was the result of a flaw in the car's design.\n\"If the customer used the technology in an inappropriate way then the user is still liable,\" said Mr Coelingh.\n\"Likewise if a third party vehicle causes the crash, then it would be liable.\"\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nLike Volvo, Audi has also complained about inconsistent regulations governing driveless cars\nWill this actually make a difference?\nMaybe.\nProf Sandor Veres, director of autonomous systems at Sheffield University, told the BBC: \"This bold move by Volvo can pave the way for global legislation, as if other manufacturers take similar undertakings then legislation can be made simple.\"\nBut Mr Gardener said the move would not get around underlying safety concerns about driverless cars.\nWhile manufacturers claim autonomous vehicles could eventually improve road safety, a number have been involved in accidents involving such cars during tests.\n\"There's not much value in a manufacturer saying we'll be responsible for thousands of accidents but then there being thousands of accidents each year,\" said Mr Gardener.\nHe said it was also unclear whether automatically holding manufacturers liable would stand up in court - at least in the UK.\n\"In this country, the party causing an accident is responsible once all of the circumstances of the particular case have been examined,\" he said.\n\"But moving to a strict manufacturer liability approach would remove the need to consider who is responsible for the collision. This is something not currently recognised in the UK approach to liability.\"\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nA driverless Mercedes-Benz on display at Consumer Electronics Show, Shanghai, in May\nWhen will we see driverless cars on our roads?\nIn truth, we are only likely to see widespread use of driverless technology when such grey areas are resolved.\nBut according to some, that could be sooner than previously expected.\nAt the Frankfurt Auto show last month, US secretary of transportation Anthony Foxx said he expects driverless cars to be in use all over the world in 10 years.\nTesla founder Elon Musk has predicted his firm will have approval for its automated vehicles as early as 2019.\nAnd many other manufacturers plan to launch driverless cars in the near future, including Toyota which on Wednesday said it expected to be selling such vehicles by 2020.\nMore on this story\nToyota tests driverless car\nPublished\n7 October 2015\nGoogle's driverless car is so boring\nPublished\n2 October 2015\nCould driverless cars own themselves?\nPublished\n16 February 2015","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news article discusses the issue of liability for accidents involving driverless cars. Volvo has announced that it will accept full liability for accidents involving its driverless cars, making it one of the first car companies to do so. The article also mentions Google making similar claims. The lack of clear regulations in the US regarding testing and certification of autonomous vehicles, as well as the uncertainty over liability for driverless car crashes, is highlighted. Volvo's chief technical officer, Erik Coelingh, mentions that driverless technology will never be perfect and raises the question of responsibility in case of accidents. The article discusses Volvo's approach to liability, stating they would accept it only if it's due to a flaw in the car's design. This news is relevant to self-driving car crashes as it discusses the responsibility and liability aspects associated with accidents involving autonomous vehicles.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-34475031","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":52,"Time":"22 March 2018","Title":"Uber self-driving crash: Footage shows moment before impact","Content":"Uber self-driving crash: Footage shows moment before impact\nPublished\n22 March 2018\nShare\nMedia caption,\nUber dashcam footage shows moment before fatal impact\nPolice have released two videos showing the moments leading up to a fatal crash involving a self-driving Uber car in Tempe, Arizona, on Sunday.\nIn the 14-second video, the autonomous vehicle is seen failing to slow down before hitting Elaine Herzberg, 49, who is walking her bike across the road.\nOne video shows dashcam footage of the impact. The other, an Uber operator monitoring the car's controls.\nUber has suspended self-driving tests in North America following the crash.\nIn footage released on Wednesday by the Tempe police department, the human Uber operator sitting inside the Volvo appears to be looking down at something while the vehicle is travelling in autonomous mode.\nMoments later, the woman appears visibly shocked as she looks up to see Ms Herzberg crossing the highway in their path seconds before impact.\nIMAGE SOURCE,\nREUTERS\nImage caption,\nUber has been testing autonomous Volvo vehicles\n\"The video is disturbing and heartbreaking to watch, and our thoughts continue to be with Elaine's loved ones,\" Uber said in a statement.\n\"Our cars remain grounded, and we're assisting local, state and federal authorities in any way we can,\" the statement added.\nPolice said the accident happened on Sunday night, adding that Ms Herzberg had not been using a pedestrian crossing.\nMs Herzberg was taken to a local hospital following the collision but died of her injuries.\nThe US National Highway Traffic Safety Administration and the National Transportation Safety Board earlier said they would be investigating the incident in Tempe.\nWhile self-driving cars have been involved in multiple accidents, it is thought to be the first time an autonomous car has been involved in a fatal collision with a pedestrian.\nMore on this story\nUber halts self-driving tests after death\nPublished\n20 March 2018\nToyota suspends US driverless car tests\nPublished\n20 March 2018","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is about a self-driving car crash involving an Uber vehicle in Tempe, Arizona. The incident occurred on Sunday, March 22, 2018, and resulted in a fatal collision with a pedestrian, Elaine Herzberg, who was walking her bike across the road. The article provides detailed information about the crash, including the release of dashcam footage showing the moments leading up to the impact. The news also mentions that Uber suspended self-driving tests in North America following the crash and that the incident prompted investigations by the US National Highway Traffic Safety Administration and the National Transportation Safety Board. This information clearly fits the category of self-driving car crashes.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/world-us-canada-43497364","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":48,"Time":"Tue 8 May 2018 12.00 CEST","Title":"Uber’s self-driving car saw the pedestrian but didn’t swerve – report","Content":"An Uber self-driving test car which killed a woman crossing the street detected her but decided not to react immediately, a report has said.\nThe car was travelling at 40mph (64km\/h) in self-driving mode when it collided with 49-year-old Elaine Herzberg at about 10pm on 18 March. Herzberg was pushing a bicycle across the road outside of a crossing. She later died from her injuries.\nAlthough the car’s sensors detected Herzberg, its software which decides how it should react was tuned too far in favour of ignoring objects in its path which might be “false positives” (such as plastic bags), according to a report from the Information. This meant the modified Volvo XC90 did not react fast enough.\nThe report also said the human safety driver was not paying close enough attention to intervene before the vehicle struck the pedestrian.\nArizona suspended Uber’s self-driving vehicle testing after the incident. The company later settled with Herzberg’s family.\nUber and the US National Transportation Safety Board (NTSB) are investigating the incident. Uber has already reached its preliminary conclusion, according to the report. A comprehensive NTSB report is expected later.\n“We’re actively cooperating with the NTSB in their investigation. Out of respect for that process and the trust we’ve built with NTSB, we can’t comment on the specifics of the incident,” an Uber spokesperson said in a statement. “In the meantime, we have initiated a top-to-bottom safety review of our self-driving vehicles programme, and we have brought on former NTSB chair Christopher Hart to advise us on our overall safety culture. Our review is looking at everything from the safety of our system to our training processes for vehicle operators, and we hope to have more to say soon.”\nThe collision marked the first fatality attributed to a self-driving car, the development of which has frequently been labelled as the only way to eliminate road deaths for those inside and outside the car.\nThe incident was not the first controversy to involve Uber’s self-driving efforts, which the company sees as key to its survival as a ride-sharing or taxi firm. The company has been involved in a long-running battle with former Google self-driving car outfit Waymo over theft of technology around Anthony Levandowski.\nUber’s self-driving technology was also called 5,000 times worse than Waymo’s in an independent analysis in 2017, while it has had legal tussles with various US states where it has tried to test vehicles.\nTesla car that crashed and killed driver was running on Autopilot, firm says\nGoogle sibling Waymo launches fully autonomous ride-hailing service","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is about a self-driving car crash involving an Uber self-driving test car. The incident occurred when the self-driving car, operating at 40mph in self-driving mode, collided with a pedestrian, Elaine Herzberg, who was pushing a bicycle across the road outside of a crossing. Although the car's sensors detected the pedestrian, its software decided not to react immediately, and the safety driver was not paying close enough attention to intervene before the collision. This incident marked the first fatality attributed to a self-driving car. Additionally, the news mentions that Uber and the US National Transportation Safety Board (NTSB) are investigating the incident, and Uber has initiated a safety review of its self-driving vehicles program.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/may\/08\/ubers-self-driving-car-saw-the-pedestrian-but-didnt-swerve-report","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":55,"Time":"Published 3:19 PM GMT+2, March 28, 2017","Title":"Self-driving car crash comes amid debate about regulations","Content":"PHOENIX (AP) — A crash that caused an Uber self-driving SUV to flip onto its side in a Phoenix suburb serves as a stark reminder of the challenges surrounding autonomous vehicles in Arizona, a state that has gone all-in to entice the company by promising minimal government regulation.\nFriday night’s crash was blamed on the driver of an oncoming SUV that turned left in front of the Uber vehicle carrying two test drivers and no passengers. There were no serious injuries and the driver of the other car was cited for a moving violation. But images of Uber’s Volvo SUV rolled onto its side reverberated heavily on social media.\nUber responded by briefly suspending its self-driving cars in its three testing locations — Arizona, San Francisco and Pittsburgh — as it investigated the accident.\nUber’s self-driving car program is rolling out amid questions about how much government regulation it should endure on issues such as accidents, insurance and reporting instances in which the person behind the wheel in test cars needs to take control of the vehicle.\nThe San Francisco-based startup endured a shaky December rollout in California — including running red lights — that culminated in a standoff between Uber and state regulators who wanted more transparency and reporting.\nArizona Gov. Doug Ducey seized the opportunity and used lax regulations to entice Uber, which decided to ship more than a dozen SUVs to metro Phoenix.\n“California may not want you, but Arizona does,” said Ducey, who took the first ride as a passenger in Uber’s self-driving cars last month.\nUber spokeswoman Taylor Patterson said the company is operating more than a dozen of the 21 vehicles it has registered in Arizona. Some pick up passengers.\nIn Arizona, companies such as Uber only need to carry minimum liability insurance policies to operate self-driving cars. They are not required to track crashes like the one that occurred in Tempe on Friday or report any information to the state.\nThat means that self-driving test cars are essentially treated like all other cars on the road.\nDucey spokesman Daniel Scarpinato said in a March 3 interview that the cars are safe and there is sufficient oversight under existing automobile rules.\n“There’s a driver in the car,” he said. “The state oversight is: There are not cars without drivers in them.”\nJohn Simpson of the California-based advocacy group Consumer Watchdog said Ducey has abandoned his responsibility to protect the public by buying into the hype surrounding Uber.\n“It’s a fundamental responsibility of a governor of a state to make sure that when companies are using the state’s public highways as their own private laboratories, that there is some obligation to protect public safety,” Simpson said. “There are no rules in Arizona.”\nIn March, Uber obtained permits for two of its Volvo SUVS to again hit the streets in San Francisco.\nCalifornia’s rules for autonomous vehicles require a $5 million insurance policy, and the companies must reports accidents to the state within 10 days and release an annual tally documenting how many times test drivers had to take over.\nAlso, unlike in Arizona and Pennsylvania, passengers are not allowed to ride in autonomous vehicles in California.\nDucey doesn’t believe self-driving car testing needs extra regulations because drivers can take over if something goes wrong, but his office said Monday after the accident that “public safety remains our top priority and we will continue to monitor the situation closely.”\nKevin Biesty, deputy director for policy for the Arizona Department of Transportation, said the state could set up a system to monitor local police accident reports involving self-driving cars but chose not to do so.\n“At this point we don’t see an issue if the vehicles are being operated safely — they’ll be responsible for whatever issues arise, just like any driver,” Biesty said.\nUber’s SUVs have been tooling around Phoenix and Tempe for more than three months, and police in both cities said they knew of no accidents before Friday.\nOther companies testing self-driving cars in Arizona include Waymo, a Google spinoff company, and General Motors. Intel has a fleet of self-driving cars that are being tested, although they are not used in autonomous mode on city streets, company spokeswoman Danielle Mann said.\nThere’s no Arizona state data showing how many accidents the cars may have been involved in or caused.\nPolice in suburban Chandler said the Google cars have been in at least four wrecks over the past three years. None of the GM cars have been involved in accidents, said Kevin Kelly, the company’s spokesman for advanced technology projects.\nUber’s recent crash comes amid a series of public-relations woes at the company, including an upheaval of its executive ranks and allegations that it routinely ignores sexual harassment.\nThe New York Times also revealed the company’s use of the “Greyball” program that helped Uber identify law enforcement agents who may be trying to catch it operating illegally in some places. The company’s chief security officer, Joe Sullivan, subsequently promised Uber would no longer use the program.","Relevant":true,"Overview":"Here's the JSON format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to self-driving car crashes. It describes a crash involving an Uber self-driving SUV in a Phoenix suburb. The crash occurred when an oncoming SUV turned left in front of the Uber vehicle, causing it to flip onto its side. The news also mentions that Uber suspended its self-driving cars temporarily in response to the accident. Additionally, there is discussion about government regulation related to self-driving cars, accidents, insurance, and reporting requirements for autonomous vehicles, making this news directly related to self-driving car technology and its challenges.\"\n}\nThe news is indeed about a self-driving car crash involving an Uber vehicle, and it provides specific information about the crash and the surrounding regulatory issues.","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/5e2cdcd3ef4f466180a84559692ff2db","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":51,"Time":"3:00 PM GMT+1•December 14, 2021","Title":"California suspends Pony.ai driverless test permit after crash","Content":"cameraImage Credits: Pony.ai\nPony.ai, a Chinese autonomous driving startup with an office in the U.S., has paused its driverless pilot fleet in California six months after it was approved by local regulators to test autonomous vehicles without a human safety driver.\nOn November 19, the California Department of Motor Vehicles notified Pony.ai that it was suspending its driverless testing permit following a reported collision in Fremont on October 28, the DMV told TechCrunch in a statement.\nPony.ai has 10 Hyundai Kona electric cars registered under its driverless testing permit. This suspension does not impact Pony.ai’s permit for testing with a safety driver, said the DMV.\nAccording to Pony.ai’s collision report, the incident took place on a clear morning when its driverless vehicle was changing lanes using the autonomous mode.\n“Recently, one of our vehicles experienced an incident in Fremont, California, involving a collision with a lane divider and street sign. No other vehicles were involved and no injuries occurred,” a Pony.ai spokesperson told TechCrunch.\n“We immediately launched an investigation, and are in contact with the California DMV about the incident,” the spokesperson said.\nWhile other autonomous vehicles had reported crashes, most have occurred while the safety driver has been operating the vehicle manually or when another vehicle has struck an AV from behind. This incident stands out because the vehicle was in autonomous mode and didn’t involve any other vehicle.\nThe incident puts a question mark on Pony.ai’s autonomous driving capabilities. The startup’s AVs have “successfully driven over 750,000 real-world miles in California since 2017,” said its spokesperson.\nFounded in 2016 by Baidu veterans, Pony.ai is among a group of AV startups that have R&D teams and testing fleets in both China and California. It has attracted heavyweight investors like Toyota and Sequoia Capital, with over $1 billion in total raise and a valuation of $5.3 billion as of February. Just recently, alongside Baidu, it was approved to operate commercial drivered autonomous cars within a demo zone in Beijing.\nThe company has also faced obstacles in recent months. It had halted plans to list through a SPAC in New York after failing to reassure Beijing it would not be targeted by the U.S. government, Reuters reported in August. Last month, TechCrunch reported that the company’s autonomous trucking business had lost a few key executives, leaving the newly minted unit in limbo. As competition heats up, the startup has a lot to prove to its investors that it has competitive technologies and a viable commercial future.","Relevant":true,"Overview":"Based on the provided news article, it appears to be relevant to self-driving car crashes. Here's the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"Pony.ai, a Chinese autonomous driving startup, had its driverless testing permit in California suspended by the California Department of Motor Vehicles (DMV) following a reported collision in Fremont on October 28. According to Pony.ai's collision report, the incident occurred when its driverless vehicle was changing lanes using the autonomous mode, and it collided with a lane divider and street sign. Importantly, this incident stands out because the vehicle was in autonomous mode and did not involve any other vehicle. No injuries occurred, but the suspension of Pony.ai's driverless testing permit raises questions about the company's autonomous driving capabilities and safety. This news is directly related to self-driving car crashes as it involves a collision during autonomous operation.\"\n}\nThe news article reports a specific incident where a self-driving vehicle from Pony.ai was involved in a collision while operating in autonomous mode, leading to the suspension of their driverless testing permit. This information confirms that the news is about a self-driving car crash and provides specific details about the incident.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2021\/12\/14\/pony-ai-suspension-driverless-pilot-california\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":49,"Time":"Fri 22 Jun 2018 10.32 CEST","Title":"Driver was streaming The Voice when Uber self-driving car crashed, say police","Content":"The “safety” driver behind the wheel of a self-driving Uber that hit and killed a pedestrian was streaming the television show The Voice on her phone at the time of the crash, police have said.\nThe collision that killed Elaine Herzberg, 49, who was crossing the road at night in Tempe, Arizona, was “entirely avoidable”, a police report said, if Rafaela Vasquez had been paying attention.\nInstead, she repeatedly looked down at her phone, glancing up just half a second before the car hit Herzberg. Police said she could faces charges of\nvehicular manslaughter, but it would be for prosecutors to decide.\nThe Uber car was in autonomous mode at the time of the crash, but Uber, like other self-driving car developers, requires a back-up driver in the car to intervene when the autonomous system fails or a tricky driving situation occurs.\nVasquez looked up just 0.5 seconds before the crash, after keeping her head down for 5.3 seconds, the police report said. Uber’s self-driving Volvo SUV was travelling at just under 44mph.\nPolice obtained records from Hulu, an online streaming service, which showed Vasquez’s account was playing the television talent show The Voice the night of the crash for about 42 minutes, ending at 9.59pm, which “coincides with the approximate time of the collision”, the report says.\nVasquez could not immediately be reached for comment and Reuters could not locate her attorney.\nThe crash dealt Uber a major setback in its efforts to develop self-driving cars, and the company ended its autonomous car testing programme in Arizona after the incident.\nUber declined to comment on the police report. The company prohibits the use of any mobile device by safety drivers while the self-driving cars are on a public road, and drivers are told they can be fired for violating this rule.\nA spokeswoman last month said the company was undergoing a “top-to-bottom safety review”.\nPolice said video from inside the car showed Vasquez was looking down during the trip, and her face “appears to react and show a smirk or laugh at various points during the times that she is looking down”.\nThe report found that Vasquez “was distracted and looking down” for close to seven of the nearly 22 minutes prior to the collision.\nAccording to a report last month by the US National Transportation Safety Board, which is also investigating the crash, Vasquez told federal investigators she had been monitoring the self-driving interface in the car and that neither her personal nor business phones were in use until after the crash.\nThat report showed Uber had disabled the emergency braking system in the Volvo, and Vasquez began braking less than a second after hitting Herzberg.\nHerzberg, who was homeless, was walking her bicycle across the street, outside a crossing on a four-lane road, on the night of 18 March when she was struck by the front right side of the Volvo.\nThe police report faulted Herzberg for “unlawfully crossing the road at a location other than a marked crosswalk”.","Relevant":true,"Overview":"Based on the provided news article, it is relevant to self-driving car crashes. Here is the JSON format response with specific information:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Driver was streaming The Voice when Uber self-driving car crashed, say police.\",\n        \"Description\": \"The news is about a fatal crash involving a self-driving Uber car in Tempe, Arizona. The crash resulted in the death of Elaine Herzberg, who was hit by the Uber car while crossing the road at night. The key details that make this news relevant to self-driving car crashes include:\",\n        \"Details\": [\n            \"The self-driving Uber car was in autonomous mode at the time of the crash, indicating it was a self-driving car involved.\",\n            \"The 'safety' driver, Rafaela Vasquez, was supposed to intervene if needed but was distracted by streaming the television show 'The Voice' on her phone at the time of the crash.\",\n            \"The police report stated that the collision was 'entirely avoidable' if the safety driver had been paying attention.\",\n            \"The police report revealed that Vasquez looked up just half a second before the car hit Herzberg after keeping her head down for 5.3 seconds.\",\n            \"Records from Hulu showed that Vasquez was streaming 'The Voice' around the time of the collision, corroborating her distraction.\",\n            \"Uber had disabled the emergency braking system in the Volvo, and Vasquez began braking less than a second after hitting Herzberg, which raises concerns about the car's autonomous system.\",\n            \"The incident led Uber to end its autonomous car testing program in Arizona, indicating the significant impact on self-driving car development.\"\n        ]\n    }\n}\nThis response provides specific information from the news article that supports the relevance of the news to self-driving car crashes, including details about the autonomous mode, the distracted safety driver, and the consequences of the crash on Uber's self-driving car program.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/jun\/22\/driver-was-streaming-the-voice-when-uber-self-driving-car-crashed-say-police","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":58,"Time":"July 29, 2023","Title":"Backup driver in 2018 Uber self-driving crash pleads guilty","Content":"Companies\nUber Technologies Inc\nFollow\nJuly 28 (Reuters) - The backup safety driver behind the wheel of a self-driving Uber Technologies (UBER.N) test vehicle that struck and killed a woman in Tempe, Arizona, in 2018 pleaded guilty on Friday and was sentenced to probation, prosecutors said.\nRafaela Vasquez, who was charged with negligent homicide in 2020, pleaded guilty to endangerment, a reduced charge, and was sentenced to three years of supervised probation, the Maricopa County Attorney's office said. Uber and a lawyer for Vasquez did not immediately respond to requests for comment.\nElaine Herzberg died after she was struck while walking a bicycle across a street at night. The first recorded death involving a self-driving vehicle prompted significant safety concerns about the nascent autonomous vehicle industry.\n\"The defendant in this matter was responsible for the operation of a vehicle on our city streets that ended with a woman being killed,\" Maricopa County Attorney Rachel Mitchell said in a statement. \"We believe the judge ordered an appropriate sentence based on the mitigating and aggravating factors.\"\nAdvertisement · Scroll to continue\nA Tempe police report said Vasquez was repeatedly looking down instead of keeping her eyes on the road. Prosecutors in March 2019 said Uber was not criminally liable in the crash.\nPolice said previously the crash was \"entirely avoidable\" and that Vasquez was streaming \"The Voice\" TV program at the time of the crash.\nIn 2019, the National Transportation Safety Board (NTSB) faulted Vasquez's inactions and Uber for inadequate attention to safety and decisions in the company's autonomous vehicle development.\nAdvertisement · Scroll to continue\nThe NTSB said the probable cause was Vasquez's failure to monitor the driving environment \"because she was visually distracted throughout the trip by her personal cell phone.\" She was supposed to act in the event of an emergency.\nUber made a series of development decisions that contributed to the crash's cause, the NTSB said. The software in the modified Volvo XC90 did not properly identify Herzberg as a pedestrian and did not address \"operators' automation complacency.\"\nAdvertisement · Scroll to continue\nIn 2020, Uber announced the sale of its autonomous driving unit to self-driving car startup Aurora for $4 billion.\nReporting by David Shepardson; editing by Jonathan Oatis and Richard Chang\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here is the JSON format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"This news is about a self-driving car crash. It discusses the case of the backup safety driver behind the wheel of a self-driving Uber test vehicle that struck and killed a woman in Tempe, Arizona, in 2018. The driver, Rafaela Vasquez, pleaded guilty to endangerment, a reduced charge, and was sentenced to three years of supervised probation. The incident involved the death of Elaine Herzberg, who was struck while walking a bicycle across a street at night. The news also highlights the National Transportation Safety Board's findings, which faulted Vasquez for failure to monitor the driving environment due to being visually distracted by her personal cell phone and Uber for inadequate attention to safety and development decisions in the company's autonomous vehicle program. The news provides specific details about the incident, the charges, and the consequences, making it highly relevant to self-driving car crashes.\"\n}\nThe provided news is indeed relevant to self-driving car crashes as it discusses a specific incident involving a self-driving Uber vehicle and provides detailed information about the incident and its consequences.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/backup-driver-2018-uber-self-driving-crash-pleads-guilty-2023-07-28\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":54,"Time":"Fri 1 Jul 2016 01.14 CEST","Title":"Tesla driver dies in first fatal crash while using autopilot mode","Content":"The first known death caused by a self-driving car was disclosed by Tesla Motors on Thursday, a development that is sure to cause consumers to second-guess the trust they put in the booming autonomous vehicle industry.\nThe 7 May accident occurred in Williston, Florida, after the driver, Joshua Brown, 40, of Ohio put his Model S into Tesla’s autopilot mode, which is able to control the car during highway driving.\nWhat happens when Tesla’s AutoPilot goes wrong: owners post swerving videos\nRead more\nAgainst a bright spring sky, the car’s sensors system failed to distinguish a large white 18-wheel truck and trailer crossing the highway, Tesla said. The car attempted to drive full speed under the trailer, “with the bottom of the trailer impacting the windshield of the Model S”, Tesla said in a blogpost.\nA police report in the Levy County Journal said the top of the vehicle “was torn off by the force of the collision”. The truck driver, Frank Baressi, 62, of Tampa, Florida, was uninjured, the Journal reported.\nAmerica’s National Highway Traffic Safety Administration (NHTSA) has opened an inquiry into the accident.\nBrown owned a technology company called Nexu Innovations and was a Tesla enthusiast who posted videos of his car on autopilot on YouTube. One video showed his car avoiding a collision on a highway, racking up 1m views after it was tweeted by Tesla CEO Elon Musk.\nOwner video of Autopilot steering to avoid collision with a truckhttps:\/\/t.co\/FZUAXSjlR7\n— Elon Musk (@elonmusk) April 17, 2016\nIn its 537-word statement on the incident, the electric vehicle company repeatedly went out of its way to shift blame for the accident. The first paragraph notes that this was Tesla’s first known autopilot death in some 130 million miles driven by its customers. “Among all vehicles in the US, there is a fatality every 94 million miles,” the company then notes.\nIt goes on to say that the car’s autonomous software is designed to nudge consumers to keep their hands on the wheels to make sure they’re paying attention. “Autopilot is getting better all the time, but it is not perfect and still requires the driver to remain alert,” the company said.\nAt the end of its blogpost, Tesla stated: “The customer who died in this crash had a loving family and we are beyond saddened by their loss,” the company said. “He was a friend to Tesla and the broader [electric vehicle] community, a person who spent his life focused on innovation and the promise of technology and who believed strongly in Tesla’s mission.”\nOur condolences for the tragic loss https:\/\/t.co\/zI2100zEGL\n— Elon Musk (@elonmusk) June 30, 2016\n“Preliminary reports indicate the vehicle crash occurred when a tractor-trailer made a left turn in front of the Tesla at an intersection on a non-controlled access highway. The driver of the Tesla died due to injuries sustained in the crash,” the NHTSA said.\nThe car continued to travel after passing under the truck’s trailer, veered off the road, and then crashed through two fences and into a power pole, the local police report said. Brown died at the scene.\nThe accident comes at a time when Americans have just started to become more comfortable with letting machines take the wheel. Tesla has generated enormous fanfare with its autopilot mode and inspired consumers – despite the company’s warnings – to see just how much they can do while letting the car drive.\nA popular video posted online appeared to show a man taking a nap as his car navigated busy traffic. One member of a Tesla owner message board recently quipped that he uses his free hands to play air guitar to Billy Idol songs. Another claimed that during a road trip, he noticed Tesla’s autopilot camera struggling to pick out lines during bright sunlight in the morning or at dusk.\nOther carmakers have said they will be releasing autonomous vehicles in the near future. Within a year, General Motors plans to test self-driving taxis with the ride-hailing app Lyft. One startup backed by Google is testing completely autonomous trucks in Nevada that will let drivers rest as they complete cross-country shipping routes.\nMusk recently hinted that the company plans to include its autopilot feature in its upcoming Model 3, which at $35,000 is being marketed to middle-class drivers.\nStatistically, self-driving cars are about to kill someone. What happens next?\nRead more\nThe appeal of the technology is obvious. And Tesla isn’t wrong when it says that statistically, vehicles driving themselves have a much better safety record than ones driven by humans.\nBut the Tesla accident offers the first test of how consumers will react when reminded that they are putting their lives in the hands of computer code when they turn over control of the wheel.\nBrown, for his part, appeared to believe that he was safe when letting go of the wheel. In one of the YouTube videos he posted of his Model S driving itself in slow-moving traffic, he said, “It takes all the stress out of it.”\n“You get to your destination slightly slower but at least now you don’t have to worry about anything. Just let it go.”","Relevant":true,"Overview":"Based on the provided news article, it is indeed about a self-driving car crash. Here's the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is about a fatal crash involving a Tesla vehicle using autopilot mode. The accident occurred in Williston, Florida, on May 7th, where the Tesla Model S driven by Joshua Brown was in autopilot mode. The car's sensors failed to distinguish a large truck crossing the highway, leading to a collision where the top of the vehicle was torn off. Joshua Brown died at the scene. The National Highway Traffic Safety Administration (NHTSA) opened an inquiry into the accident. The article discusses Tesla's response, statistics related to self-driving cars, and the perception of safety among consumers.\"\n}\nThis information confirms that the news is relevant to self-driving car crashes and provides specific details about the incident.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/jun\/30\/tesla-autopilot-death-self-driving-car-elon-musk","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":60,"Time":null,"Title":"Uber driver was streaming Hulu show just before self-driving car crash - police report","Content":"By Heather Somerville, David Shepardson\nSAN FRANCISCO, June 21 (Reuters) - Police in Tempe, Arizona said evidence showed the “safety” driver behind the wheel of a self-driving Uber was distracted and streaming a television show on her phone right up until about the time of a fatal accident in March, deeming the crash that rocked the nascent industry “entirely avoidable.”\nA 318-page report from the Tempe Police Department, released late on Thursday in response to a public records request, said the driver, Rafaela Vasquez, repeatedly looked down and not at the road, glancing up just a half second before the car hit 49-year-old Elaine Herzberg, who was crossing the street at night.\nAccording to the report, Vasquez could face charges of vehicle manslaughter. Police said that, based on testing, the crash was “deemed entirely avoidable” if Vasquez had been paying attention.\nPolice obtained records from Hulu, an online service for streaming television shows and movies, which showed Vasquez’s account was playing the television talent show “The Voice” the night of the crash for about 42 minutes, ending at 9:59 p.m., which “coincides with the approximate time of the collision,” the report says.\nIt is not clear if Vasquez will be charged, and police submitted their findings to county prosecutors, who will make the determination. The Maricopa County Attorney’s Office referred the case to the Yavapai County Attorney’s office because of a conflict and that office could not be reached late Thursday.\nVasquez could not immediately be reached for comment and Reuters could not locate her attorney.\nThe Uber car was in autonomous mode at the time of the crash, but Uber, like other self-driving car developers, requires a back-up driver in the car to intervene when the autonomous system fails or a tricky driving situation occurs.\nVasquez looked up just 0.5 seconds before the crash, after keeping her head down for 5.3 seconds, the Tempe Police report said. Uber’s self-driving Volvo SUV was traveling at just under 44 miles-per-hour.\nUber declined to comment.\nLast month, an Uber spokeswoman said the company was undergoing a “top-to-bottom safety review,” and had brought on a former federal transportation official to help improve the company’s safety culture. The company prohibits the use of any mobile device by safety drivers while the self-driving cars are on a public road, and drivers are told they can be fired for violating this rule.\nPolice said a review of video from inside the car showed Vasquez was looking down during the trip, and her face “appears to react and show a smirk or laugh at various points during the times that she is looking down.” The report found that Vasquez “was distracted and looking down” for close to seven of the nearly 22 minutes prior to the collision.\nTempe Police Detective Michael McCormick asked Hulu for help in the investigation, writing in a May 10 email to the company that “this is a very serious case where the charges of vehicle manslaughter may be charged, so correctly interpreting the information provided to us is crucial.” Hulu turned over the records on May 31.\nAccording to a report last month by the National Transportation Safety Board, which is also investigating the crash, Vasquez told federal investigators she had been monitoring the self-driving interface in the car and that neither her personal nor business phones were in use until after the crash. That report showed Uber had disabled the emergency braking system in the Volvo, and Vasquez began braking less than a second after hitting Herzberg.\nHerzberg, who was homeless, was walking her bicycle across the street, outside of a crosswalk on a four-lane road, the night of March 18 when she was struck by the front right side of the Volvo.\nThe police report faulted Herzberg for “unlawfully crossing the road at a location other than a marked crosswalk.”\nIn addition to the report, police released on Thursday a slew of audio files of 911 calls made by Vasquez, who waited at the scene for police, and bystanders the night of the crash; photographs of Herzberg’s damaged bicycle and the Uber car; and videos from police officers’ body cameras that capture the minutes after the crash, including harrowing screams in the background.\nThe crash dealt Uber a major setback in its efforts to develop self-driving cars, and the company shuttered its autonomous car testing program in Arizona after the incident. It says it plans to begin testing elsewhere this summer, although in some cities it will have to first win over increasingly wary regulators. (Reporting by Heather Somerville and David Shepardson; editing by Neil Fullick)\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Based on the provided news article, it appears to be related to a self-driving car crash. Here's the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Description\": \"The news is about a self-driving car crash involving an Uber vehicle in Tempe, Arizona.\",\n        \"Details\": [\n            \"The police report indicates that the 'safety' driver of the self-driving Uber, Rafaela Vasquez, was distracted and streaming a television show on her phone just before the crash.\",\n            \"The crash resulted in the death of a pedestrian, Elaine Herzberg, who was crossing the street at night.\",\n            \"The report states that the crash was 'deemed entirely avoidable' if the driver had been paying attention.\",\n            \"Evidence shows that Vasquez looked up just 0.5 seconds before the crash, after keeping her head down for 5.3 seconds, while the self-driving Uber was traveling at just under 44 miles-per-hour.\",\n            \"The Uber car was in autonomous mode at the time of the crash, with a back-up driver required in the car to intervene when necessary.\",\n            \"The police report also mentions that Vasquez had been watching the television show 'The Voice' on Hulu just before the collision, and this coincided with the approximate time of the crash.\",\n            \"The report further mentions the possibility of Vasquez facing charges of vehicle manslaughter.\",\n            \"Uber had disabled the emergency braking system in the self-driving Volvo involved in the crash.\",\n            \"The crash led to Uber discontinuing its autonomous car testing program in Arizona.\"\n        ]\n    }\n}\nSo, the news is indeed about a self-driving car crash involving a distracted safety driver streaming a television show, and it provides detailed information about the circumstances of the crash.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/uber-selfdriving-crash-idUSL1N1TO05R","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":56,"Time":"2:55 PM GMT+2•October 9, 2015","Title":"Driverless Car Accident Reports Make Unhappy Reading For Humans","Content":"As technology giants accelerate humanity towards a driverless car future, where we are conditioned to keep our eyeballs on our devices while algorithms take the wheel and navigate the vagaries of the open road, safety questions crash headlong into ethical and philosophical considerations.\nEarlier this year Google blogged about the eleven “minor accidents” its driverless cars had been involved in over six years of testing — laying the blame for all 11 incidents at the hands of the other human drivers. Which sounds great for the technology on the surface. But in reality it underlines the inherent complexities of blending two very different styles of driving — and suggests that robot cars might actually be too cautious and careful.\nCombine that cautious, by-the-book approach with human drivers’ tendency to take risks and cut corners, and well, that, in itself, might indicate driverless cars’ risk aversion is an accident waiting to happen (at least when human drivers are also in the mix).\nGoogle is now trying to train its cars to drive “a bit more humanistically”, as a Google driverless car bod put it this summer, using a word that seems better suited to the lexicon of a robot. Which boils down to getting robots to act a bit more aggressively at the wheel. Truly these are strange days.\nAutonomous vehicles navigating open roads guided only by algorithmic smarts is certainly an impressive technical achievement. But successfully integrating such driverless vehicles into the organic, reactive chaos of (for now) human-motorist dominated roads will be an even more impressive achievement — and we’re not there yet. Frankly the technical progress achieved thus far, by Google and others in this field, may prove the far easier portion of what remains a very complex problem.\nThe last mile of driverless cars is going to require an awful lot of engineering sweat, and regulatory and society accord about acceptable levels of risk (including very sizable risks to a whole swathe of human employment). Self-driving car-makers accepting blanket liability for accidents is one way the companies involved are trying to accelerate the market.\nAs you’d expect, California has been at the forefront of fueling tech developments here. Its DMV is currently developing regulations for what it dryly dubs the “post-testing deployment of autonomous vehicles” — a process that’s, unsurprisingly given the aforementioned complexities, lagging far behind schedule, with no draft rules published yet, despite them being slated to arrive at the start of this year.\nThe DMV has just published all the official accident reports involving autonomous vehicles tested on California’s roads, covering the period from last September to date, on its website. This data mostly pertains to Google’s driverless vehicles, with eight of the nine reports involving Mountain View robot cars. The other one is an autonomous vehicle made by Delphi Automatic.\nThe reports appear to support Google’s claims that human error by the drivers of the non-autonomous cars is, on the surface, causing accidents. However the difficulties caused by the co-mingling of human and robot driving styles is also in ample evidence.\nIn one report, from April this year, a low-speed rear-shunt occurred when a robot car — in the midst of attempting to turn right at an intersection — applied the brakes to avoid an oncoming car, after initially creeping forward. The human-driven car behind it, also trying to turn right and presumably encouraged by the Lexus creeping forward, then “failed to brake sufficiently” and so collided with the rear of the Google Lexus.\nIn another report, from June this year, a Google Lexus traveling in autonomous vehicle mode was also shunted from behind at low speed by a human-driven car. In this instance the robot car was obeying a red stop sign that was still showing for the lane it was occupying. The human driver behind was apparently spurred on to drive into the back of the stationary Lexus because of a green light appearing — albeit for a left-turn lane (whereas the two cars were actually occupying the straight ahead lane).\nA third report, from this July, details how another Google Lexus was crashed into from behind by a human driver — this time after decelerating to a stop in traffic because of stopped traffic ahead of a green lit traffic intersection. Presumably the human driver was paying more attention to the green traffic signal than to the changing road conditions.\nMost of the accidents detailed in the reports occurred at very low speeds. But that might be more a consequence of the type of road-testing driverless cars are currently engaged in, if the focus of current tests for makers is urban navigation and all its messy complexities. While Google’s cars being involved in the majority of the reports is likely down to the company clocking up the most driverless mileage, having been committed to the space for so many years.\nBack in May Google said its 20+ self-driving cars were averaging around 10,000 self-driven miles per week. The fleet had clocked up almost a million miles over a six year testing period at that point, so has likely added a further 200,000 miles or so since then — assuming rates of testing remained the same.\nAll the DMV’s Google-related accident reports pertain to this year, with six accident reports covering the first half of the year, including two in June and two in April.\nThere are currently 10 companies approved by the DMV to test driverless cars on California’s roads: Volkswagen Group of America, Mercedes Benz, Google, Delphi Automotive, Tesla Motors, Bosch, Nissan, Cruise Automation, BMW and Honda.\nApple also apparently recently met with the DMV to discuss the department’s forthcoming driverless vehicle regulations — adding more fuel to rumors Cupertino is also working on developing a (self-driving?) electric car.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving car crashes because it discusses multiple incidents involving autonomous vehicles, particularly Google's driverless cars, and their interactions with human-driven cars. The news provides specific examples of accidents and collisions between autonomous vehicles and human-driven vehicles, highlighting the challenges of blending two different driving styles. It mentions accidents caused by the autonomous car applying brakes to avoid collisions, following traffic signals correctly, and navigating traffic. These incidents demonstrate the complexities and potential risks associated with self-driving cars operating alongside human drivers on the road.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2015\/10\/09\/dont-blame-the-robot-drivers\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":64,"Time":"4 October 2018","Title":"Honda to invest $2.8bn in GM's self-driving car unit","Content":"Honda to invest $2.8bn in GM's self-driving car unit\nPublished\n4 October 2018\nShare\nIMAGE SOURCE,\nREUTERS\nImage caption,\nGM has done extensive testing of its Cruise robot cars in cities and on busy roads\nHonda is to invest $2.75bn (£2.1bn) and take a stake in General Motors' self-driving unit, GM Cruise, as firms continue to team up in the race to develop autonomous vehicles.\nThe Japanese carmaker said it wanted to develop a self-driving car that could be mass produced.\nIt comes as Toyota and Softbank created a new venture to develop transport services using autonomous vehicles.\nA series of accidents have left some consumers wary of driverless cars.\n'Mobility services'\nUnder the tie-up with GM, Honda will contribute approximately $2bn over 12 years to self-driving vehicle initiatives, which together with a $750m equity investment in Cruise, brings its total commitment to the project to $2.75bn, the two firms said in a statement.\nAnalysts have said that GM is among the leaders in the development of self-driving vehicles.\nEarlier this year, Japan's Softbank invested $2.25bn in GM Cruise. Honda's investment in GM Cruise, together with Softbank's recent investment, values the firm at $14.6bn.\nAmerica's self-driving car race speeds up\nWill 5G be necessary for self-driving cars?\nApple self-driving car in minor crash\nSoftbank has also announced a partnership with Toyota to provide \"new mobility services\", which includes the development of autonomous services.\nThe Japanese pair have created a joint venture partner called MONET - short for mobility services - that will start with 2bn yen in capital.\nIt plans to use Toyota's electric cars to provide autonomous services in the coming years.\n\"Possibilities include demand-focused just-in-time mobility services, such as meal delivery vehicles where food is prepared on the move, hospital shuttles where onboard medical examinations can be performed, mobile offices,\" the companies said.\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nJapanese carmaker Toyota developing technologies for self-driving vehicles\nThe rush to develop self-driving technology comes amid some consumer concern over safety.\nUber briefly suspended self-driving cars tests in March after a fatal accident in the US, while a self-driving car owned by Apple was involved in an accident this month.\nWho is to blame for 'self-driving car' deaths?\nSan Francisco-based GM Cruise, which developed Chevy's first driverless cars, argues the technology it is working on allows self-driving cars to see more than a human driver would.\nIn each car, 10 cameras are installed that take pictures at a frequency of 10 shots per second.\n\"We see more of what is going on around the car at any given time than a driver can,\" GM Cruise said.\nMore on this story\nWill 5G be necessary for self-driving cars?\nPublished\n27 September 2018\nApple self-driving car in minor crash\nPublished\n1 September 2018\nAmerica's self-driving car race speeds up\nPublished\n31 May 2018\nTwo self-drive car crashes reported\nPublished\n24 January 2018","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about Honda's $2.8 billion investment in GM's self-driving car unit, GM Cruise. Honda's investment is aimed at developing a self-driving car that can be mass-produced. This investment is part of the ongoing race to develop autonomous vehicles, with GM Cruise being a leader in self-driving technology. The news also mentions Softbank's investment in GM Cruise and the partnership between Toyota and Softbank to develop autonomous services. Furthermore, the article discusses the technology used by GM Cruise, which includes installing 10 cameras in each car to provide better visibility than a human driver. All of this information is directly related to self-driving cars and their development.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/business-45728169","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":57,"Time":"Wed 21 Mar 2018 01.25 CET","Title":"'Uber should be shut down': friends of self-driving car crash victim seek justice","Content":"Friends of the first known pedestrian to be killed by a self-driving car have called for Uber to be held accountable as questions mount about how the autonomous technology failed to stop the vehicle from hitting a human in its path.\nTwo days after an Uber SUV fatally struck the 49-year-old Elaine Herzberg in Tempe, Arizona, while traveling in autonomous mode, friends of the victim have argued that the ride-share company should face consequences and criticized government officials for encouraging car companies to test the vehicles on the state’s public roads.\n“This shouldn’t have ever happened,” said Carole Kimmerle, a Mesa resident who said she had been friends with Herzberg for more than 10 years and had previously lived with her. “I think this should be a negligent homicide … and the government should also be held accountable.”\nElaine Herzberg had struggled with homelessness, but was turning her life around, friends say. Photograph: Courtesy of Carole Kimmerle\nHerzberg’s loved ones said they were still in shock on Tuesday after police announced that the Uber car, an SUV Volvo, was driving roughly 40 miles per hour on its own and did not appear to slow down when it collided with the victim, who was walking her bicycle in front of the car at 10pm on Sunday. There was a human operator in the front seat, but police said the car was in autonomous mode, which meant the radar technology may not have detected the pedestrian or the vehicle did not stop for another reason.\nTempe police said Herzberg was not in a crosswalk when she was hit, though some have argued that the car still should have stopped. Uber said it was temporarily pausing its self-driving operations in Phoenix and other cities, but the company has not commented on the cause of the crash.\nAs federal investigators have begun their inquiry, local police officials have appeared to cast blame on the victim, saying Uber may not have been at fault, sparking further backlash from the woman’s friends.\n“Uber should be shut down for it,” one friend, Deniel Klapthor, told the Guardian. “There has to be a bigger punishment than not allowing them to drive it on the street.”\nKimmerle added, “She was not in anyway unsafe. She rode a bike everywhere. She was very cautious of the laws.”\nHerzberg had struggled with homelessness, according to her friends, who said she had recently turned her life around and was in the process of starting a new job.\nIf her family were to pursue a civil case, attorneys could potentially make a range of negligence claims, said Bryant Walker Smith, an assistant professor at the University of South Carolina and a legal expert on autonomous cars.\nDepending on what might have gone wrong, the victim’s family could argue that a number of players were liable, including the car maker, the operator behind the wheel, the manufacturers of various specific technologies, and Uber itself, he said.\n“The attorney would want to say to a hypothetical jury, ‘These are really scary systems. They demand the utmost care and responsibility.’ And the lawyer would suggest that’s not the case here,” said Smith.\nRyan Calo, a University of Washington law professor and self-driving expert, predicted the company would try to resolve any case quickly and privately: “Uber will settle this immediately for an undisclosed amount of money.”\nWho's driving? Autonomous cars may be entering the most dangerous phase\nRead more\nThe first reported fatal self-driving car crash happened in 2016 when a Tesla on “autopilot” did not detect a white truck in its path. There have since been a series of high-profile incidents involving Teslas, Ubers and other companies, and some have raised concerns that even if self-driving technology is a safer mode of travel, the industry is entering a particularly dangerous phase of development when the vehicles aren’t yet fully autonomous and require humans to intervene.\nArizona has lured self-driving car operators to the state by arguing it has fewer regulations than other jurisdictions – a fact that upset Herzberg’s friends, who said the government should have done more to prevent these kinds of crashes.\n“In Tempe, they’re everywhere,” said Jerry Higgins, another friend of Herzberg, who said he keeps his eye on the self-driving cars and said he was struggling to understand how the collision occurred: “Don’t they have a driver in the car that’s supposed to keep stuff like this from happening? … I don’t see how they didn’t work something like this into the programs.”\nHerzberg loved to read and write and was always generous and offering to help others even when she was dealing with her own struggles, said Kimmerle.\n“She was very loving,” she said. “The world lost a good person.”\nContact the author: sam.levin@theguardian.com","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving car crashes because it discusses the first known pedestrian fatality involving a self-driving car, specifically an Uber SUV in autonomous mode. The article provides details about the incident, including the victim's name (Elaine Herzberg), the circumstances of the crash, and the fact that there was a human operator in the front seat but the car was in autonomous mode. It also mentions the concerns and criticisms from friends of the victim, the temporary pause of Uber's self-driving operations, and the ongoing investigations. Therefore, this news is directly related to self-driving car crashes.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/mar\/20\/uber-self-driving-car-crash-death-arizona-elaine-herzberg","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":66,"Time":"February 17, 2023","Title":"Tesla recalls 362,000 U.S. vehicles over Full Self-Driving software","Content":"Companies\nTesla Inc\nFollow\nWASHINGTON, Feb 16 (Reuters) - Tesla Inc (TSLA.O) will recall more than 362,000 U.S. vehicles to update its Full Self-Driving (FSD) Beta software after U.S. regulators said on Thursday the driver assistance system did not adequately adhere to traffic safety laws and could cause crashes.\nThe National Highway Traffic Safety Administration (NHTSA) said the Tesla software allows a vehicle to \"exceed speed limits or travel through intersections in an unlawful or unpredictable manner increases the risk of a crash.\"\nAdvertisement · Scroll to continue\nTesla said it disagreed with NHTSA's analysis but ultimately acquiesced to the safety agency's January request. The electric vehicle manufacturer has previously clashed with NHTSA, which has a number of pending Tesla probes, over other safety issues and recall demands.\nTesla, whose shares closed down 5.7% at $202.04, will release an over-the-air (OTA) software update free of charge. The EV maker said it is not aware of any injuries or deaths that may be related to the recall issue. The automaker said it had 18 warranty claims.\nAdvertisement · Scroll to continue\nThe recall covers 2016-2023 Model S and Model X vehicles, 2017-2023 Model 3, and 2020-2023 Model Y vehicles equipped with FSD Beta software or pending installation.\nU.S. senators Ed Markey and Richard Blumenthal, both Democrats, said the recall was \"long overdue,\" adding, \"Tesla must finally stop overstating the real capabilities of its vehicles.\"\nThis is a fresh setback for Tesla's driver assistance system. Chief Executive Elon Musk has repeatedly missed his own targets to achieve self-driving capability, which he has touted as a potential cash cow.\nAdvertisement · Scroll to continue\nThe move is a rare intervention by federal regulators in a real-world testing program that the company sees as crucial to the development of cars that can drive themselves.\nThe recall comes less than two weeks before the company's March 1 investor day, during which Musk is expected to promote the EV maker's artificial intelligence capability and plans to expand its vehicle lineup.\nWhile Tesla's Autopilot feature assists with steering, accelerating and braking for other vehicles and pedestrians within its lane, the company says FSD is a more advanced system “designed to provide more active guidance and assisted driving” under active supervision of the driver.\nAdvertisement · Scroll to continue\nTesla reported having $2.9 billion in deferred revenue at the end of 2022 related to \"access to our FSD features, internet connectivity, free Supercharging programs and over-the-air software updates primarily on automotive sales.\"\nTesla could not be reached for comment, but Musk tweeted on Thursday that the word \"recall\" for an over-the-air software update is \"anachronistic and just flat wrong!\"\n[1\/4]A Tesla Model S charges at a Tesla supercharger station in Cabazon, California, U.S. May 18, 2016. REUTERS\/Sam Mircovich Acquire Licensing Rights\nTesla released FSD Beta to nearly all of its 400,000 FSD customers in the United States and Canada in the fourth quarter, when it recognized FSD revenue of $324 million. It said it expects to recognize nearly $1 billion of deferred revenue that remains over time as software updates are delivered.\nONGOING PROBES\nMusk has positioned FSD technology as one of several artificial intelligence initiatives at Tesla.\nLast May, in an interview with members of a Tesla owners club, Musk called full self driving \"essential\" for the company. \"It’s really the difference between Tesla being worth a lot of money or worth basically zero.\"\nNHTSA has an ongoing investigation it opened in 2021 into 830,000 Tesla vehicles with driver assistance system Autopilot over a string of crashes with parked emergency vehicles. NHTSA is reviewing whether Tesla vehicles adequately ensure drivers are paying attention. NHTSA said on Thursday despite the FSD recall its \"investigation into Tesla’s Autopilot and associated vehicle systems remains open and active.\"\nTesla said in \"certain rare circumstances ... the feature could potentially infringe upon local traffic laws or customs while executing certain driving maneuvers.\"\nPossible situations where the problem could occur include traveling or turning through certain intersections during a yellow traffic light and making a lane change out of certain turn-only lanes to continue traveling straight, NHTSA said.\nNHTSA said \"the system may respond insufficiently to changes in posted speed limits or not adequately account for the driver's adjustment of the vehicle's speed to exceed posted speed limits.\"\nLast year, Tesla recalled nearly 54,000 U.S. vehicles with FSD Beta software that may allow some models to conduct \"rolling stops\" and not come to a complete stop at some intersections, posing a safety risk, NHTSA said.\nTesla and NHTSA say FSD's advanced driving features do not make the cars autonomous and require drivers to pay attention.\nIn December, NHTSA opened two new special investigations into crashes involving Tesla vehicles, including an eight-vehicle crash in San Francisco on the Bay Bridge in which a driver reported the FSD feature had malfunctioned.\nSince 2016, NHTSA has opened more than three dozen investigations involving Tesla crashes where advanced driver assistance systems were suspected of use and 19 deaths were reported.\nReporting by David Shepardson in Washington Additional reporting by Joseph White in Detroit and Hyun Joo Jin in San Francisco Editing by Ben Klayman, Peter Henderson and Matthew Lewis\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Based on the provided news article, it is indeed relevant to self-driving car crashes. Here is the JSON format response as requested:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"News_title\": \"Tesla recalls 362,000 U.S. vehicles over Full Self-Driving software\",\n        \"Reason_for_relevance\": \"The news is relevant to self-driving car crashes because it discusses Tesla's recall of over 362,000 U.S. vehicles due to issues with its Full Self-Driving (FSD) Beta software. The National Highway Traffic Safety Administration (NHTSA) stated that the FSD software could cause crashes as it allows the vehicles to exceed speed limits and travel through intersections in an unlawful or unpredictable manner, increasing the risk of a crash. Tesla has also had previous safety issues and recall demands related to its autonomous driving features.\",\n        \"Affected_vehicle_models\": \"The recall covers 2016-2023 Model S and Model X vehicles, 2017-2023 Model 3, and 2020-2023 Model Y vehicles equipped with FSD Beta software or pending installation.\",\n        \"Comments_from_senators\": \"U.S. senators Ed Markey and Richard Blumenthal described the recall as 'long overdue' and emphasized that Tesla must stop overstating the real capabilities of its vehicles.\",\n        \"Tesla's response\": \"Tesla will release an over-the-air (OTA) software update free of charge to address the issue and disagrees with NHTSA's analysis but ultimately acquiesced to the safety agency's request.\"\n    }\n}\nThe provided news article discusses Tesla's recall of a significant number of vehicles due to issues with its Full Self-Driving (FSD) Beta software, which is directly related to self-driving car safety concerns. It includes details about why the news is relevant to self-driving car crashes, the affected vehicle models, comments from U.S. senators, and Tesla's response to the situation.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/tesla-recalls-362000-us-vehicles-over-full-self-driving-software-2023-02-16\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":61,"Time":"7:41 AM GMT+2•September 24, 2016","Title":"A Google self-driving car was involved in crash in Mt. View today","Content":"cameraImage Credits: Steve Jurvetson, retouched image by Mariordo\n(opens in a new window)\nunder a CCA2 - Generic\n(opens in a new window)\nlicense.\nA Google self-driving Lexus RX 450h was involved in a crash with a van in Mountain View, Calif. on Friday afternoon, according to local police. Another driver ran a red light and crashed into the car. Thankfully, nobody was injured in the accident.\nGoogle issued the following statement with details about the crash:\n“A Google vehicle was traveling northbound on Phyllis Ave. in Mountain View when a car heading westbound on El Camino Real ran a red light and collided with the right side of our vehicle. Our light was green for at least six seconds before our car entered the intersection. Thousands of crashes happen everyday on U.S. roads, and red-light running is the leading cause of urban crashes in the U.S. Human error plays a role in 94% of these crashes, which is why we’re developing fully self-driving technology to make our roads safer.”\nTwo local TV stations, KRON and KPIX, reported that the Google autonomous vehicle had been “in control,” or in its self-driving mode at the time of the crash.\nHowever, the self-driving car was reportedly manned by a Google employee who took over its operation, and applied the brakes when the other car’s driver began crossing an intersection, apparently running a red light and colliding with Google’s vehicle.\nThe crash comes just after the U.S. Department of Transportation made some major autonomous vehicle policy announcements earlier this week, as TechCrunch then reported.\nAmong other things, the DOT called on self-driving auto makers and related technology firms to prioritize safety, and share data with the government and each other within reason.\nGoogle issues reports voluntarily every month reflecting data on the number of miles their autonomous vehicles have traveled, how many cars they have on the road, and details about any accidents in which they were involved.\nGenerally, DOT officials say they want to ensure the U.S. can be a leader rather than a laggard in autonomous vehicles, while protecting the traveling public.\n Updates: This post has been updated with Google’s official statement about the accident. Earlier, the post’s headline was edited to emphasize that a human driver ran a red light and collided with Google’s self-driving vehicle. ","Relevant":true,"Overview":"Based on the provided news article, here is the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Description\": \"A Google self-driving car was involved in a crash in Mountain View, California.\",\n        \"Incident Details\": \"Another driver ran a red light and collided with the self-driving car. The self-driving car was reportedly in its self-driving mode at the time of the crash, but a Google employee took over and applied the brakes when the other car's driver ran a red light. Fortunately, nobody was injured in the accident.\",\n        \"Context\": \"The news article discusses an incident where a Google self-driving car was involved in a crash due to another driver running a red light. This incident is directly related to self-driving car technology and the safety concerns associated with it.\"\n    }\n}\nIn this case, the news is relevant to self-driving car crashes because it describes an accident involving a Google self-driving car and provides specific information about the incident, including the cause of the crash and the fact that the self-driving car was in its self-driving mode at the time.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2016\/09\/23\/a-google-self-driving-car-crashed-in-mt-view-today\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":62,"Time":"Mon 14 Mar 2016 12.14 CET","Title":"US government: 'it's not a surprise Google's car crashed'","Content":"The fact that a Google self-driving car had a crash is “not a surprise” according to the US transport secretary Anthony Foxx.\nTalking to the BBC at South by Southwest in Austin Texas, Foxx discussed the future of road transportation and the recent crash caused by Google’s autonomous SUV, which pulled out and struck a bus.\nFoxx said: “It’s not a surprise that at some point there would be a crash of any technology that’s on the road.\n“But I would challenge one to look at the number of crashes that occurred on the same day that were the result of human behaviour.”\nSeven US cities, including Austin, Columbus, Denver, Kansas City, Pittsburgh, Portland and San Francisco, are in the running for a $40m government grant to fund smart city technologies, including driverless cars.\nFoxx said that he expected the autonomous technology not to be perfect, but that it could prevent up to 80% of crashes in the US. For the foreseeable future there is likely to be a mix of human driven and driverless cars, but the future for the subsequent generations might be different.\nBut Foxx said: “My kids don’t know what a typewriter is. They don’t even know what it is to roll up a window in a car, because they’re all motorised these days.”\nGoogle’s driverless Lexus SUV – without a scratch. Photograph: Elijah Nouvelage\/Reuters\nWhat form the autonomous future holds is still up for debate, with car manufacturers designing and building cars along the traditional model of ownership and others including Google, pitching an idea closer to a shared car service – taxis without drivers.\nMany cars on the road today are capable of at least some form of autonomous driving, from adaptive cruise control and emergency automated braking to Tesla Motor’s piloted driving of its Model S electric car, which can drive, change lanes and take turns without the driver needing to actively participate.\nFoxx said: “Driverless technology presents a lot of potential for disruption on a number of fronts. It’s unclear to me now exactly how that future unfolds.”\nGeorge Osborne to back driverless car trials on UK motorways\nYour next car will be hacked. Will autonomous vehicles be worth it?","Relevant":true,"Overview":"Based on the provided news article, it does mention a crash involving a Google self-driving car. Therefore, it is relevant to self-driving car crashes. Here is the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news article discusses a crash involving a Google self-driving car. The US transport secretary, Anthony Foxx, commented on the crash, stating that it's not surprising that any technology on the road would eventually have a crash. The crash involved a Google autonomous SUV that pulled out and struck a bus. Foxx also mentioned that while autonomous technology may not be perfect, it has the potential to prevent up to 80% of crashes in the US. This information clearly indicates the relevance of the news to self-driving car crashes.\"\n}\nThe news is relevant to self-driving car crashes because it specifically mentions a crash involving a Google self-driving car and provides information about the incident and related discussions by the US transport secretary.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/mar\/14\/us-government-anthony-foxx-not-surprise-google-driverless-car-crashed","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":63,"Time":"Tue 1 Mar 2016 08.28 CET","Title":"Google self-driving car collides with bus in California, accident report says","Content":"One of Google’s self-driving cars has collided with a public bus in Mountain View, an accident report has revealed, in what appears to be the first example of one of the company’s prototype cars causing an accident.\nThe incident happened on 14 February and was reported to California’s department of motor vehicles in an accident report that the agency posted on 29 February.\nThe car was rolling at 2mph (3kph) and the bus at 15mph. No one was injured.\nThe report does not address fault. However, Google wrote that its car was trying to get around some sandbags on a street when its left front struck the right side of the bus.\nThe car’s test driver, who under state law must be in the front seat to grab the wheel when needed, thought the bus would yield and did not have control when the collision happened, according to Google’s report.\nIf it is determined the Google vehicle was at fault, it would be the first time one of its SUVs caused an accident while in autonomous mode.\nJessica Gonzalez, a spokeswoman from the DMV, said the agency hoped to speak with Google on Monday about what went wrong.\nIn a detailed statement from its monthly report on its self-driving car project, Google said that the incident happened on El Camino Real, a busy six-lane boulevard with hundreds on intersections. The car was following a recent change to its programming, following “the spirit of the road” as well as the traffic code by hugging the far side of the right-turn lane to allow other cars to pass on the left.\n“It then detected sandbags near a storm drain blocking its path, so it needed to come to a stop. After waiting for some other vehicles to pass, our vehicle, still in autonomous mode, began angling back toward the center of the lane at around 2mph – and made contact with the side of a passing bus traveling at 15mph. Our car had detected the approaching bus, but predicted that it would yield to us because we were ahead of it,” Google said.\n“Our test driver, who had been watching the bus in the mirror, also expected the bus to slow or stop. And we can imagine the bus driver assumed we were going to stay put. Unfortunately, all these assumptions led us to the same spot in the lane at the same time. This type of misunderstanding happens between human drivers on the road every day.”\nThis type of misunderstanding happens between human drivers on the road every day.\nGoogle's monthly self-driving car report\nGoogle says it has refined its software following the incident, acknowledging that buses and other large vehicles are less likely to yield. “In this case we clearly bear some responsibility because if our car hadn’t moved there would’t have been a collision.”\n“We hope to handle situations like this more gracefully in the future.”\nHilary Rowen, a partner at the insurance regulation practice Sedgwick LLP and an expert in the issue of self-driving cars and legal responsibility, said the case is a good example of a conundrum that will soon be common.\n“Here, the software didn’t avoid the accident, but the human could have taken over,” she said. “Who’s at fault – the driver, the bus driver, or the software?\nRowen said in real world situations, both the driver and injured party will actually be incentivized to blame the software which, if found to be guilty, will leave the driver’s record clear and likely have a higher payout for the injured party.\n“Everybody’s going to be blaming the software all the time,” Rowen said. “All the time.” Rowen still thinks autonomous car insurance will be cheaper than human-driven car insurance because humans aren’t very good drivers.\n“At a very visceral level, people will accept a higher chance of being maimed or killed by a human being than they will by being maimed or killed by software,” she said. “The self-driving car will likely be able to make better risk calculations.”\nGoogle has been testing two dozen Lexus SUVs outfitted with sensors and cameras near the tech firm’s Silicon Valley headquarters.\nGoogle cars have been involved in more than a dozen collisions. In most cases, Google’s cars were rear-ended. No one has been seriously injured.\nGoogle reports self-driving car mistakes: 272 failures and 13 near misses\nRead more","Relevant":true,"Overview":"Based on the provided news article, it appears to be about a self-driving car accident involving a Google self-driving car and a public bus in California. Here's the JSON format response:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Summary\": \"A Google self-driving car collided with a public bus in California.\",\n    \"Date\": \"The incident happened on 14 February and was reported on 29 February.\",\n    \"Speed\": \"The self-driving car was rolling at 2mph, and the bus was at 15mph.\",\n    \"Injuries\": \"No one was injured in the collision.\",\n    \"Description\": \"Google's car was attempting to navigate around sandbags when it struck the right side of the bus. The car was in autonomous mode at the time, and its test driver did not have control during the collision.\",\n    \"Fault\": \"The report does not address fault, but Google acknowledged some responsibility and mentioned refining its software to handle such situations better in the future.\"\n  }\n}\nSo, this news is relevant to self-driving car crashes because it describes an incident involving a self-driving car (Google's prototype) colliding with a bus. It provides specific information about the incident, including the date, speeds of the vehicles, lack of injuries, and details about the collision itself.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/feb\/29\/google-self-driving-car-accident-california","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":70,"Time":"Updated 11:09 AM EDT, Fri September 18, 2020","Title":"Uber self-driving car operator charged in pedestrian death","Content":"Washington, DC\nCNN\n — \nThe Uber test driver who was responsible for monitoring one of the company’s self-driving cars that hit and killed a pedestrian in 2018 was charged with negligent homicide this week.\nRafaela Vasquez was watching television on her smartphone when the Uber self-driving vehicle struck Elaine Herzberg, who was crossing a road in Tempe, Arizona, according to a National Transportation Safety Board investigation. It was the first fatality involving a fully autonomous vehicle.\nVasquez pleaded not guilty to the charge, according to the Maricopa County Attorney’s Office, and the court ordered her released with ankle monitoring. Vasquez’s attorney did not immediately respond to a request for comment.\nThe NTSB investigation released in November 2019 concluded that the crash was caused because Vasquez was distracted by her phone, and that Uber’s inadequate safety culture contributed to the crash. Furthermore, the NTSB found that Uber’s system could not correctly classify and predict the path of a pedestrian crossing midblock, which occurred in the incident involving Vasquez.\nUber had also deactivated a forward collision warning and automatic emergency braking system on the Volvo, according to the NTSB. Uber made revisions to its program following guidance from the NTSB.\n“Over the last 20 months, we have provided the NTSB with complete access to information about our technology and the developments we have made since the crash,” Uber’s Nat Beuse, who leads self-driving safety efforts, said in a statement at the time. “While we are proud of our progress, we will never lose sight of what brought us here or our responsibility to continue raising the bar on safety.”\nA grand jury in Maricopa County, Arizona charged Vasquez with the felony long after Uber had been cleared of any potential charges. In March 2019, the Yavapai County Attorney’s Office concluded there was no basis for criminal liability against Uber. It declined to detail how its decision was made, or answer any questions on the case. Uber also declined to comment.\nLiability questions surrounding self-driving cars continue to be up in the air, but experts say this case is unlikely to set a significant precedent for the future of self-driving laws, given the circumstances.\nVasquez was an Uber employee who was supposed to be monitoring the vehicle’s performance, and taking control if necessary for safety’s sake. If self-driving vehicles go mainstream, there won’t be test drivers behind the wheel, and experts expect companies to be liable for crashes. Some companies are developing vehicles without human controls, making it impossible for passengers to control a vehicle.\nEd Walters, who teaches a class on autonomous vehicles at Georgetown Law, said it’s unclear who would be charged if Vasquez hadn’t allegedly been watching TV on her phone.\n“The fact she was watching TV makes her an easy and maybe convenient person to accept responsibility,” Walters said. “Remove that fact and it could easily be Uber.”\nBryant Walker Smith, a professor at the University of South Carolina School of Law who studies autonomous vehicles, expressed concern about the signal it sends to prosecute only the driver.\n“It will come across to many that that’s where all the fault is, and the companies behind these don’t have responsibility,” Smith said. “Uber’s testing program created circumstances where this was foreseeable and almost inevitable. Someone spends their whole day seeing a vehicle drive well and starts assuming that it will drive well.”\nOther experts in autonomous vehicle law felt differently about the charge.\n“Uber trains and requires its safety drivers to remain alert.  If one of them does not do so, it is difficult to say Uber committed a crime,” said Todd Benoff, a lawyer at Alston & Bird. “If the driver had been drinking that would also be a crime, but not one that Uber should be charged for.”\nUber did face the risk of a civil lawsuit, and settled with Herzberg’s family fewer than two weeks after the crash. Details of the agreement weren’t released.","Relevant":true,"Overview":"Here's the JSON format response based on the provided news article:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Summary\": \"The news is relevant to self-driving car crashes.\",\n        \"Details\": [\n            \"The news reports an incident involving an Uber self-driving car that hit and killed a pedestrian in 2018.\",\n            \"The Uber test driver, Rafaela Vasquez, responsible for monitoring the self-driving car, was charged with negligent homicide.\",\n            \"The incident occurred when Vasquez was watching television on her smartphone, indicating potential negligence in monitoring the self-driving car.\",\n            \"The National Transportation Safety Board (NTSB) investigation concluded that Vasquez's distraction by her phone contributed to the crash.\",\n            \"The NTSB also found that Uber's inadequate safety culture and technology played a role in the accident.\",\n            \"Uber had deactivated some safety systems on the self-driving vehicle, including forward collision warning and automatic emergency braking.\",\n            \"While Uber had been cleared of potential charges, Vasquez faced charges related to her role in the incident.\",\n            \"Experts have differing opinions on who should be held responsible, with some suggesting it could be Uber's responsibility due to the testing program's circumstances.\",\n            \"The case highlights ongoing liability questions surrounding self-driving cars and their impact on future self-driving laws.\",\n            \"Uber settled with the family of the pedestrian shortly after the crash in a civil lawsuit.\"\n        ]\n    }\n}\nThis news article is relevant to self-driving car crashes because it discusses an incident where an Uber self-driving car struck and killed a pedestrian. It provides detailed information about the incident, including the role of the test driver's distraction, Uber's technology, and the resulting legal consequences and discussions surrounding liability.","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2020\/09\/18\/cars\/uber-vasquez-charged\/index.html","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":71,"Time":"Wed 24 Oct 2018 19.00 CEST","Title":"Who should AI kill in a driverless car crash? It depends who you ask","Content":"Moral responses to unavoidable damage vary greatly around the world in a way that poses a big challenge for companies planning to build driverless cars, according to new research.\nThe researchers, from Massachusetts Institute of Technology and other institutions, presented variations of the classic “trolley problem” thought experiment almost 40m times to millions of volunteers from all around the world.\nIn the traditional thought experiment, participants are asked to consider whether they would reroute a runaway trolley car which is about to hit and kill five people, directing it on to a siding where it would kill only one person. In the new quiz, dubbed “Moral Machine”, the researchers instead asked volunteers to consider what a self-driving car should do in examples from more than 26 million variations of the same question.\nShould a car with three occupants, an adult man and woman and a child, swerve into a wall, killing them all, in order to avoid hitting three elderly people, two men and a woman? Should an unoccupied car swerve and kill an unemployed adult man, a child and a cat in order to save an adult man and woman and a child? Does the answer change if the pedestrian light is red? What if one of the people is unfit, or pregnant?\nResponses to those questions varied greatly around the world. In the global south, for instance, there was a strong preference to spare young people at the expense of old – a preference that was much weaker in the far east and the Islamic world. The same was true for the preference for sparing higher-status victims – those with jobs over those who are unemployed.\nThe TV comedy The Good Place tackles the trolley problem.\nWhen compared with an adult man or woman, the life of a criminal was especially poorly valued: respondents were more likely to spare the life of a dog (but not a cat).\nThe researchers, whose work is published in the journal Nature, also note “some striking peculiarities, such as the strong preference among those in the global south for sparing women and fit characters.\n“Only the (weak) preference for sparing pedestrians over passengers and the (moderate) preference for sparing the lawful over the unlawful appear to be shared to the same extent in all clusters.”\nThe data comes with caveats, of course. Unlike traditional polling, the volunteers were entirely self-selected, reached in large numbers thanks to the viral nature of the “Moral Machine” quiz, which was covered by technology news sites like The Next Web and Business Insider. That means that, for instance, the data is likely to skew towards the wealthy in nations with weak internet penetration. More generally, they write, “most users on Moral Machine are male, went through college, and are in their 20s or 30s”.\nNonetheless, they argue that the findings should prompt policymakers and auto engineers to consider embedding some moral intuitions into self-driving cars. “Before we allow our cars to make ethical decisions, we need to have a global conversation to express our preferences to the companies that will design moral algorithms, and to the policymakers that will regulate them,” they write.\nAmong some autonomous vehicle engineers, however, that view is disputed. Speaking to the Guardian just after the Moral Machine quiz was first released, Andrew Chatham, a principal engineer on Google’s self-driving car project, said the problem has little bearing on actual design.\n“It takes some of the intellectual intrigue out of the problem, but the answer is almost always ‘slam on the brakes’,” he said. “You’re much more confident about things directly in front of you, just because of how the system works, but also your control is much more precise by slamming on the brakes than trying to swerve into anything. So it would need to be a pretty extreme situation before that becomes anything other than the correct answer.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving car considerations and ethical dilemmas associated with autonomous vehicles. It discusses the moral challenges posed by self-driving cars and the variations of the classic 'trolley problem' thought experiment presented to millions of volunteers worldwide. The article explores the global variations in responses to scenarios involving self-driving cars, such as whether a car should swerve to avoid hitting certain individuals or objects, even at the expense of other lives. It also mentions the need for a global conversation about embedding moral intuitions into self-driving cars and highlights differing viewpoints among engineers, including Google's self-driving car project, on the practical application of ethical considerations in self-driving car design.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/oct\/24\/who-should-ai-kill-in-a-driverless-car-crash-it-depends-who-you-ask","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":74,"Time":"Thu 22 Mar 2018 02.02 CET","Title":"Video released of Uber self-driving crash that killed woman in Arizona","Content":"Video of the first self-driving car crash that killed a pedestrian showed how the autonomous Uber failed to slow down as it fatally hit a 49-year-old woman walking her bike across the street.\nThe newly released footage of the collision that killed Elaine Herzberg in Tempe, Arizona, on Sunday night has raised fresh questions about why the self-driving car did not stop when a human entered its path and has sparked scrutiny of regulations in the state, which has encouraged testing of the autonomous technology.\n“It’s just awful,” Tina Marie Herzberg White, a stepdaughter of the victim, told the Guardian on Wednesday. “There should be a criminal case.”\nPolice have released two videos of the case – one outside and one showing the interior of the Volvo SUV. The four-second exterior video showed the car driving down a somewhat dark and largely empty street as it collided into the woman walking directly in its path.\nTempe Police Vehicular Crimes Unit is actively investigating\nthe details of this incident that occurred on March 18th. We will provide updated information regarding the investigation once it is available. pic.twitter.com\/2dVP72TziQ\n— Tempe Police (@TempePolice) March 21, 2018\nThe 14-second video inside the car showed the operator, identified by police as Rafaela Vasquez, 44, appearing to look at something inside the vehicle and not at the road at the time of the collision. She alternated between looking down and looking forward and appeared shocked at the last minute just as the car failed to stop.\nUber typically refers to its operators in the front of the autonomous cars as “safety drivers” who can intervene and take control of the car, but generally let the vehicles run in autonomous mode. The cars rely on radar sensors that are meant to detect pedestrians, cyclists, cars and other obstacles.\nIt is unclear what went wrong in this case. It was around 10pm at the time of the crash, and the video showed the woman appearing in view a second or so before the collision. She was not walking in a crosswalk when the car hit her, though Herzberg’s loved ones and some autonomous driving experts have argued that the technology still should have detected her.\nLocal prosecutors will decide whether criminal charges are warranted. Some have argued that under new rules issued by Arizona’s governor, a strong proponent of the technology, a company like Uber could possibly be criminally liable if an autonomous car negligently killed someone. But the local police chief, Sylvia Moir, suggested in an interview that she believed Uber was not at fault.\n“It’s absolutely ridiculous,” said White, who said she was shocked to learn that an autonomous car had killed Herzberg, who she had known for more than 20 years. “I can’t believe that the [driver] that was in the car did not see her.”\nCompanies manufacturing the technology have argued that self-driving cars are safer than humans, but skeptics have pointed out that the industry is entering a dangerous phase while the cars are not yet fully autonomous, but human operators are not fully engaged.\nWhite said she did not know whether Herzberg’s immediate family might pursue a civil case: “Ain’t no amount of money in the world going to bring her back.”\nUber has temporarily suspended its self-driving program, but has not commented on what caused the crash. The company said in a statement on Wednesday night: “The video is disturbing and heartbreaking to watch, and our thoughts continue to be with Elaine’s loved ones. Our cars remain grounded, and we’re assisting local, state and federal authorities in any way we can.”\nWhite said she did not believe the cars should be allowed to return: “This might be the first, but I’ll tell you what, there will be more.”","Relevant":true,"Overview":"Based on the provided news article, it appears to be about a self-driving car crash involving an Uber vehicle that resulted in the death of a pedestrian. Here's the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is about a self-driving car crash involving an Uber vehicle. The incident occurred in Tempe, Arizona, and resulted in the death of a 49-year-old woman named Elaine Herzberg who was walking her bike across the street. The video footage of the crash shows that the autonomous Uber failed to slow down and hit the pedestrian. The article discusses the release of both exterior and interior videos, with the operator inside the car, Rafaela Vasquez, appearing to be distracted and not looking at the road at the time of the collision. It also mentions the ongoing investigation and the debate surrounding the responsibility of Uber and the safety of self-driving technology.\"\n}\nSo, the news is relevant to self-driving car crashes, and the specific information provided relates to the incident, the victim, the circumstances of the crash, and the investigation.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/mar\/22\/video-released-of-uber-self-driving-crash-that-killed-woman-in-arizona","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":76,"Time":"8:43 PM GMT+2•July 1, 2022","Title":"Behind the scenes of Waymo’s worst automated truck crash","Content":"cameraImage Credits: Waymo\nThe most serious crash to date involving a self-driving truck might have resulted in only moderate injuries, but it exposed how unprepared local government and law enforcement are to deal with the new technology.\nOn May 5, a Class 8 Waymo Via truck operating in autonomous mode with a human safety operator behind the wheel was hauling a trailer northbound on Interstate 45 toward Dallas, Texas. At 3:11 p.m., just outside Ennis, the modified Peterbilt was traveling in the far right lane when a passing truck and trailer combo entered its lane.\nThe driver of the Waymo Via truck told police that the other semi truck continued to maneuver into the lane, forcing Waymo’s truck and trailer off the roadway. She was later taken to a hospital for injuries that Waymo described in its report to the National Highway Traffic Safety Administration as “moderate.” The other truck drove off without stopping.\nWhile Waymo’s autonomous semi truck was not at fault in the hit and run, the incident highlights gaps in reporting mechanisms, and raises questions about how ready the public and law enforcement are to cope with heavy, fast-moving vehicles that have no human driver.\nThe stakes for the autonomous trucking industry, which is still in its infancy, couldn’t be any higher. One crash, even if the company is not at fault, could tarnish the public’s image of the technology. \nWaymo’s trucking origins\nWaymo started testing its driverless technology with semi trucks in 2017, beginning in California and Arizona. At the time, it was in the middle of an epic legal battle with Uber over technology allegedly taken from Waymo by engineer Anthony Levandowski, and subsequently purchased by Uber as part of self-driving truck startup Otto.\nWaymo’s self-driving trucks, which are part of a delivery and logistics division the company calls Waymo Via, rely on similar technologies to its robotaxis: a suite of sensors, including cameras, radars and lidars, and powerful on-board computers. All have qualified truck drivers — known as autonomous specialists — in the driver’s seat.  \nIn 2018, Waymo began hauling freight in Georgia, and it branded its delivery business Waymo Via in 2020. It then expanded into New Mexico and Texas, and inked deals with logistics companies like J.B. Hunt, UPS and C.H. Robinson. Earlier this month, it committed to a long-term strategic partnership with Uber and announced a pilot delivery program with home goods e-tailer Wayfair.\nThat pilot is due to start in July on the same stretch of I-45 highway where the May crash occurred. \nInside the crash\nUsing reports from local police and the Department of Transportation, and data supplied by Waymo to NHTSA, TechCrunch has attempted to reconstruct the worst self-driving truck crash on U.S. roads to date.\nAccording to Waymo, the Peterbilt 579 truck was not carrying freight for any customers or partners; it was conducting “standard” testing with a weighted load.\nBehind the wheel was a 40-year-old autonomous specialist with a decade of truck driving experience; there was also a software operator on board. Like many workers in Waymo vehicles, both were actually employed by Transdev, a multinational transit and mobility company.\nAlthough the ultimate aim of automated trucks is to eliminate, or at least greatly reduce, staffing costs, self-driving truck startups today operate with a safety driver and an engineer or technician on board.\nWaymo reported that its truck was driving in autonomous mode at 62 miles per hour, slightly below the speed limit, when the other truck entered its lane and forced it off the road. \nA Waymo Via self-driving truck was hit by another semi truck in May 2022. Image Credits: Ennis Police Department\nWaymo told TechCrunch that the safety operator did not take control of the truck from its autonomous system. \n“The technology was not a factor, as this collision was caused by a human driver of another vehicle when they crossed the lane line and collided with the cab of Waymo’s vehicle and continued driving,” spokesperson Katherine Barna wrote in an email.\nEnnis PD photos, obtained under public records laws, show the Waymo truck and trailer by the side of the highway. They appear to have been prevented from sliding onto a parallel suburban road by a crash barrier. An Ennis police officer noted the truck itself sustained only minor damage: one picture shows damage to the truck’s lidar laser-ranging sensor.\nPhoto of Waymo Via’s lidar sensor, which was damaged in a crash in Texas. Image Credits: Ennis Police Department\nThe driver, however, was taken to a nearby hospital with unspecified, moderate injuries. The attending officer classified the incident as a hit and run. Waymo told TechCrunch that it understands the driver is doing well, following their injury. The driver did not respond to a request from TechCrunch for comment.\nBecause the system was active during at least some of the 30 seconds preceding the collision, Waymo was required to report it to NHTSA, to comply with the agency’s Standing General Order on Crash Reporting for automated vehicles.\nGaps in the system\nThere are no checkboxes on a Texas Department of Transportation crash report to record whether the vehicles involved are operating with full or partial automation, and that information was not recorded in the narrative section of the Waymo crash report.\nEnnis PD Detective Paul Asby, who later investigated the incident, told TechCrunch that he did not know the truck was operating autonomously at the time of the collision.\nAt the hospital, the Waymo driver told police the hit-and-run vehicle belonged to Helwig Trucking, a local carrier with about 15 trucks. (Waymo also confirmed that the truck’s cameras captured enough details to identify the other vehicle.) Helwig did not respond to a request for comment.\nThe driver left her phone number with the police and was released from the hospital, and the Waymo truck was towed away. Waymo also provided a contact number to the police. Detective Asby was assigned to the case, and quickly established that the crash was the fault of the Helwig driver. He contacted the company to get its side of the story, and its insurance details. But when it came to Waymo, Asby met a wall of silence.\n“I was going to speak to the driver because she was taken to the hospital but I’ve tried to contact her cell phone and it says it’s not a valid number,” he said. “The same thing for the passenger who was in there with her.”\nSubsequent calls to Waymo itself went unanswered. “They never did return my calls. I inactivated the case, but the insurance information is in there if they want it,” he says. “Maybe they’re so rich they don’t care.” \nWaymo told TechCrunch that it is not aware of any attempt by Ennis PD to contact it for information, and that it did not have any need to contact the department itself. \nHow it’s going\nThe Ennis crash is not the only one to have involved a Waymo semi truck. In February, a similar Waymo Peterbilt 579 traveling southbound on Interstate 10 near Sacaton, Arizona, was struck by a box truck traveling in the adjacent lane, and which had just also hit a motor coach. The Waymo vehicle was traveling 50 mph in a 75 mph limit zone. TechCrunch was not immediately able to source a police report detailing the crash; there were no reported injuries.\nIf Waymo had not been required to report the crashes to NHTSA, there is a chance they might never have come to light. The official crash reports gathered by Texas, which has welcomed multiple self-driving truck operations to its highways, appear insufficient to fully record incidents involving driverless vehicles. Local law enforcement has historically been similarly ill-equipped to deal with driving systems instead of driving humans. \nWaymo is trying to close those gaps, says Barna. “Waymo has built the Waymo Driver to interact with First Responders; and has worked closely with public safety officials to ensure the safe introduction of our technology in every market that we operate in,” she told TechCrunch.  “We have a team with decades of law enforcement experience that has provided training to hundreds of officers and firefighters in California, Arizona and Texas detailing best practices for safe interactions with Waymo vehicles.”\n“We’ve got a mountain of work to do integrating these things into society,” said Steve Viscelli, a sociologist at the University of Pennsylvania who studies trucking, and acts as an advisor to Aurora’s self-driving truck effort. “We need to talk a lot more about what they mean for supply chains, for workers and for the highway. There are a lot of people who are going to do stupid and aggressive stuff around them because they don’t like self-driving vehicles.”\nWaymo has told the U.S. Department of Transportation that it has 47 trucks, which have driven more than 1.6 million miles. It would not disclose to TechCrunch how many of those miles were driven under some level of automated control.\nAutomated trucking companies have “got the basic driving stuff down,” says Viscelli. “It’s what happens with the family on vacation and the tire’s off, or when there’s construction that changes the shape of the road, or debris on the highway. It’s when you have confidence in those issues that’s going to determine when they’re on the road. But I would not be surprised to see trucks without drivers on lanes next year.”\nUpdated: TechCrunch updated the article to reflect that Waymo also provided a contact number to the police.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is about a self-driving truck crash involving a Class 8 Waymo Via truck operating in autonomous mode with a human safety operator behind the wheel. The crash occurred on May 5, just outside Ennis, Texas, when another semi truck entered the Waymo truck's lane, forcing it off the roadway. The Waymo Via truck driver reported moderate injuries, and the other truck drove off without stopping. While Waymo's autonomous truck was not at fault, this incident highlights the challenges of dealing with self-driving vehicles and the potential risks associated with them. It also discusses the technology and circumstances surrounding the crash, including the speed of the Waymo truck and the fact that the safety operator did not take control of the truck from its autonomous system. Additionally, the news highlights gaps in reporting mechanisms for autonomous vehicle crashes and the challenges faced by law enforcement in dealing with self-driving trucks. The article provides specific details about the crash, including the location, timing, and the condition of the vehicles involved, making it relevant to self-driving car crashes.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/07\/01\/behind-the-scenes-of-waymos-worst-automated-truck-crash\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":67,"Time":"Updated 11:23 PM GMT+2, July 28, 2023","Title":"The backup driver in the 1st death by a fully autonomous car pleads guilty to endangerment","Content":"PHOENIX (AP) — The backup Uber driver for a self-driving vehicle that killed a pedestrian in suburban Phoenix in 2018 pleaded guilty Friday to endangerment in the first fatal collision involving a fully autonomous car.\nMaricopa County Superior Court Judge David Garbarino, who accepted the plea agreement, sentenced Rafaela Vasquez, 49, to three years of supervised probation for the crash that killed 49-year-old Elaine Herzberg. Vasquez told police that Herzberg “came out of nowhere” and that she didn’t see Herzberg before the March 18, 2018, collision on a darkened Tempe street.\nVasquez had been charged with negligent homicide, a felony. She pleaded guilty to an undesignated felony, meaning it could be reclassified as a misdemeanor if she completes probation.\nAuthorities say Vasquez was streaming the television show “The Voice” on a phone and looking down in the moments before Uber’s Volvo XC-90 SUV struck Herzberg, who was crossing with her bicycle.\nRELATED COVERAGE\nCongress urged to revive long-stalled debate about regulating self-driving vehicles\nTesla shouldn’t call driving system Autopilot because humans are still in control, Buttigieg says\nCruise wants to test self-driving cars all over California\nVasquez’s attorneys said she was was looking at a messaging program used by Uber employees on a work cellphone that was on her right knee. They said the TV show was playing on her personal cellphone, which was on the passenger seat.\nDefense attorney Albert Jaynes Morrison told Garbarino that Uber should share some blame for the collision as he asked the judge to sentence Vasquez to six months of unsupervised probation.\n“There were steps that Uber failed to take,” he said. By putting Vasquez in the vehicle without a second employee, he said. “It was not a question of if but when it was going to happen.”\nProsecutors previously declined to file criminal charges against Uber, as a corporation. The National Transportation Safety Board concluded Vasquez’s failure to monitor the road was the main cause of the crash.\n“The defendant had one job and one job only,” prosecutor Tiffany Brady told the judge. “And that was to keep her eyes in the road.”\nMaricopa County Attorney Rachel Mitchell said in a statement after the hearing that her office believes the sentence was appropriate “based on the mitigating and aggravating factors.”\nThe contributing factors cited by the NTSB included Uber’s inadequate safety procedures and ineffective oversight of its drivers, Herzberg’s decision to cross the street outside of a crosswalk and the Arizona Department of Transportation’s insufficient oversight of autonomous vehicle testing.\nThe board also concluded Uber’s deactivation of its automatic emergency braking system increased the risks associated with testing automated vehicles on public roads. Instead of the system, Uber relied on the human backup driver to intervene.\nIt was not the first crash involving an Uber autonomous test vehicle. In March 2017, an Uber SUV flipped onto its side, also in Tempe when it collided with another vehicle. No serious injuries were reported, and the driver of the other car was cited for a violation.\nHerzberg’s death was the first involving an autonomous test vehicle but not the first in a car with some self-driving features. The driver of a Tesla Model S was killed in 2016 when his car, operating on its Autopilot system, crashed into a semitrailer in Florida.\nNine months after Herzberg’s death, in December 2019, two people were killed in California when a Tesla on Autopilot ran a red light, slammed into another car. That driver was charged in 2022 with vehicular manslaughter in what was believed to be the first felony case against a motorist who was using a partially automated driving system.\nIn Arizona, the Uber system detected Herzberg 5.6 seconds before the crash. But it failed to determine whether she was a bicyclist, pedestrian or unknown object, or that she was headed into the vehicle’s path, the board said.\nThe backup driver was there to take over the vehicle if systems failed.\nThe death reverberated throughout the auto industry and Silicon Valley and forced other companies to slow what had been a fast march toward autonomous ride-hailing services. Uber pulled its self-driving cars out of Arizona, and then-Gov. Doug Ducey prohibited the company from continuing its tests of self-driving cars.\nVasquez had previously spent more than four years in prison for two felony convictions — making false statements when obtaining unemployment benefits and attempted armed robbery — before starting work as an Uber driver, according to court records.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is about a self-driving car-related incident. It discusses the case of the backup Uber driver for a self-driving vehicle that killed a pedestrian in suburban Phoenix in 2018. The driver, Rafaela Vasquez, pleaded guilty to endangerment in the first fatal collision involving a fully autonomous car. The news provides details about the incident, including the circumstances leading to the crash, Vasquez's actions before the collision, and the legal proceedings surrounding the case. It also mentions the National Transportation Safety Board's conclusion that Vasquez's failure to monitor the road was the main cause of the crash and highlights the debate about responsibility and safety in self-driving car technology.\"\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/autonomous-vehicle-death-uber-charge-backup-driver-1c711426a9cf020d3662c47c0dd64e35","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":72,"Time":"Published 9:06 PM GMT+2, December 22, 2022","Title":"US probing automated driving system use in 2 Tesla crashes","Content":"The U.S. government’s highway safety agency said Thursday it will send teams to investigate two November crashes in California and Ohio involving Teslas that may have been operating on automated driving systems.\nThe probes bring to 35 the number of crashes investigated by the National Highway Traffic Safety Administration since 2016 in which either Tesla’s “Full Self-Driving” or “Autopilot” systems likely were in use. Nineteen people were killed in the crashes.\nThe California crash occurred on Thanksgiving Day involving eight vehicles on the San Francisco-Oakland Bay Bridge. The driver told authorities that the Tesla Model S was using the company’s “Full Self-Driving” software, according to Highway Patrol report obtained by CNN.\nThe Ohio crash happened Nov. 18 near Toledo, when a Tesla Model 3 crashed into an Ohio Highway Patrol SUV stopped on a roadway with its emergency lights flashing.\nA message was left Thursday seeking comment from Tesla on the latest NHTSA action. The company based in Austin, Texas, has disbanded its media relations department.\nThe agency said Thursday that it sent the team to the California crash after gathering information from law enforcement officers and Tesla.\nThe eight-vehicle crash happened about noon, closing two lanes and clogging traffic on the holiday. Nine people were treated for minor injuries including a child who was hospitalized, according to CNN, which got a copy of the crash report through a public records request.\nThe Tesla Model S driver reportedly told the California Highway Patrol that the company’s “Full Self-Driving” system was operating when the crash occurred, and that it braked unexpectedly while traveling at 55 miles per hour (88.5 kilometers per hour). The Model S shifted into the far left lane, then braked to 20 mph, causing the pileup, CNN said the report stated.\nIn the crash near Toledo, an Ohio State Highway Patrol trooper and another motorist were injured when a Tesla Model 3 struck the rear of the police SUV parked on a road with its emergency lights flashing.\nThe Toledo Blade reported that the highway patrol report didn’t address whether any automated systems were operating.\nNHTSA has been investigating Tesla automated systems for 6 1\/2 years without taking enforcement action.\nMichael Brooks, executive director of the nonprofit Center for Auto Safety, a watchdog group, said it’s long past time for the agency to seek a recall.\n“The question is what’s the threshold number of injuries and deaths and cars driving stupidly that we have to see before NHTSA finds that there’s some sort of defect in these cars?” Brooks asked.\nHe said NHTSA has only published information on one of the Tesla crash investigations and called on the agency to be more transparent.\nA message was left for a NHTSA spokeswoman seeking comment on the length of the investigations.\nIn addition to the crash investigation teams, NHTSA already has two formal investigations open into Tesla’s advanced driver assistance systems. One involves complaints that the vehicles brake unexpectedly for no reason, and the other stems from crashes into emergency vehicles parked on roadways with lights flashing.\nNHTSA hasn’t made public any enforcement actions on either probe. The agency also has ordered automakers and tech companies to report crashes involving automated systems.\nIn the “phantom braking” investigation, the agency said that more than 750 Tesla owners have complained that cars operating on the partially automated driving systems have suddenly stopped on roadways for no apparent reason.\nIn opening the probe, the agency said it was looking into vehicles equipped with automated driver-assist features such as adaptive cruise control and “Autopilot,” which allows them to automatically brake and steer within their lanes.\n“Complainants report that the rapid deceleration can occur without warning, and often repeatedly during a single drive cycle,” the agency said.\nMany owners wrote in their complaints that they feared a rear-end crash on a freeway.\nAlthough Tesla calls one of its systems “Full Self Driving,” even CEO Elon Musk concedes that it’s not ready to drive itself. The other system, called “Autopilot,” keeps a car centered in its lane and a distance away from vehicles in front of it. In both cases Tesla says these are driver-assist systems and that drivers must be ready to intervene at all times.\nCritics say humans become too reliant on the systems when they’re not capable of handling all situations.","Relevant":true,"Overview":"The provided news is about incidents involving Tesla vehicles and their automated driving systems. Here's the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": [\n        {\n            \"Incident\": \"California Crash on Thanksgiving Day\",\n            \"Details\": \"The driver reported that the Tesla Model S was using the company's 'Full Self-Driving' software when the crash occurred. The vehicle braked unexpectedly while traveling at 55 miles per hour (88.5 kilometers per hour), shifted lanes, and braked to 20 mph, causing a pileup involving eight vehicles on the San Francisco-Oakland Bay Bridge. Nine people were treated for minor injuries, and the driver claimed the system was in operation.\"\n        },\n        {\n            \"Incident\": \"Ohio Crash on Nov. 18\",\n            \"Details\": \"A Tesla Model 3 crashed into an Ohio Highway Patrol SUV with its emergency lights flashing. The report does not specify whether any automated systems were operating, but this incident involved a Tesla vehicle, potentially under autopilot or similar automation.\"\n        },\n        {\n            \"NHTSA Investigations\": \"The National Highway Traffic Safety Administration (NHTSA) has been investigating Tesla automated systems for over 6 years, with multiple crashes involving Tesla's 'Full Self-Driving' or 'Autopilot' systems likely in use, resulting in 19 fatalities. NHTSA has also opened formal investigations into complaints of unexpected braking and crashes into emergency vehicles.\"\n        },\n        {\n            \"Safety Concerns\": \"Critics have raised concerns about the safety of Tesla's automated systems, and there are calls for NHTSA to seek a recall and be more transparent in its investigations. Tesla emphasizes that these systems are driver-assist and that drivers must be ready to intervene at all times.\"\n        }\n    ]\n}\nIn summary, the news is relevant to self-driving car crashes because it discusses incidents involving Tesla vehicles using their automated driving systems, specifically the \"Full Self-Driving\" and \"Autopilot\" features. It also highlights ongoing investigations by the NHTSA into these incidents and related safety concerns.","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/technology-business-california-571b7fcc31cb51a7cb9d3ee47f98d4fa","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":81,"Time":"Updated 8:01 PM EDT, Tue August 22, 2023","Title":"GM’s Cruise slashed fleet of robotaxis by 50% in San Francisco after collisions","Content":"CNN\n — \nCalifornia authorities have asked General Motors to “immediately” take some of its Cruse robotaxis off the road after autonomous vehicles were involved in two collisions – including one with an active fire truck – last week in San Francisco.\nCalifornia’s Department of Motor Vehicles confirmed to CNN that it is investigating “recent concerning incidents involving Cruise vehicles in San Francisco.”\n“The DMV is in contact with Cruise and law enforcement officials to determine the facts and requested Cruise to immediately reduce its active fleet of operating vehicles by 50% until the investigation is complete and Cruise takes appropriate corrective actions to improve road safety,” the department said in a statement.\nThat means Cruise, which is the self-driving subsidiary of General Motors, can have no more than 50 driverless cars in operation during the day, and 150 in operation at night, according to the department.\nThe California DMV said that Cruise has agreed to the request, and a spokesperson from Cruise told CNN that the company is investigating the firetruck crash as well.\nAccidents come just after Cruise’s big win in San Francisco\nThe accidents come less than two weeks after California regulators officially gave the green light for Cruise and competitor Waymo to charge money for robotaxi trips around San Francisco at any time of day. Prior to the approval, Cruise was only authorized to offer fared passenger service from driverless cars overnight from 10 pm to 6 am, when there are fewer pedestrians or traffic that could confuse the autonomous vehicle’s software.\nThe collisions, which both occurred on Thursday, reveal potential risks of driverless technology.\nIn a blog post, Cruise’s general manager for San Francisco said the firetruck crash occurred when an emergency vehicle that appeared to be en route to an emergency scene moved into an oncoming lane of traffic to bypass a red light. Cruise’s driverless car identified the risk, the blog post said, but it “was ultimately unable to avoid the collision.”\nThat crash resulted in one passenger being taken to the hospital via ambulance for seemingly minor injuries, according to the company.\nCruise told CNN the other crash on Thursday took place when another car ran a red light “at a high rate of speed.”\n“The AV detected the vehicle and braked but the other vehicle made contact with our AV. There were no passengers in our AV and the driver of the other vehicle was treated and released at the scene,” Hannah Lindow, a Cruise spokesperson, told CNN.\nIt is unclear whether the two accidents would have been avoided had there been a human driver rather than an autonomous vehicle (AV) involved – but the crashes were not the only two incidents involving Cruise’s driverless cars in San Francisco last week.\nOn Tuesday, Cruise confirmed on X, formerly known as Twitter, that one of its driverless taxis drove into a construction area and stopped in wet concrete.\n“This vehicle has already been recovered and we’re in communication with the city about this,” the company said.\nRoadblocks for autonomous vehicles\nThe recent events underscore the challenges of creating safe, fully driverless passenger vehicles.\nGeneral Motors acquired Cruise Automation in 2016 for $1 billion, solidifying its place in the autonomous vehicles race, but many companies have since scaled back, or abandoned their driverless car ambitions. The endeavor has proven costly, and mastering all situations that humans might face behind the wheel is difficult and time-consuming.\nRidesharing giants Uber and Lyft have both sold autonomous vehicle units in recent years. Even Tesla CEO Elon Musk, who has been optimistic about autonomous vehicle technology, has yet to fully deliver on his promise.\nTesla vehicles now come with the option to add a “full self-driving” feature in beta-testing for $15,000, but drivers must agree to “stay alert, keep your hands on the steering wheel at all times and maintain control of your car.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n\"General Motors' Cruise robotaxis were involved in two collisions in San Francisco.\",\n\"One of the collisions involved an active fire truck.\",\n\"California's Department of Motor Vehicles is investigating these incidents.\",\n\"California authorities have requested Cruise to reduce its active fleet of operating vehicles by 50% during the investigation.\",\n\"Cruise, the self-driving subsidiary of General Motors, agreed to the request.\",\n\"The collisions occurred shortly after California regulators allowed Cruise to charge money for robotaxi trips around San Francisco at any time of day.\",\n\"The accidents reveal potential risks of driverless technology, as the car was unable to avoid the collision with the firetruck despite identifying the risk.\",\n\"One passenger was taken to the hospital due to the firetruck collision.\",\n\"Another collision involved a car running a red light at a high rate of speed.\",\n\"There was an additional incident where a driverless taxi drove into a construction area and stopped in wet concrete.\",\n\"These incidents highlight the challenges of creating safe, fully driverless passenger vehicles.\",\n\"Several companies have faced difficulties and setbacks in the development of autonomous vehicle technology.\"\n]\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2023\/08\/21\/tech\/gm-cruise-to-cut-robotaxi-fleet-in-san-francisco-after-collisions\/index.html","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":77,"Time":"Sun 26 Mar 2017 13.29 CEST","Title":"Uber suspends fleet of self-driving cars following Arizona crash","Content":"Uber has suspended its fleet of self-driving cars while it investigates a crash in Arizona involving one of its vehicles.\nPhotographs posted online show the car lying on its side next to another badly damaged vehicle. There were no serious injuries as a result of the incident.\nThe Uber Technologies car – a Volvo SUV – was carrying two engineers in the front and no backseat passengers but it is not yet clear whether the car was in self-driving mode at the time of the crash.\nThe incident is the latest blow for the car-hailing app which has been left reeling by a series of setbacks in recent weeks including the departure of its president, Jeff Jones.\nInitial police reports suggest the collision was caused by a person who failed to give way to the self-driving car rather than a malfunction by the Uber vehicle.\nHowever, pending further investigation, the company has removed all its self-driving cars from the road in Arizona, as well as test sites in Pennsylvania and California – all three states where it operated the driverless vehicles.\nA spokesperson for the police in Tempe, Arizona, said the crash happened when another car “failed to yield” to an Uber car at a left turn. Josie Montenegro said: “There was a person behind the wheel. It is uncertain at this time if they were controlling the vehicle at the time of the collision.”\nUber self-driving cars always have a human in the driving seat who can take over the controls.\nThe latest incident follows recent difficult months for the firm, which has seen a number of high level executives quit and has faced criticism over workplace practices and ethics.\nMeanwhile legal issues have been hampering progress with the testing of its autonomous vehicle technology in California.","Relevant":true,"Overview":"Based on the provided news article, it appears to be related to self-driving car crashes. Here's the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Uber suspends fleet of self-driving cars following Arizona crash\",\n        \"Description\": \"Uber has suspended its fleet of self-driving cars while it investigates a crash in Arizona involving one of its vehicles. Photographs posted online show the car lying on its side next to another badly damaged vehicle. There were no serious injuries as a result of the incident. The Uber Technologies car – a Volvo SUV – was carrying two engineers in the front and no backseat passengers but it is not yet clear whether the car was in self-driving mode at the time of the crash. Initial police reports suggest the collision was caused by a person who failed to give way to the self-driving car rather than a malfunction by the Uber vehicle. However, pending further investigation, the company has removed all its self-driving cars from the road in Arizona, as well as test sites in Pennsylvania and California – all three states where it operated the driverless vehicles. A spokesperson for the police in Tempe, Arizona, said the crash happened when another car 'failed to yield' to an Uber car at a left turn. Josie Montenegro said: 'There was a person behind the wheel. It is uncertain at this time if they were controlling the vehicle at the time of the collision.' Uber self-driving cars always have a human in the driving seat who can take over the controls.\"\n    }\n}\nThe news discusses Uber's suspension of its self-driving car fleet following a crash in Arizona. It mentions that there were no serious injuries but raises questions about whether the self-driving car was in control at the time of the crash. Additionally, the initial police reports suggest that the collision was caused by a person failing to yield to the self-driving car, indicating a potential self-driving car-related incident.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2017\/mar\/26\/uber-suspends-self-driving-cars-arizona-crash-volvo-suv","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":84,"Time":"MARCH 31, 2018","Title":"More family members of woman killed in Uber self-driving car crash hire lawyer","Content":"By David Schwartz\nPHOENIX (Reuters) - More family members of a woman killed by an Uber Technologies Inc [UBER.UL] self-driving vehicle have hired legal counsel, indicating the ride services firm’s legal problems may not be over in the first fatality caused by an autonomous car.\nA still frame taken from video released March 21, 2018 shows the exterior view of the self-driving Uber vehicle leading up to the fatal collision in Tempe, Arizona, U.S. on March 18, 2018. Tempe Police Department\/Handout via REUTERS\nPhoenix attorney Patrick McGroder said he has been retained by the mother, father and son of Elaine Herzberg, 49, who died after being struck by an Uber self-driving SUV while walking across a street in the suburb of Tempe earlier this month.\nOn Thursday, a different law firm representing Herzberg’s daughter and husband said it had reached a settlement with Uber. The terms were not given.\nThat settlement does not affect the new case, McGroder said in an email on Friday.\n“We are in the initial stages of investigation,” he said, noting he and his two partners will represent the family members and that they had sent a letter of representation to Uber this week.\nA spokesman for Uber declined to comment.\nThe fallout from the accident could stall the development and testing of self-driving vehicles, designed to eventually perform far better than human drivers and to sharply reduce the number of motor vehicle fatalities that occur each year.\nUber has suspended its testing in the wake of the incident. Toyota Motor Corp and chipmaker Nvidia Corp have also suspended self-driving testing on public roads, as they and others await the results of investigations into the Tempe accident, believed to be the first death of a pedestrian struck by a self-driving vehicle.\nThe March 18 fatality near downtown Tempe also presents an unprecedented liability challenge because self-driving vehicles, which are still in the development stage, involve a complex system of hardware and software often made by outside suppliers.\nHerzberg was pushing a bicycle while walking across a four-lane road outside a crosswalk when she was struck. Video footage from a dash-mounted camera inside the vehicle, released by Tempe police, showed the SUV traveling along a dark street when the headlights suddenly illuminated Herzberg in front of the SUV.\nOther footage showed that in the seconds before the accident, the human safety driver behind the wheel was mostly looking down, not at the road.\nReporting by David Schwartz in Phoenix; Writing by Chris Prentice; Editing by Susan Thomas\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Based on the provided news article, it appears to be related to a self-driving car crash involving an Uber vehicle. Here is the JSON format response:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"This news is relevant to self-driving car crashes as it discusses a fatal accident involving an Uber self-driving vehicle. The article mentions that Elaine Herzberg, a pedestrian, was killed when she was struck by an Uber self-driving SUV while walking across a street in Tempe. It also mentions that the incident led to a legal case, with family members hiring an attorney to represent them. The article highlights that this was the first fatality caused by an autonomous car. Additionally, it discusses the fallout from the accident, including the suspension of self-driving testing by Uber, Toyota, and Nvidia, as well as the complexity of liability associated with self-driving technology. The article also mentions video footage from a dash-mounted camera inside the vehicle, which showed the moments leading up to the collision, and the human safety driver not paying full attention to the road.\"\n}\nSo, the news is relevant to self-driving car crashes, and the specific information provided in the news article confirms this relevance.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-uber-idUSKBN1H700H","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":85,"Time":"MARCH 27, 2017","Title":"Uber resumes self-driving program three days after Arizona crash","Content":"By Heather Somerville, Gina Cherelus\n(Reuters) - Uber Technologies Inc [UBER.UL] put its self-driving cars back on the road on Monday, voicing confidence in its autonomous vehicle program three days after one of its cars was involved in a crash in Tempe, Arizona.\nUber wrapped up a brief investigation and cleared its autonomous cars to resume driving in the three cities where it operates a self-driving pilot program - Tempe, San Francisco and Pittsburgh, Pennsylvania - an Uber spokeswoman, who declined to be named, told Reuters.\nOn Friday, Uber suspended its pilot program in the three cities after the crash, in which a human-driven vehicle “failed to yield” to an Uber vehicle while making a turn, said Josie Montenegro, a spokeswoman for Tempe’s police department. The Uber car was in self-driving mode at the time of the crash. “The vehicles collided, causing the autonomous vehicle to roll onto its side,” Montenegro said in an email. “There were no serious injuries.”\nA driver and an engineer were in the front seats of the Uber Volvo SUV at the time, a standard requirement for the self-driving cars. The back seat was unoccupied.\nA full police report on the crash is expected as early as the middle of this week, said Detective Lily Duran of the Tempe Police Department. She said the Uber vehicle was not at fault in the collision. Photos and a video posted on Twitter by Fresco News showed a Volvo SUV flipped on its side after an apparent collision involving two other, slightly damaged cars. Uber said the images appeared to be from the Tempe crash scene.\nThere are no reported collisions involving Uber self-driving cars in Pittsburgh, where the company launched its first program in September, Pittsburgh police spokeswoman Sonya Toler said.\nExperts in autonomous vehicles say crashes involving such cars will happen, but as autonomous technology improves so too will the safety of everyone on the road.\n“Driverless cars keep getting better the more they drive, whereas humans have a roughly constant safety record over the years,” said Hod Lipson, a professor of mechanical engineering and roboticist at Columbia University, who estimates there are about 23,000 traffic fatalities per week globally.\n“The idea that somehow a human driver makes the drive more secure is false comfort, and potentially dangerously misleading,” he said.\nUber’s driverless car program has had its share of controversy. In December, Uber moved its cars from San Francisco to Arizona after a standoff with the California Department of Motor Vehicles. Uber had refused to apply for the necessary permit to test autonomous vehicles on public roads, and the DMV revoked the registration of 16 Uber self-driving cars.\nUber conceded and applied for and received the permit earlier this month.\nThe company also faces a lawsuit from Alphabet Inc's GOOGL.O self-driving car unit, Waymo, which accuses Uber of stealing designs for technology critical to autonomous cars known as Lidar. Uber has said Waymo's claims are false. The lawsuit set in motion what many in the self-driving sector predict will be a closely watched, acrimonious trade secrets battle between the two tech rivals.\nReporting by Heather Somerville in San Francisco and Gina Cherelus in New York; Additional reporting by Jessica Toonkel in New York and Aishwarya Venugopal in Bengaluru; Editing by Leslie Adler\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Based on the provided news article, it is indeed about a self-driving car crash. Here's the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news article discusses a recent self-driving car crash involving Uber's autonomous vehicle program in Tempe, Arizona. On Friday, Uber suspended its self-driving pilot program in Tempe, San Francisco, and Pittsburgh after one of its self-driving cars was involved in a crash. The collision occurred when a human-driven vehicle failed to yield to the Uber self-driving vehicle while making a turn, causing the autonomous vehicle to roll onto its side. Fortunately, there were no serious injuries. The article also mentions that Uber cleared its autonomous cars to resume driving after a brief investigation and that the Uber vehicle was not at fault in the collision. A police report on the crash is expected soon. The article provides details about the circumstances of the crash, the locations where Uber's self-driving pilot program operates, and the controversies surrounding Uber's autonomous vehicle program.\"\n}\nSo, the news is relevant to self-driving car crashes, and the specific information related to the self-driving car crash has been detailed in the JSON response.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-uber-tech-crash-idUSKBN16Y1WB","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":87,"Time":"September 1, 2021","Title":"U.S. identifies 12th Tesla Autopilot car crash involving emergency vehicle","Content":"WASHINGTON, Sept 1 (Reuters) - U.S. auto safety regulators on Wednesday said they had identified a 12th crash involving Tesla Inc (TSLA.O) vehicles using advanced driver assistance systems in incidents involving emergency vehicles and demanded the automaker answer detailed questions about its Autopilot system.\nThe National Highway Traffic Safety Administration (NHTSA) on Aug. 16 said it had opened a formal safety probe into Tesla driver assistance system Autopilot after 11 crashes. The probe covers 765,000 U.S. Tesla vehicles built between 2014 and 2021.\nThe 12th occurred in Orlando on Saturday, NHTSA said. The agency sent Tesla a detailed 11-page letter on Tuesday with numerous questions it must answer, as part of its investigation.\nTesla's Autopilot handles some driving tasks and allows drivers to keep their hands off the wheel for extended periods. Tesla says Autopilot enables vehicles to steer, accelerate and brake automatically within their lane.\nAdvertisement · Scroll to continue\nTesla did not respond to a request seeking comment. The company could face civil penalties of up to $115 million if it fails to fully respond to the questions, NHTSA said.\nTesla shares closed down 0.2% at $734.09 on Wednesday.\nOn Saturday, the Florida Highway Patrol said the car of a Florida trooper who had stopped to assist a disabled motorist on a major highway was struck by a Tesla that the driver said was in Autopilot mode. According to a police report released on Wednesday, the trooper \"narrowly missed being struck as he was outside of his patrol car.\"\nAdvertisement · Scroll to continue\nNHTSA said earlier it had reports of 17 injuries and one death in the 11 crashes. A December 2019 crash of a Tesla Model 3 left a passenger dead after the vehicle collided with a parked fire truck in Indiana.\nNHTSA's request for information asks Tesla to detail how it detects and responds to emergency vehicles, as well as flashing lights, road flares, cones and barrels and to detail the impact of low light conditions.\nAdvertisement · Scroll to continue\nNHTSA said previously that most of the 11 incidents occurred after dark.\nTesla in July introduced an option for some customers to subscribe to its advanced driver assistance software, dubbed \"Full Self-Driving capability.\" Tesla said the current features \"do not make the vehicle autonomous.\"\nNHTSA is seeking information on the \"date and mileage at which the 'Full Self Driving' (FSD) option was enabled\" for all vehicles, along with all consumer complaints, field reports, crash reports and lawsuits.\nAdvertisement · Scroll to continue\nThe agency also wants Tesla to explain how it prevents use of the system outside areas where it is intended.\nAmong the detailed questions, NHTSA also asked Tesla to explain \"testing and validation required prior to the release of the subject system or an in-field update to the subject system, including hardware and software components of such systems.\"\nTesla must respond to NHTSA's questions by Oct. 22, it said, and it must disclose plans for any changes to Autopilot within the next 120 days.\nReporting by David Shepardson; Editing by Barbara Lewis and Bill Berkrot\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Based on the provided news article, it appears to be about self-driving car-related incidents involving Tesla vehicles. Here is the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to self-driving car crashes as it discusses the 12th crash involving Tesla vehicles using advanced driver assistance systems, specifically Autopilot. In this incident, a Tesla vehicle in Autopilot mode struck a Florida trooper's car, who had stopped to assist a disabled motorist on a major highway. The news also mentions that the National Highway Traffic Safety Administration (NHTSA) has opened a formal safety probe into Tesla's Autopilot system after a series of crashes involving emergency vehicles. NHTSA's request for information includes details on how Tesla detects and responds to emergency vehicles, flashing lights, and low light conditions. The article indicates that there have been previous crashes, injuries, and even one death related to Tesla's Autopilot system.\"\n}\nSo, the news is relevant to self-driving car crashes, specifically incidents involving Tesla's Autopilot system and emergency vehicles.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/us-identifies-12th-tesla-assisted-systems-car-crash-involving-emergency-vehicle-2021-09-01\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":91,"Time":"6 November 2019","Title":"Uber in fatal crash had safety flaws say US investigators","Content":"Uber in fatal crash had safety flaws say US investigators\nPublished\n6 November 2019\nShare\nIMAGE SOURCE,\nREUTERS\nAn Uber self-driving test vehicle that hit and killed a woman in 2018 had software problems, according to US safety investigators.\nElaine Herzberg, 49, was hit by the car as she was crossing a road in Tempe, Arizona.\nThe US National Transportation Safety Board (NTSB) found the car failed to identify her properly as a pedestrian.\nThe detailed findings raised a series of safety issues but did not determine the probable cause of the accident.\nThe safety board is expected to make that finding when it meets on 19 November.\nThe findings, released on Tuesday, may also be used to help shape recommendations for the developing autonomous driving industry. The sector has come under sharp scrutiny in the wake of the accident.\nThe fatal crash occurred in March 2018, and involved a Volvo XC90 that Uber had been using to test its self-driving technology.\nJust before the crash, Ms Herzberg had been walking with a bicycle across a poorly lit stretch of a multi-lane road.\nUber 'not criminally liable' for car death\nUber crash driver was 'watching TV'\nUber warned about self-drive car crashes\nAccording to the NTSB, Uber's test vehicle failed to correctly identify the bicycle as an imminent collision until just before impact.\nBy that time, it was too late for the vehicle to avoid the crash.\n\"The system design did not include a consideration for jaywalking pedestrians,\" the NTSB said.\nMedia caption,\nUber dashcam footage shows moment before fatal impact\nThe report also said there were 37 crashes of Uber vehicles in self-driving mode between September 2016 and March 2018.\nIn a statement, Uber said: \"We deeply value the thoroughness of the NTSB's investigation into the crash and look forward to reviewing their recommendations\".\nEarlier this year, prosecutors ruled that the company is not criminally liable for the death of Ms Herzberg.\nHowever, the car's back-up driver could still face criminal charges.\nDash-cam footage released by police after the incident appeared to show the vehicle's back-up driver, Rafaela Vasquez, taking her eyes off the road moments before the accident.\nFurther records from the streaming service Hulu suggested that Ms Vasquez had been streaming a television talent show on a phone at the time of the crash.\nFollowing the crash, authorities in Arizona suspended Uber's ability to test self-driving cars on the state's public roads.\nThe company subsequently pulled the plug on its autonomous car operation in Arizona, although the company later resumed tests in Pennsylvania\nMore on this story\nUber 'not criminally liable' for car death\nPublished\n6 March 2019\nUber crash driver was 'watching TV'\nPublished\n22 June 2018\nWho is to blame for 'self-driving car' deaths?\nPublished\n22 May 2018","Relevant":true,"Overview":"Based on the provided news article, it is relevant to self-driving car crashes. Here is the JSON format response with specific information:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news article discusses a fatal accident involving an Uber self-driving test vehicle that occurred in 2018. The accident involved the vehicle hitting and killing a pedestrian, Elaine Herzberg, as she was crossing a road in Tempe, Arizona. US safety investigators from the National Transportation Safety Board (NTSB) found that the self-driving car had software problems and failed to identify the pedestrian properly. The NTSB's detailed findings raised safety issues related to the self-driving technology. The report also mentioned that Uber's test vehicle failed to correctly identify an imminent collision with a bicycle. The article highlights that the self-driving industry has come under scrutiny due to this accident. Additionally, it mentions that Uber's ability to test self-driving cars in Arizona was suspended after the crash.\"\n}\nThe news is relevant to self-driving car crashes because it discusses a fatal accident involving a self-driving Uber vehicle and provides specific information about the software problems, the failure to identify a pedestrian, and the safety issues related to self-driving technology.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/business-50312340","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":83,"Time":"Wed 9 Mar 2016 21.01 CET","Title":"Google self-driving car caught on video colliding with bus","Content":"Newly released video shows the moment a Google self-driving car learned the hard way not to tussle with a public bus.\nThe collision happened on Valentine’s Day 2016, when a Google vehicle struck the side of a public bus in the Silicon Valley city of Mountain View. Footage recorded by cameras on the bus shows a Lexus SUV, which Google had outfitted with sensors and cameras that let it drive itself, edging into the path of the bus that was rolling by at about 15mph.\n\nIt was the first time in several years of testing on public roads that a Google self-driving car prototype caused a crash. Google has blamed other drivers for past collisions during testing, but, in a first, accepted at least partial responsibility for this crash.\nDriverless cars are the future. We're living in the motorised middle ages\nMartin Belam\nRead more\nThe Santa Clara Valley transportation authority released the video and post-crash photos this week to the Associated Press under a public records request.\n\nThough it was a low-speed collision, the impact crumpled the Lexus’ front left side, flattened the tire, and tore off the radar Google had installed to help the SUV perceive its surroundings. The Lexus had to be towed.\nNeither the Google employee in the driver’s seat – who was required to be there under California law to take the wheel in an emergency – nor the 16 people on the bus were injured.\n\nThe transit agency has concluded based on the footage that the bus driver was not responsible, spokeswoman Stacey Hendler Ross said. An independent claims adjuster has not yet determined liability, she said.\n\nThe 14 February crash is the first in which Google has acknowledged its car made a mistake that led to a collision. Google has reported that between September 2014 and November, its prototypes drove themselves about 400,000 miles on city streets near its Silicon Valley headquarters without causing a collision.\n\nIts fleet has likely driven an additional 100,000 miles since then, though the company won’t be specific. According to Google’s accounting, its cars have been hit nearly a dozen times on the streets in or around its Mountain View headquarters since road testing began in spring 2014.\n\nThe Lexus intended to turn right off a major boulevard but stopped after detecting sandbags around a storm drain near the intersection, according to an accident report Google filed with the California department of motor vehicles.\n\nPhotos show two small, black sandbags on either side of a drain at the curb. The right lane was wide enough to let some cars turn and others go straight, but to avoid driving over the sandbags, the Lexus needed to slide to its left within the lane. The bus and several other cars that drove straight were to the left of the Lexus, in the same lane.\nGoogle self-driving car collides with bus in California, accident report says\nRead more\nWhen the light turned green, several cars ahead of the bus passed the SUV. Google has said that both the car’s software and the person in the driver’s seat thought the bus would let the Lexus into the flow of traffic. The Google employee did not try to intervene before the crash.\n\n“This is a classic example of the negotiation that’s a normal part of driving – we’re all trying to predict each other’s movements. In this case, we clearly bear some responsibility, because if our car hadn’t moved there wouldn’t have been a collision,” Google wrote of the incident.\n\nThe footage shows angles from eight onboard cameras. In one clip, passengers gazing out on a sunny afternoon are jolted to attention by a scraping, crunching sound and the impact, which causes several hand straps to sway.\n\nIn another clip, the Lexus RX450 can be seen bouncing off the side of the bus. Photos show a long scratch mark traversing the side of the bus. The radar unit ended up wedged in the crack where two side passenger doors of the bus join, cracking a glass panel.\n\nGoogle has said that safety is its guiding principle in developing the cars, and that once mature, the technology promises to reduce collisions and deaths dramatically.\nThe software that controls the cars is programmed to follow all traffic laws and to drive conservatively, the company said.\n\nThe law-abiding algorithms that rule the SUV’s onboard computer make its decision to try and slip in front of the bus surprising.\n\nFootage from a forward-facing camera on the bus shows there was not much more than a full car length between the bus and the GMC Yukon in front of it.\n\nGoogle said it has tweaked its software to “more deeply understand that buses and other large vehicles are less likely to yield to us than other types of vehicles”.\nGoogle submitted an accident report to the California department of motor vehicles and also detailed the incident in the February edition of its monthly self-driving car report.","Relevant":true,"Overview":"Based on the provided news article, the news is relevant to self-driving car crashes. Here's the JSON format response:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Title\": \"Google self-driving car caught on video colliding with bus\",\n    \"Description\": \"The collision happened on Valentine’s Day 2016, when a Google vehicle struck the side of a public bus in the Silicon Valley city of Mountain View.\",\n    \"Details\": \"The news article describes a collision involving a Google self-driving car and a public bus. It happened on Valentine's Day in 2016 in Mountain View, California. The article provides specific information about the incident, including the fact that this was the first time a Google self-driving car prototype caused a crash. It mentions the damage to the Lexus SUV equipped with self-driving technology, the acceptance of partial responsibility by Google, and the circumstances leading to the collision, such as the car detecting sandbags and attempting to merge into traffic. The news also discusses the aftermath and Google's efforts to improve safety.\",\n    \"Source\": \"The Santa Clara Valley transportation authority released the video and post-crash photos this week to the Associated Press under a public records request.\"\n  }\n}\nThe news article is relevant to self-driving car crashes because it provides specific details about a collision involving a Google self-driving car and a public bus, including the date, location, and circumstances of the crash, as well as information about the damage and liability.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/mar\/09\/google-self-driving-car-crash-video-accident-bus","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":89,"Time":"6:54 PM GMT+2•June 22, 2018","Title":"Uber safety driver of fatal self-driving crash was watching Hulu, not the road","Content":"cameraImage Credits: Tempe Police Department\nA safety driver operating an Uber self-driving vehicle looked down at a phone that was streaming The Voice on Hulu 204 times during a 43-minute test drive that ended when pedestrian Elaine Herzberg was struck and killed in Tempe, Arizona, according to a 318-page police report reviewed by TechCrunch.\nThe Tempe Police Department released late Thursday evening the report on the fatal self-driving car crash that occurred in a Phoenix suburb in March. The lengthy report reveals that safety driver Rafaela Vasquez was streaming the show The Voice on her phone at the time of the crash.\nPolice determined that Vasquez’s eyes were off the road for 3.67 miles of the 11.8 total miles driven, or about 31 percent of the time.\nBased on the data, police reported that Vasquez could have avoided hitting Herzberg if her eyes were on the road. The case has been submitted to the Maricopa County Attorney’s office for review against Vasquez, who could face charges of vehicular manslaughter.\n“We continue to cooperate fully with ongoing investigations while conducting our own internal safety review,” an Uber spokeswoman said. “We have a strict policy prohibiting mobile device usage for anyone operating our self-driving vehicles. We plan to share more on the changes we’ll make to our program soon.”\nUber has hired former National Transportation Safety Board chair Christopher Hart as an adviser on the company’s overall safety culture. The company is reviewing internal processes, including its safety driver training practices.\nWhile the report reveals the actions of the safety driver, questions are still swirling around Uber’s self-driving technology system in the modified Volvo XC90. A preliminary report by the NTSB found Uber’s modified Volvo XC90’s LiDAR and radar first spotted an object in its path about six seconds before the crash. The self-driving system first classified the pedestrian as an unknown object, then as a vehicle and then as a bicycle. At 1.3 seconds before impact, the self-driving system determined that an emergency braking maneuver was needed to mitigate a collision, according to the NTSB. But Uber had disabled Volvo’s emergency braking system so it didn’t work when the vehicle was under computer control to reduce the potential for “erratic behavior.”\nThe accident occurred March 18 at about 10 p.m. when an Uber self-driving vehicle struck 49-year-old pedestrian Herzberg on Mill Avenue, just south of Curry Road, according to the Tempe Police Department. The vehicle was in autonomous mode at the time of the collision.\nUber immediately halted public testing of its self-driving vehicles following the crash. At the time, Uber was testing autonomous vehicles on public roads in the Phoenix suburb of Tempe, Pittsburgh, San Francisco and Toronto.\nArizona Gov. Doug Ducey later suspended Uber from testing its self-driving cars in Arizona.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is about a fatal self-driving car crash involving an Uber self-driving vehicle. The safety driver of the self-driving vehicle was distracted and watching Hulu on her phone during a test drive. The crash resulted in the death of a pedestrian, Elaine Herzberg, in Tempe, Arizona. The police report states that the safety driver's eyes were off the road for approximately 31 percent of the time, covering a distance of 3.67 miles. It is mentioned that the safety driver could have avoided hitting the pedestrian if her attention had been on the road. The case has been submitted for review, and the safety driver may face charges of vehicular manslaughter. Additionally, the news discusses the actions of the self-driving system, which first classified the pedestrian incorrectly and failed to initiate emergency braking, as Uber had disabled the vehicle's emergency braking system. This incident led to Uber suspending its self-driving vehicle testing in multiple locations, including Arizona.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2018\/06\/22\/uber-safety-driver-of-fatal-self-driving-crash-was-watching-hulu-not-the-road\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":86,"Time":"Wed 15 Jun 2022 22.27 CEST","Title":"Teslas account for 273 of nearly 400 US crashes involving driver-assist systems","Content":"US automakers reported nearly 400 crashes in 10 months involving vehicles with partially automated driver-assist systems, including 273 with Teslas, according to statistics released by federal safety regulators.\nTesla investigation deepens after more than a dozen US ‘Autopilot’ crashes\nRead more\nThe National Highway Traffic Safety Administration (NHTSA) cautioned against using the numbers to compare automakers, saying it did not weight them by the number of vehicles from each manufacturer or how many miles those vehicles traveled.\nAutomakers reported crashes from July 2021 through 15 May 2022 under an order from the agency.\nSteven Cliff, the NHTSA administrator, said: “As we gather more data, NHTSA will be able to better identify any emerging risks or trends and learn more about how these technologies are performing in the real world.”\nTesla crashes happened while vehicles were using Autopilot, Full Self-Driving, Traffic Aware Cruise Control or other driver-assist systems that have some control over speed and steering. The company has about 830,000 such vehicles on the road.\nThe next closest of a dozen automakers that reported crashes was Honda, with 90. Honda says it has about 6m vehicles on US roads with such systems. Subaru was next with 10. All other automakers reported five or fewer crashes.\nIn a June 2021 order, NHTSA told more than 100 automakers and tech companies to report serious crashes within one day of learning about them and to disclose less-serious crashes by the 15th day the following month.\nSix people were killed in crashes involving driver-assist systems and five were seriously hurt, NHTSA said. Of the deaths, five were in Teslas and one was reported by Ford. Three serious injuries were in Teslas. Honda and Ford each reported one.\nTesla’s crash number may appear elevated because it gets real-time crash reports. Other automakers do not, so their reports may come slower or crashes may not be reported at all, NHTSA said.\nTesla’s crashes accounted for nearly 70% of 392 reported. Although Tesla calls its systems Autopilot and Full Self-Driving, it says the vehicles cannot drive themselves and the drivers must be ready to intervene at all times.\nA message was left seeking comment from Tesla.\nAuto safety advocates said driver-assist and self-driving systems have potential to save lives, but not until NHTSA sets minimum performance standards and requires safety improvements.\n“It’s clear that US road users are unwitting participants in beta testing of automated driving technology,” said Cathy Chase, president of Advocates for Highway and Auto Safety.\nEd Markey, a US senator from Massachusetts, said although the NHTSA data had limitations, it was not isolated evidence that Tesla has ignored regulations and putting the public in danger.\nThere have been “a never ending parade of reports” of Teslas on automated systems rolling through stop signs or braking for no reason, he said. NHTSA also is investigating Teslas that crash into parked emergency vehicles.\n“As today’s data suggests, this contempt for auto safety laws has real-world consequences,” Markey said, urging NHTSA to take enforcement action.\nNHTSA has used the data to seek a recall, open investigations and provide information for existing inquiries, officials said.\n“This will help our investigators quickly identify potential defect trends that can emerge,” Cliff said. “These data will also help us identify crashes that we want to investigate and provide more information about how people in other vehicles interact with the vehicles.”\nNHTSA’s order also covered companies running fully autonomous vehicles: 25 reported a total of 130 crashes. Google spinoff Waymo led with 62, followed by Transdev Alternative Services with 34 and General Motors-controlled Cruise with 23.\nWaymo said it had more than 700 autonomous vehicles in its fleet. The company is running a ride-hailing service in Arizona and testing one in California. The company said all crashes happened at low speeds, with air bags inflating in two.\nIn 108 crashes involving fully autonomous vehicles, no fatalities were reported and there was only one serious injury. In most crashes, vehicles were struck from the rear.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about self-driving car crashes involving vehicles with partially automated driver-assist systems, including Teslas. It states that US automakers reported nearly 400 crashes in 10 months involving such vehicles, with 273 of them involving Teslas. The crashes with Teslas happened while vehicles were using Autopilot, Full Self-Driving, Traffic Aware Cruise Control, or other driver-assist systems that have some control over speed and steering. The report also mentions that six people were killed and three were seriously injured in these crashes, with five of the deaths occurring in Teslas. Tesla's crash number may appear elevated because it receives real-time crash reports. Additionally, it highlights that Tesla calls its systems Autopilot and Full Self-Driving but emphasizes that the vehicles cannot drive themselves, and drivers must be ready to intervene at all times. The news also includes comments from auto safety advocates and US senators, suggesting concerns about the safety and regulation of self-driving technologies.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/us-news\/2022\/jun\/15\/tesla-us-car-carashes-driver-assist-systems","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":93,"Time":"Updated 3:14 PM EDT, Wed June 15, 2022","Title":"Teslas using driver-assist systems were involved in 273 crashes over the past 9 months, according to NHTSA","Content":"Washington, DC\nCNN Business\n — \nThe National Highway Traffic Safety Administration released on Wednesday nine months of crash data from vehicles using driver-assist technologies like Tesla Autopilot as well as fully autonomous vehicles like Waymo’s robotaxis.\nNHTSA broke crash data into two categories based on the level of the autonomous systems: driver-assist systems – which offer speed and steering input – and fully autonomous technologies, which are intended to one day safely function without human intervention. NHTSA found that there have been 367 crashes in the last nine months involving vehicles that were using these driver-assist technologies. 273 of the incidents involved a Tesla system, either its “full self-driving” software or its precursor, Tesla Autopilot.\nThere were 130 crashes involving fully automated driving systems, 62 of which were Waymo crashes. Transdev, a shuttle operator, reported 34 crashes, and Cruise, which offers robotaxis for General Motors in San Francisco, reported 23.\nThe data lacks critical context like fleet size or the number of miles traveled, making it impossible to fairly compare the safety of the different technologies. Not all relevant crashes may be included in the data set, NHTSA said, because crash data recording may vary widely among manufacturers.\nA Tesla dashboard is shown as the Autopilot software disengages during a drive in Brooklyn, New York.\nChristopher Goodney\/Bloomberg\/Getty Images\n“I would advise caution before attempting to draw conclusions based only on the data we’re releasing. In fact, the data alone may raise more questions than they answer,” NHTSA administrator Steven Cliff told reporters in a briefing Tuesday.\nTwo of the technologies with the most reported crashes are also two of the most commonly used systems. Tesla Autopilot, for example, comes standard on all of its vehicles, unlike competing driver-assist systems from other automakers. Drivers describe using Autopilot regularly because they say it can make them feel less fatigued after long drives. Waymo, the other company with the most total crashes, operates the most extensive robotaxi service in the country, with operations in much of metropolitan Phoenix, Arizona and San Francisco.\nFor the first time, automakers and robotaxi operators have had to report to NHTSA data about crashes involving these vehicles. NHTSA says it will use the data to identify safety issues and intervene as necessary. Pony.ai, which is testing robotaxis in California, recalled three of its vehicles this year following data NHTSA gathered from this process.\nOf the 497 crashes total, 43% occurred in California. The state is home to Silicon Valley, making it a hotspot for testing new technologies.\nNHTSA found that of the 367 driver-assist crashes reported, there were six fatalities and five serious injuries.\nFederal investigators expand probe into Tesla Autopilot crashes\nThe safety risks of these new technologies have drawn the attention of safety advocates for years. There are not specific regulations for driver-assist systems, leaving automakers to market and describe the systems as they so choose.\nTesla’s Autopilot and “full self-driving” software have been especially controversial. NHTSA’s investigation into Teslas rear-ending first responders’ vehicles was expanded last week and could lead to a recall.\nThe National Transportation Safety Board has investigated fatal crashes involving Autopilot and called for the automaker to make changes, such as developing technology to more effectively sense the driver’s level of engagement and alert them when their engagement is lacking.\nTesla has released data since 2018 claiming that Autopilot has a lower crash rate per mile than typical driving. But safety experts caution that Tesla’s analysis compares apples to oranges, as most Autopilot driving takes place on highways, where crash rates per mile are much lower than all driving.\nTesla states that drivers using Autopilot must remain alert and be prepared to take full control of the vehicle at a moment’s notice. However, drivers using technologies like Autopilot risk becoming distracted, experts say.\nWaymo says it sees value in nationally standardized and uniform crash reporting as autonomous driving is developed.\nJustin Sullivan\/Getty Images\nA 2021 MIT study found that Tesla drivers looked away from the road more frequently while using Autopilot than when driving without the driver-assist system.\nNHTSA said that its investigation into Teslas rear-ending emergency vehicles while using Autopilot found that in 37 of 43 crashes with detailed car log data available, drivers had their hands on the wheel in the last second prior to the collision.\nFor years, Tesla detected torque on the wheel to determine if a driver was engaged. It’s begun to use an in-car camera for detecting distraction, which many safety experts say is a superior method, as cameras can track eye movement.\n“We see value in having nationally standardized and uniform crash reporting during this early stage of the development and deployment of autonomous driving technology, and there’s public benefit in NHTSA sharing its findings,” Waymo said in response to the data. Tesla did not respond to a request for comment.","Relevant":true,"Overview":"Based on the provided news article, here is the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Summary\": \"The National Highway Traffic Safety Administration (NHTSA) released nine months of crash data involving vehicles using driver-assist technologies, including Tesla Autopilot and fully autonomous vehicles like Waymo's robotaxis.\",\n        \"Details\": [\n            \"The news is relevant to self-driving car crashes because it discusses 367 crashes that occurred in the last nine months involving vehicles using driver-assist technologies, and 273 of these incidents involved a Tesla system, either its 'full self-driving' software or Tesla Autopilot.\",\n            \"The article also mentions six fatalities and five serious injuries in the 367 driver-assist crashes reported, highlighting the safety concerns associated with these technologies.\",\n            \"While the data lacks critical context like fleet size or the number of miles traveled, it provides valuable insights into the frequency of crashes involving self-driving features.\",\n            \"The article discusses the controversy and investigations surrounding Tesla's Autopilot system, including NHTSA's expanded investigation into Tesla rear-ending first responders' vehicles and the National Transportation Safety Board's calls for improvements in sensing driver engagement.\",\n            \"It also mentions the value of nationally standardized crash reporting as autonomous driving technology is developed, indicating the relevance of the data to ongoing discussions about regulating and improving self-driving technologies.\"\n        ]\n    }\n}\nIn summary, the news is relevant to self-driving car crashes as it provides data on crashes involving vehicles using driver-assist technologies, with a significant number of incidents related to Tesla systems. It also highlights safety concerns and ongoing investigations related to these technologies.","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2022\/06\/15\/cars\/nhtsa-autopilot-robotaxi-crash-data\/index.html","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":97,"Time":"MARCH 20, 2018","Title":"Fatal U.S. self-driving auto accident raises novel legal questions","Content":"By Tina Bellon\nNEW YORK (Reuters) - The death of a pedestrian hit by a self-driving Uber vehicle in Arizona this week could offer a test of who can be held legally responsible for accidents when a human is no longer at the wheel, legal experts said.\nTraffic passes an intersection just north of the location where a woman pedestrian was struck and killed by an Uber self-driving sport utility vehicle in Tempe, Arizona, U.S., March 19, 2018. REUTERS\/Rick Scuteri\nAny litigation that arises from the accident, the first fatality involving a fully autonomous vehicle, could pit the ride-hailing service against technology suppliers and the vehicle’s manufacturer, legal experts said. It could also provide a window into confidential indemnification agreements that companies developing self-driving car systems may have reached to shield themselves legally.\nIn Sunday’s accident, Elaine Herzberg, 49, was walking her bicycle outside the crosswalk on a four-lane road in the Phoenix suburb of Tempe when she was hit by the Volvo XC90 SUV, which was traveling in autonomous mode at about 40 miles per hour, police said.\nTempe Police Chief Sylvia Moir, who saw video of the crash, told the San Francisco Chronicle late Monday that it “would have been difficult to avoid this collision in any kind of mode (autonomous or human-driven) based on how she came from the shadows right into the roadway.”\nThe Tempe Police Department said in a statement later that the department does not determine fault in vehicle collisions. “Ultimately the investigation will be submitted to the Maricopa County Attorney’s Office for review and any potential charges,” it said.\nIt was not clear whether a lawsuit would be filed on behalf of Herzberg’s estate.\nBryant Walker Smith, a law professor at the University of South Carolina with expertise in self-driving cars, said most states, including Arizona, require drivers to exercise due care to avoid hitting pedestrians on a roadway, even if the pedestrians use an unmarked crosswalk.\nSergei Lemberg, a California-based lawyer who has brought several lawsuits against self-driving car manufacturers, said that Uber, Volvo, which manufactured the sport utility vehicle, and any companies that supplied self-driving technology could potentially be held liable if litigation ensued from the Tempe accident. The “safety” driver who was behind the wheel but who appears not to have operated the vehicle could also be named as a defendant, Lemberg said.\n“Suing all these parties would be my top thought right now,” he said.\nRELATED COVERAGE\nFactbox: How United States, others regulate autonomous vehicle testing\nUber said in a statement it was fully cooperating with local authorities investigating the accident.\nVolvo, the Swedish car brand owned by China’s Geely, confirmed its vehicle was involved in the crash but said the software controlling the SUV was not its own. Volvo said in 2015 it would accept full liability when one of its vehicles equipped with its own self-driving system was in autonomous mode. The company said in 2017 it had agreed to supply Uber with 24,000 self-driving vehicles.\nUntil now, most litigation over non-fatal accidents involving self-driving vehicles has been confidentially settled, according to legal experts. A case in which a motorcyclist sued General Motors GM.N after a collision last year with one of the company's autonomous test cars in San Francisco is pending.\nGM has said in a statement regarding the lawsuit that in its collision report the San Francisco Police Department determined that the autonomous vehicle was not at fault in the accident.\nLegal experts said liability in the Tempe case would depend on the facts of the accident and the results of U.S. National Highway Safety Administration and National Transportation Safety Board investigations.\nWITH NO DRIVER, SPOTLIGHT ON DESIGN\nCar accident litigation usually turns on whether a driver acted negligently, or failed to exercise a reasonable level of care.\nBy contrast, a lawsuit involving an autonomous vehicle could revolve around whether the self-driving system had a design defect, legal experts said.\nDesign defect claims do not require a finding of fault or negligence. To prevail, a plaintiff must show only that a product had an inherent design defect that would render it unsafe.\nAutomakers and software writers could counter with detailed data gathered by on-board sensors on how cars behaved during a collision, experts said, to show that it was impossible for the vehicle to avoid a collision and that all the systems functioned properly.\nOne question that would likely arise in litigation is whether Uber, Volvo or any companies that supplied self-driving technology entered into indemnification agreements, in which an entity assumes another company’s liability costs.\nPaul Rafferty, a California-based lawyer who represents carmakers, said such agreements in the autonomous vehicle world are confidential and disputes over indemnification are generally arbitrated or mediated behind closed doors.\nVolvo declined to comment on whether it had entered into indemnification agreements with Uber.\nJennifer Dukarski, a Detroit-based lawyer representing suppliers of autonomous-vehicle technology, said the lawsuit filed by the San Francisco motorcyclist against GM raised only negligence claims.\nSo if any litigation in the Arizona accident alleges design defect claims, it could set legal precedent and be “very impactful” on the nascent autonomous vehicle industry, she said.\nRegardless of who is at fault, the University of South Carolina’s Smith said he would expect Uber, which has faced past scrutiny over its business practices, to settle any litigation quickly to avoid the public spotlight.\n“Only if Uber believes that it was wholly without fault could I see this case going to trial,” he said.\nReporting by Tina Bellon in New York; Additional reporting by Paul Lienert in Detroit; Editing by Noeleen Walder and Frances Kerry\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the JSON format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"This news is about a fatal self-driving car accident involving a self-driving Uber vehicle in Arizona. The accident resulted in the death of a pedestrian, Elaine Herzberg, who was hit by the self-driving Volvo XC90 SUV while walking her bicycle outside the crosswalk. The vehicle was traveling in autonomous mode at about 40 miles per hour. Legal experts are discussing the potential liability of various parties involved, including Uber, Volvo (the vehicle manufacturer), and companies supplying self-driving technology. The news also mentions the possibility of a lawsuit and how it could set legal precedent in the autonomous vehicle industry. Additionally, the news highlights the challenges of determining liability in self-driving car accidents, specifically regarding design defects and indemnification agreements.\"\n}\nThe news is relevant to self-driving car crashes as it discusses a fatal accident involving a self-driving vehicle and the legal implications surrounding it.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-autos-selfdriving-uber-liability-anal-idUSKBN1GW2SP","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":90,"Time":"Thu 11 Jan 2018 13.43 CET","Title":"Ford-backed self-driving car in crash that sent two to hospital","Content":"A self-driving test car from Ford-backed startup Argo was involved in an accident on Wednesday that sent two people to hospital.\nThe car, a modified Ford Fusion, was struck by a box van running a red light in the East Allegheny area of Pittsburgh. The collision smashed in the doors on the passenger side and blew out the back window, according to local reports.\nA Pittsburgh city spokesperson said two of the four occupants of the vehicle were injured and transported to hospital in a stable condition before being released later on Wednesday.\n“We’re aware that an Argo AI test vehicle was involved in an accident. We’re gathering all the information. Our initial focus is on making sure that everyone involved is safe,” a Ford spokesperson said in a statement.\nA #selfdriving @argoai_llc vehicle involved in accident near 16th St Bridge in #Pittsburgh. @PghPolice on scene. Follow #wpxi for more details. #autonomous #BreakingNews pic.twitter.com\/5oBeHkapyw\n— Jason G (@wpxijg) January 10, 2018\nIt is not yet known whether charges will be filed against the driver of the box van, or whether the Argo car was in self-driving mode at the time of the accident. Ford declined to comment on whether further testing would be suspended, but its team were involved in exhibiting self-driving technology at CES in Las Vegas this week, including an autonomous pizza delivery vehicle made in partnership with Dominos.\nFord invested $1bn into Pittsburgh-based startup Argo AI in February 2016 with plans for creating the self-driving technology to drive the automotive company’s autonomous vehicles.\nThe crash happened less than a mile away from Argo AI’s headquarters.\nAccidents involving self-driving cars have so far been relatively minor, with few producing injuries requiring hospital treatment. A test driver for Google’s self-driving car voluntarily went to hospital after a collision in September 2016, but most incidents, such as the self-driving bus crash in Las Vegas in November, are not much more than fender benders.\nThis isn’t the first incident involving autonomous vehicles in Pittsburgh, as the city has stepped up efforts to promote self-driving car research. In September, an Uber self-driving Volvo XC90, which was being driven by a human at the time, was involved in an accident, grounding the firm’s autonomous testing for several hours.\nWhy did Ford build a ‘fake driverless car’ using a man dressed as a seat?","Relevant":true,"Overview":"Based on the provided news article, it appears to be about a self-driving car crash. Here's the JSON format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"A self-driving test car from Ford-backed startup Argo was involved in an accident on Wednesday that sent two people to the hospital. The car, a modified Ford Fusion, was struck by a box van running a red light in the East Allegheny area of Pittsburgh. The collision smashed in the doors on the passenger side and blew out the back window. Two of the four occupants of the vehicle were injured and transported to the hospital in a stable condition before being released later on Wednesday. It is not yet known whether charges will be filed against the driver of the box van, or whether the Argo car was in self-driving mode at the time of the accident.\"\n}\nThe news article contains specific information about a self-driving test car (a modified Ford Fusion) being involved in an accident, causing injuries to two individuals. This information is relevant to self-driving car crashes.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/jan\/11\/ford-self-driving-car-crash-hospital-argo-ai-pittsburgh","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":98,"Time":"MARCH 5, 2019","Title":"Uber not criminally liable in fatal 2018 Arizona self-driving crash: prosecutors","Content":"By David Shepardson, Heather Somerville\n(Reuters) - Uber Technologies Inc is not criminally liable in a March 2018 crash in Tempe, Arizona, in which one of the company’s self-driving cars struck and killed a pedestrian, prosecutors said on Tuesday.\nUber not criminally liable in fatal crash\n Uber not criminally liable in fatal crash\n01:49\nThe Yavapai County Attorney said in a letter made public that there was “no basis for criminal liability” for Uber, but that the back-up driver, Rafaela Vasquez, should be referred to the Tempe police for additional investigation.\nProsecutors’ decision not to pursue criminal charges removes one potential headache for the ride-hailing company as the company’s executives try to resolve a long list of federal investigations, lawsuits and other legal risks ahead of a hotly anticipated initial public offering this year.\nThe crash involved a Volvo XC90 sport utility vehicle that Uber was using to test self-driving technology. The fatal accident was a setback from which the company has yet to recover; its autonomous vehicle testing remains dramatically reduced.\nThe accident was also a blow to the entire autonomous vehicle industry and led other companies to temporarily halt their testing. Scrutiny has mounted on the nascent technology, which presents fatal risks but has minimal oversight from regulators.\nVasquez, the Uber back-up driver, could face charges of vehicular manslaughter, according to a police report in June. Vasquez has not previously commented and could not immediately be reached on Tuesday.\nBased on a video taken inside the car, records collected from online entertainment streaming service Hulu and other evidence, police said last year that Vasquez was looking down and streaming an episode of the television show “The Voice” on a phone until about the time of the crash. The driver looked up a half-second before hitting Elaine Herzberg, 49, who died from her injuries.\nFILE PHOTO: Traffic passes an intersection just north of the location where a woman pedestrian was struck and killed by an Uber self-driving sport utility vehicle in Tempe, Arizona, U.S., March 19, 2018. REUTERS\/Rick Scuteri\/File Photo\nPolice called the incident “entirely avoidable.”\nYavapai County Attorney’s Office, which examined the case at the request of Maricopa County where the accident occurred, did not explain the reasoning for not finding criminal liability against Uber. Yavapai sent the case back to Maricopa, calling for further expert analysis of the video to determine what the driver should have seen that night.\nAn Uber spokeswoman declined to comment on the letter.\nThe National Transportation Safety Board and National Highway Traffic Safety Administration are still investigating.\nThe Maricopa County Attorney’s Office did not immediately comment on Tuesday.\nUber in December filed confidentially for an initial public offering and is expected to seek a valuation of up to $120 billion. Its self-driving program, which costs hundreds of millions of dollars and does not generate revenue yet, is likely to come under scrutiny by investors.\nThe ride-hailing company, which last year lost about $3.3 billion, is betting on a transition to self-driving cars to eliminate the need to pay drivers.\nAt an autonomous vehicles conference in Silicon Valley last week, industry leaders lamented the loss of confidence from the public, regulators and investors that lingers a year after the Uber crash. There is no consensus on safety standards for the industry.\nIn March 2018, authorities in Arizona suspended Uber’s ability to test its self-driving cars. Uber also voluntarily halted its entire autonomous car testing program and left Arizona.\nIn December, Uber resumed limited self-driving car testing in Pittsburgh, restricting the cars to a small loop they can drive only in good weather. The company is now testing with two people in the front seat and more strictly monitors safety drivers. The company also said last year it made improvements to the vehicles’ self-driving software.\nUber has not resumed testing in San Francisco or Toronto, where it previously had programs.\nReporting by David Shepardson. Additional reporting by Heather Somerville in San Francisco and David Schwartz in Phoenix; Editing by Grant McCool and Cynthia Osterman\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Based on the provided news article, it is relevant to self-driving car crashes, and here is the JSON format response as requested:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is about a March 2018 self-driving car crash in Tempe, Arizona, involving an Uber self-driving car that struck and killed a pedestrian, Elaine Herzberg. Prosecutors have stated that Uber is not criminally liable for the crash. The article also mentions that the backup driver, Rafaela Vasquez, could face charges of vehicular manslaughter, as evidence suggests she was distracted, looking down, and streaming a TV show on her phone at the time of the crash. This incident had a significant impact on Uber's autonomous vehicle testing, leading to a suspension of their self-driving car testing program in Arizona. It also drew attention to the safety concerns and regulatory oversight of self-driving technology in the autonomous vehicle industry.\"\n}\nSo, the news is relevant to self-driving car crashes, and the specific information provided in the article supports this relevance.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-uber-crash-autonomous\/uber-not-criminally-liable-in-fatal-2018-arizona-self-driving-crash-prosecutors-idINKCN1QM2O8","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":92,"Time":"5:51 AM GMT+2•August 22, 2022","Title":"Human drivers in UK won’t be liable for accidents when vehicle is self-driving","Content":"cameraImage Credits: Alexander Spatari \/ Getty Images\nManufacturers operating self-driving vehicles in the U.K. will be liable for a vehicle’s actions when in autonomous mode, according to the country’s new roadmap to achieve a widespread rollout of self-driving vehicles by 2025.\nThe British government unveiled the roadmap over the weekend, announcing $119 million in funding for AV projects and an additional $41 million for research to support safety and new legislation.\nThe roadmap stated that new legislation will build on existing laws and state that manufacturers are responsible for the actions of self-driving vehicles, “meaning a human driver would not be liable for incidents related to driving while the vehicle is in control of driving.”\nThis distinction could set a precedent globally, where liability in the case of autonomous vehicle accidents can still be somewhat of a gray area. When an autonomous Uber test vehicle killed a pedestrian in 2017, the human safety operator in the vehicle was charged with negligent homicide with a dangerous instrument. Uber faced no charges. And after a deadly crash in 2019 involving a Tesla vehicle operating in Autopilot, the driver, not Tesla, was charged with two felony counts of vehicular manslaughter with gross negligence.\nAs the country prepares to write new laws in regard to AVs, Britain has opened a consultation period on a “safety ambition” for self-driving vehicles to be as safe as a human driver. The results from the consultation would help form the standards for operating autonomous vehicles on public roads, as well as potential sanctions if those standards are not met.\n“It is still quite a big leap from assisted driving, where the driver is still in control, to self-driving, where the car takes control. It is important that the government does study how these vehicles would interact with other road users on different roads and changing weather conditions,” AA president, Edmund King, said in a statement. “However the ultimate prize, in terms of saving thousands of lives and improving the mobility of the elderly and the less mobile, is well worth pursuing.”\nAside from drafting legislation to ensure the safety of autonomous technology, the U.K. wants to take advantage of the emerging market for AVs, which it values at around $50 billion and estimates could create 38,000 new jobs. Of the total investment amount, about $24 million will be used to help kick-start commercial self-driving services, such as autonomous grocery delivery or shuttle pods in airports. Another $7 million will be put toward further market research and to support commercialization.\n“We want the UK to be at the forefront of developing and using this fantastic technology, and that is why we are investing millions in vital research into safety and setting the legislation to ensure we gain the full benefits that this technology promises,” said transport secretary Grant Shapps in a statement.\nThe government said vehicles that can drive themselves on motorways could be available to buy within the next year for drivers with valid driver’s licenses. While Teslas are some of the best-selling vehicles in the U.K., current legislation does not allow for British drivers to try out Tesla’s full self-driving beta, its advanced driver assistance system (ADAS) that comes with a variety of automated driving functions. New legislation could open the door not only for ride-hail, delivery and public transportation, but also for cars equipped with higher levels of ADAS.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving car-related matters. It discusses liability for self-driving vehicles in the UK, where manufacturers will be held responsible for the actions of self-driving vehicles when in autonomous mode. This means that human drivers would not be liable for incidents related to driving while the vehicle is in control. The news also mentions the UK's roadmap for self-driving vehicle deployment by 2025, funding for AV projects, research support, and the importance of new legislation to ensure the safety of autonomous technology. Additionally, it highlights the potential for self-driving vehicles on UK roads, job creation, and the development of the AV market.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/08\/21\/human-drivers-in-uk-wont-be-liable-for-accidents-when-vehicle-is-self-driving\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":88,"Time":"Published 12:17 AM GMT+2, October 19, 2022","Title":"11 more crash deaths are linked to automated-tech vehicles","Content":"DETROIT (AP) — Eleven people were killed in U.S. crashes involving vehicles that were using automated driving systems during a four-month period earlier this year, according to newly released government data, part of an alarming pattern of incidents linked to the technology.\nTen of the deaths involved vehicles made by Tesla, though it is unclear from the National Highway Traffic Safety Administration’s data whether the technology itself was at fault or whether driver error might have been responsible.\nThe 11th death involved a Ford pickup. The automaker said it has to report fatal crashes to the government quickly, but it later determined that the truck was not equipped with its partially automated driving system.\nThe deaths included four crashes involving motorcycles that occurred during the spring and summer: two in Florida and one each in California and Utah. Safety advocates note that the deaths of motorcyclists in crashes involving Tesla vehicles using automated driver-assist systems such as Autopilot have been increasing.\nThe new fatal crashes are documented in a database that NHTSA is building in an effort to broadly assess the safety of automated driving systems, which, led by Tesla, have been growing in use. Tesla alone has more than 830,000 vehicles on U.S. roads with the systems. The agency is requiring auto and tech companies to report all crashes involving self-driving vehicles as well as autos with driver assist systems that can take over some driving tasks from people.\nThe 11 new fatal crashes, reported from mid-May through September, were included in statistics that the agency released Monday. In June, the agency released data it had collected from July of last year through May 15.\nThe figures that were released in June showed that six people died in crashes involving the automated systems, and five were seriously hurt. Of the deaths, five occurred in Teslas and one a Ford. In each case, the database says that advanced driver assist systems were in use at the time of the crash.\nMichael Brooks, executive director of the nonprofit Center for Auto Safety, said he is baffled by NHTSA’s continued investigations and by what he called a general lack of action since problems with Autopilot began surfacing back in 2016.\n“I think there’s a pretty clear pattern of bad behavior on the part of Tesla when it comes to obeying the edicts of the (federal) safety act, and NHTSA is just sitting there,” he said. “How many more deaths do we need to see of motorcyclists?”\nBrooks noted that the Tesla crashes are victimizing more people who are not in the Tesla vehicles.\n“You’re seeing innocent people who had no choice in the matter being killed or injured,” he said.\nA message was left Tuesday seeking a response from NHTSA.\nTesla’s crash number may appear elevated because it uses telematics to monitor its vehicles and obtain real-time crash reports. Other automakers lack such capability, so their crash reports may emerge more slowly or may not be reported at all, NHTSA has said.\nNHTSA has been investigating Autopilot since August of last year after a string of crashes since 2018 in which Teslas collided with emergency vehicles parked along roadways with flashing lights on. That investigation moved a step closer to a recall in June, when it was upgraded to what is called an engineering analysis.\nIn documents, the agency raised questions about the system, finding that the technology was being used in areas where its capabilities are limited and that many drivers weren’t taking steps to avoid crashes despite warnings from the vehicle.\nNHTSA also reported that it has documented 16 crashes in which vehicles with automated systems in use hit emergency vehicles and trucks that were displaying warning signs, causing 15 injuries and one death.\nThe National Transportation Safety Board, which also has investigated some of the Tesla crashes dating to 2016, has recommended that NHTSA and Tesla limit Autopilot’s use to areas where it can safely operate. The NTSB also recommended that NHTSA require Tesla to improve its systems to ensure that drivers are paying attention. NHTSA has yet to act on the recommendations. (The NTSB can make only recommendations to other federal agencies.)\nMessages were left Tuesday seeking comment from Tesla. At the company’s artificial intelligence day in September, CEO Elon Musk asserted that, based on the rate of crashes and total miles driven, Tesla’s automated systems were safer than human drivers — a notion that some safety experts dispute.\n“At the point of which you believe that adding autonomy reduces injury and death, I think you have a moral obligation to deploy it,” Musk said. “Even though you’re going to get sued and blamed by a lot of people. Because the people whose lives you saved don’t know that their lives were saved. And the people who do occasionally die or get injured, they definitely know, or their state does, that it was, whatever, there was a problem with Autopilot.”\nTeslas with automated systems have driven more than 3 million vehicles on the road, Musk said.\n“That’s a lot of miles driven every day. And it’s not going to be perfect. But what matters is that it is very clearly safer than not deploying it.”\nIn addition to Autopilot, Tesla sells “Full Self-Driving” systems, though it says the vehicles cannot drive themselves and that motorists must be ready to intervene at all times.\nThe number of deaths involving automated vehicles is small compared with the overall number of traffic deaths in the U.S. Nearly 43,000 people were killed on U.S. roads last year, the highest number in 16 years, after Americans returned to the roads as the pandemic eased. Authorities blamed reckless behavior such as speeding and driving while impaired by drugs or alcohol for much of the increase.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is about self-driving car crashes. The news reports that eleven people were killed in U.S. crashes involving vehicles that were using automated driving systems during a four-month period earlier this year. Ten of the deaths involved vehicles made by Tesla. The report also mentions that safety advocates note that the deaths of motorcyclists in crashes involving Tesla vehicles using automated driver-assist systems, such as Autopilot, have been increasing. The National Highway Traffic Safety Administration (NHTSA) is building a database to assess the safety of automated driving systems, which includes these fatal crashes. The report provides statistics on the number of deaths and injuries related to automated systems, mentions NHTSA's investigation into Autopilot crashes, and includes recommendations from the National Transportation Safety Board (NTSB) regarding the safe use of Autopilot. Overall, the news provides detailed information on self-driving car crashes and their implications.\"\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/technology-business-traffic-government-and-politics-a16c1aba671f10a5a00ad8155867ac92","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":100,"Time":null,"Title":"Toyota halts all self-driving e-Palette vehicles after Olympic village accident","Content":"Aug 27 (Reuters) - Toyota Motor (7203.T) said on Friday it had suspended all self-driving e-Palette transportation pods at the Tokyo Paralympic Games village, a day after one of the vehicles collided with and injured a visually impaired pedestrian.\nIn a YouTube video, Toyota Chief Executive Akio Toyoda apologized for the incident and said he offered to meet the person but was unable to do so.\n“A vehicle is stronger than a person, so I was obviously worried about how they were,” he said, answering questions about the incident.\nAdvertisement · Scroll to continue\nToyoda said the accident showed the difficulty for the self-driving vehicle to operate in the special circumstances of the village during the Paralympics with people there who are visually impaired or have other disabilities.\n“It shows that autonomous vehicles are not yet realistic for normal roads,” he said.\nThe vehicle had stopped at a T junction and was about to turn under manual control of the operator, who was using the vehicle's joystick control, when the vehicle hit the athlete going at around 1 or 2 kilometres an hour, Toyoda said. He said Paralympic officials had told him that the athlete, who remained conscious, was taken to the athlete village medical centre for treatment and was able to walk back to their residence.\nAdvertisement · Scroll to continue\nToyota Motor Corporation President Akio Toyoda, arrives in their e-Palette autonomous concept vehicle to give his presentation at the Tokyo Motor Show, in Tokyo, Japan October 23, 2019.... Acquire Licensing Rights Read more\nThe athlete is due to compete on Saturday at 1030 local time, Toyoda said.\nThe e-Palette, a fully autonomous battery-electric vehicle, was adapted specifically for use during the Tokyo Olympic and Paralympic Games, with large doors and electric ramps to allow groups of athletes to board quickly.\nThe company said it was cooperating with a local police probe to determine the cause of the accident, adding that it would also conduct its own investigation.\nAdvertisement · Scroll to continue\nIn its statement, the Japanese automaker said it would continue to coordinate closely with the Tokyo Organising Committee of the Olympic and Paralympic Games to prevent any further incidents.\nParalympic Games organizers were not immediately available for comment.\nThe 2020 Paralympic Games began on Tuesday in Tokyo, even as Japan struggles with its worst COVID-19 outbreak, with record daily cases and an overwhelmed medical system. read more\nReporting by Sanjana Shivdas in Bengaluru and Tim Kelly in Tokyo; Editing by Ramakrishnan M., Joseph White and Steve Orlofsky\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here's the analysis of the provided news article in JSON format:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Reason\": \"The news is about self-driving car-related incident.\",\n    \"Details\": [\n      \"Toyota has suspended all self-driving e-Palette transportation pods at the Tokyo Paralympic Games village.\",\n      \"One of the self-driving vehicles collided with and injured a visually impaired pedestrian.\",\n      \"Toyota's Chief Executive, Akio Toyoda, apologized for the incident and expressed concern for the injured person.\",\n      \"The incident highlights the challenges self-driving vehicles face in special circumstances, such as the Paralympics village with visually impaired individuals.\",\n      \"The vehicle was under manual control at the time of the accident, but it was part of the autonomous e-Palette fleet.\",\n      \"Toyota is cooperating with a local police probe to determine the cause of the accident and conducting its own investigation.\",\n      \"The news mentions the adaptation of the e-Palette as a fully autonomous vehicle for use during the Tokyo Olympic and Paralympic Games.\"\n    ]\n  }\n}\nThis news is relevant to self-driving car crashes because it reports an incident involving a self-driving e-Palette vehicle colliding with a visually impaired pedestrian in the Tokyo Paralympic Games village. Toyota suspended all self-driving e-Palette vehicles as a result of this incident, and there are discussions about the challenges of autonomous vehicles operating in special circumstances. The news also mentions investigations into the accident and the adaptation of the e-Palette as an autonomous vehicle for the Olympics.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/toyota-halts-all-self-driving-e-pallete-vehicles-after-olympic-village-accident-2021-08-27\/","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":99,"Time":"Thu 12 Jul 2018 11.27 CEST","Title":"Uber scales back self-driving car tests in wake of fatal crash","Content":"Uber laid off 100 of its self-driving car backup drivers in Pittsburgh on Wednesday as it scales back its testing in the wake of its fatal crash in March.\nThe ride-hailing firm made 55 new mission specialist positions available to replace them, according to a report by Quartz, with the intention of returning to on-the-road testing but with a reduced fleet of cars.\nThe new drivers will be trained in both public road and track testing, and are expected to provide a greater level of technical feedback than the company’s previous safety backup drivers. Uber has used mission specialists before to operate its cars in difficult scenarios and with new equipment on test tracks.\n“Our team remains committed to building safe self-driving technology, and we look forward to returning to public roads in the coming months,” said an Uber spokesperson.\nUber suspended its self-driving operations after the fatal crash in Tempe, Arizona, in March. An investigation into the crash found that the car spotted the pedestrian but chose not to react. According to police, the safety driver that was meant to take evasive action was watching a TV programme on her phone at the time of the crash.\nThe company was later banned from operating self-driving cars in Arizona, forcing Uber to shutdown its testing programme and lay off 300 staff in May. Uber pulled all testing in San Francisco, Toronto and Pittsburgh as a result of the crash. The company has said it intends to restart testing in Pittsburgh, its first self-driving development site, before the end of 2018.\nUber has operated with a single backup driver in its self-driving test cars since November 2017, having previously required two people in the vehicles at all times. Google sibling Waymo’s test cars also operate with single drivers but the majority of other companies still operate with two test drivers at any one time.\nUber crash shows ‘catastrophic failure’ of self-driving technology, experts say","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving car crashes because it discusses Uber's decision to scale back its self-driving car tests in the wake of a fatal crash that occurred in March. The crash investigation found that the self-driving car spotted a pedestrian but chose not to react, leading to the fatal incident. Uber suspended its self-driving operations, was banned from operating self-driving cars in Arizona, and laid off staff as a result of the crash. The article also mentions that Uber plans to restart testing in Pittsburgh, its first self-driving development site, before the end of 2018. Overall, the news provides specific information about the fatal crash, Uber's response, and its impact on self-driving car testing.\"\n}\ncase_text_end=======","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/jul\/12\/uber-scales-back-self-driving-car-tests-in-wake-of-fatal-crash","query_name":"updated_file_self-driving_car_crashes.xlsx"}
{"num":15,"Time":"16 August 2021","Title":"Tesla Autopilot: US opens official investigation into self-driving tech","Content":"Tesla Autopilot: US opens official investigation into self-driving tech\nPublished\n16 August 2021\nShare\nIMAGE SOURCE,\nREUTERS\nThe US federal agency in charge of road safety is opening an official investigation into Tesla's \"self-driving\" Autopilot system.\nThe National Highway Traffic Safety Administration (NHTSA) said it was acting following 11 Tesla crashes since 2018 involving emergency vehicles.\nIn some cases, the Tesla vehicles \"crashed directly into the vehicles of first responders\", it said.\nThe investigation will cover roughly 765,000 Tesla cars made since 2014.\nThat includes those in the Model Y, Model X, Model S and Model 3, the NHTSA said - the entire current range.\n'Control at all times'\nThe agency was primarily concerned with an apparent inability of Tesla vehicles to cope with vehicles stopped in the road - specifically emergency vehicles attending an incident.\nAmong the list of cases was one where a Tesla \"ploughed into the rear\" of a parked fire engine attending an accident, and another in which a parked police car was struck.\nThe NHTSA said it was opening its preliminary investigation into \"the technologies and methods used to monitor, assist, and enforce the driver's engagement\", while using Autopilot.\nIt said that in the 11 crashes that prompted its investigation, either Autopilot or a system called Traffic Aware Cruise Control had been active \"just prior\" to the collisions.\nTesla Autopilot 'tricked' to run without driver\nTesla crash driver 'was playing video game'\nThe assistive technology allows the car to automatically steer, accelerate and brake.\nBut it has come under fire for being misleading, as it does not automatically drive the car and drivers are required to maintain control and attention at all times.\nTesla has marketed the feature as an \"Autopilot\" and promised \"full self-driving\", which is now available to some users in a beta version.\nUsers have abused the system frequently in the past, with examples ranging from using their phones while the car drives unattended to switching car seats and leaving no driver at the wheel.\nMedia caption,\nThis driver received a ban after being caught in the passenger seat in England\nIn a statement, an NHTSA spokesperson said: \"No commercially available motor vehicles today are capable of driving themselves. Every available vehicle requires a human driver to be in control at all times.\"\nThe investigation's supporting documents do, however, note the challenging circumstances involved in many of the collisions.\n\"Most incidents took place after dark and the crash scenes encountered included scene control measures such as first responder vehicle lights, flares, an illuminated arrow board, and road cones,\" it reads.\nIt comes days ahead of an event to showcase the car company's software.\nChief executive Elon Musk had previously announced 19 August as \"Tesla AI Day\", which he said would showcase the progress of the firm's artificial intelligence systems - with a view to recruiting AI experts to the firm.\nTesla disbanded its public relations team in October 2020 and cannot be reached for comment.\nRelated Topics\nCar Technology\nTesla\nDriverless cars\nArtificial intelligence\nUnited States\nCar industry\nMore on this story\nTesla hit police car 'while using Autopilot'\nPublished\n30 May 2018\nTesla Autopilot 'tricked' to run without driver\nPublished\n23 April 2021\nTesla's 'Autopilot' misleading, Germany rules\nPublished\n15 July 2020\nTesla crash driver 'was playing video game'\nPublished\n26 February 2020","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it reports an official investigation by the National Highway Traffic Safety Administration (NHTSA) into Tesla's Autopilot system. The investigation was initiated due to 11 Tesla crashes since 2018 involving emergency vehicles, with some cases where Tesla vehicles 'crashed directly into the vehicles of first responders.' The NHTSA is concerned about the inability of Tesla vehicles to cope with vehicles stopped in the road, especially emergency vehicles attending incidents. Autopilot or Traffic Aware Cruise Control had been active 'just prior' to these collisions. Additionally, the news highlights concerns about Tesla's marketing of the feature as 'Autopilot' and 'full self-driving,' which has led to users misusing the system, such as driving unattended or leaving no driver at the wheel. This information demonstrates a clear connection to self-driving technology malfunctions.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-58232137","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":16,"Time":"Updated 8:50 PM EDT, Thu April 22, 2021","Title":"Tesla owners warned of ‘full self-driving’ risks even before fatal crash","Content":"Washington, DC\nCNN Business\n — \nTesla’s “full self-driving” feature has attempted to drive under a railroad crossing arm while a speeding train passes. It’s nearly driven head on into a concrete wall of a parking garage, attempted ill-advised left turns, clipped at least one curb, and at least one driver was able to set a maximum speed of 90 mph on a street where the posted speed limit was 35 mph, according to videos posted on social media.\nThese drivers knew they weren’t using a foolproof system, and that there would be glitches as they had agreed to test early versions of the regularly updating “full self-driving” software for Tesla. The company warned them of limitations, and their need to be attentive.\nExperts worry that the name of the feature implies a greater functionality than what Tesla is actually offering. But the risks of “full self-driving” don’t appear to be holding Tesla back from a broad beta release of the feature. Tesla is preparing a wide rollout even as some of the Tesla loyalists testing the feature raise concerns about what will come next.\nSome Tesla enthusiasts spoke out even before two people were killed in a Tesla over the weekend when it crashed into some trees. Police said that one occupant had been in the front passenger seat, and the other had been in one of the rear seats. There was no one in the driver’s seat, the police said. The National Highway Traffic Safety Administration said Monday that is investigating the crash.\nTwo people died in a Tesla crash in Spring, Texas, over the weekend.\nThe police statement that there was no driver behind the wheel suggests that Autopilot, the widely available precursor to “full self-driving,” may have been active and, if so, was being used inappropriately.\nTesla CEO Elon Musk said Monday that data logs recovered so far show Autopilot was not enabled. But Musk did not rule out that future findings could reveal Autopilot was in use. He also did not share an alternative theory for the crash.\nTesla did not respond to multiple requests for comment, and generally does not engage with the professional news media.\nThe long road to “full self-driving”\nTesla says that the “full self-driving” system can change lanes, navigate roads, and stop for traffic signals. Tesla has promised the feature since 2016, but the company only began to let a small group of drivers test an early version of it last fall. Musk said that about 2,000 Tesla owners were testing “full self-driving” as of March. The company is preparing a wider rollout with what it calls a significantly upgraded system than the one seen in the videos already, and with Musk tweeting that he would be “surprised” if a wide beta release isn’t available by some time in June.\nThough the name implies a high degree of autonomy, drivers must stay alert, keep their hands on the wheel and maintain control of their cars while using the function, according to Tesla. While the initial rollout was rocky last October, its beta testers have described it as improving in social media posts, and Musk has said on Twitter that it is “getting mature.”\nBut the system’s limitations have concerned some of Tesla’s enthusiastic supporters. YouTube videos of “full self-driving” in beta testing have shown the steering wheel jerk back and forth unpredictably.\nTeslas using a version of the “full self-driving” beta have at times attempted seemingly dangerous left turns – pulling in front of looming high-speed traffic, or slowly making a turn, triggering uncomfortable drivers to push the accelerator to get out of harm’s way.\nTesla’s full self-driving software, or FSD, is technically a driver-assist system, so American regulators allow beta versions of it to be tested on public roads. There are stiffer restrictions on driver-assist systems in Europe, where Tesla offers a more limited suite of autonomous driving features.\nAnd even when the system does appear to be working as intended, Tesla says that drivers are supposed to remain attentive and be prepared to take over at any time. But some worry that these guidelines won’t be heeded.\nCalling for caution\nAI DRIVR, a YouTuber who posts Tesla videos and is testing “full self-driving” already, has said on social media that he’s nervous about a large population getting the feature, and says people are bound to abuse it.\nLike other social media users who post frequently about Tesla’s “full self-driving” software, AI DRIVR said he had an NDA, and, when contacted by CNN, he said he was not able to speak to CNN directly.\n“Please let’s not screw this up and make Tesla regret their decision and the freedom that they are giving people,” AI DRIVR said.\nHe pointed to the controversial video in which a young man whose Tesla is using Autopilot, the company’s precursor to “full self-driving,” climbs out of the driver’s seat and lies down under a blanket in the back of the Tesla as it appears to drive down a highway. Tesla has safeguards in place to prevent misuse of Autopilot, such as requiring a seatbelt to be on, and detecting torque on the steering wheel, but a driver could work around the safety measures. The man who goes by Mr. Hub on YouTube, did not respond to a request for comment.\n“This kid is playing Russian roulette without even realizing it,” AI DRIVR said of the video.\nIn a series of tweets in March, Musk said that there have been no accidents with FSD though he did not give details on how he was defining “accident.” But AI DRIVR posted a video in which his car hit a curb making a turn while in FSD mode. He said his vehicle was not damaged because of a plastic protection device that he’d previously installed, and which could be replaced.\nPushing Tesla's FSD BETA over the limits\n“The beta is at a point where it can behave amazingly well and then the next second does something very unpredictable,” he said in a YouTube video. One shortcoming he claimed he experienced while using the beta version of “full self-driving” was his Tesla sometimes swerving on highways around semi trucks, when there was no clear reason to do so. In a YouTube video he speculated that one of the Tesla’s side cameras could be to blame as it’s obstructed by the trucks. AI DRIVR did not post video footage of his Tesla behaving in this way.\nRaj Rajkumar, a Carnegie Mellon University professor who studies autonomous vehicles, told CNN Business that the camera on the side of the Tesla may essentially see a flat surface (the side of the truck) with the same color and texture, and incorrectly conclude that something is very close.\nTesla, like other self-driving companies, uses cameras to see objects. Tesla says its vehicles have eight cameras, 12 ultrasonic sensors and a radar. But Tesla says it does not rely on lidar and plans to soon stop using radar. Both are sensors that are standard in the rest of the industry, and helpful in complementing the limitations of cameras, such as the challenges of seeing certain objects, like tractor-trailers. Teslas have been involved in high-profile deadly crashes, in which they failed to see the side of a tractor-trailer. Autopilot was found by the National Transportation Safety Board to have been used against Tesla’s own guidelines, and Tesla had apparently not restricted such use. Tesla said following the first NTSB investigation in 2017 that Autopilot is not fully self-driving technology and drivers need to remain attentive. It did not comment when the NTSB reiterated its findings in 2020 following another investigation.\nAn instrument panel with the Tesla Motors Inc. 8.0 software update illustrates the road ahead using radar technology inside a Model S P90D vehicle in the Brooklyn borough of New York, U.S., on Tuesday, Sept. 20, 2016.\n“Their side cameras very likely do not sense depth,” Rajkumar said. “With this ambiguity, the Tesla software may be concluding that it is best to be conservative and swerve.”\nTesla has a radar, but that is forward looking, so not aimed at trucks next to it. Ultrasonics are on all sides of the Tesla, but they’re really only useful for parking, Rajkumar said.\nRajkumar said that because “full self-driving” has “a lot of problems,” based on his assessment of beta testers’ YouTube footage, Tesla will need to prioritize what problems it addresses first and may not have had time to fully address the issue yet. Rajkumar has not tested the beta version of “full self-driving” himself.\nRajkumar said that one of the problems of “full self-driving” is its own name, which like Autopilot, he says, is extremely misleading. Drivers will get complacent and tragic crashes will happen, he said.\n“I have wondered for a long time why the Federal Trade Commission does not consider this as deceptive advertising, and why NHTSA has not forced Tesla to not use these names from a public safety standpoint,” Rajkumar said.\nThe National Highway Traffic Safety Administration said that it will take action as appropriate to protect the public against risks to safety, but that it does not have authority over advertising and marketing claims and directed questions to the Federal Trade Commission, which does provide oversight of this kind. The Federal Trade Commission declined to comment.\nJames Hendler, who studies artificial intelligence at Rensselaer Polytechnic Institute told CNN Business that another plausible explanation for Teslas allegedly swerving near semi trucks is that the angle that the sun reflecting off trucks makes the Tesla think the semis are extremely close.\n“These cars don’t think in terms we can understand. They can’t explain why they did it,” Hendler said.\nKeeping an eye on drivers\nThe concerns of Tesla owners echo the concerns of autonomous driving experts, who have long warned that “full self-driving” oversells what Teslas are capable of. There are also questions about if Tesla has sufficient driver monitoring systems to prevent abuse of “full self-driving.”\nAn MIT study of 19 drivers last year found that Tesla owners were more likely to look off-road when they use Autopilot, the precursor to “full self-driving,” compared to when they were in manual driving mode. Researchers said that more should be done to keep drivers attentive.\nRajkumar, the Carnegie Mellon professor, said that Tesla would be better off with a driver monitoring system similar to one used by GM, which uses an in-vehicle camera and infrared lights to monitor driver attention.\n“[It would] avoid the many shenanigans that some Tesla vehicle operators do to circumvent paying attention,” Rajkumar said.\nTeslas have a camera mounted in the passenger cabin that could theoretically monitor a driver. But Tesla does not appear to be using that camera to check if beta testers pay attention. Two beta testers of “full self-driving” have said that they have at times blocked their cameras: one, who posts on YouTube as “Dirty Tesla,” and “K10,” a Twitter-based Tesla enthusiast who has said she’s testing “full self-driving.”\n“They’re definitely not using it yet because I blocked mine, and they haven’t said anything,” Dirty Tesla said in an interview last month. “If they want it, they’ll let me know.”\nDirty Tesla declined to answer follow-up questions from CNN.\nMusk said on Twitter last month that Tesla has revoked the beta program from cars “where drivers did not pay sufficient attention to the road.” But CNN Business could not independently confirm that Tesla has revoked “full self-driving” access to a driver.\nThe feature will cost $10,000, but monthly subscriptions will be a more affordable way to use “full self-driving” for a short period of time, like a summer road trip. Musk has said they’ll be offered by July.\nTesla Raj, another YouTuber with early access to “full self-driving,” said in a recent video that there have been instances when he felt he was in danger of hitting another vehicle, or another vehicle hitting him, and he needed to take control of the car.\nPreparing for Tesla FSDbeta\n“Please be careful, please be responsible,” Tesla Raj said in his video.\nRicky Roy, who calls himself a huge Tesla fan, and an investor in the company, posted a video recently called, “the truth about Tesla full self-driving.” He said that important questions were getting lost in “crazy excitement about [a] future of robotaxis that will make people millions.”\nRoy alluded to Musk’s 2019 prediction that there would be a million robotaxis operating in 2020. Musk has said that “full self-driving” would make Teslas appreciating assets. Roy said in his video that he feared people would mistake Tesla’s “full self-driving,” which still requires a human driver ready to intervene at any time, for a fully autonomous vehicle, which does not need human supervision.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it highlights several incidents involving Tesla's 'full self-driving' feature attempting risky maneuvers. These incidents include trying to drive under a railroad crossing arm while a speeding train passes, nearly driving head-on into a concrete wall of a parking garage, making ill-advised left turns, clipping curbs, and allowing a driver to set a maximum speed of 90 mph on a street with a posted speed limit of 35 mph. Additionally, the news mentions concerns raised by experts and Tesla enthusiasts about the limitations and potential dangers of the 'full self-driving' feature, as well as an investigation by the National Highway Traffic Safety Administration into a fatal crash involving a Tesla with no one in the driver's seat. These incidents and concerns highlight the malfunctioning and potential risks associated with self-driving technology.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2021\/04\/21\/tech\/tesla-full-self-driving-launch\/index.html","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":12,"Time":"3:20 AM GMT+1•February 13, 2023","Title":"Tesla’s biggest hater airs Super Bowl ad against FSD","Content":"cameraImage Credits: The Dawn Project\nThis article has been updated with the cost of The Dawn Project’s Super Bowl ad. \nSafety advocacy group The Dawn Project is taking its campaign to ban Tesla’s Full Self-Driving (FSD) system to the Super Bowl.\nThe 30-second ad, which is broadcasting to millions of football fans, including political leaders in Washington, D.C. and state capitals like Austin, Tallahassee, Albany, Atlanta and Sacramento, outlines several alleged critical safety defects of Tesla FSD, the automaker’s advanced driver assistance system (ADAS).\nFSD is not actually fully self-driving, although it can perform some automated driving tasks like maneuvering through city streets and highways without driver input. The $15,000 system isn’t perfect, though, and drivers must remain alert to take over in case the system malfunctions or comes across something it can’t handle. There have been several reports of accidents occurring while Autopilot, Tesla’s lower-level ADAS, was engaged. As a result, Tesla has been criticized, investigated and sued for falsely marketing the capabilities of its automated driving systems.\nThis most recent critique comes as Tesla has recently released its latest version of FSD to around 400,000 drivers in North America, renewing concerns of the system’s safety. Last month, a Tesla engineer testified that a 2016 demo in which the company claimed its car was driving itself was actually staged.\nThe Super Bowl ad, which cost $598,000, features a collection of incriminating videos of Teslas behaving erratically while a voiceover claims FSD will “run down a child in a school crosswalk, swerve into oncoming traffic, hit a baby in a stroller, go straight past stopped school buses, ignore ‘do not enter’ signs, and even drive on the wrong side of the road.”\nThe Dawn Project asserts that Tesla’s “deceptive marketing” and “woefully inept engineering” is endangering the public, and calls on the National Highway Traffic Safety Administration and the Department of Motor vehicles to turn off FSD until all of the safety defects are fixed.\nThe Dawn Project’s founder, Dan O’Dowd, is also the CEO of Green Hill Software, a company that builds operating systems for embedded safety and security systems, as well as its own automated driving systems. That fact at once lends credence to the organization’s potential subject matter expertise, and makes it clear that Green Hill is in competition with Tesla’s FSD. Last year, The Dawn Project took out a full-page ad in The New York Times claiming Tesla’s FSD has a “critical malfunction every eight minutes.”\nO’Dowd, who ran for a seat in the U.S. Senate last November and lost, says he’s making the investment in the new ad campaign because he wants to put pressure on politicians to prioritize ADAS safety. Some politicians like Sens. Richard Blumenthal (D-Conn.) and Edward J. Markey (D-Mass.) have called for more oversight on Tesla’s tech, but the issue hasn’t exactly gone mainstream.\nAfter The Dawn Project aired a commercial last summer showing a Tesla Model 3 striking four different child-sized mannequins while driving a test track in California, Tesla sent the organization a cease-and-desist letter. The letter refuted all of the campaign’s claims, doubled down on Tesla’s commitment to safety and called to question The Dawn Project’s methodology.\n“The purported tests misuse and misrepresent the capabilities of Tesla’s technology, and disregard widely recognized testing performed by independent agencies as well as the experiences shared by our customers,” wrote Dinna Eskin, a Tesla lawyer, in last year’s cease-and-desist. “In fact, unsolicited scrutiny of the methodology behind The Dawn Project’s tests has already (and within hours of you publicly making defamatory allegations) shown that the testing is seriously deceptive and likely fraudulent.”\nTesla supporters also rushed to defend the technology, including one investor who tested the FSD beta using his own kid. O’Dowd offered to run the test with Musk and other critics in person to prove the accuracy and methodology of his tests.\n“Tesla continues to focus on features and marketing gimmicks, not fixing critical safety defects,” said O’Dowd in a statement. “Elon even stated that Tesla’s priorities were Smart Summon, Autopark and Optimus, not making sure that FSD will not run down children. It is clear that the priorities at Tesla are wrong, and it is time for the regulator to step in and switch the software off until all of the issues we have identified are fixed.”\nTesla hasn’t responded publicly to the Super Bowl ad, but CEO Elon Musk replied to a tweet showing the ad with the Rolling on the Floor Laughing emoji. Tesla disbanded its PR department in 2020, so TechCrunch couldn’t reach out for a comment.\nIn addition to the Super Bowl ad, The Dawn Project is also taking out a series of full-page ads in Politico and running additional TV ads in Washington, D.C., “where regulators are located,” that will call on FSD to be disabled until critical safety defects are fixed.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving technology malfunctions because it discusses safety advocacy group The Dawn Project airing a Super Bowl ad against Tesla's Full Self-Driving (FSD) system. The news outlines several alleged critical safety defects of Tesla FSD, including instances of Tesla vehicles behaving erratically, swerving into oncoming traffic, ignoring traffic signs, and driving on the wrong side of the road. It also mentions previous reports of accidents occurring while Tesla's lower-level ADAS (Autopilot) was engaged, resulting in criticism, investigations, and lawsuits against Tesla for falsely marketing the capabilities of its automated driving systems. The Dawn Project is calling for the National Highway Traffic Safety Administration and the Department of Motor Vehicles to turn off FSD until all safety defects are fixed, emphasizing concerns about the system's safety. Additionally, the news highlights the founder of The Dawn Project, Dan O'Dowd, who has a background in automated driving systems and competition with Tesla's FSD, adding credibility to the organization's claims.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2023\/02\/12\/tesla-dawn-project-super-bowl-fsd\/","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":18,"Time":"December 22, 2022","Title":"Tesla driver in multi-car crash told police self-driving software malfunctioned","Content":"WASHINGTON, Dec 21 (Reuters) - The driver of a 2021 Tesla (TSLA.O) Model S involved in an eight-vehicle crash last month on San Francisco's Bay Bridge told police he was in Full-Self Driving (FSD) mode which had malfunctioned, according to a police report made public Wednesday.\nThe Thanksgiving Day crash on Interstate-80 near Treasure Island resulted in two juveniles being transported to a local hospital for treatment of minor injuries and led to lengthy delays on the bridge.\nChief Executive Elon Musk has touted Tesla \"Full Self-Driving\" software as a potential cash cow for the world's biggest electric carmaker. But Tesla's advanced driver assistance systems - and Musk's claims about them - face growing legal, regulatory and public scrutiny.\nTesla sells the $15,000 FSD software as an ad-on which enables its vehicles to change lanes and park autonomously. That complements its standard \"Autopilot\" feature, which enables cars to steer, accelerate and brake within their lanes without driver intervention.\nAdvertisement · Scroll to continue\nThe Tesla driver told police the FSD malfunctioned but police were unable to determine if the software was in operation or if his statement was accurate, according to the report which was made public after a Reuters request.\nThe police report said the vehicle made an unsafe lane change and was slowing to a stop, which led to another vehicle hitting the Tesla and a chain reaction of additional crashes.\nAdvertisement · Scroll to continue\nThe police report said if FSD malfunctioned, the driver should have manually taken control of the vehicle.\nTesla did not respond to a request for comment.\nThe National Highway Traffic Safety Administration (NHTSA), which has been investigating the automaker's advanced driver assistance systems, did not comment.\nTesla's says \"Full Self-Driving\" gives access to more advanced driver assistance features but emphasizes \"all Tesla vehicles require active driver supervision and are not autonomous.\"\nAdvertisement · Scroll to continue\nNational Transportation Safety Board chair Jennifer Homendy has questioned Tesla's marketing the feature as \"full self-driving,\" when it is incapable of that and said Tesla must do more to ensure people do not misuse the feature.\nReporting by David Shepardson and Hyunjoo Jin; Editing by Lincoln Feast.\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Based on the provided news article, it appears to be relevant to self-driving technology malfunctions. Here is the JSON-formatted response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": [\n        \"The driver of a 2021 Tesla Model S was involved in an eight-vehicle crash while in Full-Self Driving (FSD) mode, which he claimed had malfunctioned.\",\n        \"The crash resulted in two juveniles being transported to a local hospital for minor injuries and caused lengthy delays on the bridge.\",\n        \"The police report mentioned that the vehicle made an unsafe lane change and was slowing to a stop, leading to another vehicle hitting the Tesla and causing additional crashes.\",\n        \"The driver told police that the FSD had malfunctioned, but it was unclear if the software was indeed in operation or if his statement was accurate.\",\n        \"The National Transportation Safety Board chair has criticized Tesla's marketing of the feature as 'full self-driving' when it is incapable of that.\"\n    ]\n}\nThis news is relevant to self-driving technology malfunctions because it discusses a Tesla driver's claim that their Full-Self Driving (FSD) mode malfunctioned, leading to a multi-car crash. The specific information provided in the response includes details about the crash, the driver's statement about the FSD malfunction, and criticism of Tesla's marketing of the feature.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/tesla-driver-multi-car-crash-told-police-self-driving-software-malfunctioned-2022-12-22\/","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":8,"Time":"Thu 22 Dec 2016 02.52 CET","Title":"Uber cancels self-driving car trial in San Francisco after state forces it off road","Content":"California has forced Uber to remove its self-driving vehicles from the road, canceling the company’s controversial pilot program in San Francisco after a week of embarrassing reports of traffic violations and repeated legal threats from state officials.\nThe department of motor vehicles (DMV) announced late on Wednesday that it had revoked the registration of 16 autonomous Uber cars, which the corporation deployed without proper permits last week and which were caught on numerous occasions running red lights.\nSelf-driving cars: Uber's open defiance of California shines light on brazen tactics\nRead more\nUber, which had previously declared that its rejection of government regulations was an “important issue of principle”, confirmed that it has stopped its pilot in a statement, adding: “We’re now looking at where we can redeploy these cars but remain 100 percent committed to California and will be redoubling our efforts to develop workable statewide rules.”\nDMV officials and the state attorney attorney general, Kamala Harris, have noted that Uber must get a testing permit for its Volvo XC90s, which are navigated by a computer system but have a driver in the front seat who can intervene when needed.\n“It was determined that the registrations were improperly issued for these vehicles because they were not properly marked as test vehicles,” the DMV said in a statement. “Concurrently, the department invited Uber to seek a permit so their vehicles can operate legally in California.”\nThe removal of the cars is a major blow to Uber in its home town and a sign that California will not allow the company to skirt regulations – a tactic that has been a central component of the corporation’s rapid growth.\nAn Uber self-driving car in San Francisco. The company had quietly launched its pilot program last week. Photograph: Eric Risberg\/AP\nThe car-sharing company’s autonomous cars were first unveiled in Pittsburgh in the fall, and Uber quietly started testing the technology on the roads of San Francisco in September.\nThe company officially launched a pilot program accessible to riders last week, claiming that it did not need self-driving permits since the cars have humans inside monitoring. Anthony Levandowski, head of Uber’s advanced technology group, also justified the company’s unpermitted vehicles by claiming that they have “state-of-the-art” technology and “core safety capabilities”.\nBut within hours of the formal pilot program’s start, a video emerged of one of the vehicles running a red light in downtown San Francisco as a pedestrian began to cross the street. Reports of similar errors quickly spread on social media, and Uber blamed the mistakes on “human error”, saying the self-driving technology was not at fault and that they would be suspending drivers.\nThat defense earned widespread criticism from transportation experts and safety advocates, who argued that Uber should take responsibility for the actions of its cars and should collaborate with regulators instead of brazenly defying them.\nOne witness said he saw a self-driving Uber run a red light and nearly cause a collision while the vehicle was in self-driving mode.\nUber also admitted to the Guardian this week that the self-driving technology had a “problem” with bike lanes, raising concerns about serious safety risks in a city where there are an estimated 82,000 bike trips each day.\nThe DMV has repeatedly pointed out that 20 manufacturers have gone through proper protocols and obtained permits for self-driving technology.\n“California’s testing regulations for autonomous vehicles strike a balance between protecting public safety and embracing innovation,” the statement said. “Uber is welcome to test its autonomous technology in California like everybody else.”\nWitness says self-driving Uber ran red light on its own, disputing Uber's claims\nRead more\nThe DMV statement also said it could take less than 72 hours for Uber to get its permits after completing an application: “The department stands ready to assist Uber in obtaining a permit as expeditiously as possible.”\nSome have suggested that Uber has aggressively tried to avoid permits so that it does not have to face the public scrutiny of a regulatory process.\nCompanies with permits are required to disclose crashes to the government and submit annual “disengagement reports”, which outline situations when a human operator intervened, sometimes due to technology failures.\nSan Francisco’s mayor, Ed Lee, said he supported the removal of the cars in a statement: “I have always been a strong supporter of innovation and autonomous vehicle development and testing, but only under conditions that put human, bicyclist and pedestrian safety first.”\nJohn M Simpson, privacy project director for Consumer Watchdog, an advocacy group that has staunchly opposed the pilot program, praised the “DMV’s decisive action”, but said he still had doubts about Uber’s cars and practices.\n“Given Uber’s safety record over the last two days,” he said, “where they’ve been blasting through red lights and threatening cyclists’ safety, we have fundamental questions about whether the robot cars are even ready to be tested on public highways.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n\"The news is about Uber's self-driving car trial in San Francisco being canceled by the state of California.\",\n\"The cancellation was due to embarrassing reports of traffic violations and repeated legal threats from state officials.\",\n\"The Department of Motor Vehicles (DMV) revoked the registration of 16 autonomous Uber cars for deploying them without proper permits and running red lights.\",\n\"Uber claimed that it did not need self-driving permits because the cars had humans inside monitoring, but videos and reports showed instances of self-driving cars running red lights and causing safety concerns.\",\n\"The DMV repeatedly emphasized the need for permits and proper protocols for self-driving technology in California.\",\n\"San Francisco's mayor and advocacy groups expressed concerns about safety and supported the removal of Uber's self-driving cars.\"\n]\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/dec\/21\/uber-cancels-self-driving-car-trial-san-francisco-california","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":5,"Time":"Fri 16 Dec 2016 12.00 CET","Title":"Self-driving cars: Uber's open defiance of California shines light on brazen tactics","Content":"Uber has launched an aggressive battle with California over its controversial self-driving cars, with regulators and consumer advocates accusing the corporation of flagrantly violating the law, endangering public safety and mistreating drivers.\nThe intense fight with the state – which ignited hours after numerous self-driving cars were caught running red lights in Uber’s home town – has exposed what critics say are the unethical and illegal tactics that the company has repeatedly used to grow its business.\nUber blames humans for self-driving car traffic offenses as California orders halt\nRead more\nThe ride-sharing company, which launched semi-autonomous vehicles in San Francisco without permits this week, was ordered by the California department of motor vehicles (DMV) to immediately remove the cars from the road or face legal action.\nBut Uber, which has not publicly responded to the state’s demands, blamed the traffic light violations on “human error” and suspended the drivers who were monitoring the cars. This bold deflection of blame further highlights the corporation’s refusal to take responsibility for potential faults in its technology and raises questions about the dangers of prematurely rolling out self-driving vehicles.\n“How many people are they going to kill before they understand they’re not doing the right thing?” said John M Simpson, privacy project director with Consumer Watchdog, a non-profit that has called for Uber to face consequences for side-stepping regulations. “If you’re going to use public highways as your own private laboratory, you’ve got an obligation to follow the rules.”\nUber’s open defiance of California regulators marks the latest case of a “sharing economy” corporation ignoring government under the guise of “disruption” and “innovation”. Uber has long claimed that it is a technology “platform” and not a transportation company and thus does not have to classify its drivers as employees or follow traditional taxicab regulations.\nThat strategy has resulted in more than 70 lawsuits in federal courts and hefty settlements, along with claims from opponents that the company is abusing workers’ rights and failing to ensure the safety of riders.\nThe San Francisco self-driving car scandal centers on Uber’s Volvo XC90s, which can navigate on their own while licensed drivers sit at the wheel and take control when necessary. The company first piloted semi-autonomous vehicles in Pittsburgh in August.\nIn Pennsylvania, some drivers responded to the program with shock and concern and wondered how quickly they could lose their jobs to full automation.\nThe abrupt rollout in San Francisco and subsequent suspensions has led some California drivers to question whether Uber will toss them aside when the technology malfunctions or drivers aren’t properly trained and make a mistake.\nUber self-driving car drives through red light in San Francisco Guardian\n“Uber itself is a very unethical company,” said Travis Taborek, a 26-year-old Bay Area resident, who drives full time for Uber and its competitor, Lyft. “If you launch technology like this on the scale of a city, then you need to go through proper channels. That’s how people are protected.”\nIn the long term, he added, “I am concerned self-driving cars are going to put a lot of people out of work.”\nOne San Francisco Uber driver, who requested anonymity for fear of retaliation, followed a self-driving car for about 15 minutes on Wednesday and filmed its movements. The footage shows multiple minor violations or potentially dangerous moves, such as failing to fully brake at a stop sign or cutting off a bus.\n“I’d expect better behavior from a company with such clout in the transportation industry,” he said.\nUber has argued that it does not need permits since its cars aren’t fully autonomous, though the DMV noted that 20 companies have followed proper procedures and were approved to test self-driving vehicles in California.\nUber did not respond to repeated inquiries about the state’s order and lawsuit threat. In a statement to the Guardian, a spokesperson claimed that two cars running red lights were not part of the pilot and weren’t carrying customers.\n“These incidents were due to human error. This is why we believe so much in making the roads safer by building self-driving Ubers.” A spokesperson also claimed the cars require “human monitoring and intervention in many conditions, including bad weather”.\nAn Uber driverless car is displayed in a garage in San Francisco. Photograph: Eric Risberg\/AP\nExperts note that Uber has an obvious financial interest in a system with minimal regulations. By ignoring California regulators, the corporation is attempting to preemptively create a framework in which self-driving cars are treated similarly to traditional vehicles, said Arun Sundararajan, a New York University business professor and expert on the sharing economy.\n“The act of asking for permission is sort of a tacit admission that this needs to be regulated,” he said. “They are playing a long game here … They’re trying to define what the regulatory space is going to look like.”\nMichael Gumora, a longtime San Francisco Uber driver who runs a website called RideshareReport.com, said it was a public relations ploy to announce suspensions and say the drivers erred when the automation system clearly didn’t work properly.\n“It’s a human error, but the vehicle and the technology didn’t compensate,” he said. “The technology itself wasn’t able to avoid running the red light.”\nTaborek said publicly admonishing the operators was another example of Uber mistreating drivers. “I would take anything Uber says with massive grains of salt ... I’d be willing to bet good money that the technology is at fault.”\nUber’s response is reminiscent of statements from Tesla earlier this year after a driver using its “autopilot” technology died in a crash. The corporation denied responsibility and defended its technology even though the car’s system failed to detect a truck in front of it.\n“You put unsafe vehicles on the road and then you blame a human,” said Simpson, who argued that San Francisco police should respond to Uber’s rogue operations by impounding the vehicles operating without a permit.\nConsumer Watchdog also called for criminal charges against Uber’s CEO Travis Kalanick for violating DMV requirements.\n“This is essentially driving without a license,” Simpson said. “It’s really unconscionable.”\nContact the author: sam.levin@theguardian.com","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving technology malfunctions because it discusses Uber's aggressive battle with California over its self-driving cars and reports on incidents where numerous self-driving cars were caught running red lights. Uber's response to the traffic light violations, blaming them on 'human error' and suspending the drivers who were monitoring the cars, raises concerns about the functionality and safety of their self-driving technology. The article also highlights criticism from consumer advocates and experts who question the company's tactics and its refusal to take responsibility for potential faults in its technology.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/dec\/16\/uber-self-driving-cars-california-illegal-unethical-tactics","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":22,"Time":"April 6, 2023","Title":"Tesla workers shared sensitive images recorded by customer cars","Content":"Summary\nCompanies\nPrivate camera recordings, captured by cars, were shared in chat rooms: ex-workers\nCirculated clips included one of child being hit by car: ex-employees\nTesla says recordings made by vehicle cameras ‘remain anonymous’\nOne video showed submersible vehicle from James Bond film, owned by Elon Musk\nLONDON\/SAN FRANCISCO, April 6 (Reuters) - Tesla Inc assures its millions of electric car owners that their privacy “is and will always be enormously important to us.” The cameras it builds into vehicles to assist driving, it notes on its website, are “designed from the ground up to protect your privacy.”\nBut between 2019 and 2022, groups of Tesla employees privately shared via an internal messaging system sometimes highly invasive videos and images recorded by customers’ car cameras, according to interviews by Reuters with nine former employees.\nAdvertisement · Scroll to continue\nSome of the recordings caught Tesla customers in embarrassing situations. One ex-employee described a video of a man approaching a vehicle completely naked.\nAlso shared: crashes and road-rage incidents. One crash video in 2021 showed a Tesla driving at high speed in a residential area hitting a child riding a bike, according to another ex-employee. The child flew in one direction, the bike in another. The video spread around a Tesla office in San Mateo, California, via private one-on-one chats, “like wildfire,” the ex-employee said.\nAdvertisement · Scroll to continue\nOther images were more mundane, such as pictures of dogs and funny road signs that employees made into memes by embellishing them with amusing captions or commentary, before posting them in private group chats. While some postings were only shared between two employees, others could be seen by scores of them, according to several ex-employees.\nTesla states in its online “Customer Privacy Notice” that its “camera recordings remain anonymous and are not linked to you or your vehicle.” But seven former employees told Reuters the computer program they used at work could show the location of recordings – which potentially could reveal where a Tesla owner lived.\nAdvertisement · Scroll to continue\nOne ex-employee also said that some recordings appeared to have been made when cars were parked and turned off. Several years ago, Tesla would receive video recordings from its vehicles even when they were off, if owners gave consent. It has since stopped doing so.\n“We could see inside people's garages and their private properties,” said another former employee. “Let's say that a Tesla customer had something in their garage that was distinctive, you know, people would post those kinds of things.”\nAdvertisement · Scroll to continue\nTesla didn't respond to detailed questions sent to the company for this report.\nAbout three years ago, some employees stumbled upon and shared a video of a unique submersible vehicle parked inside a garage, according to two people who viewed it. Nicknamed “Wet Nellie,” the white Lotus Esprit sub had been featured in the 1977 James Bond film, “The Spy Who Loved Me.”\nThe vehicle’s owner: Tesla Chief Executive Elon Musk, who had bought it for about $968,000 at an auction in 2013. It is not clear whether Musk was aware of the video or that it had been shared.\nThe submersible Lotus vehicle nicknamed \"Wet Nellie\" that featured in the 1977 James Bond film, \"The Spy Who Loved Me,” and which Tesla chief executive Elon Musk purchased in 2013. Tim Scott... Acquire Licensing Rights Read more\nMusk didn’t respond to a request for comment.\nTo report this story, Reuters contacted more than 300 former Tesla employees who had worked at the company over the past nine years and were involved in developing its self-driving system. More than a dozen agreed to answer questions, all speaking on condition of anonymity.\nReuters wasn’t able to obtain any of the shared videos or images, which ex-employees said they hadn’t kept. The news agency also wasn’t able to determine if the practice of sharing recordings, which occurred within some parts of Tesla as recently as last year, continues today or how widespread it was. Some former employees contacted said the only sharing they observed was for legitimate work purposes, such as seeking assistance from colleagues or supervisors.\nLABELING PEDESTRIANS AND STREET SIGNS\nThe sharing of sensitive videos illustrates one of the less-noted features of artificial intelligence systems: They often require armies of human beings to help train machines to learn automated tasks such as driving.\nSince about 2016, Tesla has employed hundreds of people in Africa and later the United States to label images to help its cars learn how to recognize pedestrians, street signs, construction vehicles, garage doors and other objects encountered on the road or at customers’ houses. To accomplish that, data labelers were given access to thousands of videos or images recorded by car cameras that they would view and identify objects.\nFILE PHOTO: The forward-facing camera of a Tesla Model S containing Autopilot features is pointed out during a 2015 Tesla event in Palo Alto. REUTERS\/Beck Diefenbach Acquire Licensing Rights\nTesla increasingly has been automating the process, and shut down a data-labeling hub last year in San Mateo, California. But it continues to employ hundreds of data labelers in Buffalo, New York. In February, Tesla said the staff there had grown 54% over the previous six months to 675.\nTwo ex-employees said they weren’t bothered by the sharing of images, saying that customers had given their consent or that people long ago had given up any reasonable expectation of keeping personal data private. Three others, however, said they were troubled by it.\n“It was a breach of privacy, to be honest. And I always joked that I would never buy a Tesla after seeing how they treated some of these people,” said one former employee.\nAnother said: “I’m bothered by it because the people who buy the car, I don't think they know that their privacy is, like, not respected … We could see them doing laundry and really intimate things. We could see their kids.”\nOne former employee saw nothing wrong with sharing images, but described a function that allowed data labelers to view the location of recordings on Google Maps as a “massive invasion of privacy.”\nDavid Choffnes, executive director of the Cybersecurity and Privacy Institute at Northeastern University in Boston, called sharing of sensitive videos and images by Tesla employees “morally reprehensible.”\n“Any normal human being would be appalled by this,” he said. He noted that circulating sensitive and personal content could be construed as a violation of Tesla’s own privacy policy — potentially resulting in intervention by the U.S. Federal Trade Commission, which enforces federal laws relating to consumers’ privacy.\nA spokesperson for the FTC said it doesn’t comment on individual companies or their conduct.\nTo develop self-driving car technology, Tesla collects a vast trove of data from its global fleet of several million vehicles. The company requires car owners to grant permission on the cars’ touchscreens before Tesla collects their vehicles’ data. “Your Data Belongs to You,” states Tesla’s website.\nFILE PHOTO: A Tesla Model 3 vehicle is shown using the Autopilot Full Self Driving Beta software (FSD) while navigating a city road in Encinitas, California, February 2023. REUTERS\/Mike Blake Acquire Licensing Rights\nIn its Customer Privacy Notice, Tesla explains that if a customer agrees to share data, “your vehicle may collect the data and make it available to Tesla for analysis. This analysis helps Tesla improve its products, features, and diagnose problems quicker.” It also states that the data may include “short video clips or images,” but isn’t linked to a customer’s account or vehicle identification number, “and does not identify you personally.”\nCarlo Piltz, a data privacy lawyer in Germany, told Reuters it would be difficult to find a legal justification under Europe’s data protection and privacy law for vehicle recordings to be circulated internally when it has “nothing to do with the provision of a safe or secure car or the functionality” of Tesla's self-driving system.\nIn recent years, Tesla’s car-camera system has drawn controversy. In China, some government compounds and residential neighborhoods have banned Teslas because of concerns about its cameras. In response, Musk said in a virtual talk at a Chinese forum in 2021: “If Tesla used cars to spy in China or anywhere, we will get shut down.”\nElsewhere, regulators have scrutinized the Tesla system over potential privacy violations. But the privacy cases have tended to focus not on the rights of Tesla owners but of passers-by unaware that they might be being recorded by parked Tesla vehicles.\nIn February, the Dutch Data Protection Authority, or DPA, said it had concluded an investigation of Tesla over possible privacy violations regarding “Sentry Mode,” a feature designed to record any suspicious activity when a car is parked and alert the owner.\nReuters Image Acquire Licensing Rights\n“People who walked by these vehicles were filmed without knowing it. And the owners of the Teslas could go back and look at these images,” said DPA board member Katja Mur in a statement. “If a person parked one of these vehicles in front of someone’s window, they could spy inside and see everything the other person was doing. That is a serious violation of privacy.”\nThe watchdog determined it wasn’t Tesla, but the vehicles’ owners, who were legally responsible for their cars’ recordings. It said it decided not to fine the company after Tesla said it had made several changes to Sentry Mode, including having a vehicle’s headlights pulse to inform passers-by that they may be being recorded.\nA DPA spokesperson declined to comment on Reuters findings, but said in an email: “Personal data must be used for a specific purpose, and sensitive personal data must be protected.”\nREPLACING HUMAN DRIVERS\nTesla calls its automated driving system Autopilot. Introduced in 2015, the system included such advanced features as allowing drivers to change lanes by tapping a turn signal and parallel parking on command. To make the system work, Tesla initially installed sonar sensors, radar and a single front-facing camera at the top of the windshield. A subsequent version, introduced in 2016, included eight cameras all around the car to collect more data and offer more capabilities.\nMusk’s future vision is eventually to offer a “Full Self-Driving” mode that would replace a human driver. Tesla began rolling out an experimental version of that mode in October 2020. Although it requires drivers to keep their hands on the wheel, it currently offers such features as the ability to slow a car down automatically when it approaches stop signs or traffic lights.\nTesla's Autopilot system\nThis excerpt from the owner’s manual for the Tesla Model X explains the car’s Autopilot system, including the cameras that record video of the vehicle’s surroundings. Reuters found that Tesla... Acquire Licensing Rights Read more\nIn February, Tesla recalled more than 362,000 U.S. vehicles to update their Full Self-Driving software after the National Highway Traffic Safety Administration said it could allow vehicles to exceed speed limits and potentially cause crashes at intersections.\nAs with many artificial-intelligence projects, to develop Autopilot, Tesla hired data labelers to identify objects in images and videos to teach the system how to respond when the vehicle was on the road or parked.\nTesla initially outsourced data labeling to a San Francisco-based non-profit then known as Samasource, people familiar with the matter told Reuters. The organization had an office in Nairobi, Kenya, and specialized in offering training and employment opportunities to disadvantaged women and youth.\nIn 2016, Samasource was providing about 400 workers there for Tesla, up from about an initial 20, according to a person familiar with the matter.\nBy 2019, however, Tesla was no longer satisfied with the work of Samasource’s data labelers. At an event called Tesla AI Day in 2021, Andrej Karpathy, then senior director of AI at Tesla, said: “Unfortunately, we found very quickly that working with a third party to get data sets for something this critical was just not going to cut it … Honestly the quality was not amazing.”\nA former Tesla emp loyee said of the Samasource labelers: “They would highlight fi re hydrants as pedestrians … They would miss objects all the time. Their skill level to draw boxes was very low.”\nSamasource, now called Sama, declined to comment on its work for Tesla.\nTesla decided to bring data labeling in-house. “Over time, we’ve grown to more than a 1,000-person data labeling (organization) that is full of professional labelers who are working very closely with the engineers,” Karpathy said in his August 2021 presentation.\nKarpathy didn’t respond to requests for comment.\nTesla’s own data labelers initially worked in the San Francisco Bay area, including the office in San Mateo. Groups of data labelers were assigned a variety of different tasks, including labeling street lane lines or emergency vehicles, ex-employees said.\nAt one point, Teslas on Autopilot were having difficulty backing out of garages and would get confused when encountering shadows or objects such as garden hoses. So some data labelers were asked to identify objects in videos recorded inside garages. The problem eventually was solved.\nIn interviews, two former employees said in their normal work duties they were sometimes asked to view images of customers in and around their homes, including inside garages.\n“I sometimes wondered if these people know that we're seeing that,” said one.\n“I saw some scandalous stuff sometimes, you know, like I did see scenes of intimacy but not nudity,” said another. “And there was just definitely a lot of stuff that like, I wouldn't want anybody to see about my life.”\nAs an example, this person recalled seeing “embarrassing objects,” such as “certain pieces of laundry, certain sexual wellness items … and just private scenes of life that we really were privy to because the car was charging.”\nMEMES IN THE SAN MATEO OFFICE\nTesla staffed its San Mateo office with mostly young workers, in their 20s and early 30s, who brought with them a culture that prized entertaining memes and viral online content. Former staffers described a free-wheeling atmosphere in chat rooms with workers exchanging jokes about images they viewed while labeling.\nAccording to several ex-employees, some labelers shared screenshots, sometimes marked up using Adobe Photoshop, in private group chats on Mattermost, Tesla’s internal messaging system. There they would attract responses from other workers and managers. Participants would also add their own marked-up images, jokes or emojis to keep the conversation going. Some of the emojis were custom-created to reference office inside jokes, several ex-employees said.\nOne former labeler described sharing images as a way to “break the monotony.” Another described how the sharing won admiration from peers.\n“If you saw something cool that would get a reaction, you post it, right, and then later, on break, people would come up to you and say, ‘Oh, I saw what you posted. That was funny,’” said this former labeler. “People who got promoted to lead positions shared a lot of these funny items and gained notoriety for being funny.”\nSome of the shared content resembled memes on the internet. There were dogs, interesting cars, and clips of people recorded by Tesla cameras tripping and falling. There was also disturbing content, such as someone being dragged into a car seemingly against their will, said one ex-employee.\nVideo clips of crashes involving Teslas were also sometimes shared in private chats on Mattermost, several former employees said. Those included examples of people driving badly or collisions involving people struck while riding bikes – such as the one with the child – or a motorcycle. Some data labelers would rewind such clips and play them in slow motion.\nFILE PHOTO: A camera is seen on the body of a Tesla electric vehicle (EV) during a media day for the Auto Shanghai show, April 2021. REUTERS\/Aly Song Acquire Licensing Rights\nAt times, Tesla managers would crack down on inappropriate sharing of images on public Mattermost channels since they claimed the practice violated company policy. Still, screenshots and memes based on them continued to circulate through private chats on the platform, several ex-employees said. Workers shared them one-on-one or in small groups as recently as the middle of last year.\nOne of the perks of working for Tesla as a data labeler in San Mateo was the chance to win a prize – use of a company car for a day or two, according to two former employees.\nBut some of the lucky winners became paranoid when driving the electric cars.\n“Knowing how much data those vehicles are capable of collecting definitely made folks nervous,\" one ex-employee said.\nReported by Steve Stecklow and Waylon Cunningham in London and Hyunjoo Jin in San Francisco. Edited by Peter Hirschberg.\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab\nSteve Stecklow\nThomson Reuters\nPulitzer Prize-winning global investigative reporter based in London whose work has included Facebook’s failure to combat hate speech in Myanmar, U.S. college admissions fraud, how Iran's Supreme Leader secretly controls a multi-billion-dollar corporate empire, and sanctions busting by Chinese companies in Iran. Previously worked at The Wall Street Journal and The Philadelphia Inquirer.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving technology malfunctions because it reveals that Tesla employees privately shared highly invasive videos and images recorded by customers' car cameras. These shared recordings include incidents such as crashes and road-rage incidents, with one instance involving a Tesla driving at high speed in a residential area hitting a child riding a bike. This incident reflects a malfunction or misuse of Tesla's vehicle cameras and could be indicative of issues with the self-driving technology's data privacy and security.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/technology\/tesla-workers-shared-sensitive-images-recorded-by-customer-cars-2023-04-06\/","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":23,"Time":"September 19, 2022","Title":"Insight: Truly autonomous cars may be impossible without helpful human touch","Content":"MILTON KEYNES, England, Sept 12 (Reuters) - Autonomous vehicle (AV) startups have raised tens of billions of dollars based on promises to develop truly self-driving cars, but industry executives and experts say remote human supervisors may be needed permanently to help robot drivers in trouble.\nThe central premise of autonomous vehicles - that computers and artificial intelligence will dramatically reduce accidents caused by human error - has driven much of the research and investment.\nAdvertisement · Scroll to continue\nBut there is a catch: Making robot cars that can drive more safely than people is immensely tough because self-driving software systems simply lack humans' ability to predict and assess risk quickly, especially when encountering unexpected incidents or \"edge cases.\"\n\"Well, my question would be, 'Why?'\" said Kyle Vogt, CEO of Cruise, a unit of General Motors (GM.N), when asked if he could see a point where remote human overseers should be removed from operations.\nAdvertisement · Scroll to continue\n\"I can provide my customers peace of mind knowing there is always a human there to help if needed,\" Vogt said. \"I don't know why I'd ever want to get rid of that.\"\nThis is the first time Cruise has acknowledged the long-term need for remote human operators.\nLike air traffic controllers, such human supervisors could be sitting tens of hundreds of miles away monitoring video feeds from multiple AVs, sometimes with a steering wheel, ready to step in and get stuck robot drivers moving again - AVs invariably stop when they cannot figure out what to do.\nAdvertisement · Scroll to continue\nAlphabet Inc's (GOOGL.O) Waymo and Argo, which is backed by Ford Motor Co (F.N) and Volkswagen AG (VOWG_p.DE), declined to comment when asked the same question.\nGM recalled and updated software in 80 Cruise self-driving vehicles this month after a June crash in San Francisco left two people injured. U.S. safety regulators said the recalled software could \"incorrectly predict\" an oncoming vehicle's path, and Cruise said the unusual scenario would not recur after the update. read more\nAdvertisement · Scroll to continue\nFor some, the idea that human supervisors could be here to stay raises more doubts about the technology.\nTruly autonomous vehicles are far behind the optimistic rollout schedules predicted just a few years ago.\nIn 2018, GM sought U.S. government approval for a fully autonomous car without a steering wheel, brake or accelerator pedals that would enter its commercial ride-sharing fleet in 2019. That vehicle, the Cruise Origin, now is not slated to begin production until spring 2023, Vogt said.\nIn 2019, Tesla Inc (TSLA.O) CEO Elon Musk promised a million robotaxis \"next year for sure\" - though his company's \"Full Self Driving\" offering has been criticized because its cars are not capable of driving themselves without a human behind the wheel and ready to take manual control in an emergency.\nIn a June interview on YouTube, Musk said developing self-driving cars was \"way harder than I originally thought, by far.\" But when asked for a timeline, he said Tesla could make it \"this year.\"\nTesla did not respond to a request for comment for this story.\nThe undelivered promise of true autonomy has raised the stakes for the AV industry.\n\"If these companies don't succeed over the next two years, they're not going to exist anymore,\" said Mike Wagner, CEO of Edge Case Research, which helps AV companies assess, manage and insure risk. \"It's a case of put up or shut up at this point.\"\nREMOTE HUMANS WATCHING\nMany AV startups today use humans as remote supervisors, alongside safety drivers sitting behind the wheel. read more\nThose remote humans are an additional expense, but help self-driving cars handle edge cases. These could include something as basic as an unfamiliar set of lane closures during road construction, or erratic, unpredictable behavior by pedestrians or human drivers.\nWhen a robot driver encounters an edge case, \"it puts its hands up and says, 'I don't know what's going on,'\" said Koosha Kaveh, CEO of Imperium Drive, which is using humans as remote operators for cars in the English city of Milton Keynes. Over time, those people will act as \"air traffic controllers,\" supervising a growing number of autonomous cars.\nCruise's Vogt says the company's AVs on the roads in San Francisco currently rely on humans less than 1% of the time. But across hundreds, thousands or even millions of AVs, that would add up to a significant amount of time stopped on the road waiting for human guidance.\n[1\/5]An operator controls a Fetch driverless car from the office of Imperium Drive, during driverless car trials, in Milton Keynes, Britain, June 8, 2022. REUTERS\/Andrew Boyers Acquire Licensing Rights\nImperium Drive's Kaveh said as more self-driving cars - which are more predictable than humans - hit the roads the number of edge cases will drop, \"but you will never get to zero edge cases.\"\n\"Even decades from now you will not get to 100% truly autonomous vehicles,\" Kaveh added.\nNevertheless, competition is rising. Some Chinese cities are pushing to allow active AV testing more quickly.\nThe need to tackle edge cases and cut the costs of everything from sensors to the number of humans in the loop in order to get to market has also intensified because investor funding for autonomous cars has plummeted.\nDoubt has crept in as investors puzzle over how soon autonomous business will turn profitable. Simpler or slower AVs like trucks or last-mile delivery services operating on highways or on set, low-speed routes are likely to reach profitability first, but will still take years to get there.\nOverall investment in future mobility startups has slowed, with AV-focused companies hit especially hard, representing less than 10% of venture investment in the second quarter, according to investor website PitchBook. (Graphic: https:\/\/tmsnrt.rs\/3Rzy04y)\nReuters Graphics\nInvestment in AV startups in the quarter dropped to $958 million. Just two years ago AV investment was booming, as Alphabet's Waymo raised $3 billion, Didi's AV unit raised $500 million and Amazon.com Inc (AMZN.O) acquired AV startup Zoox for $1.3 billion, according to PitchBook.\n'RUSH TO MARKET'\nAutonomous systems are not as capable as people because their \"perception and prediction algorithms are not as good as how a human brain processes and decides,\" said Chris Borroni-Bird, an independent consultant who previously led advanced-vehicle programs at GM and Waymo.\nFor instance, a human when seeing a ball roll into the road - harmless by itself - will assume it could be followed by a child and hit the brakes far quicker than an AV, Borroni-Bird said.\n\"I am concerned that AV companies will rush to market without proving the safety is better than human-driven vehicles,\" he added.\nThe problem is there are \"tens of billions of potential edge cases\" that AVs could encounter, said James Routh, CEO of AB Dynamics (ABDP.L), which conducts tests and runs simulations on cars including on the advanced driver-assistance systems (ADAS) that are the foundation of autonomous driving features.\nAuto data startup Wejo Group Ltd (WEJO.O) receives 18 billion data points daily from millions of connected cars and is helping with simulations for AVs, said Sarah Larner, executive vice president for strategy and innovation.\n\"But there are so many variables such as weather, you can take an edge case and then have to layer in all the different variants,\" she said. \"It's truly millions of outputs.\"\nDRIVERLESS DELIVERY\nIn its track tests for cars, AB Dynamics employs a robot arm that it plans to retrofit on slow-moving mining and agricultural trucks to make them largely autonomous.\nRouth envisages a remote team of humans supervising fleets of, for instance, self-driving mining trucks operating in closed environments.\nHe does not see that scenario working for vehicles in faster, more open environments because it could be difficult for remote human supervisors to react quickly enough to dangers.\nWithin the next 12 months, British online food delivery and technology company Ocado Group Plc (OCDO.L) will roll out a small fleet of driverless delivery vehicles with autonomous vehicle software startup Oxbotica - backed by remote human supervisors - that will operate on just a few streets on set routes in a small UK city and never drive at speeds above 30 miles (48 km) per hour.\n\"At 30 miles an hour, if a vehicle panics, it can hit the emergency brake and seek help,\" Ocado's head of advanced technology, Alex Harvey, said. \"This feels like a very viable strategy at low speed.\"\n\"But you can't play that game on a motorway,\" Harvey added, because hard stops in edge cases would pose a safety risk.\nHarvey said it should take around five years for Ocado to develop a profitable driverless delivery system. More than half of Ocado's UK customers could be reached with AVs driving no more than 40 mph he said. Eventually, the service could be rolled out to Ocado clients like U.S. retail chain Kroger Co. (KR.N)\nReporting by Nick Carey in Milton Keynes, England, and Paul Lienert in Detroit Editing by Ben Klayman, Matthew Lewis and Louise Heavens\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it discusses the challenges and limitations of autonomous vehicles. It highlights that the central premise of autonomous vehicles, which is reducing accidents caused by human error through computer and AI systems, may not be achievable without the permanent assistance of human supervisors. It mentions that self-driving software systems struggle to predict and assess risk quickly, especially in unexpected incidents or 'edge cases.' The article also talks about how remote human overseers may be necessary to step in and assist when self-driving cars encounter difficulties. It provides examples of self-driving technology malfunctions, such as GM recalling and updating software in 80 Cruise self-driving vehicles after a crash caused by incorrect predictions, and mentions other companies like Waymo and Argo declining to comment on the issue. Overall, the news highlights the ongoing challenges and doubts surrounding the development of fully autonomous vehicles and the potential need for human intervention due to technology limitations.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/technology\/truly-autonomous-cars-may-be-impossible-without-helpful-human-touch-2022-09-12\/","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":24,"Time":"Wed 8 Mar 2023 16.48 CET","Title":"Investigation launched into complaints of Tesla steering wheels coming off mid-drive","Content":"US auto safety regulators have opened an investigation into Tesla’s Model Y SUV after getting two complaints that the steering wheels can come off while being driven.\nThe National Highway Traffic Safety Administration (NHTSA) says the investigation covers an estimated 120,000 vehicles from the 2023 model year.\nThe agency says in both cases the Model Ys were delivered to customers with a missing bolt that holds the wheel to the steering column. A friction fit held the steering wheels on, but they separated when force was exerted while the SUVs were being driven.\nElon Musk overstated Tesla’s autopilot and self-driving tech, new lawsuit says\nRead more\nThe agency says in documents posted on its website on Wednesday that both incidents happened while the SUVs had low mileage on them.\nMessages were left seeking comment from Tesla, which has disbanded its media relations department.\nIn one complaint filed with NHTSA, an owner said he was driving with his family on Route 1 in Woodbridge, New Jersey, when the steering wheel suddenly came off on 29 January, five days after the vehicle was purchased. The owner wrote that there were no cars behind him, and he was able to pull toward the road divider. There were no injuries.\nThe complaint has a link to a Twitter posting from the owner that included a video of the detached steering wheel and pictures of the white Tesla being towed.\nAt first, a Tesla service center gave the owner a cost estimate of $103.96 to repair the problem. The service center apologized in what appear to be text messages posted on Twitter.\nWhen the owner wrote that he had lost faith in Tesla and asked for a refund, the service center removed the charge and wrote that Tesla doesn’t have a return policy, but he could contact the sales and delivery team.\nThe man was later given the option of keeping the car or getting it replaced with a new one, according to his post on Twitter. He posted that Tesla did replace his car.\nDetached steering wheels are rare in the auto industry, but not unprecedented. In February, Nissan recalled about 1,000 Ariya electric vehicles because the wheels could come off the steering column due to a loose bolt.\nYet the latest NHTSA investigation adds to a long string of problems that Tesla is having with the US road safety agency. In the past three years it has opened investigations of Tesla’s Autopilot driver-assist system crashing into parked emergency vehicles, and problems with suspensions. At least 14 Teslas have crashed into emergency vehicles while using the Autopilot system.\nThe agency also is investigating complaints that Teslas can brake suddenly for no reason.\nIn February, NHTSA pressured Tesla into recalling nearly 363,000 vehicles with Full Self-Driving software because the system can break traffic laws. The system, which cannot drive itself, is being tested on public roads by as many as 400,000 Tesla owners. But NHTSA said in documents that it can make unsafe actions such as traveling straight through an intersection while in a turn-only lane, going through a yellow traffic light without proper caution or failing to respond to changes in posted speed limits.\nThe US justice department also has asked Tesla for documents from Tesla about Full Self-Driving and Autopilot.\nNHTSA has sent investigators to 35 Tesla crashes in which automated systems are suspected of being used. Nineteen people have died in those crashes, including two motorcyclists.\nSince January of 2022, Tesla has issued 20 recalls, including several that were required by NHTSA. The recalls include one from January of last year for Full Self-Driving vehicles being programmed to run stop signs at slow speeds.","Relevant":true,"Overview":"Here is the JSON-formatted response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it discusses an investigation into Tesla's Model Y SUV due to complaints about the steering wheels coming off mid-drive. The National Highway Traffic Safety Administration (NHTSA) has received two complaints where the steering wheels separated from the steering column during driving. These incidents were attributed to missing bolts that should hold the steering wheels in place. This issue directly pertains to vehicle safety systems and can be considered a malfunction related to self-driving technology as it affects the vehicle's control and safety mechanisms. Additionally, the news mentions previous investigations and issues related to Tesla's Autopilot and Full Self-Driving software, further highlighting the relevance of self-driving technology malfunctions.\"\n}\nIn summary, the news is relevant to self-driving technology malfunctions because it discusses a safety investigation related to steering wheel detachment in Tesla's Model Y SUVs, which can be considered a malfunction of vehicle safety systems, including self-driving technology.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2023\/mar\/08\/tesla-steering-wheels-fall-off-investigation","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":14,"Time":"Updated 6:21 PM GMT+2, August 5, 2023","Title":"Recalling a wild ride with a robotaxi named Peaches as regulators mull San Francisco expansion plan","Content":"SAN FRANCISCO (AP) — I won’t forget the first time I took a ride in a car without anyone sitting in the driver’s seat.\nIt happened one night last September when a Chevy Bolt named Peaches picked me up outside a San Francisco bar. Our ensuing half-hour ride together produced, at first, a titillating display of technology’s promise. Then an unexpected twist made me worry that the encounter had turned into a mistake I would regret.\nPeaches and I were getting along great for most of our time together, as the car deftly navigated through hilly San Francisco streets similar to those Steve McQueen careened through during the famous chase scene in the 1968 film “Bullitt.” Unlike McQueen, Peaches never exceeded 30 mph (48 kph) because of restrictions imposed by state regulators on a ride-hailing service operated by Cruise, a General Motors subsidiary, since it won approval to transport fare-paying passengers last year.\nIt was all going so smoothly that I was starting to buy into the vision of Cruise and Waymo, a self-driving car pioneer spun off from a Google project that is also trying launch a ride-hailing service in San Francisco.\nOTHER NEWS\nDianne Feinstein was at the center of a key LGBTQ+ moment. She’s being lauded as an evolving ally\nThe Dianne Feinstein they knew: Women of the Senate remember a tireless fighter and a true friend\nPhotographs documented US Sen. Dianne Feinstein’s groundbreaking career in politics\nThe theory fueling the ambition is that driverless cars will be safer than vehicles operated by frequently distracted and occasionally intoxicated humans — and, in the case of robotaxis, be less expensive and more convenient to ride in than automobiles that require a human behind the wheel.\nThe concept does sound good. And the technology to pull it off is advancing steadily, just like other artificial intelligence applications such as chatbots that can write college-level essays and produce impressive pieces of art within seconds.\nBut when something goes awry, as it did near the end of my encounter with Peaches, that sense of astonishment and delight can evaporate very quickly.\nAnd even though none of the Cruise and Waymo driverless vehicles have been involved in major accidents in San Francisco, the robotaxis have been malfunctioning frequently enough to have triggered an intense resistance to proposed expansion that would allow them to operate around-the-clock throughout the city.\nAfter postponing two previous votes on the proposed expansion in June and July amid the robotaxi backlash, the California Public Utilities Commission is scheduled to tackle the thorny issue Thursday — unless information presented at a Monday status conference prompts another delay.\nDESTINATION: UNCERTAIN\nMy September ride with Peaches didn’t end well.\nAs we approached my designated drop-off location near the Fairmont Hotel — where presidents have stayed and the late Tony Bennett first sang “I Left My Heart In San Francisco” — Peaches advised me to gather my belongings and prepare to get out of the car.\nWhile I grabbed my bag as the robotaxi appeared to be pulling over to the curb, it suddenly sped up and inexplicably started driving away in the opposite direction.\nAfter seeing the dashboard display screen indicating I was now somehow an estimated 20 minutes away from my destination, I grew frantic. I asked Peaches what was going on. There was no response, so I used a feature on Cruise’s ride-hailing center that enables a passenger to contact a person at a call center.\nThe Cruise representative confirmed that Peaches had gotten confused, apologized and assured me the robotaxi had been reprogrammed to get me to my original destination.\nIndeed, the car did seem to be headed back to where I requested. Then it started doing the old same thing again, making me wonder whether Peaches might like me a little too much to let me go. Feeling more like I was stuck on Mr. Toad’s Wild Ride at Disneyland than riding in an artificially intelligent car, I contacted Cruise’s call center. Peaches, they told me apologetically, seemed to be malfunctioning.\nSuddenly, Peaches came to a halt right in the middle of the street. I bolted from the Bolt, marooned several blocks from my destination shortly before 10 p.m.\nFortunately, I know my way around San Francisco, so I walked the rest of the way to where I needed to be. But what if this had happened to tourists? Would they know where to go? How would they feel being forced to walk around a strange neighborhood in a big city late at night?\nMAYBE DON’T STOP HERE\nWhen I discussed the incident during an interview for a story about robotaxis, Cruise CEO Kyle Vogt apologized and assured me the problem had been fixed. Sure enough, I was picked up and dropped off at my designated destinations in rides I took in two different Cruise robotaxis — one named Cherry and the other Hollandaise — on a mid-February night in San Francisco.\nYet other problems apparently persist.\nIn the first five months of this year alone, city transportation officials said they logged reports of more than 240 incidents in which a Cruise or Waymo vehicle may have created a safety hazard. The transportation officials believe the actual number of problems may be even higher because state regulators don’t currently require Cruise or Waymo to disclose every incident involving erratic behavior in their respective fleets.\nCruise and Waymo contend that the problems cited by San Francisco officials have been overblown and are stepping up their efforts to counter the criticism.\nIn full-page ads that recently ran in several newspapers, Cruise declared, “Humans are terrible drivers,” while trumpeting its robotaxis as a safer alternative. And Waymo’s co-CEO Tekedra Mawakana recently wrote an opinion piece in the San Francisco Chronicle asserting that the company’s technology is “mature enough to make a meaningful impact on road safety.”\nAs for my night with Peaches? Whenever I reminisce about the ride, I am also reminded of another trip to New York that I took two days after the robotaxi couldn’t deliver me to my destination.\nAfter I landed at JFK Airport, I hopped into an old-fashioned taxi driven by a fellow named Talid. I remember having a pleasant conversation with Talid, who chuckled as I recounted what happened with Peaches. At the end of the ride, Talid dropped me off at Grand Central Terminal, as I had requested. Then his cab drove off — with, of course, a human still behind the wheel.","Relevant":true,"Overview":"Based on the provided news article, here is the JSON-formatted response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": [\n        \"The news is about a robotaxi named Peaches that experienced a malfunction during a ride in San Francisco.\",\n        \"The robotaxi initially demonstrated promising self-driving technology, navigating through hilly San Francisco streets.\",\n        \"However, near the end of the ride, the robotaxi suddenly sped up and drove in the opposite direction, causing confusion and concern for the passenger.\",\n        \"The passenger had to contact the ride-hailing center, which confirmed the robotaxi's confusion and malfunction.\",\n        \"The robotaxi later came to a halt in the middle of the street, leaving the passenger stranded several blocks from the destination.\",\n        \"This incident is indicative of a malfunction in self-driving technology, raising concerns about the reliability and safety of robotaxis.\"\n    ]\n}\nIn summary, the news is indeed relevant to self-driving technology malfunctions as it describes a malfunction that occurred during a ride with a robotaxi named Peaches in San Francisco. The specific information highlights the malfunction's details, such as the sudden change in direction, the passenger's distress, and the robotaxi coming to a halt, all of which point to a self-driving technology malfunction.","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/selfdriving-cars-taxi-robotaxis-cruise-waymo-b09b2ebd5d130d48154e5f4b27f6f358","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":27,"Time":"7 November 2017","Title":"No safety driver in Waymo's self-driving car taxi fleet","Content":"No safety driver in Waymo's self-driving car taxi fleet\nPublished\n7 November 2017\ncomments\nComments\nShare\nMedia caption,\nThese taxis will not have a human behind the wheel\nBy Dave Lee\nNorth America technology reporter\nSelf-driving car company Waymo has said it is confident enough in its technology to ditch the human safety driver and open up its fleet to the public.\nCompanies testing autonomous cars typically have a human on hand ready to step in if the car malfunctions.\nBut Waymo, owned by Google parent company Alphabet, said it no longer needed that protection - though at first one of its employees will ride in the back with customers.\nThe service will be made available in Phoenix, Arizona over the next few months, Waymo’s chief executive John Krafcik said.\nIt is the not the first time Waymo has demonstrated cars without human drivers - even as a back-up - on public roads.\nIts driverless prototype has been tested over short journeys on pre-defined routes, including taking a blind man to a doctor’s appointment.\nIMAGE SOURCE,\nWAYMO\nImage caption,\nThe self-driving fleet will run on highly-modified Fiat minivans\nBut this latest move will greatly increase the ambition, and risk, of Waymo’s technology - it will eventually cover an area the size of Greater London, the company said.\n'Shared mobility'\nMembers of the public will be riding in Fiat Chrysler Pacifica minivans. Initially a Waymo employee will travel with the customers, but not behind the wheel as has been the case previously. Eventually the public will be allowed to travel alone.\nThe self-driving taxi fleet will at first be free to use, but the company envisions it will charge for the journeys at a later date.\nWaymo is a company created out of Google’s self-driving programme, and was seen as a way to step up efforts to commercialise the firm’s industry-leading efforts in autonomy.\nTo date, Waymo has conducted 3.5m miles on US public roads in a variety of scenarios. But unlike other companies working this kind of technology, such as Tesla, Waymo's products have not had a commercial application, and therefore, were making no money back on what has been an almost-decade-long, expensive investment.\n“Because we see so much potential in shared mobility, the first way people will get to experience Waymo’s fully self-driving technology will be as a driverless service,” Mr Krafcik said told an audience at Web Summit, a technology conference being held in Lisbon this week.\nOver time it is expected that self-driving technology could dramatically improve the safety on our roads. But experts have said the public should not expect this problem to be solved quickly.\nInstances of people being killed in accidents involving self-driving cars will likely increase, argued Nidhi Kalra, co-author on a new study looking at self-driving safety.\nBut her report said that should not be a reason to delay adoption.\n“Waiting for highly autonomous vehicles that are many times safer than human drivers misses opportunities to save lives,” the report concluded.\n\"It is the very definition of allowing perfect to be the enemy of good.”\nFollow Dave Lee on Twitter @DaveLeeBBC\nYou can reach Dave securely through encrypted messaging app Signal on: +1 (628) 400-7370\nMore on this story\nBaidu fund heats up driverless race\nPublished\n21 September 2017\nTesla to unveil electric lorry in October\nPublished\n14 September 2017\nWaymo retires Firefly driverless car\nPublished\n13 June 2017\nView comments","Relevant":true,"Overview":"Based on the provided news article, it appears to be relevant to self-driving technology, specifically the decision made by Waymo, a self-driving car company, to operate its self-driving car taxi fleet without a human safety driver. Here's the JSON-formatted response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to self-driving technology as it discusses Waymo's decision to operate its self-driving car taxi fleet without a human safety driver. Waymo, owned by Google's parent company Alphabet, expressed confidence in its technology and stated that it no longer needed a human safety driver. Initially, one of its employees will ride in the back with customers, but eventually, the public will be allowed to travel alone in the self-driving taxis. This development signifies a significant step in self-driving technology, as it eliminates the need for human intervention in case of malfunctions, which is a key aspect of self-driving technology. Waymo's decision to operate without a safety driver is a noteworthy milestone in the advancement of autonomous vehicles.\"\n}\nSo, the news is relevant to self-driving technology, and the specific information is detailed above.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-41909594","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":26,"Time":"Updated 11:08 AM EDT, Wed October 13, 2021","Title":"Waymo’s self-driving taxi struggles with left turns and puddles. But it’s still winning over some Arizona riders","Content":"Washington, DC\nCNN\n — \nWaymo says its self-driving vans have given tens of thousands of rides since the company launched a public robotaxi service in Chandler, Arizona a year ago.\nCNN Business tested Waymo’s service this month and spoke with eight Waymo customers who have used the service over the past year. They say they’re generally very satisfied and prefer it to Uber, Lyft or car ownership. Their stories suggest that there’s demand for robotaxis, but there are also significant barriers to self-driving cars becoming a mainstay in Americans lives anytime soon.\nA rider can request a ride via the Waymo app and is directed to walk to a pick-up point where the robotaxi can safely stop to pick them up. A Waymo Chrysler Pacifica minivan pulls up, displaying the riders initials on the dashboard so that it can’t be confused with another Waymo van.\n“Good afternoon,” a recording greets riders as they enter. “This car is all yours with no one up front.”\nThe vans still have a steering wheel, which turns right and left as the car navigates. A piece of plastic separates the front seats from passengers in the back, where riders are supposed to stay. Riders hit a button to start their trip.\nTrips are generally longer and less direct than an Uber or Lyft trip. The taxis still aren’t using shared turn lanes, the company said. That forces them to sometimes take roundabout routes, like three right turns instead of one left turn.\nWaymo says its vehicles have driven more than 20 million autonomous miles on public roads, but rainy days and puddles that linger on roads afterwards can still confuse the robotaxis enough that Waymo has to revert to an older, less impressive version of its robotaxis, featuring a human test driver behind the wheel.\nThe robotaxi service in Arizona is the culmination of 12 years of work since Google created its self-driving car project, which was later re-named Waymo, and made a separate subsidiary of Alphabet. The project began with grand ambition and promises. Google co-founder Sergey Brin said in 2012 that “you can count on one hand the number of years until ordinary people can experience this.”\nThe reality has turned out differently. Following years of promise, Waymo offers a robotaxi service in a 50 square mile portion of Chandler, Arizona, and has yet to offer fully autonomous rides to the public elsewhere.\nThere are advantages to a computer driving the car over a human, customers say. Some like riding alone, rather than being driven by a stranger. Waymo’s prices can be cheaper and the vehicles all drive with the same cautious personality, unlike ridehail drivers, whose driving styles vary, the riders say.\nThey say they’ve seen Waymo improve. Two riders said they had experienced sudden braking in Waymo taxis that appeared to be due to birds near the car, as no other vehicles were around. But both riders said the cars are no longer braking sharply when near birds. Waymo declined to comment on the riders’ experiences with sudden braking around birds. The Waymo vans are also more confident driving in crowded parking lots, according to riders.\nBut much work remains to be done. Waymo remains focused on learning to improve its services, rather than profitability according to Waymo co-CEO Tekedra Mawakana.\nWaymo struck a deal in 2018 with Fiat Chrysler to add up to 62,000 Pacifica Hybrids to its fleet. Three years later, the fleet remains at only slightly more than 700 vehicles. Former CEO John Krafcik, who led the deal, exited the role earlier this year.\nThree rights make a left turn\nChandler resident John Mitkowski gave his car to his college-age daughter after Waymo began offering rides. He figured he could use a combination of Waymo, Uber and Lyft to get around. He rides Waymo to work, run errands and go golfing, and calls it his favorite mode of transportation.\nHe’s taken more than 400 rides, more than anyone using the service, according to Waymo, which introduced CNN Business to Mitkowski.\nHe told CNN Business that he’ll only use Lyft or Uber today if Waymo has no cars available. He arranges his schedule to be somewhat flexible in case a Waymo car is immediately available or not available for 15 minutes.\nHe does work on his laptop during trips, so he doesn’t mind if they’re slightly longer than in an Uber or Lyft. He said he saves money with Waymo because there’s no need to tip a robotaxi. Riders told CNN Business that Waymo prices were generally comparable to Uber and Lyft.\nWaymo riders said they regularly draw attention from people who approach them with questions after drop-offs or attempt to record trips while driving alongside Waymos.\n“In the corner of my eye, I’ll notice a car driving erratically. I’ll look up from my laptop and it’s like, ‘What is he doing?’” Mitkowski said. “My car might be trying to make a right, but the guy doesn’t see it because he’s trying to record me and the no driver in the car.”\nWaymo’s vehicles avoid making a left turn from a busy street into his neighborhood. Mitkowski said that instead they take a slight detour into a neighborhood across the busy street so that they can cross the busy street at a traffic signal.\n“The object of their game appears to be the safest route,” Mitkowski said. “How do we not put the driver, the car and the passenger in danger?”\nWaymo rider Sophia Lovasz told CNN Business that a recent trip that would usually be a 10-minute drive took her 27 minutes with Waymo.\nHow Tesla can sell 'full self-driving' software that doesn't really drive itself\n“If you had somewhere to be at a specific time, you may not want to call a Waymo,” Lovasz said.\nEven so, Lovasz said she’s a very satisfied Waymo customer and views it as like a better second car for her family. She said she prefers it to Uber and Lyft and feels more independent in it.\nLovasz took her first ride this April, requesting a ride from the public library in Tempe, which neighbors Chandler. Lovasz said Waymo asked her to cross a seven-lane road and wait on a residential street across from the library to be picked up.\nLovasz crossed the street and walked into the neighborhood, waiting on a narrow median as the sun set, she said.\n“That was a little bit awkward as a Black woman just standing in the middle of some neighborhood where no one’s seen me before. I was really uncomfortable,” Lovasz said.\nWaymo has since updated its service so that riders can be picked up directly at the library. A CNN Business reporter requested a ride from the library last week. Following a 15-minute wait, a Waymo van appeared about to turn into the library.\nInstead it drove past the library, turned around, drove past again rather than making a left turn into the library, and then took a circuitous 15-minute route before finally entering the library.\nA Waymo spokeswoman explained that the Waymo van appeared unable to change lanes to get in the proper lane to make its initial turn into the library. After doubling back, the Waymo van didn’t make a left into the library because its vehicles aren’t using shared turn lanes. The spokeswoman said such turns are tricky because local law requires drives to exit the lane within a few hundred feet, which can lead to tight turns.\nEven so, Chandler is well-suited to self-driving cars in many ways, according to autonomous vehicles experts.\nThere’s little inclement weather, which is challenging for self-driving cars. Waymo has human test drivers behind the wheel on days with expected rain, and often afterwards. Light reflecting off puddles can make driving difficult for the vehicles.\nRiders on Waymo can hit buttons in the van's cabin to start and end their ride.\nAnother advantage of Chandler is the lack of pedestrians and cyclists. CNN Business encountered a single cyclist and no pedestrians, aside from in a Whole Foods parking lot, during its two rides,\nRiders told CNN Business they’ve seen Waymo vehicles get more confident and assertive in parking lots, where the vehicles have to deal with crossing pedestrians and vehicles backing out. But there are still problems that sometimes emerge.\nRajan Phadnis said that when he was dropped at a Walmart recently, the vehicle stopped in a way that parked in a driver in the lot. Phadnis said the driver of the vehicle yelled at him, making him feel uncomfortable, as he couldn’t control and move the Waymo vehicle.\nHe contacted Waymo’s support, and said after a couple minutes the Waymo vehicle moved out of the way.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it mentions that Waymo's self-driving vans still struggle with certain aspects of driving, such as left turns and puddles. Specifically, the article states that rainy days and puddles on roads can confuse the robotaxis enough that Waymo has to revert to an older, less impressive version of its robotaxis, featuring a human test driver behind the wheel. This highlights the malfunction of self-driving technology when faced with adverse weather conditions and the need for human intervention in such situations.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2021\/10\/12\/tech\/waymo-one-year\/index.html","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":31,"Time":"April 7, 2023","Title":"GM's Cruise recalls 300 self-driving vehicles to update software after bus crash","Content":"WASHINGTON, April 7 (Reuters) - General Motors' (GM.N) robotaxi unit Cruise LLC is recalling the automated driving software in 300 vehicles after one of its driverless vehicles crashed into the back of a San Francisco bus.\nThe March 23 collision was the fault of a software error in a Cruise automated vehicle (AV) that inaccurately predicted the movement of an articulated San Francisco Municipal Transit Authority bus, Cruise said on Friday. The crash caused moderate damage to the Cruise but did not result in any injuries.\nCruise in a National Highway Traffic Safety Administration (NHTSA) filing on Friday said the software was updated on March 25 to address concerns that the system \"may inaccurately predict the movement of articulated vehicles such as buses and tractor trailers.\"\n\"Fender benders like this rarely happen to our AVs, but this incident was unique,\" Cruise CEO Kyle Vogt said in a blog post. \"We do not expect our vehicles to run into the back of a city bus under any conditions, so even a single incident like this was worthy of immediate and careful study.\"\nAdvertisement · Scroll to continue\nCruise in a separate filing with California said the vehicle was traveling on Haight Street when a bus stopped ahead of it and the Cruise struck the rear bumper.\nVogt said, \"The bus’s behavior was reasonable and predictable. It pulled out into a lane of traffic from a bus stop and then came to a stop. Although our car did brake in response, it applied the brakes too late and rear-ended the bus at about 10 mph.\"\nAdvertisement · Scroll to continue\nThe driverless vehicle's view of the bus’s front section became fully blocked as the bus pulled out in front of the AV.\n\"Since the AV had previously seen the front section and recognized that the bus could bend, it predicted that the bus would move as connected sections with the rear section following the predicted path of the front section,\" Vogt said, adding that it was the only crash of its kind that the company has experienced.\nAdvertisement · Scroll to continue\nCruise said after the update it determined the crash would not recur.\nCruise in September disclosed that it recalled and updated software in 80 self-driving vehicles after a June crash in San Francisco that left two people injured.\nNHTSA last year said the software could \"incorrectly predict\" an oncoming vehicle's path.\nNHTSA in December opened a formal safety probe into the Cruise autonomous driving system after it received reports of incidents in which self-driving Cruise vehicles \"may engage in inappropriately hard braking or become immobilized.\"\nReporting by David Shepardson; Editing by David Goodman, Chizu Nomiyama and Mark Porter\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Based on the provided news article, it appears to be relevant to self-driving technology malfunctions. Here is the JSON-formatted response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": [\n        \"General Motors' Cruise LLC is recalling the automated driving software in 300 vehicles after a driverless vehicle crashed into the back of a San Francisco bus due to a software error.\",\n        \"The crash caused moderate damage to the Cruise vehicle but did not result in any injuries.\",\n        \"The software update on March 25 aimed to address concerns that the system may inaccurately predict the movement of articulated vehicles such as buses.\",\n        \"Cruise CEO Kyle Vogt mentioned that such incidents are rare, and they do not expect their vehicles to run into the back of a city bus under any conditions.\",\n        \"The software issue led to a collision that was the only crash of its kind that the company has experienced.\",\n        \"Cruise had previously recalled and updated software in 80 self-driving vehicles after a June crash in San Francisco, and NHTSA had expressed concerns about the software's ability to predict oncoming vehicle paths.\",\n        \"NHTSA had opened a formal safety probe into the Cruise autonomous driving system after reports of incidents involving self-driving Cruise vehicles engaging in inappropriately hard braking or becoming immobilized.\"\n    ]\n}\nThis news is relevant to self-driving technology malfunctions because it describes a specific incident where a self-driving vehicle crashed into the back of a bus due to a software error. It also mentions that the software update was made to address concerns related to the accuracy of predicting the movement of articulated vehicles like buses. Furthermore, it highlights previous incidents and investigations related to self-driving technology malfunctions, confirming the relevance of the news to this topic.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/technology\/gm-self-driving-unit-cruise-recalls-300-vehicles-after-crash-2023-04-07\/","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":37,"Time":null,"Title":"Tesla crash raises concerns about autonomous vehicle regulation","Content":"By David Shepardson, Bernie Woodall\n(Reuters) - The fatal crash of a Tesla Motors Inc Model S in Autopilot mode has turned up pressure on auto industry executives and regulators to ensure that automated driving technology is deployed safely.\nThe first such known accident, which occurred in Florida in May, has highlighted tensions surrounding efforts to turn over responsibility for braking, steering and driving judgments to machines. It may delay the U.S. government’s plan to outline guidelines for self-driving cars this month.\nThe cause of the Model S crash is still under investigation by federal and Florida state authorities, which are looking into whether the driver was distracted before his 2015 Model S went under a truck trailer.\nShares of Tesla and Mobileye NV, the maker of the camera vision system used in the Model S, rose on Friday as analysts said the accident was likely a short-term setback. The stocks fell in after-hours trading on Thursday after an investigation of the crash was made known.\nAdvocates of automating driving point to research that shows 90 per cent of accidents are caused by human mistakes. But machines can also make mistakes, or encounter situations they are not designed to handle.\nOn Friday, the U.S. National Highway Traffic Safety Administration (NHTSA) said U.S. traffic deaths rose by 7.7 percent to 35,200 in 2015 - the highest annual tally since 2008 and biggest single-year jump since 1966. Federal officials and industry executives say that toll could be cut by technology such as brakes that automatically engage when sensors detect an impending crash.\nIn March, 20 automakers agreed with regulators to make automatic emergency braking standard on nearly all U.S. vehicles by 2022, a move that could prevent thousands of rear-end crashes annually.\nBut automakers have issued numerous recalls for problems with such systems. Honda Motor Co recalled nearly 50,000 Acura SUVs and cars in June 2015 because the system can apply the brakes when it detects a vehicle accelerating and is driving along a metal fence or guardrail.\nU.S. Transportation Secretary Anthony Foxx told Reuters earlier this year he planned to propose regulatory guidelines by mid-July to clear the way for wider deployment of automated driving systems.\n“This technology is coming,” Foxx said. “Ready or not, it’s coming.”\nOn Wednesday, Foxx said the guidelines could take more time, and cautioned there are questions “that are probably unanswerable at this point.”\nNHTSA said in a report in March that there are relatively few hurdles to fully autonomous vehicles being used on U.S. roads, as long as vehicle design “allows a human driver to operate the vehicle with a wheel and pedals.”\nAt a conference in Detroit last month, NHTSA chief Mark Rosekind said he would accept technology that was “two times” better than conventional vehicles at preventing collisions.\nHours before the crash became public knowledge on Thursday, U.S. National Transportation Safety Board Chairman Christopher Hart said driverless cars will not be perfect.\n“There will be fatal crashes, that’s for sure,” Hart told the audience at the National Press Club in Washington, but added that will not derail the move toward driverless cars, even if the vehicles are not ready: “This train has left the station.”\nAlphabet Inc’s Google unit and other companies are racing to get self-driving cars on U.S. roads. Google has logged about 1.5 million miles of test driving, but has not said when it would offer its technology for sale.\n“We have a responsibility to get this out there as soon as we can and really as soon as we have data that says we’re better than the current system of flawed human drivers,” Google’s self-driving car CEO John Krafcik said last month in Washington.\nFormer NHTSA chief David Strickland, who is heading a self-driving coalition including Google and Ford Motor Co, told Reuters on Friday he does not “think this crash is going to change the arc for the entire industry ... and our pathway toward full self-driving.”\nAutomakers have wide latitude to install systems that intervene when drivers are not attentive - from lane-keeping systems to automatic emergency braking - and do not need prior approval from regulators, even if the systems are described as in “beta”, or public testing mode.\nFormer NHTSA chief Joan Claybrook said in an interview the agency needs to set performance standards for electronic systems like Autopilot. “It’s the like Wild West. The regulatory system is not being used,” Claybrook said.\nTesla’s Autopilot system “is explicitly denoted as a beta product,” said Jason Corso, an associate professor of electrical and computer engineering at the University of Michigan. The accident is a “wake-up call that significant further study is needed to model the sensors and the underlying recognition technologies on which these systems rely,” he said.\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the JSON-formatted response based on the provided news article:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it discusses the fatal crash of a Tesla Model S in Autopilot mode. This incident highlights concerns about the safety of automated driving technology. The article mentions that the crash is still under investigation to determine whether the driver was distracted, which is a common issue with autonomous systems. Additionally, it raises questions about the readiness of self-driving technology and the need for regulatory guidelines to ensure safe deployment. The article also notes that machines can make mistakes, emphasizing the potential for malfunctions in self-driving systems. Overall, the news provides specific information about a real-world incident involving self-driving technology, making it relevant to self-driving technology malfunctions.\"\n}\nSo, the news is relevant to self-driving technology malfunctions as it discusses a fatal crash involving a Tesla Model S in Autopilot mode and highlights various issues and concerns related to the technology.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/tesla-autopilot-idUSL1N19N24O","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":25,"Time":"Published 12:17 AM GMT+2, October 19, 2022","Title":"11 more crash deaths are linked to automated-tech vehicles","Content":"DETROIT (AP) — Eleven people were killed in U.S. crashes involving vehicles that were using automated driving systems during a four-month period earlier this year, according to newly released government data, part of an alarming pattern of incidents linked to the technology.\nTen of the deaths involved vehicles made by Tesla, though it is unclear from the National Highway Traffic Safety Administration’s data whether the technology itself was at fault or whether driver error might have been responsible.\nThe 11th death involved a Ford pickup. The automaker said it has to report fatal crashes to the government quickly, but it later determined that the truck was not equipped with its partially automated driving system.\nThe deaths included four crashes involving motorcycles that occurred during the spring and summer: two in Florida and one each in California and Utah. Safety advocates note that the deaths of motorcyclists in crashes involving Tesla vehicles using automated driver-assist systems such as Autopilot have been increasing.\nThe new fatal crashes are documented in a database that NHTSA is building in an effort to broadly assess the safety of automated driving systems, which, led by Tesla, have been growing in use. Tesla alone has more than 830,000 vehicles on U.S. roads with the systems. The agency is requiring auto and tech companies to report all crashes involving self-driving vehicles as well as autos with driver assist systems that can take over some driving tasks from people.\nThe 11 new fatal crashes, reported from mid-May through September, were included in statistics that the agency released Monday. In June, the agency released data it had collected from July of last year through May 15.\nThe figures that were released in June showed that six people died in crashes involving the automated systems, and five were seriously hurt. Of the deaths, five occurred in Teslas and one a Ford. In each case, the database says that advanced driver assist systems were in use at the time of the crash.\nMichael Brooks, executive director of the nonprofit Center for Auto Safety, said he is baffled by NHTSA’s continued investigations and by what he called a general lack of action since problems with Autopilot began surfacing back in 2016.\n“I think there’s a pretty clear pattern of bad behavior on the part of Tesla when it comes to obeying the edicts of the (federal) safety act, and NHTSA is just sitting there,” he said. “How many more deaths do we need to see of motorcyclists?”\nBrooks noted that the Tesla crashes are victimizing more people who are not in the Tesla vehicles.\n“You’re seeing innocent people who had no choice in the matter being killed or injured,” he said.\nA message was left Tuesday seeking a response from NHTSA.\nTesla’s crash number may appear elevated because it uses telematics to monitor its vehicles and obtain real-time crash reports. Other automakers lack such capability, so their crash reports may emerge more slowly or may not be reported at all, NHTSA has said.\nNHTSA has been investigating Autopilot since August of last year after a string of crashes since 2018 in which Teslas collided with emergency vehicles parked along roadways with flashing lights on. That investigation moved a step closer to a recall in June, when it was upgraded to what is called an engineering analysis.\nIn documents, the agency raised questions about the system, finding that the technology was being used in areas where its capabilities are limited and that many drivers weren’t taking steps to avoid crashes despite warnings from the vehicle.\nNHTSA also reported that it has documented 16 crashes in which vehicles with automated systems in use hit emergency vehicles and trucks that were displaying warning signs, causing 15 injuries and one death.\nThe National Transportation Safety Board, which also has investigated some of the Tesla crashes dating to 2016, has recommended that NHTSA and Tesla limit Autopilot’s use to areas where it can safely operate. The NTSB also recommended that NHTSA require Tesla to improve its systems to ensure that drivers are paying attention. NHTSA has yet to act on the recommendations. (The NTSB can make only recommendations to other federal agencies.)\nMessages were left Tuesday seeking comment from Tesla. At the company’s artificial intelligence day in September, CEO Elon Musk asserted that, based on the rate of crashes and total miles driven, Tesla’s automated systems were safer than human drivers — a notion that some safety experts dispute.\n“At the point of which you believe that adding autonomy reduces injury and death, I think you have a moral obligation to deploy it,” Musk said. “Even though you’re going to get sued and blamed by a lot of people. Because the people whose lives you saved don’t know that their lives were saved. And the people who do occasionally die or get injured, they definitely know, or their state does, that it was, whatever, there was a problem with Autopilot.”\nTeslas with automated systems have driven more than 3 million vehicles on the road, Musk said.\n“That’s a lot of miles driven every day. And it’s not going to be perfect. But what matters is that it is very clearly safer than not deploying it.”\nIn addition to Autopilot, Tesla sells “Full Self-Driving” systems, though it says the vehicles cannot drive themselves and that motorists must be ready to intervene at all times.\nThe number of deaths involving automated vehicles is small compared with the overall number of traffic deaths in the U.S. Nearly 43,000 people were killed on U.S. roads last year, the highest number in 16 years, after Americans returned to the roads as the pandemic eased. Authorities blamed reckless behavior such as speeding and driving while impaired by drugs or alcohol for much of the increase.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving technology malfunctions because it reports on eleven fatal crashes involving vehicles using automated driving systems. Ten of these deaths involved vehicles made by Tesla, although it's unclear from the National Highway Traffic Safety Administration's data whether the technology itself was at fault or if driver error might have been responsible. The report also mentions that the deaths included crashes involving motorcycles, and safety advocates note an increasing trend in motorcyclists' deaths in crashes involving Tesla vehicles using automated driver-assist systems such as Autopilot. The news discusses NHTSA's ongoing investigations into these incidents and concerns raised by experts about the safety of Tesla's Autopilot system.\"\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/technology-business-traffic-government-and-politics-a16c1aba671f10a5a00ad8155867ac92","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":29,"Time":"Tue 15 Aug 2023 01.33 CEST","Title":"Robotaxi breakdowns cause mayhem in San Francisco days after expansion vote","Content":"After months of debate, the California Public Utilities Commission voted on Thursday to approve an expansion that allows vehicles on the streets at all hours of the day. This weekend, as videos of robotaxi malfunctions began appearing on social media, opponents say they are seeing their fears realized.\nSan Francisco to get round-the-clock robo taxis after controversial vote\nRead more\nOn Friday, amid increased traffic from the Outside Lands music festival, a number of self-driving cars seemed to glitch. One TikTok user recorded a Cruise vehicle causing “mayhem” outside of the festival where it was stuck at an angle in the middle of the street. “They’re causing mad confusion over here,” the user said. A Twitter user shared a video of a Cruise vehicle nearly running over a family on a crosswalk over the weekend. In San Francisco’s North Beach neighborhood, as many as 10 Cruise cars blocked a main thoroughfare, stoking anger from locals.\nAaron Peskin, who represents the neighborhood on the San Francisco board of supervisors said he received a number of complaints from his constituents that night. He said on Twitter that the snafu underscored the concerns he and others raised ahead of the CPUC vote.\n“Why do state commissioners think it’s OK to put people in danger + create traffic chaos on our neighborhoods streets?” he wrote. “We warned them + they refused to listen.”\nIn a tweet, Cruise said that the music festival caused issues with the cellphone networks the vehicles rely on for connectivity, and that it is “actively investigating and working on solutions to prevent this from happening again”. Critics pointed out that the 10-car traffic jam occurred several miles from the concert.\nWhile the companies got the green light on Thursday to expand operations, it is unclear if they have already deployed more cars on the roads. Previously, cars were limited to driving at night and, under beta testing programs, were not charging passengers for rides.\nCruise did not respond to a request for comment on whether it has increased its ridership since Thursday. A spokesperson from Waymo, the Google-owned firm that has also been testing self-drivng vehicles in San Francisco, said the company is taking an “incremental approach” to expanding its fleet in the city, but did not clarify whether it has already added more cars to the road following the vote.\nA representative from Safe Street Rebel, the anti-car activist group that has been disabling autonomous vehicles with traffic cones, said incidents like this are poised to increase. The group has started collecting reports of malfunctioning, crashing, or dangerous self-driving cars on its website.\n“Given the amount of problems on the street already caused by robot cars, we have a hunch things will not go as smoothly as GM and Google hope,” they said. “The only path forward to healthier, safer, more sustainable cities is taking cars off the street, not adding new ones. And of course, more cones.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions. It describes multiple incidents of robotaxi malfunctions, including a Cruise vehicle causing 'mayhem' outside a music festival, nearly running over a family on a crosswalk, and blocking a main thoroughfare. These incidents suggest issues with the self-driving technology's performance and safety, aligning with concerns related to self-driving technology malfunctions.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/us-news\/2023\/aug\/14\/san-francisco-robotaxi-waymo-cruise-breakdown","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":42,"Time":"19 August 2022","Title":"Driverless cars: Experts warn no easy answer to how safe they should be","Content":"Driverless cars: Experts warn no easy answer to how safe they should be\nPublished\n19 August 2022\ncomments\nComments\nShare\nIMAGE SOURCE,\nGETTY IMAGES\nBy Chris Vallance\nTechnology reporter\nThere is no easy answer as to how safe self-driving cars should be, an adviser to a new government-backed report has told the BBC.\nThe report, from the Centre for Data Ethics and Innovation, warns that it might not be enough for self-driving cars to be safer than normal cars.\nIt says the public may have little tolerance for driverless car crashes - even if they are safer on average.\nIt comes as the government sets out its plans for self-driving cars.\nNo self-driving cars are currently allowed on UK roads, but the government has now said the first such vehicles could be on UK roads by 2025.\nSome cars, coaches and lorries with self-driving features could even be operating on motorways in the next year, the Department for Transport said.\nThe government's plans include a \"safety ambition\" for vehicles: that they should be as safe as a competent human driver.\nIt says this will inform standards that need to be reached in order to be allowed to self-drive on the roads - and manufacturers could face sanctions if they are not met.\nBut the CDEI, a government expert body that leads its work on trustworthy innovation using data and AI, says the question of how safe autonomous vehicles should be is not one science alone can answer.\nIt says the public may have little tolerance for crashes that are seen as the fault of \"faceless technology companies or lax regulation\" even if, it adds, on average driverless cars are safer than humans.\nAnd if the public expect self-driving cars to be as safe as trains or planes, it would require a 100-fold increase in average safety over manually driven cars, it warns.\n\"What we wanted to do was say there's not an easy answer to this question\" said Professor Jack Stilgoe of University College London who advised the CDEI. He suggested that establishing how safe they should be was a democratic decision.\nThe CDEI says it is also important to consider how risk is distributed between different groups. Even if there are improvements in overall safety, \"some groups may see substantial safety improvements while others see none or even face new hazards\".\nBiased drivers\nThe report advises that other risks will need scrutiny as the technology is rolled out.\nOne is the potential for bias in algorithms controlling the cars.\nIt warns that some groups, such as wheelchair users, may be underrepresented in data used to train the software algorithms which control the cars - potentially causing bias.\nThe report also says that self-driving cars should be clearly identified, and that \"people have a right to know what sort of agents they are sharing the road with\".\nAn academic survey quoted by the CDEI in the report suggested that 86% of the public agreed with this.\nProfessor Stilgoe said there were also serious moral questions about how the testing of self-driving vehicles is conducted on public roads, as other road users could in effect become participants in these trials whether they liked it or not.\n\"There is something quite important about the ethical principle of informed consent,\" he said.\nThe technology might result in pressure to alter roads and the rules of the road to suit self-driving cars.\nProfessor Stilgoe said these needed to be debated and discussed transparently.\n\"The danger is sort of sleepwalking in to a world in which these changes happen in order to suit one mode of transport - and the benefits then don't get spread very widely,\" he said.\nSelf-driving road map\nOn Friday, the government is publishing a policy paper setting out planned laws ready for the introduction of self-driving vehicles on British roads.\nThe new laws will be brought forward when parliamentary time allows, the government has said.\nThe legislation is expected to state that manufacturers are responsible for the vehicle's actions when self-driving - something the Law Commission recommended earlier this year.\nIn April, proposed updates to the Highway Code were announced that would allow drivers to watch entertainment on a car's screen while the vehicle was self-driving.\nInitially this would likely be whilst the vehicle was driving at slow speed on a motorway, such as in congested traffic.\nVehicles that can drive themselves on motorways could be available to purchase within the next year, the government reiterated.\nThe government has also repeated its goal of a wider rollout of self-driving technologies by 2025 and £100m of funding for industry and research.\nThatcham Research, the motor insurers' automotive research centre, welcomed the government's ambition but warned \"complete clarity around the driver's legal responsibilities\" was needed, along with transparency on how the technology is marketed, \"how the dealer describes systems when handing over the keys and how the self-driving system itself communicates with the driver\".\nRelated Topics\nDriverless cars\nMore on this story\nWatching TV to be allowed in self-driving cars\nPublished\n20 April 2022\nMajor legal changes needed for driverless cars\nPublished\n26 January 2022\nView comments","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving technology malfunctions because it discusses the challenges and safety concerns associated with self-driving cars. The report from the Centre for Data Ethics and Innovation raises concerns about the public's tolerance for self-driving car crashes, even if these cars are statistically safer than human-driven cars. It highlights the complexity of determining how safe self-driving cars should be, indicating that it's not solely a scientific matter. Additionally, the news mentions the potential for bias in the algorithms controlling self-driving cars, which can lead to safety issues, and the importance of clear identification of self-driving cars on the road. Overall, the article provides insights into various safety-related aspects of self-driving technology.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-62598618","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":43,"Time":"SEPTEMBER 29, 2017","Title":"Lyft IPO puts investors in self-driving cars as well as ride services","Content":"By Paul Lienert, Joseph White\nDETROIT (Reuters) - An initial public offering by Lyft Inc will give investors a way to jump into self-driving cars, although the ride services company and rival Uber Technologies Inc may have to wait years before sending a driverless robotaxi to a customer.\nFILE PHOTO: An illuminated sign appears in a Lyft ride-hailing car in Los Angeles, California, U.S. September 21, 2017. REUTERS\/Chris Helgren\/File Photo\nOn Thursday, Reuters reported that San Francisco-based Lyft is close to hiring an IPO advisory firm as a first concrete step toward becoming publicly listed.\nLyft would establish a public valuation for ride services startups that has been elusive. Lyft was valued at $7.5 billion in its latest fundraising, while larger rival Uber is valued at $68 billion. Some question whether that is fair, given the range of scandals at Uber this year. In August, Uber’s new CEO Dara Khosrowshahi set a new tentative timeline for Uber’s IPO of between 18 and 36 months.\nA ride services IPO tests the belief of many auto industry insiders that individual auto ownership will wane as people will sell their cars and rely on “mobility services” such as ride services, car share, bike share - and self-driving vehicles.\nA Lyft IPO “is going to tell us what the valuations of these mobility services companies really are,” said Mike Ramsey, research director at Gartner.\nA public offering by Lyft, which offers similar services to Uber and markets itself as a friendlier company, is “a proxy for whether next-generation mobility is as disruptive as private investors think it is,” said venture capitalist Evangelos Simoudis of Synapse Partners.\nSelf-driving technology is being developed and tested by dozens of companies, although it is not ready for real-world use.\nSelf-driving cars are expected to reinforce the move away from individual car ownership, by driving down the cost of a ride and driving up the cost of vehicles.\nAuto industry executives estimate the cost of driving a mile in an owner-driven vehicle is about 60 cents to $1.00, and a mile with a Lyft or Uber driver is about $2.50 to $3.00.\nReplacing the human driver with artificial intelligence and electronics could eliminate about two-thirds of the cost, they say, and Lyft is making an early, important bet on taking advantage of that change.\nAn IPO that values Lyft higher than its current $7.5 billion “is a step in a long journey toward self-driving” vehicles, said Jeff Schuster, senior vice president of LMC Automotive.\nLyft already has struck agreements for automakers Ford Motor Co F.N, Tata Motors Ltd's TAMO.NS Jaguar Land Rover and General Motors Co GM.N; and technology companies Waymo, a unit of Alphabet Inc GOOGL.O, Drive.ai and nuTonomy to test autonomous vehicles in its network. That could eventually lead to the technology companies and fleet owners using Lyft as their network provider, although some, including GM, may also create their own services.\nUber, by comparison, has a much deeper financial commitment to developing self-driving technology. “Partner with as many tech companies as possible is the right move,” given the number of developers, Ramsey said. But, he added, it would likely be the end of the next decade before there were substantial numbers of self-driving cars creating revenue.\nReporting by Paul Lienert in Detroit; Editing by Peter Henderson and Phil Berlowitz\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions as it discusses the development and testing of self-driving technology by various companies, including Lyft. It mentions that self-driving technology is not yet ready for real-world use, indicating the possibility of malfunctions or issues in the technology's development. However, the specific information about self-driving technology malfunctions is not provided in this news article.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-lyft-ipo-investment\/lyft-ipo-puts-investors-in-self-driving-cars-as-well-as-ride-services-idUSKCN1C41IY","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":36,"Time":"12:17 AM GMT+2•August 11, 2022","Title":"Ralph Nader asks NHTSA to recall Tesla’s ‘dangerous and irresponsible’ FSD","Content":"cameraImage Credits: Getty Images\nRalph Nader, a political and consumer advocate and former presidential candidate, has issued a statement calling Tesla’s “so-called” full self-driving (FSD) technology “one of the most dangerous and irresponsible actions by a car company in decades.”\nNader is calling on the National Highway Traffic Safety Administration (NHTSA) to use its safety recall authority to order that FSD technology be removed in every Tesla. Per CEO Elon Musk’s recent statements, that’s about 100,000 vehicles.\nThe author of the bestselling book “Unsafe at Any Speed,” which criticized the American auto industry, cited research that found FSD malfunctions every eight minutes. That research was published in January by The Dawn Project, an organization aiming to ban unsafe software from safety critical systems that put out a full-page ad in The New York Times slating Tesla’s FSD, which analyzed data from 21 YouTube videos of Tesla owners using FSD beta software.\n“This nation should not allow this malfunctioning software which Tesla itself warns may do the ‘wrong thing at the worst time’ on the same streets where children walk to school,” wrote Nader. “Together we need to send an urgent message to the casualty-minded regulators that Americans must not be test dummies for a powerful, high-profile corporation and its celebrity CEO. No one is above the laws of manslaughter.”\nNader’s callout comes as Tesla is gearing up to release the next version of its FSD software, version 10.69, on August 20. Musk tweeted out the announcement, saying nothing about the next iteration’s capabilities other than: “This release will be big.” During Tesla’s Q2 earnings call, Musk also said Tesla would increase the price of the software and that the automaker was hoping to “solve full self-driving” by this year.\nFSD Beta 10.69 drops on 8\/20\n— Elon Musk (@elonmusk) August 7, 2022\nReally, Nader should be targeting Tesla’s Autopilot, as well. Tesla and Musk have been adamant in the past that FSD has not been responsible for any crashes or deaths. (However, a recent YouTube video from AI Addict shows a Tesla in FSD mode colliding with a bike lane barrier post.) Autopilot, on the other hand, has likely been the cause of several crashes. NHTSA is currently investigating 16 crashes in which Tesla owners were potentially engaging Autopilot and then crashed into stationary emergency vehicles, resulting in 15 injuries and one fatality. Since 2016, there have been 38 special investigations into crashes involving Tesla vehicles, of which 18 were fatal.\nOther automakers have come out with similar ADAS technology, and based on NHTSA’s recent ADAS crash report, appear to have far fewer crashes. It’s difficult to compare how dangerous Tesla’s technology is in relation to its rivals, in part because there are far more ADAS-equipped Teslas on the road than any other vehicle.\nNHTSA did not respond immediately for a request for comment.\nIn a pair of July 28 filings, the California Department of Motor Vehicles accused Tesla of false advertising to promote its Autopilot and FSD technologies — both of which are advanced driver assistance systems and do not provide full autonomous driving. While Tesla’s website states that “the currently enabled features require active driver supervision and do not make the vehicle autonomous,” the DMV told the Los Angeles Times that the disclaimer “contradicts the original untrue or misleading labels and claims, which is misleading, and does not cure the violation.”\nThe California DMV also said earlier this year that it was revisiting its approach to regulating Tesla’s autonomous vehicle technology, as the agency does with every other company that claims to pursue full self-driving and does public road testing. Tesla has gotten away without reporting crashes and system failures to the DMV for so long because its systems fall under the ADAS category, which requires a human driver must be present. However, after reviewing dozens of videos showing “dangerous use” of that technology — and such use is informed by the way Tesla and its CEO Elon Musk speak about the technology — the DMV decided to reevaluate.\nThis reevaluation is ongoing, and the DMV told TechCrunch it could not comment until it is complete. That said, based on the DMV’s most recent claims that Tesla is falsely advertising, Tesla could be facing revocation of its licenses to make or sell its cars in California, in the worst case. That probably won’t happen, but if it did, it would spell trouble for the EV maker. California is home to Tesla’s most loyal buyer base.\nMusk has had a fraught relationship with the state ever since May 2020, when Alameda County ordered Tesla to shutter its Fremont factory to stop the spread of COVID. In October last year, Musk announced Tesla would be moving its headquarters to Austin, Texas.\nThe celebrity executive has also repeatedly underlined the importance of FSD to the company, saying in June that without it, Tesla is “worth basically zero.” It is likely based on the belief by many, including Nader, that FSD is not what it’s cracked up to be; Nader went on to tweet Wednesday that Tesla’s stock is vastly overvalued.\n“Tesla and @elonmusk exposed the technological stagnation of the auto companies and broke ground with EVs and other climate-benign technologies,” tweeted Nader. “However, a fast moving company can not obscure wildly speculative stock valuation on top of a general stock market bubble that could implode on pension and mutual fund savings of millions of Americans. Fundamentals can’t be ignored.” ","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving technology malfunctions because Ralph Nader, a prominent political and consumer advocate, has called Tesla's full self-driving (FSD) technology 'one of the most dangerous and irresponsible actions by a car company in decades.' Nader cited research that found FSD malfunctions every eight minutes, which suggests a significant issue with the technology. He has called on the National Highway Traffic Safety Administration (NHTSA) to recall FSD technology in Tesla vehicles, which indicates a concern about its safety and functionality. Additionally, the mention of a Tesla in FSD mode colliding with a bike lane barrier post in a YouTube video further highlights potential malfunctions in self-driving technology.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/08\/10\/ralph-nader-asks-nhtsa-to-recall-teslas-dangerous-and-irresponsible-fsd\/","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":44,"Time":null,"Title":"Tesla automated parking problems seen liability of app 'driver' for now","Content":"By Suzanne Barlyn\n(Reuters) - Tesla Inc owners summoning their driverless cars in parking lots are likely liable for crashes, lawyers said after a series of internet videos showed problems with cars running new software.\nTesla vehicles are shown on a delivery truck in Los Angeles, California, September 27. 2019. REUTERS\/Mike Blake\nIf the accidents pile up, though, Tesla itself is sure to be brought into a legal fight, insurance industry experts said.\nThe incidents highlight a shifting landscape for long-held assumptions about auto insurance and accident blame as more car manufacturers offer features that can automate parallel parking, avoid collisions, and take over steering during traffic, among other things.\nA Tesla software update last week added a so-called Smart Summon feature for some customers. When the car is within 200 feet and in their line of sight, they can use a phone app to summon the vehicle in a parking lot. Users start the car by holding down a button and stop the car by releasing it, Tesla said in instructions, warning users to be careful.\nU.S. regulators are looking into parking lot crashes involving Tesla cars driving themselves to their owners, the National Highway Traffic Safety Administration (NHTSA) said on Wednesday.\n“This was an interesting one to explain to my insurance,” wrote one user in YouTube and Twitter video posts of a Tesla scraping against a garage door frame while exiting in summon mode. “Silly feature cost me time and money.”\nTesla did not respond to a request for comment but Chief Executive Elon Musk on Wednesday tweeted that there were more than 550,000 Smart Summon uses in the first few days.\nInsurance claims for such incidents will go through the car owner’s traditional auto coverage, said Jennifer Dukarski, a lawyer in Ann Arbor, Michigan, who represents automotive suppliers in disputes about safety and autonomous car technology.\n“But as the number of incidents build, you’ll find someone who will entertain a class action [lawsuit] dealing with a product defect,” Dukarski said.\nAutomated features also raise the question of whether the owner or the auto manufacturer is at fault when accidents occur.\n“The rationale for the owner being responsible is that the laws have not caught up to autonomous vehicles,” said Keith McKenna, an insurance lawyer in New York who represents corporate policyholders.\nIf fully-automated cars someday becomes the norm, liability will shift to manufacturers and their insurance, McKenna said.\nThe technology is not sophisticated enough to be deemed “driverless” under California’s motor vehicle rules, the California Department of Motor Vehicles said, essentially leaving the app user outside of the car responsible for accidents.\nStill, limitations to the app and other features make Tesla more responsible when accidents occur, said Mike Morgan, a Florida personal injury lawyer. His firm sued Tesla last year on behalf of a client who blames Tesla for his car crashing into a disabled vehicle.\n“You are still responsible for your car and  must monitor it and its surroundings at all times and be within your line of sight because it may not detect all obstacles,” Tesla said in instructions for the new feature.\nBut it may be too late to avoid a crash when a user attempts to stop the car, Morgan said.\n“These vehicles provide a false sense of human control,” Morgan said.\nReporting by Suzanne Barlyn; Additional reporting by Tina Bellon in New York; Editing by Peter Henderson and Nick Zieminski\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it discusses problems with Tesla's new software, specifically the Smart Summon feature. This feature allows Tesla owners to summon their vehicles in a parking lot using a phone app. Several incidents and internet videos have shown issues with cars using this feature, including scraping against garage door frames and potential accidents. The article also mentions that U.S. regulators are investigating parking lot crashes involving Tesla cars driving themselves to their owners. These incidents highlight potential malfunctions in the self-driving technology, specifically related to Tesla's Smart Summon feature.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-tesla-safety-insurance-idUSKBN1WJ29Z","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":47,"Time":"AUGUST 9, 2017","Title":"Exclusive: Tesla developing self-driving tech for semi-truck, wants to test in Nevada","Content":"By Marc Vartabedian\nSAN FRANCISCO (Reuters) - Tesla Inc is developing a long-haul, electric semi-truck that can drive itself and move in “platoons” that automatically follow a lead vehicle, and is getting closer to testing a prototype, according to an email discussion of potential road tests between the car company and the Nevada Department of Motor Vehicles (DMV), seen by Reuters.\nFILE PHOTO - Elon Musk, founder, CEO and lead designer at SpaceX and co-founder of Tesla, speaks at the International Space Station Research and Development Conference in Washington, U.S. on July 19, 2017. REUTERS\/Aaron P. Bernstein\/File Photo\nMeanwhile, California officials are meeting with Tesla on Wednesday “to talk about Tesla’s efforts with autonomous trucks,” state DMV spokeswoman Jessica Gonzalez told Reuters.\nThe correspondence and meeting show that Tesla is putting self-driving technology into the electric truck it has said it plans to unveil in September, and is advancing toward real-life tests, potentially moving it forward in a highly competitive area of commercial transport also being pursued by Uber Technologies Inc [UBER.UL] and Alphabet Inc’s Waymo.\nAfter announcing intentions a year ago to produce a heavy-duty electric truck, Musk tweeted in April that the semi-truck would be revealed in September, and repeated that commitment at the company’s annual shareholder meeting in June, but he has never mentioned any autonomous-driving capabilities.\nTesla has been a leader in developing self-driving technology for its luxury cars, including the lower-priced Model 3, which it is beginning to manufacture.\nSeveral Silicon Valley companies developing autonomous driving technology are working on long-haul trucks. They see the industry as a prime early market for the technology, citing the relatively consistent speeds and little cross-traffic trucks face on interstate highways and the benefits of allowing drivers to rest while trucks travel.\nSome companies also are working on technology for “platooning”, a driving formation where trucks follow one another closely. If trucks at the back of the formation were able to automatically follow a lead vehicle, that could cut the need for drivers.\nSilicon Valley startup Peloton Technology, for example, is working with several truck makers including Volvo on its platooning system, which it sees as a precursor to autonomy.\nTesla’s high-flying shares, up almost 70 percent this year, closed down 0.5 percent at $363.53 on Nasdaq, but rose slightly after hours.\nPROTOTYPE TESTS\nAn email exchange in May and June between Tesla and Nevada DMV representatives included an agenda for a June 16 meeting, along with the Nevada Department of Transportation, to discuss testing of two prototype trucks in Nevada, according to the exchange seen by Reuters.\n“To insure we are on the same page, our primary goal is the ability to operate our prototype test trucks in a continuous manner across the state line and within the States of Nevada and California in a platooning and\/or Autonomous mode without having a person in the vehicle,” Tesla regulatory official Nasser Zamani wrote to Nevada DMV official April Sanborn. He made no reference to any dates for potential road tests.\nNo companies yet have tested self-driving trucks in Nevada without a person in the cab. On July 10, Zamani inquired further to the Nevada DMV about terms for a testing license, an email seen by Reuters shows.\nCalifornia DMV spokeswoman Gonzalez said that Tesla had requested a meeting on Wednesday to introduce new staff and talk about Tesla’s efforts with autonomous trucks. She said that the DMV was not aware of the level of autonomy in the trucks.\nTesla declined to comment on the matter, referring Reuters to the previous statements by Musk, who has discussed the truck in tweets and at the annual shareholder meeting.\nNevada officials confirmed the meeting with Tesla had occurred and said that Tesla had not applied for a license so far. They declined to comment further.\nSKEPTICS\nMusk has said that potential customers are eager to get a Tesla electric long-haul truck, but he faces doubt that the company can deliver.\nWhile established trucking companies and truck manufacturing startups have poured resources into electrifying local package delivery fleets, battery range limitations have largely kept the industry from making electric trucks that travel across swaths of the country.\nLithium ion battery researcher Venkat Viswanathan of Carnegie Mellon University said electric long-haul trucking is not economically feasible yet.\n“Your cargo essentially becomes the battery,” Viswanathan said of the massive batteries that would be needed to make range competitive with diesel.\nDiesel trucks used for cross-country hauls by United Parcel Service Inc can travel up to 500 miles (800 km) on a single tank, according to UPS’s director of maintenance and engineering, international operations, Scott Phillippi. By comparison, the company’s electric local package delivery trucks travel up to 80 miles on a full charge.\nReporting By Marc Vartabedian; Additional reporting by Alexandria Sage; Editing by Peter Henderson and Bill Rigby\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology, as it discusses Tesla's development of a long-haul, electric semi-truck with self-driving capabilities. The article mentions that Tesla is getting closer to testing a prototype of the self-driving semi-truck and that it plans to use self-driving technology in this vehicle. Additionally, it talks about Tesla's correspondence with the Nevada Department of Motor Vehicles regarding testing autonomous trucks and platooning technology. While it doesn't provide specific details about malfunctions, it does relate to the development and testing of self-driving technology in the commercial transport sector.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/cbusiness-us-tesla-truck-autonomous-idCAKBN1AP2GD-OCABS","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":39,"Time":"Sun 1 May 2022 12.00 CEST","Title":"California tech titan’s Senate run has one target: Tesla’s self-driving software","Content":"Dan O’Dowd is hardly the first California tech titan to bankroll his own campaign for high political office. What makes him unusual is that he has no interest in winning the US Senate seat he is vying for, or even in challenging the other candidates competing in the 7 June primary.\nForget Twitter: Elon Musk has his hands full keeping Tesla on the road\nRead more\nO’Dowd, a software entrepreneur with a 40-year history of working on military, aerospace and other commercial contracts, is running, rather, out of frustration at his fellow tech entrepreneur, Elon Musk, whom he accuses of endangering road safety with a driver assistance software package he’s put in his Tesla electric cars.\nO’Dowd doesn’t deny that this is a strikingly narrow platform on which to run for public office. He’s aware, too, that there are risks as well as potential rewards in using a political campaign to take a swing at the world’s richest man – especially now that Musk is dominating headlines as the prospective new owner of Twitter.\nBut O’Dowd is also unapologetic about being a single-issue candidate. His mission, he says, is to ensure that government regulators become much tougher with the “move fast and break things” ethos that has inspired Musk and many other tech pioneers over the past two decades. He’s spent $650,000 on advertising so far and seems poised to spend a lot more over the next six weeks.\nDan O’Dowd is running for US Senate, but has no interest in winning. Photograph: Courtesy Dan O'Dowd Senate Campaign\nAnd, in his mind, it’s not just about Musk. O’Dowd believes that the problems he’s documented with Tesla’s “full self-driving” software package – problems that, according to publicly available video footage, have caused vehicles to veer unexpectedly into the wrong lane, turn the wrong way, crash into poles and endanger other road users – are emblematic of a broader and increasingly serious problem.\nIn a world increasingly dependent on computers to run critical machinery, O’Dowd says, it’s vital that the software is built as securely and reliably as possible, only Silicon Valley rarely sees it that way. If we can’t stop semi-autonomous cars crashing into things, he argues, how are we supposed to keep our power grids, hospitals, office computers and other vital systems safe from cyber attack and other threats?\nO’Dowd has spent his career aspiring to build “software that never fails and can’t be hacked” for projects including fighter jets, nuclear bombers and a space exploration vehicle for Nasa, so the issue is deeply personal to him.\n“The bigger picture,” he said, “is about software and computers in general. It’s about making computers safe for humanity.”\nPolitical analysts have been scratching their heads over this pitch from the moment O’Dowd announced his late entry into the Senate race on 19 April. Was this really a politically motivated campaign, they asked, or just a branding exercise to drum up business for O’Dowd’s Santa Barbara-based company, Green Hills Software, and whose clients include several of Tesla’s car making rivals including General Motors, BMW and Daimler?\nDan O’Dowd has accused his fellow tech entrepreneur of endangering road safety. Photograph: Aly Song\/Reuters\nWas O’Dowd targeting Tesla because he really thought it was behaving worse than the dozens of other companies working to develop a self-driving car, or was he merely piggy-backing on Musk’s name recognition to attract media attention?\n“It’s unusual for a campaign to be so singularly focused on a very discrete commercial issue,” said Dan Weiner, director of the elections and government program at the Brennan Center for Justice. “You could say this is the apotheosis of a system in which corporate interests have free rein to engage with the political process.”\nO’Dowd insisted that his campaign had nothing to do with commercial self-interest. Rather, he said, he found Tesla’s “full self-driving” software package more alarming than anything else in commercial use because it was, in his words, “amazingly terrible” – a car guidance system that, according to his analysis, goes wrong every eight minutes, whereas similarly experimental guidance systems run by competitors including the Google subsidiary Waymo typically go tens of thousands of miles before encountering problems. O’Dowd was similarly dismissive of the notion, promoted by Tesla, that such problems can be fixed by patching the software with online upgrades.\nIt’s unusual for a campaign to be so singularly focused on a very discrete commercial issue\nDan Weiner\nHis analysis is far from a consensus position in the industry. Many experts say that everyone is struggling to crack the problem of producing a reliable self-driving car and that the problem of cybersecurity – making sure a bad actor cannot gain control of a fleet of tens of thousands of cars through their operating software – is a particularly vexing one across the board.\nChris Clark, a software security expert with Synopsys, a California company that tests software and designs computer systems for the car industry, said he did not think O’Dowd’s rollout campaign ad, showing the Tesla “full self-driving” program malfunctioning over and over, was especially fair.\n“You could make a similar video about just about any other company out there,” he said, adding that by his count the number of enterprises working on a self-driving car was close to 300. “This is an industry-wide challenge … The driver is always supposed to be paying attention and protect the vehicle if it does something unexpected.”\nClark also took issue with O’Dowd’s argument that government regulators are not focusing enough on road safety as car companies grow increasingly experimental with their automated features. “Yes, it is the wild west,” he said, “but there are sheriffs in town to ensure the industry does what it can to protect consumers.”\nAccording to Dan O’Dowd’s analysis, Tesla’s self-driving feature goes wrong every eight minutes. Photograph: Bloomberg\/Getty Images\nBoth the federal National Highway Transport Safety Administration and the California Department of Motor Vehicles are investigating Tesla’s driver enhancement software following a flurry of documented malfunctions. Meanwhile, Lena Gonzalez, the chair of the transportation committee in the California state senate, is pushing to close a loophole whereby Tesla is exempted from reporting crash data on its “full self-driving” package. “Senator Gonzalez believes protecting California drivers is of the utmost importance,” her office said in a statement.\nIt’s not entirely clear what else O’Dowd thinks government regulators should be doing, or how he can further his cause by running for a senate seat that the incumbent, Alex Padilla, is expected to win easily. It’s more common for influential industry players to lobby government directly (O’Dowd said he’s tried), and if that fails, to sponsor a ballot initiative calling for a specific change in public policy.\nO’Dowd said: “I didn’t think of a ballot initiative.”\nO’Dowd was eloquent about the urgent need to use the highest grade software to build critical infrastructure – a theme he has spent years building into a public campaign he calls the Dawn Project. But he struggled to square that lofty goal with his focus on petty online squabbles, in which Musk supporters accuse him of being slow, out of touch, and worse, and he fires back that Musk’s cars are being driven by “artificial stupidity”.\nMusk himself had a go at O’Dowd in January, after the Dawn Project took out a full-page advertisement in the New York Times to attack him. “Green Hills software is a pile of trash,” Musk tweeted back. He has kept quiet ever since.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it discusses concerns and criticisms surrounding Tesla's self-driving software package. The article highlights issues documented with Tesla's 'full self-driving' software, including incidents where it has caused vehicles to veer unexpectedly into the wrong lane, turn the wrong way, crash into poles, and endanger other road users. These incidents suggest malfunctions and safety concerns related to self-driving technology.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/us-news\/2022\/may\/01\/dan-odowd-us-senate-tesla-self-driving-elon-musk","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":50,"Time":"FEBRUARY 28, 2019","Title":"BMW, Daimler pool resources on automated driving technology","Content":"By Edward Taylor\nFRANKFURT (Reuters) - Daimler and BMW deepened their alliance on Thursday to share spiraling development costs for highly automated driving technologies, even as each carmaker pursues separate efforts to develop fully self-driving cars.\nFILE PHOTO: The BMW iNEXT electric autonomous concept car is introduced during a BMW press conference at the Los Angeles Auto Show in Los Angeles, California, U.S. November 28, 2018. REUTERS\/Mike Blake\/File Photo\nThe enormous cost of designing and building computer-powered vehicles has already prompted Honda to pool its efforts with General Motors, while Volkswagen is pursuing talks with Ford about an alliance on autonomous cars.\nBMW and Daimler deepened their alliance for similar reasons, said Michael Hafner, head of automated driving at Mercedes-Benz research and development said in a blog post which accompanied a joint press release by the companies on Thursday.\n“We have learned that the development of these systems is a bit like climbing a mountain,” he said.\n“Taking the first few meters from the base station to the summit seems easy. But the closer you come to the goal, the thinner the air around you becomes, the more strength is required for each further step, and the more complex become the challenges you have to resolve.”\nIt made sense to distribute the technological and financial challenges of automated driving, Hafner said, so BMW and Daimler will jointly develop technology to enable automated driving on highways.\nPOTENTIAL GROWTH\n“Initially, the focus will be on advancing the development of next-generation technologies for driver assistance systems, automated driving on highways and parking features,” the companies said in the statement.\n“In addition, the two partners plan to discuss the possibility of extending their collaboration to cover higher levels of automation, both on highways and in urban areas.”\nBMW and Daimler’s move comes as even deep pocketed technology companies struggle to gain traction in autonomous driving. Apple Inc said on Wednesday it planned to lay off 190 employees in its self-driving car program, Project Titan.\nThe market for advanced driver assistance systems and autonomous vehicles is expected to grow to $96 billion in 2025 and $290 billion in 2035 from about $3 billion in 2015, according to Goldman Sachs.\nBMW and Daimler already cooperate in high-definition mapping with HERE and in the area of procurement, and earlier this month unveiled a joint ride-hailing, parking and electric car charging business.\nThey said on Thursday their new partnership will center on so-called level 3 and level 4 automated driving technologies, including cars that still require steering wheels and drivers.\nDaimler will pursue a separate development alliance for level 5 robotaxis between its luxury brand Mercedes-Benz and supplier Robert Bosch. Level 5 cars require no driver.\nBMW, for its part, continues its development alliance for robotaxis with Israeli autonomous vehicle tech company Mobileye and chip maker Intel, with the aim of putting autonomous cars on the road by 2021.\nReporting by Edward Taylor; editing by Thomas Seythal\/Ludwig Burger and Emelia Sithole-Matarise\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it discusses the challenges and costs associated with developing highly automated driving technologies. While it doesn't explicitly mention malfunctions, it highlights the difficulties and complexities involved in advancing automated driving systems. The article mentions that the development of these systems is likened to climbing a mountain, suggesting that there are significant challenges and obstacles to overcome. Additionally, the mention of cooperation between BMW and Daimler to jointly develop technology for automated driving on highways implies that there are technical hurdles to be addressed. Overall, this news provides insights into the challenges and complexities of self-driving technology, which can indirectly relate to malfunctions and issues in the field.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/cbusiness-us-bmw-daimler-autonomous-idCAKCN1QH13O-OCABS","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":51,"Time":"18 February 2022","Title":"Tesla investigated over 'phantom braking' problem","Content":"Tesla investigated over 'phantom braking' problem\nPublished\n18 February 2022\nShare\nIMAGE SOURCE,\nGETTY IMAGES\nThe US government is investigating reports of Tesla cars braking unexpectedly on motorways.\nThe so-called \"phantom braking\" problem is being looked at by US regulator the National Highway Traffic Safety Administration (NHTSA).\nIt received 354 complaints in the past nine months and its investigation will cover approximately 416,000 Tesla Model 3 and Model Y vehicles from 2021-22.\nDrivers say the issue occurs using the Autopilot driver assistance system.\nThe feature gives the vehicle control over some elements of braking and steering when driving, although it is not a substitute for a human driver.\nDespite the name, Tesla recommends drivers remain vigilant and supervise their vehicle, noting the Autopilot ADAS system \"does not make the vehicle autonomous\".\nTesla is currently under investigation by the NHTSA over two other matters.\nIn December 2021, it disabled its Passenger Play feature that allowed games to be played on its touchscreen while the car is in motion, leading to an open investigation covering an estimated 580,000 vehicles.\nAnd last August, the NHTSA started to look into the role of the Autopilot system in 11 crashes involving emergency vehicles, covering approximately 765,000 Tesla cars.\nThe NHTSA's Office of Defects Investigation (ODI) has begun a \"preliminary evaluation\" into Tesla over the complaints. This is the stage before the agency could officially issue a recall of the vehicles.\nIt says there have been no crashes, injuries or fatalities as a result of the incidents.\n\"The complaints allege that while utilising the ADAS features including adaptive cruise control, the vehicle unexpectedly applies its brakes while driving at highway speeds,\" the report says.\n\"Complainants report that the rapid deceleration can occur without warning, at random, and often repeatedly in a single drive cycle.\n\"ODI is opening this preliminary evaluation to determine the scope and severity of the potential problem and to fully assess the potential safety-related issues.\"\nThe NHTSA makes customer complaints publicly available on its website, so consumers can compare vehicle safety.\nIn one complaint from 11 February 2022, the driver says: \"Heavy braking occurs for no apparent reason and with no warning, resulting in several near misses for rear end collisions... this issue has occurred dozens of times during my five months and 10,000-mile ownership.\"\nIn another dated 3 February 2022, the user complains of \"phantom braking for no apparent reason\", stating that their car suddenly decelerated from 73mph down to 59mph \"in two seconds\".\nThe BBC has approached Tesla for comment.\nRelated Topics\nCar Technology\nTesla\nUnited States\nMore on this story\nTesla recalls cars that may roll past stop signs\nPublished\n2 February 2022\nTesla to recall 475,000 cars in the US\nPublished\n30 December 2021\nTesla withdraws self-driving beta after problems\nPublished\n25 October 2021","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it reports an investigation by the US government into 'phantom braking' problems with Tesla cars, specifically involving the Autopilot driver assistance system. The report mentions that drivers have experienced unexpected and rapid deceleration without warning while using adaptive cruise control and other Autopilot features. This behavior is indicative of a malfunction in the self-driving technology, as it is not behaving as expected and poses potential safety risks.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-60432351","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":40,"Time":"Fri 3 Jun 2022 15.25 CEST","Title":"Tesla cars on autopilot have stopped on highways without cause, owners report","Content":"More than 750 Tesla owners have complained to US safety regulators that cars operating on the automaker’s partially automated driving systems have suddenly stopped on roadways for no apparent reason.\nThe National Highway Traffic Safety Administration revealed the number in a detailed information request letter to Tesla that was posted on Friday on the agency’s website.\nThe 14-page letter dated 4 May asks the automaker for all consumer and field reports it has received about false braking, as well as reports of crashes, injuries, deaths and property damage claims. It also asks whether the company’s “full self driving” and automatic emergency braking systems were active at the time of any incident.\nThe agency began investigating phantom braking in Tesla’s Models 3 and Y last February after getting 354 complaints. The inquiry covers an estimated 416,000 vehicles from the 2021 and 2022 model years. In February, the agency said it had no reports of crashes or injuries.\nThe letter gives Tesla a deadline of 20 June to respond to the information request but says the company can ask for an extension.\nA message was left early on Friday seeking comment from Tesla.\nIn opening the investigation, the agency said it was looking into vehicles equipped with automated driver-assist features such as adaptive cruise control and “autopilot”, which allows them to automatically brake and steer within their lanes.\n“Complainants report that the rapid deceleration can occur without warning, and often repeatedly during a single drive cycle,” the agency said.\nMany owners wrote in their complaints that they feared a rear-end crash on a freeway.\nIn the letter, NHTSA asks for the initial speed when the cars began to brake, the final speed and the average deceleration. It also asks if the automated systems detected a target obstacle, and whether Tesla has video of the braking incidents.\nThe agency is now seeking information on warranty claims for phantom braking including the owners’ names and what repairs were made. It is also seeking information on Tesla’s sensors, any testing or investigations into the braking problems, or if any modifications were made.\nThe letter focuses on Tesla’s testing of the automated systems when it comes to detecting metal bridges, S-shaped curves, oncoming and cross traffic and different sizes of vehicles including large trucks. The agency also wants information on how cameras deal with reflections, shadows, glare and blockage due to snow or heavy rain.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving technology malfunctions. The article reports that more than 750 Tesla owners have complained to US safety regulators that cars operating on the automaker’s partially automated driving systems have suddenly stopped on roadways for no apparent reason. The National Highway Traffic Safety Administration is investigating phantom braking in Tesla’s Models 3 and Y from the 2021 and 2022 model years. Complaints indicate that rapid deceleration can occur without warning during a single drive cycle, which poses a safety risk on highways. The investigation also covers automated driver-assist features such as adaptive cruise control and 'autopilot,' which are part of self-driving technology. The agency is requesting data on the automated systems' behavior, including speed changes, detection of obstacles, and camera performance in various conditions. This information strongly suggests self-driving technology malfunctions.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2022\/jun\/03\/tesla-cars-autopilot-stopped-highways-without-cause","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":56,"Time":null,"Title":"RPT-Apple working with Hertz to test self-driving technology - BBG","Content":"By Reuters Staff\n(Repeats with no change to text)\nJune 26 (Reuters) - Apple Inc is leasing a small fleet of cars from rental company Hertz Global Holdings Inc to test self-driving technology, Bloomberg reported on Monday.\nHertz shares were up 13.5 percent at $10.82, while shares of Apple were slightly down.\nApple is renting Lexus RX450h sport-utility vehicles from Hertz's Donlen fleet-management unit, according to the Bloomberg report, citing documents released recently by the California Department of Motor Vehicles. (bloom.bg\/2tdqm8n)\nThe iPhone maker is concentrating on technology for self-driving cars, Chief Executive Tim Cook said earlier this month in an interview with Bloomberg.\nHertz and Apple were not immediately available for comment.\nAlphabet Inc’s self-driving car unit Waymo announced a similar partnership with Avis Budget Group Inc , earlier on Monday, to offer fleet support and maintenance services for its fleet of autonomous vehicles. (Reporting by Rishika Sadam in Bengaluru; Editing by Shounak Dasgupta)\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving technology because it reports that Apple Inc is leasing a fleet of cars from Hertz Global Holdings Inc specifically to test self-driving technology. Apple is renting Lexus RX450h sport-utility vehicles for this purpose, and the news mentions that Apple's CEO, Tim Cook, stated their focus on self-driving car technology. Additionally, Alphabet Inc’s self-driving car unit Waymo announced a similar partnership with Avis Budget Group Inc on the same day, which further highlights the relevance of self-driving technology in the news.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/apple-hertz-glo-hldg-idUSL3N1JN4DJ","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":55,"Time":null,"Title":"Continental delays autonomous investments in cost cut drive","Content":"By Reuters Staff\nFRANKFURT (Reuters) - German auto parts supplier Continental CONG.DE said it was postponing investments as its operating profit plunged by 47% in the first quarter, hit by coronavirus lockdowns which caused a 25% drop in global car production\nNew information technology projects, or plans to expand production capacity at factories have been put on hold, as well as some investments into self-driving technologies, the company said.\n“If you delay autonomous investments for level 4 and level 5 capability by six months, you have not lost the market, since this market will only emerge in 10 years,” Chief Financial Officer Wolfgang Schaefer told Reuters.\nUpon being asked whether Continental faced liquidity problems and may apply for state-backed loans, Schaefer said, “No.”\nReporting by Edward Taylor in Frankfurt and Jan Schwartz in Hamburg; editing by Thomas Seythal\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Based on the provided news article, here is the JSON-formatted response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to self-driving technology malfunctions because Continental, a German auto parts supplier, has announced the postponement of investments in self-driving technologies. They mentioned delaying investments in autonomous capabilities for level 4 and level 5, indicating a slowdown in the development of advanced autonomous driving systems. This decision to delay self-driving technology investments is a significant aspect of the news that aligns with self-driving technology malfunctions.\"\n}\nThe news is relevant to self-driving technology malfunctions because it discusses the postponement of investments in self-driving technologies by Continental, which indicates a potential setback or delay in the advancement of autonomous driving systems.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-continental-results-investments-idUSKBN22J0WW","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":58,"Time":"MAY 3, 2018","Title":"Toyota builds self-driving test track for 'edge case' scenarios","Content":"By Nick Carey\nThe Toyota company logo is see on a Yaris model car that is on display at Toyota's automobile manufacturing plant before the visit by the French president in Onnaing, France, January 22, 2018. REUTERS\/Pascal Rossignol\nDETROIT (Reuters) - Toyota Motor Corp said on Thursday it is building a closed-course test facility in Michigan for its self-driving vehicle technology that will replicate “edge case” driving scenarios that are too dangerous to conduct on public roads.\nThe facility at Ottawa Lake, which is being built by the Toyota Research Institute, will go into operation in October.\n“This new site will give us the flexibility to customize driving scenarios that will push the limits of our technology and move us closer to conceiving a human-driven vehicle that is incapable of causing a crash,” Ryan Eustice, the Toyota Research Institute’s senior vice president of automated driving, said in a statement.\nAutomakers such as General Motors Co and companies such as Alphabet Inc unit Waymo have been racing to develop self-driving cars and be the first to market with a viable product.\nBut questions about the safety of self-driving technology and oversight of developers were raised after a fatal collision between an Uber Technologies Inc self-driving vehicle and a pedestrian in Arizona in March.\nFollowing that accident, Toyota suspended all its self-driving tests on U.S. public roads in California and Michigan. Toyota has continued tests on closed courses.\nA Toyota Research Institute spokesman said halting tests on public roads has allowed the automaker to refine and upgrade its fleet of test vehicles.\n“We will resume testing on public roads in a few weeks, once these three systems have been more closely aligned,” the spokesman said.\nReporting By Nick Carey; Editing by Bernadette Baum\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it discusses Toyota building a closed-course test facility in Michigan to replicate 'edge case' driving scenarios that are too dangerous to conduct on public roads. These 'edge case' scenarios often involve situations where self-driving technology may malfunction or encounter challenges, which is why Toyota is creating this facility. The article also mentions Toyota suspending self-driving tests on public roads after a fatal collision involving a self-driving vehicle, highlighting concerns about the safety and functionality of self-driving technology.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/ctech-us-toyota-selfdriving-tests-idCAKBN1I41WL-OCATC","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":45,"Time":"Tue 1 Feb 2022 15.01 CET","Title":"Tesla recalls 54,000 vehicles with ‘Full Self-Driving’ over ‘rolling stop’ feature","Content":"Tesla is recalling nearly 54,000 cars and SUVs because their “Full Self-Driving” software lets them roll through stop signs.\nTesla sees record profit as electric car deliveries soar\nRead more\nDocuments posted on Tuesday by US safety regulators say Tesla will disable the feature with an over-the-internet software update. The “rolling stop” feature allows vehicles to go through intersections with all-way stop signs at up to 5.6mph (9 km\/h).\nTesla agreed to the recall after two meetings with the National Highway Traffic Safety Administration (NHTSA), according to documents. Tesla said it knows of no crashes or injuries caused by feature.\nThe recall covers Model S sedans and X SUVs from 2016 through 2022, as well as 2017 to 2022 Model 3 sedans and 2020 through 2022 Model Y SUVs.\nSelected Tesla drivers are “beta testing” the “Full Self-Driving” software on public roads. The company says the cars cannot drive themselves and drivers must be ready to take action at all times. A firmware release to disable the rolling stops is expected to be sent out in early February.\nA message was left seeking comment from Tesla, which has disbanded its media relations department.\nThe NHTSA said in documents that failing to stop for a sign can increase the risk of a crash.\nSafety advocates complain that Tesla should not be allowed to test the vehicles in traffic with untrained drivers and that the software can malfunction, exposing other motorists and pedestrians to danger. Most of the other auto companies with similar software test with trained human safety drivers.\nTesla introduced the “rolling stop” feature in a software update that was sent out to the testing owners on 20 October. NHTSA met with Tesla on 10 and 19 January to discuss how the software operates, the documents said. On 20 January, the company agreed to disable the rolling stops with the software update. Owners will get required notification letters on 28 March.\nThe “rolling stop” feature let the Teslas go through all-way stop signs as long as the owner enabled the function. The vehicles have to be traveling below 5.6mph while approaching the intersection and no “relevant” moving cars, pedestrians or bicyclists can be detected nearby.\nAll roads leading to the intersection had to have speed limits of 30mph or less, the documents said. The Teslas would then be allowed to go through the intersection at 0.1mph to 5.6mph without coming to a complete stop.\nAlain Kornhauser, faculty chair of autonomous vehicle engineering at Princeton University, said the recall was an example of NHTSA is doing its job as the nation’s road safety watchdog.\nTesla driver charged with vehicular manslaughter over fatal Autopilot crash\nRead more\nThe recall “shows that they can be effective even if Tesla should have been more responsible in the first place”, he said.\nIn November, the NHTSA said it was looking into a complaint from a Tesla driver that the “Full Self-Driving” software caused a crash. The driver complained that the Model Y went into the wrong lane and was hit by another vehicle.\nThe SUV gave the driver an alert halfway through the turn, and the driver tried to turn the wheel to avoid other traffic, according to the complaint. But the car took control and “forced itself into the incorrect lane”, the driver reported. No one was hurt in the 3 November crash in Brea, California, according to the complaint.\nIn December, Tesla agreed to update its less sophisticated “Autopilot” driver-assist system after the NHTSA opened an investigation. The company agreed to stop allowing video games to be played on center touch screens while its vehicles are moving.\nThe agency also is investigating why Teslas on Autopilot have repeatedly crashed into emergency vehicles parked on roadways.","Relevant":true,"Overview":"Here is the JSON-formatted response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to self-driving technology malfunctions. Tesla is recalling nearly 54,000 vehicles with 'Full Self-Driving' software due to a feature that allows them to roll through stop signs, which is a malfunction in the self-driving technology. The 'rolling stop' feature, which lets vehicles go through intersections with all-way stop signs at up to 5.6 mph, poses a safety risk, as failing to stop for a sign can increase the risk of a crash. Additionally, there have been complaints about the 'Full Self-Driving' software causing a crash, including a case where the car went into the wrong lane and was hit by another vehicle. These incidents indicate issues with the self-driving technology.\"\n}\nThe provided news is relevant to self-driving technology malfunctions because it discusses a recall of Tesla vehicles equipped with \"Full Self-Driving\" software due to a malfunctioning feature related to stop signs and mentions safety concerns and complaints about the software's performance, indicating issues with self-driving technology.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2022\/feb\/01\/tesla-recall-full-self-driving-software-stop-signs","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":61,"Time":"MARCH 1, 2017","Title":"Uber's self-driving unit quietly bought firm with tech at heart of Alphabet lawsuit","Content":"By Alexandria Sage\nSAN FRANCISCO, Feb 28 (Reuters) - A company now owned by Uber last year quietly bought a small firm specializing in sensor technology used in autonomous vehicles, giving the ride services company a patent in the technology and possibly a defense against a trade secrets theft lawsuit filed against it by rival Alphabet Inc.\nThe chief executive of little-known Tyto Lidar LLC said in a May 2016 post on LinkedIn that the company had been sold, at the same time as he and three other executives joined Otto, according to their profiles on the online business network. Official U.S. patent data shows Otto acquired Tyto technology at the same time.\nOtto, a self-driving truck startup founded by former Alphabet employees, was bought by Uber in August.\nThe unpublicized acquisition may become a factor in the high-stakes legal fight between Uber and Alphabet, the parent of Google, as the two Silicon Valley companies aggressively develop self-driving technology, widely seen as the future of private road transport in the United States.\nEqually, it may end up being a footnote in the complex litigation, which could take years to unfold.\nAlphabet’s autonomous car unit Waymo sued Uber and Otto last week, alleging that former employee Anthony Levandowski, who left Waymo to set up Otto, downloaded and stole more than 14,000 confidential files, including details on light detection and ranging sensor technology, known as Lidar, a crucial element in most self-driving car systems.\nIt claimed that without those Waymo designs, Uber could not have developed its technology as fast as it says.\nAn Uber spokesperson declined to comment on Tyto, citing the pending litigation, but called Waymo’s lawsuit “a baseless attempt to slow down a competitor.” Waymo declined to comment on Tyto.\n‘STRIKING RESEMBLANCE’\nIn its lawsuit, Waymo said that by mid-2016, Uber was “more than five years behind in the race to develop vehicle automation technology suitable for the mass market,” yet it built a Lidar system comparable to Waymo’s “in only nine months”.\nHowever, the acquisition of Tyto means that at least two executives with long experience in Lidar – one as early as 2009, according to his LinkedIn profile - transferred to Otto and then Uber. Both had previously worked at Velodyne, another Silicon Valley Lidar pioneer, according to LinkedIn.\nTyto also came to Otto with a patent for a Lidar scanner that was filed in 2013 and has since been reassigned to Uber, according to the U.S. Patent & Trademark Office website.\nEric Goldman, an intellectual property (IP) law professor at Silicon Valley’s Santa Clara University School of Law, said the Tyto acquisition and its patent “could help rebut” Waymo’s suggestion that Uber scaled up too quickly to have its own Lidar technology.\nTrade secret plaintiffs commonly make circumstantial cases, such as Waymo implying that Uber could not have developed its own technology as fast as it purported to do, Goldman said.\n“That prong of their arguments could be rebutted,” Goldman said.\nHe cautioned, however, that Tyto’s expertise and patent “may be irrelevant” if Waymo can prove its central allegation: that Levandowski downloaded confidential trade secrets before leaving the company to form Otto - and that Uber exploited this stolen information to design a Lidar circuit board with a “striking resemblance” to Waymo’s.\nIn an interview with Forbes in October that was published on Tuesday, Levandowski said Uber did not steal trade secrets from Google. “We did not steal any Google IP,” he told the magazine.\nWaymo says its patented Lidar technology is among its most valuable assets because it had successfully managed to reduce the price of the sensor by 90 percent.\nAll Lidar makers are seeking to reduce cost and size. Promotional material for Tyto from a 2015 trade conference said Tyto’s technology “enables lower cost, lighter weight and smaller size Lidar sensors.” (Reporting by Alexandria Sage; editing by Peter Henderson and Bill Rigby)\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the JSON-formatted response based on the provided news article:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"This news is relevant to self-driving technology malfunctions because it discusses an acquisition made by Uber of a small firm specializing in sensor technology used in autonomous vehicles. The acquisition potentially provides Uber with a defense against a trade secrets theft lawsuit filed by Alphabet Inc. (parent of Google). The lawsuit alleges that former employee Anthony Levandowski, who left Alphabet's autonomous car unit Waymo to set up Otto (a self-driving truck startup later acquired by Uber), downloaded and stole more than 14,000 confidential files, including details on light detection and ranging sensor technology (Lidar), which is a crucial element in most self-driving car systems. The article mentions that the acquisition of Tyto Lidar LLC and its patent for Lidar technology could help Uber rebut Waymo's suggestion that it scaled up too quickly to have its own Lidar technology. However, the outcome of the legal battle is still uncertain and could take years to unfold.\"\n}\nThis news is relevant to self-driving technology malfunctions because it is linked to allegations of trade secrets theft and misuse of Lidar technology, which are critical components in self-driving systems.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/autonomous-lawsuit-idUSL2N1GD1RO","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":66,"Time":"SEPTEMBER 18, 2017","Title":"Intel collaborates with Waymo on self-driving compute design","Content":"By Reuters Staff\nSAN FRANCISCO, Sept 18 (Reuters) - Intel Corp on Monday announced a collaboration with Alphabet’s Waymo self-driving unit, saying it had worked with the company during the design of its compute platform to allow autonomous cars to process information in real time.\nThe world’s largest computer chipmaker said its Intel-based technologies for sensor processing, general compute and connectivity were used in the Chrysler Pacifica hybrid minivans that Waymo has been using since 2015 to test its self-driving system.\n“As Waymo’s self-driving technology becomes smarter and more capable, its high-performance hardware and software will require even more powerful and efficient compute,” said Intel Chief Executive Brian Krzanich in a statement announcing the ongoing collaboration.\nIntel, which announced the $15 billion acquisition of autonomous vision company Mobileye in March, is pushing to expand its real estate in autonomous vehicles, a fast-growing industry. A collaboration with Waymo, considered by many industry experts to be at the forefront of autonomous technology, adds to its portfolio.\nThe announcement marked the first time Waymo, formerly Google’s autonomous program, has acknowledged a collaboration with a supplier. The company has done most of its development work in-house.\nIntel began supplying chips for then-Google’s autonomous program beginning in 2009, but that relationship grew into a deeper collaboration when Google began working with Fiat Chrysler Automobiles to develop and install the company’s autonomous driving technology into the automaker’s minivans.\nWaymo, which has developed its own sensors, is not using the autonomous vision system created by Mobileye..\nUnderscoring the non-exclusive partnerships and collaborations in the space, Mobileye and Intel are in an alliance with German automaker BMW and Fiat-Chrysler to create an industry-wide autonomous car platform.\nWaymo CEO John Krafcik said fast processing was crucial to the performance of its autonomous vehicles.\n“Intel’s technology supports the advanced processing inside our vehicles, with the ability to manufacture to meet Waymo’s needs at scale,” Krafcik said in a statement. (Reporting By Alexandria Sage; Editing by Gopakumar Warrier)\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology as it discusses Intel's collaboration with Waymo on self-driving compute design. It mentions how Intel's technologies for sensor processing, general compute, and connectivity were used in Waymo's autonomous minivans for testing their self-driving system. The article also highlights the importance of high-performance hardware and software for autonomous vehicles, which aligns with potential issues related to self-driving technology malfunctions. However, the news doesn't provide specific information about any malfunctions or failures.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/intel-autonomous-waymo-idUSL5N1LZ01V","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":41,"Time":"Updated 6:22 AM GMT+2, August 16, 2023","Title":"Tech company behind Kentucky school bus problems had similar issues in Ohio last year","Content":"LOUISVILLE, Ky. (AP) — The company behind a disastrous change to a Kentucky city’s school bus routes that resulted in more than a week of canceled classes had similar problems in two cities in neighboring Ohio last year.\nTouting its connections to the Massachusetts Institute of Technology, bus-routing vendor AlphaRoute pitched its mathematical models and machine-learning technology as a way of saving money and smoothing out complex bus routes in Louisville, Kentucky, and school districts across the U.S.\nBut real-world problems often got in the way.\nColumbus began running new routes planned by AlphaRoute in fall 2022 after entering into a three-year, $1.6 million contract. But there were problems from the beginning. Most importantly, the district was not able to make adjustments quickly with the company’s software. It decided to pivot mid-year to the software it was previously using from another company, Versatrans, said district spokesperson Jacqueline Bryant.\nOTHER NEWS\nRay Davis rushes for career-high 280 yards and scores 4 TDs, Kentucky dominates No. 22 Florida 33-14\nKentucky agriculture commissioner chosen to lead state’s community and technical college system\nJudges maintain bans on gender-affirming care for youth in Tennessee and Kentucky\nCincinnati Public Schools told The Associated Press in an email that it was under contract with AlphaRoute for less than one year, beginning in April 2022 at a cost of $150,000.\n“AlphaRoute provided route analysis and made efficiency recommendations. CPS was not satisfied with the results and had to reroute and physically evaluate each stop,” according to the statement.\nSeveral other districts listed as partners on the company’s website said they either no longer worked with AlphaRoute or never were its customers. The school district in Providence, Rhode Island, a listed partner, said it considered the company’s proposal in 2021 but “went in another direction.”\nAlphaRoute said in a Tuesday night written statement that it recognized the Kentucky school cancellations have been “terribly disruptive” and that it has had a team in Louisville helping to address them since Saturday.\n“We at AlphaRoute have been working alongside the district to fix as many issues as possible as fast as possible, so that service is greatly improved when schools reopen on Friday,” it said.\nIn Louisville, the transportation changes recommended by AlphaRoute for Jefferson County Public Schools proved disastrous on the first day of school. Some students were not picked up in the morning while others did not arrive home until nearly 10 p.m.\nThe fiasco resulted in hungry and tired children, angry parents and exasperated politicians. Schools had to be closed to reevaluate the transportation plan, and students will have missed more than a week of school when they begin returning on Friday as part of a staggered reopening. The fallout has included a call from some state lawmakers to explore splitting up the state’s largest school district.\nLike other districts, Jefferson County turned to AlphaRoute for ways to increase efficiency and cut the number of bus routes after a nationwide driver shortage left them scrambling for solutions to transport students. The company, based in Waltham, Massachusetts, uses computer algorithms to map out bus routes and stops.\nIn a March 2021 letter to Jefferson County seeking to justify its use as a single contractor, company co-founder and CEO John Hanlon outlined how his firm could solve some of the “daunting challenges” of a busing system he described as inefficient and one of the most complex in the country, with 65,000 daily bus riders.\nHanlon touted AlphaRoute as the only company capable of both rerouting buses and planning staggered school start times. Superintendent Marty Pollio championed the idea, saying the combination would allow for more efficient use of buses and let teenagers sleep longer so they could be more alert in school.\nA researcher who studies automation bias — in which people are prone to overly trusting the abilities of automated systems, from factory robots to ChatGPT — said what happened in Louisville fits into a broader problem with the use of artificial intelligence technology.\nStudents having to walk long distances to bus stops early in the morning might have been “algorithmically correct” because it satisfied the objectives and constraints of the algorithm under Kentucky law, “but in reality parents would not want their kids walking that far at 6 a.m.,” said Aaron Schecter, a professor of information management systems at the University of Georgia.\nSimilarly, an algorithm might satisfy its goal of minimizing total routes, to lessen the number of drivers, at the expense of another criterion such as the time it takes to transport students. Schecter said machine-learning algorithms such as AlphaRoute’s are typically trying to optimize an objective and can overlook “worst case” harms even if the average result is satisfactory.\n“The underlying principle here is that people were wooed by something that seemed sophisticated, and they trusted that AI would be a magic fix,” said Schecter, who hadn’t evaluated the specific technology used.\nAlphaRoute’s Hanlon is the former chief operating officer of Boston Public Schools and has emphasized the company’s origins as a partnership between MIT researchers and the school district.\nIn a 2019 scientific paper, a team lead by Dimitris Bertsimas, an MIT professor who is also a co-founder of AlphaRoute and its parent company, Dynamic Ideas LLC, said that using an algorithm for selecting the best school start times would empower Boston leaders “to make decisions based not on the political whims of special interest groups but on an objective standard agreed on by the community.”\nNews articles at the time said the researchers helped Boston cut 50 buses for a savings of $5 million, although transportation officials did have to vet and tweak the routes before they were used.\nHowever, Boston only ever used routing software in a limited capacity and has no relationship with AlphaRoute today, district spokesperson Max Baker said.\nIn a follow-up paper in 2020, Bertsimas and his team acknowledged that Boston didn’t follow its recommendations for changed bell times and elaborated on a number of routing challenges, from the city’s meandering topography to the equity-minded policies tracing back to racial desegregation efforts of the 1970s. But it said the experiment led it to develop a new software system that it was showing to nearly 30 school districts across 17 states.\nNearly 500,000 school buses nationwide transport 25 million students, said Molly McGee-Hewitt, executive director with the National Association for Pupil Transportation. The driver shortage is a real problem, she said, but one that can be solved by offering competitive pay and benefits and reducing bureaucratic barriers to entry.\n“You can’t have world-class schools without world-class infrastructure, and that includes transportation,” she said.\nRouting can be complicated, especially in districts that are transporting children across town to magnet schools, charter schools, special needs schools and even private schools, McGee-Hewitt said. Various software vendors have been successfully helping schools manage that challenge for years.\nIn a news conference Monday, Jefferson County Public School Superintendent Pollio said one significant deficiency was that the recommended routes weren’t accounting for the latest information. He said AlphaRoute gave the district the new routes earlier in the summer, but since then thousands of stops had been added as new students enrolled ahead of opening day or parents requested a different bus stop.\n“When stops are added to routes, we did not properly add the time that was needed for a bus driver to complete that,” he said, explaining that those extra minutes were adding up.\n“We had some room for error in our former schedule. We do not have room for error now,” he said.\nIn assessing fault for the opening day fiasco, Pollio has said he’s “not going to put it on the company. ... I said it from the very beginning, I take responsibility for it myself.”\n_____\nLoller reported from Nashville, Tenn. AP Technology Writer O’Brien reported from Providence, Rhode Island.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it discusses problems with a bus-routing vendor, AlphaRoute, that claimed to use mathematical models and machine-learning technology to optimize bus routes. However, real-world problems, including issues in Kentucky and Ohio, disrupted school bus services. In Kentucky, the changes recommended by AlphaRoute resulted in students not being picked up, long delays, and widespread disruption, leading to canceled classes for more than a week. The news highlights how the use of technology, in this case, AlphaRoute's algorithms, failed to provide efficient and reliable bus routes, causing significant malfunctions in the transportation system.\"\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/kentucky-school-bus-problems-alpharoute-a26288e7d4aa4de5b75c4f658705b19a","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":59,"Time":"February 17, 2023","Title":"Tesla recalls 362,000 U.S. vehicles over Full Self-Driving software","Content":"Companies\nTesla Inc\nWASHINGTON, Feb 16 (Reuters) - Tesla Inc (TSLA.O) will recall more than 362,000 U.S. vehicles to update its Full Self-Driving (FSD) Beta software after U.S. regulators said on Thursday the driver assistance system did not adequately adhere to traffic safety laws and could cause crashes.\nThe National Highway Traffic Safety Administration (NHTSA) said the Tesla software allows a vehicle to \"exceed speed limits or travel through intersections in an unlawful or unpredictable manner increases the risk of a crash.\"\nAdvertisement · Scroll to continue\nTesla said it disagreed with NHTSA's analysis but ultimately acquiesced to the safety agency's January request. The electric vehicle manufacturer has previously clashed with NHTSA, which has a number of pending Tesla probes, over other safety issues and recall demands.\nTesla, whose shares closed down 5.7% at $202.04, will release an over-the-air (OTA) software update free of charge. The EV maker said it is not aware of any injuries or deaths that may be related to the recall issue. The automaker said it had 18 warranty claims.\nAdvertisement · Scroll to continue\nThe recall covers 2016-2023 Model S and Model X vehicles, 2017-2023 Model 3, and 2020-2023 Model Y vehicles equipped with FSD Beta software or pending installation.\nU.S. senators Ed Markey and Richard Blumenthal, both Democrats, said the recall was \"long overdue,\" adding, \"Tesla must finally stop overstating the real capabilities of its vehicles.\"\nThis is a fresh setback for Tesla's driver assistance system. Chief Executive Elon Musk has repeatedly missed his own targets to achieve self-driving capability, which he has touted as a potential cash cow.\nAdvertisement · Scroll to continue\nThe move is a rare intervention by federal regulators in a real-world testing program that the company sees as crucial to the development of cars that can drive themselves.\nThe recall comes less than two weeks before the company's March 1 investor day, during which Musk is expected to promote the EV maker's artificial intelligence capability and plans to expand its vehicle lineup.\nWhile Tesla's Autopilot feature assists with steering, accelerating and braking for other vehicles and pedestrians within its lane, the company says FSD is a more advanced system “designed to provide more active guidance and assisted driving” under active supervision of the driver.\nAdvertisement · Scroll to continue\nTesla reported having $2.9 billion in deferred revenue at the end of 2022 related to \"access to our FSD features, internet connectivity, free Supercharging programs and over-the-air software updates primarily on automotive sales.\"\nTesla could not be reached for comment, but Musk tweeted on Thursday that the word \"recall\" for an over-the-air software update is \"anachronistic and just flat wrong!\"\n[1\/4]A Tesla Model S charges at a Tesla supercharger station in Cabazon, California, U.S. May 18, 2016. REUTERS\/Sam Mircovich Acquire Licensing Rights\nTesla released FSD Beta to nearly all of its 400,000 FSD customers in the United States and Canada in the fourth quarter, when it recognized FSD revenue of $324 million. It said it expects to recognize nearly $1 billion of deferred revenue that remains over time as software updates are delivered.\nONGOING PROBES\nMusk has positioned FSD technology as one of several artificial intelligence initiatives at Tesla.\nLast May, in an interview with members of a Tesla owners club, Musk called full self driving \"essential\" for the company. \"It’s really the difference between Tesla being worth a lot of money or worth basically zero.\"\nNHTSA has an ongoing investigation it opened in 2021 into 830,000 Tesla vehicles with driver assistance system Autopilot over a string of crashes with parked emergency vehicles. NHTSA is reviewing whether Tesla vehicles adequately ensure drivers are paying attention. NHTSA said on Thursday despite the FSD recall its \"investigation into Tesla’s Autopilot and associated vehicle systems remains open and active.\"\nTesla said in \"certain rare circumstances ... the feature could potentially infringe upon local traffic laws or customs while executing certain driving maneuvers.\"\nPossible situations where the problem could occur include traveling or turning through certain intersections during a yellow traffic light and making a lane change out of certain turn-only lanes to continue traveling straight, NHTSA said.\nNHTSA said \"the system may respond insufficiently to changes in posted speed limits or not adequately account for the driver's adjustment of the vehicle's speed to exceed posted speed limits.\"\nLast year, Tesla recalled nearly 54,000 U.S. vehicles with FSD Beta software that may allow some models to conduct \"rolling stops\" and not come to a complete stop at some intersections, posing a safety risk, NHTSA said.\nTesla and NHTSA say FSD's advanced driving features do not make the cars autonomous and require drivers to pay attention.\nIn December, NHTSA opened two new special investigations into crashes involving Tesla vehicles, including an eight-vehicle crash in San Francisco on the Bay Bridge in which a driver reported the FSD feature had malfunctioned.\nSince 2016, NHTSA has opened more than three dozen investigations involving Tesla crashes where advanced driver assistance systems were suspected of use and 19 deaths were reported.\nReporting by David Shepardson in Washington Additional reporting by Joseph White in Detroit and Hyun Joo Jin in San Francisco Editing by Ben Klayman, Peter Henderson and Matthew Lewis\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is about self-driving technology malfunctions. Tesla is recalling more than 362,000 U.S. vehicles to update its Full Self-Driving (FSD) Beta software after U.S. regulators stated that the driver assistance system did not adequately adhere to traffic safety laws and could cause crashes. The National Highway Traffic Safety Administration (NHTSA) mentioned that the Tesla software allows a vehicle to exceed speed limits or travel through intersections in an unlawful or unpredictable manner, increasing the risk of a crash. This action by Tesla is a response to issues related to its self-driving technology, indicating a malfunction in the system.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/tesla-recalls-362000-us-vehicles-over-full-self-driving-software-2023-02-16\/","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":60,"Time":"Wed 13 Mar 2019 17.31 CET","Title":"The racism of technology - and why driverless cars could be the most dangerous example yet","Content":"T\nhere is a rule for dealing with computers: garbage in, garbage out. Put the wrong number of zeroes in your Excel spreadsheet and it will unthinkingly pay your staff pennies on the pound; train a self-driving car to recognise human figures by showing it millions of pictures of white people, and it might struggle to identify pedestrians of other races.\nThat was the finding of researchers from Georgia Tech, who analysed how effective various “machine vision” systems were at recognising pedestrians with different skin tones. The results were alarming: AI systems were consistently better at identifying pedestrians with lighter skin tones than darker. And not by a little bit: one headline comparison suggests that a white person was 10% more likely to be correctly identified as a pedestrian than a black person.\nSelf-driving cars are by no means the first technology to fail when confronted by other ethnicities: Google’s image-recognition system notoriously failed to discern black people from gorillas. Almost every product design has failed to grapple with the reality of humanity, from Kodak colour film that reduced dark skin to a pitch-black smudge; to motion-activated taps and driers that refuse to acknowledge the presence of a brown hand but will trigger for a white one.\nA 10-tonne driverless truck poses a higher penalty for error, however. The good news is that most actually existing self-driving cars use more than one type of sensor, including several that do not rely on visible light at all: Tesla cars, for instance, have a radar built in to the front of the vehicle, while Google’s Waymo uses a bulky, but extraordinarily accurate Lidar system instead; think radar but with lasers.\nThe bad news is that there is strong market pressure to move towards camera-only systems because of the huge cost savings. Such systems would only hit the streets in large numbers if they proved significantly safer than human drivers, but even that raises the important question: safer for whom?","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving technology malfunctions as it discusses the potential racial bias in machine vision systems used in self-driving cars. The article highlights a study by researchers from Georgia Tech that found AI systems used in self-driving cars were consistently better at identifying pedestrians with lighter skin tones than darker ones. It emphasizes that self-driving cars are not the first technology to struggle with recognizing individuals from different ethnicities, citing examples like Google's image-recognition system failing to discern black people from gorillas. The article also raises concerns about the market pressure to move towards camera-only systems in self-driving cars, which may pose safety risks, highlighting the question of 'safer for whom?'\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/shortcuts\/2019\/mar\/13\/driverless-cars-racist","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":69,"Time":"MAY 10, 2017","Title":"Nvidia says Toyota will use its AI technology for self-driving cars","Content":"By Alexandria Sage\nThe logo of technology company Nvidia is seen at its headquarters in Santa Clara, California February 11, 2015. REUTERS\/Robert Galbraith\nSAN JOSE (Reuters) - Nvidia Corp announced a partnership with Toyota Motor Corp on Wednesday, saying the Japanese car maker would use Nvidia’s artificial intelligence technology to develop self-driving vehicle systems planned for the next few years.\nToyota will use Nvidia’s Drive PX artificial intelligence platform for its autonomous vehicles planned for market introduction, Nvidia Chief Executive Jensen Huang said in his opening keynote at the company’s GPU Technology Conference in San Jose.\nNvidia came to prominence in the gaming industry for designing graphics processing chips, but in recent years has been a key player in the automotive sector for providing the so-called “brain” of the autonomous vehicle.\nNvidia, which also has partnerships with Audi and Mercedes, is among the more popular technology partners in the self-driving car race. Its partnership with Toyota is the latest in a string of alliances between tech companies, automakers and suppliers in the fast-growing sector.\nNvidia’s Drive PX supercomputer fuses incoming data from the car’s hardware such as cameras and radar and uses artificial intelligence to help the car understand and react to its environment.\n“We’re talking about not just development now but the introduction of vehicles into the market,” said Danny Shapiro, Nvidia’s senior director of automotive. “Now we have the biggest (automaker) in Japan using our Drive PX platform.”\nIn January, Nvidia and Audi VOWG_p.DE said the German automaker would use the Drive PX to help it put autonomous vehicle on the road starting in 2020. Nvidia is also co-developing with Mercedes a project to come to market within the next 12 months, both companies said in January.\nToyota, which last year set up a U.S.-based Toyota Research Institute (TRI) to focus on AI and robotics, envisions a dual-track development of autonomous technology. It is simultaneously developing full self-driving cars while also working on what it calls “guardian angel” partially autonomous technology that may still require involvement from drivers.\nReporting by Alexandria Sage; Editing by Peter Henderson, James Dalgleish and David Gregorio\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology because it discusses Nvidia's partnership with Toyota to use Nvidia's artificial intelligence technology in developing self-driving vehicle systems. Toyota plans to use Nvidia's Drive PX artificial intelligence platform for its autonomous vehicles. This partnership highlights the integration of AI technology into self-driving car development, which is a crucial aspect of self-driving technology. It demonstrates the growing role of AI in enabling self-driving capabilities in vehicles.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/ctech-us-nvidia-toyota-self-driving-idCAKBN1862M5-OCATC","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":73,"Time":"Published 3:44 PM EST, Tue January 17, 2023","Title":"Tesla-induced pileup involved driver-assist tech, government data reveals","Content":"Washington, DC\nCNN\n — \nThe Tesla Model S that braked sharply and triggered an eight-car crash in San Francisco in November had the automaker’s controversial driver-assist software engaged within 30 seconds of the crash, according to data the federal government released Tuesday.\nThe Tesla Model S slowed to 7 mph on the highway at the time of the crash, according to the data. Publicly released video also showed the car moving into the far-left lane and braking abruptly.\nThe Tesla’s driver told authorities that the vehicle’s “full self-driving” software braked unexpectedly and triggered the pileup on Thanksgiving day. CNN Business was first to report last month the driver’s claim that “full self-driving” was active.\nThe National Highway Traffic Safety Administration then announced that it was sending a special crash investigation team to examine the incident. The agency typically conducts special investigations into about 100 crashes a year.\nThe pileup took place hours after Tesla CEO Elon Musk announced that its “full self-driving” driver-assist system was available to anyone in North America who requested it and had paid for the option. Tesla had previously restricted access to drivers with high scores on its safety rating system.\n“Full self-driving” is designed to keep up with traffic, steer in the lane and abide by traffic signals, but despite Tesla’s name for it, it requires an attentive human driver prepared to take full control of the car at any moment. It’s delighted some Tesla drivers but also alarmed others with its flaws. Drivers are warned on an in-car screen by Tesla when they install “full self-driving” that it “may do the wrong thing at the worst time.”\nTesla generally does not engage with the professional news media and did not respond to CNN’s request for comment.\n“We are proud of Autopilot’s performance and its impact on reducing traffic collisions. The benefit and promise of Autopilot is clear from the Vehicle Safety Report data that we have been sharing for 4 years,” Tesla said this month in an update to its vehicle safety data.\nTraffic safety experts have long questioned the merits of Tesla’s findings, which show fewer crashes when the driver-assist technologies are active, because among other things they’re generally used on highways where crashes are already rarer.\nTesla’s driver-assist technologies, Autopilot and “full self-driving” are already being investigated by the National Highway Traffic Safety Administration following reports of unexpected braking that occurs “without warning, at random, and often repeatedly in a single drive,” the agency has said in a statement.\nThe agency has received hundreds of complaints from Tesla users. Some have described near crashes and concerns for their safety.\nBryan Reimer, an autonomous vehicle researcher with the Massachusetts Institute of Technology’s AgeLab, told CNN Business the revelation that driver-assist technology was engaged raises questions about when NHTSA will act on its investigation, and what the future holds for Tesla’s driver-assist features.\n“How many more crashes will there be before NHTSA releases findings?” Reimer said.\nReimer said it remains to be seen if there’s a recall of any Tesla driver-assist features, and what it means for the automaker’s future. Musk has said before the company would be “worth basically zero” if it doesn’t provide “full self-driving.”\nThis story has been updated to reflect that a driver-assist system was active within 30 seconds of the crash.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it reports that a Tesla Model S involved in an eight-car pileup had the automaker’s driver-assist software engaged within 30 seconds of the crash. The Tesla Model S slowed to 7 mph on the highway at the time of the crash. The driver of the Tesla claimed that the 'full self-driving' software braked unexpectedly, leading to the pileup. This incident prompted the National Highway Traffic Safety Administration to send a special crash investigation team to examine the incident, as they are already investigating Tesla's driver-assist technologies for reports of unexpected braking. Bryan Reimer, an autonomous vehicle researcher, expressed concerns about the future of Tesla's driver-assist features and whether there might be a recall. All of this information points to a self-driving technology malfunction.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2023\/01\/17\/business\/tesla-8-car-crash-autopilot\/index.html","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":75,"Time":null,"Title":"Top safety official at Waymo self-driving unit stepping down","Content":"By David Shepardson\nWASHINGTON (Reuters) - Alphabet Inc’s Waymo self-driving unit said on Thursday that its chief safety officer, Debbie Hersman, was stepping down but would remain as a consultant to the company.\nFILE PHOTO: Three of the fleet of 600 Waymo Chrysler Pacifica Hybrid self-driving vehicles are parked and displayed during a demonstration in Chandler, Arizona, November 29, 2018. REUTERS\/Caitlin O’Hara\nHersman, the former chair of the U.S. National Transportation Safety Board (NTSB), joined the company in 2019 to oversee its product safety program.\n“We can confirm that Debbie has decided to return to her family home on the east coast and will continue on as a consultant to Waymo,” the company said in a statement.\nWaymo said Tracy Murrell, Waymo’s head of safety and sustainability, would be the company’s interim head of safety. The company said a search was under way for its next head of safety.\nA Waymo spokeswoman praised Hersman for “leading the growth of the safety team, establishing Waymo’s safety board as we responsibly progress our technology, and championing Waymo’s culture of safety.”\nFounded 11 years ago as a small project inside Google, Waymo is now widely considered the leader in developing self-driving technology. But Waymo and its principal rivals are still years away from building large-scale businesses around that technology, analysts believe.\nWaymo has said it plans to offer a range of self-driving transportation services, from its Waymo One ride-hailing business in Phoenix through the new Waymo Via logistics business that would bundle local and long-haul delivery.\nSelf-driving car safety has drawn additional scrutiny after the first-ever fatal crash of a self-driving car in March 2018 that occurred when an Uber Technologies Inc vehicle struck and killed a pedestrian in Tempe, Arizona.\nThe NTSB last year faulted Uber for inadequate attention to safety and decisions in the company’s autonomous vehicle development.\nLast week, Waymo said it raised an additional $750 million in its first external investment round, bringing the total funding to $3 billion.\nHersman’s departure was reported earlier by The Information.\nReporting by David Shepardson; Editing by Peter Cooney\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it discusses the departure of Debbie Hersman, the chief safety officer at Alphabet Inc's Waymo self-driving unit. While the article does not provide specific details about self-driving technology malfunctions, the departure of a chief safety officer raises questions about safety concerns and issues within the self-driving technology development. The article mentions the fatal crash of a self-driving car in March 2018 involving an Uber vehicle, which underscores the importance of safety in self-driving technology. Therefore, this news is relevant to self-driving technology malfunctions due to its focus on safety and the role of the chief safety officer in overseeing product safety.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-waymo-safety-idCAKBN22X2Q9","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":76,"Time":null,"Title":"In Canada, driverless cars learn to see in the snow","Content":"By Allison Lampert\nSTRATFORD, Ontario (Reuters) - A Canadian town known for its annual theater festival is emerging as a hub of “snowtonomous” research, as automakers face demands to produce self-driving cars that can perform in snow and sleet.\nA self-driving car drives during a demonstration at the Renesas Electronics autonomous vehicle test track in Stratford, Ontario, Canada, March 7, 2018. REUTERS\/Mark Blinch\nBut testing driverless cars in cold climates brings added challenges to an industry grappling with the first fatality involving an autonomous vehicle.\nOn Sunday, an Uber Technologies Inc self-driving sport utility vehicle hit and killed a woman crossing the street in Arizona.\nBlackberry BB.TO said on Wednesday it will continue testing its self-driving concept car on the road in Ottawa.The Canadian province of Ontario, which allows autonomous vehicles on roads only if there is a human driver behind the wheel, has said it would follow the Arizona investigation and take \"action if warranted.\"\nCanada’s largest city, Toronto, is one of three places where Uber tests driverless technology - the other cities are Phoenix, Arizona, and Pittsburgh, Pennsylvania.\nAutomakers, ride services companies and others are investing in self-driving technology said to reduce accidents, and the costs of a driver, with robo-taxis expected to hit the road as early as this year.\nA wider rollout, however, is expected to take years, if not decades, as companies work to prove autonomous vehicles can run not just in balmy U.S. states, where they are mostly tested, but in colder areas like Ontario and Michigan where snow and sleet can render cameras and sensors ineffective.\nRoss McKenzie, managing director of the Waterloo Centre of Automotive Research, recalled how a vehicle’s lidar, which uses laser light to help autonomous cars “see” nearby objects, once mistook an ice patch for a pothole.\nMcKenzie’s research team at the University of Waterloo responded by tweaking the sedan’s software so it would factor in the time of year and weather when driving in the snow and ice, conditions that autonomous vehicles will have to navigate to be commercially viable.\nThe car needs to consider “we have winter driving conditions because I’m in Canada, it’s November and it’s 15 below,” McKenzie said by phone.\nOntario, which has lost automaking jobs to lower-cost Mexico in recent years, is counting on the lure of its snowy winters and the strength of its automotive and software engineering sectors to establish a “Silicon North.” Since 2016, Ontario has attracted about C$1 billion ($765 million) in private-sector investment for autonomous and connected cars.\nIn Stratford, where the theater festival is an important employer of its 31,000 residents, the city-wide wireless network and weather are helping to attract companies researching connected driving technologies. Ubiquitous Wi-Fi gives companies the option to test vehicle-to-vehicle communication over a 4G band, McKenzie said.\nHARSH WINTERS\nJapanese semiconductor manufacturer Renesas Electronics Corp 6723.T chose Ontario as the site of its only autonomous test-track because of the harsh winters, along with its proximity to Blackberry's QNX unit which produces the operating system.\nThe use of high-definition maps helps self-driving vehicles more easily navigate in the snow, even when road markings are not visible, said John Buszek, Renesas’ Michigan-based director of the company’s autonomous and advanced driver-assistance division.\nRenesas’ Lincoln MKZ sedan, which does not currently drive autonomously on public roads, is still equipped with enough backup cameras and sensors to keep running even if one gets covered in sludge, Buszek said.\n“In order for the car to have enough sensors to be robust against all different types of climates it means there are going to be a lot of sensors,” he said during at a recent demonstration at the chipmaker’s track in Stratford, Ontario.\nWhile extra cameras, sensors and lidar work well on the track, consumers are less keen on the cost, appearance and inefficiency of a car covered with such lenses. Some automakers are looking at protective coatings to keep a smaller number of lenses free of sludge.\n“Ultimately those sensors that you would see on the car today have to be integrated into the car system,” said Justin Gammage, industry liaison manager at the University of Ontario Institute of Technology’s Automotive Centre of Excellence (ACE) in Oshawa, Ontario.\n“Customers will pretty much want a car to look like a car looks today, even if has all the autonomous features.”\nGammage, formerly General Motors Co's GM.N chief scientist in Canada, said researchers are still looking at non-stick coatings as a cost-effective way to protect lenses.\nHe has yet to find a one-size-fits-all-coating.\n“They all have varying degrees of performance. And it’s really dependent on what type of conditions they are working in,” he said. “One coating might work really well in snow, but not as well in rain. And vice versa.”\nWhen self-driving vehicles fail to receive enough data through their cameras, sensors and lidar due to bad weather, they simply will not be able to drive autonomously, McKenzie said.\nThe public should not expect self-driving vehicles to hit the road in conditions that humans would avoid.\n“If I’m not able to get into a car because we’re two steps short of a monsoon, why would we expect an autonomous vehicle to operate?”\n($1 = 1.3078 Canadian dollars)\nReporting by Allison Lampert; Editing by Denny Thomas, Matthew Lewis and Cynthia Osterman\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it discusses the challenges and issues faced by self-driving cars in cold climates, particularly in snowy and icy conditions. It mentions how snow and sleet can render cameras and sensors ineffective, which is a crucial aspect of self-driving technology. The article also talks about a lidar system mistaking an ice patch for a pothole, highlighting a specific instance of technology malfunction. Additionally, it mentions that self-driving vehicles may not be able to operate in adverse weather conditions, emphasizing the limitations of the technology in such situations.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/cbusiness-us-autos-selfdriving-canada-idCAKBN1GX2V9-OCABS","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":77,"Time":"Updated 9:14 PM EDT, Mon May 17, 2021","Title":"Traffic cones confused a Waymo self-driving car. Then things got worse.","Content":"Washington, DC\nCNN Business\n — \nA confused Waymo self-driving car was captured on video as it became stranded on an Arizona road earlier this month while carrying a passenger and then unexpectedly driving away as a worker from the company’s roadside assistance arrived to help.\nBut the Waymo vehicle soon became stuck farther down the road, which was lined with construction cones. The Waymo worker caught up to the vehicle, took over, and drove the paying passenger to his final destination. Waymo operates a limited ridehail service in Chandler, Arizona.\nAutonomous vehicle experts who reviewed the video footage of the 41-minute trip posted on YouTube by the Waymo passenger say it shows a series of gaffes by the Waymo self-driving technology.\n“The first one was understandable. The second was strange. The third one was jaw-dropping and the fourth one I threw up my hands,” Noah Goodall, a University of Virginia scientist who researches vehicle communication and automation, told CNN Business.\nFirst, the Waymo vehicle paused at a stop sign rather than turning onto a street lined with cones. Waymo told CNN Business that guidance provided from one of its employees to revise the car’s trajectory was “improper,” and declined to elaborate. Waymo has remote workers who can provide information and directions to the self-driving vehicles. They can not take manual control of the vehicles.\nThe car then completed the turn, but soon stopped in the road, blocking part of a lane of traffic. Construction sites are known to be a challenge for fully autonomous vehicles because they rely on detailed maps of their environment to navigate safely. When the car’s environment changes, such as with traffic cones or lane closures, it can struggle to operate at its best.\nFollowing a four-minute stop, it backed up slightly, further blocking a traffic lane. Human motorists had to cross a double yellow line to go around the Waymo vehicle. Some honked.\nA construction crew removed a cone in the Waymo vehicle’s path, but the car remained stopped.\nA few minutes later, the Waymo car pulled away, surprising a Waymo worker who was explaining to the van’s passenger, Joel Johnson, through the car’s audio system that roadside assistance was on its way. The worker encouraged Johnson to remain seated and keep his seat belt on.\n“Are we moving?” the worker asked in a confused tone.\nFurther down the road, the Waymo van halted again, amid yet more cones. It was then that a Waymo roadside assistance vehicle arrived.\nJohnson was told by the Waymo worker on the car’s audio system that the human driver would take over.\n“You better hurry up, it’s going to escape,” Johnson warned the Waymo worker. Then, as the human driver approached, the Waymo car drove away again, but only a short distance.\n“I don’t even know what’s going on anymore,” Johnson said in the video.\nJohnson told CNN Business that he arrived about 20 minutes late at his destination following the mishaps, and received a Waymo refund.\n“If you need to really be on time you don’t generally take Waymo yet,” Johnson told CNN Business in an interview.\nEven so, Johnson, who said he’s taken 146 trips in Waymo vehicles, said he doesn’t think the public should be concerned with the safety of Waymo’s self-driving cars, and he continues to ride in the vehicles.\n“I’ve seen so much impressive technology,” Johnson said of Waymo. “It seems disingenuous to write them off because of one incident.”\nWaymo said in a statement that the situation was “not ideal,” and the self-driving car had received incorrect guidance, which made it challenging for the autonomous vehicle to resume its intended route. \nBryant Walker Smith, who studies autonomous vehicles at the University of South Carolina, said he was surprised a command was never issued to halt the Waymo vehicle in place, so roadside assistance could take over.\n“There wasn’t great coordination among the in-vehicle system, this remote monitoring capacity of the operator on the line and the on-ground assistance,” Smith said.","Relevant":true,"Overview":"Based on the provided news article, it does appear to be relevant to self-driving technology malfunctions. Here's the JSON-formatted response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news discusses a series of incidents involving a Waymo self-driving car in Arizona. The Waymo vehicle made several mistakes, including pausing at a stop sign instead of turning onto a street with construction cones and subsequently blocking traffic. It also unexpectedly drove away from a Waymo worker who arrived to assist, only to become stuck again amid more cones. These incidents are indicative of self-driving technology malfunctions, including improper guidance and difficulty handling changes in its environment due to construction cones and lane closures.\"\n}\nThis news article describes a sequence of events where a Waymo self-driving car encountered difficulties in navigating through construction zones, failed to follow proper guidance, and caused disruptions on the road, all of which are indicative of self-driving technology malfunctions.","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2021\/05\/17\/tech\/waymo-arizona-confused\/index.html","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":79,"Time":null,"Title":"Renault-Nissan alliance plans self-driving cars over next 4 years","Content":"By Yasmeen Abutaleb\nSAN FRANCISCO, Jan 7 (Reuters) - The alliance between automakers Renault and Nissan will launch more than 10 cars with self-driving technology over the next four years in the United States, Europe, China and Japan, the partnership’s leader said on Thursday.\nVehicles with self-driving technology will debut this year, said Carlos Ghosn, CEO of Renault and chairman of the Renault-Nissan alliance. The cars will have a feature called “single-lane control” that allows them to drive autonomously on highways without switching lanes.\nRenault-Nissan will also launch an app for mobile devices this year that allows users to interact remotely with their cars, such as by controlling music or the car’s temperature.\nBy 2018, Ghosn said the alliance will start selling vehicles with “multiple-lane control,” meaning they can autonomously change lanes on highways and navigate heavy traffic. By 2020, the alliance will have cars that can drive through city intersections and heavy city traffic on their own.\nRenault-Nissan is a partnership between Paris-based Renault and Japanese carmaker Nissan that combined the companies’ engineering teams. They still operate as two separate companies. (Reporting By Yasmeen Abutaleb; Editing by Bill Rigby)\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving technology because it discusses the alliance between automakers Renault and Nissan's plan to launch more than 10 cars with self-driving technology over the next four years in the United States, Europe, China, and Japan. It specifically mentions the introduction of a feature called 'single-lane control' for autonomous highway driving, 'multiple-lane control' for lane-changing on highways, and the ability to drive through city intersections and heavy city traffic autonomously by 2020. These details clearly indicate the development and integration of self-driving technology into their vehicles.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/autos-nissan-autonomous-idUSL1N14R1Q620160107","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":74,"Time":"12:00 PM GMT+2•July 13, 2023","Title":"Cruise, Waymo say humans are bad drivers amid robotaxi permit delays","Content":"cameraImage Credits: Cruise\nAutonomous vehicle companies Cruise and Waymo have separately pushed a narrative this week that humans are bad drivers and that their technologies are crucial to making roads safer. The moves — full-page ads in major newspapers from Cruise and a blog post from Waymo — come as California regulators delay for a second time granting expanded permits that would give both companies authority to charge for fully driverless robotaxi rides with no human behind the wheel across San Francisco 24\/7.\nThe offensive tactics that paint human drivers as the real problem are an attempt to sway public opinion in favor of autonomous vehicle services, even as residents, safe streets advocates and city agencies like the San Francisco Municipal Transportation Authority (SFMTA) complain that malfunctioning robotaxis add to the city’s congestion problem and have impeded traffic, public transit and emergency responders.\nBoth companies currently run limited robotaxi services in the city. The California Public Utilities Commission (CPUC) had scheduled a hearing to approve permit expansions Thursday but pushed the hearing date to August 10. The agency didn’t say why exactly, only stating that the matters required “further review.”\nAs part of a push ahead of the vote next month, Cruise on Thursday took out full-page ads in the San Francisco Chronicle, the New York Times, the Los Angeles Times and the Sacramento Bee with the headline: “Humans are terrible drivers.”\n“You might be a good driver, but many of us aren’t,” reads the ad. “People cause millions of accidents every year in the US. Cruise driverless cars are designed to save lives.”\nWaymo published a blog post with a similar sentiment Tuesday. The Alphabet-owned company used its robotaxis to analyze the aggregate speeds of cars in San Francisco and Phoenix over a 10-day period, and found that vehicles speed 47% of the time. Many cars went 25 miles per hour over the posted speed limit.\nThe company cited National Highway Traffic Safety Administration data that showed in 2020, speeding accounted for one-third of all traffic fatalities and 13% of injuries in the U.S.\n“Unlike humans, the Waymo Driver is designed to follow applicable speed limits,” reads the blog. “Our driver can also detect the speed of other vehicles on the road. Doing so helps the Waymo Driver predict the likely next maneuvers of the vehicles around it and respond accordingly.”\nBoth Cruise and Waymo touted their own safety records. Cruise said its cars were involved in 92% fewer collisions as the primary contributor and 54% fewer collisions overall when benchmarked against human drivers in a comparable driving environment.\n“Local leaders and regulators need to safely explore every option possible to reverse the horrific status quo on our roads, instead of blocking a critical technology with a strong safety record,” said Drew Pusateri, a Cruise spokesperson. “Last year pedestrian deaths in the United States reached their highest levels in 40 years, often due to preventable human error, and the public deserves to know that there’s a promising emerging technology that could help improve road safety.”\nThe narratives that humans are unsafe drivers are not without truth, but that doesn’t necessarily mean robotaxis and autonomous vehicles are the solution. In fact, many safe streets advocates argue that cities should be advancing public transit and micromobility, not Big Tech solutions.\nWhile Cruise and Waymo vehicles haven’t been involved in any fatal human collisions yet, the technologies are far from perfect. There have been multiple instances of Cruise AVs malfunctioning and just stopping in the middle of roads or intersections, and a Waymo vehicle hit and killed a dog in the city last month, although that accident appeared to have been unavoidable.\nThe CPUC wouldn’t tell TechCrunch what caused its second hearing delay. The agency seemed all but ready to approve the expansion of both companies’ territories back in May when it released draft resolutions.\nIn San Francisco, Cruise’s permits currently allow it to offer a fared passenger service in limited areas of the city from 10 p.m. to 6 a.m., as well as a free passenger service throughout the city at any time of the day — both with and without a safety driver present.\nWaymo operates a fared service throughout San Francisco at any time of the day, but it’s required to have a human safety driver present in the vehicle. The company operates a fully autonomous service throughout the city as well, but that one is still free. Waymo also provides a free service with a safety driver present in parts of Los Angeles and in and around Mountain View.","Relevant":true,"Overview":"Here is the JSON-formatted response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Reason\": \"The news is relevant to self-driving technology malfunctions because it discusses the safety and performance of autonomous vehicles from Cruise and Waymo.\",\n    \"Details\": [\n      \"Both Cruise and Waymo are promoting the idea that humans are bad drivers and that their autonomous technologies are necessary for safer roads.\",\n      \"The news mentions the delay in granting expanded permits for fully driverless robotaxi rides, which implies that there are concerns about the safety and reliability of self-driving technology.\",\n      \"It highlights instances of malfunctioning robotaxis, such as Cruise AVs stopping in the middle of roads or intersections and a Waymo vehicle hitting and killing a dog.\",\n      \"Despite promoting safety records, there are concerns raised about the performance and reliability of these autonomous vehicles.\"\n    ]\n  }\n}\nThe news is relevant to self-driving technology malfunctions because it discusses the safety and performance of autonomous vehicles, including instances of malfunctioning and concerns about their reliability.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2023\/07\/13\/cruise-waymo-say-humans-are-bad-drivers-amid-robotaxi-permit-delays-san-francisco\/","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":80,"Time":null,"Title":"U.S. push for self-driving cars faces union, lawyers opposition","Content":"By David Shepardson\nWASHINGTON (Reuters) - The U.S. Senate Commerce Committee on Wednesday again rejected attempts to lift regulations to allow for the deployment of thousands of autonomous vehicles as union groups and attorneys campaign against the legislative proposal.\nSlideshow ( 2 images )\nThe committee rebuffed the bid by Republican Senator John Thune to attach measures lifting regulations on autonomous vehicles to a $78 billion surface transportation bill after he sought last month to attach it in May to a bill on China tech policy.\nThune has proposed granting the U.S. National Highway Traffic Safety Administration (NHTSA) the power to grant exemptions for tens of thousands of self-driving vehicles per manufacturer from safety standards written with human drivers in mind.\nThe surface bill, which would boost funding for Amtrak and other transportation needs, was approved by the committee on a 25-3 vote.\nThune and other lawmakers have sought for nearly five years to win approval.\nThe senator argued autonomous vehicles could help eliminate numerous deaths due to human error like distracted or impaired drivers.\nCommerce Committee Chair Maria Cantwell, a Democrat, cited recent Tesla crashes and other recent incidents involving driver assistance systems in her response.\n“It seems like every other week we’re hearing about a new vehicle that crashed when it was on Autopilot” Cantwell said. “I do think this is legislation that we can complete by the end of this year... These last issues are very thorny as it relates to legal structure.”\n“Democrats have yielded to pressure from special interests against the best interests of our economy and the American people,” Thune said. “Are we really going to continue to ignore the enormous safety benefits of these vehicles?”\nThune said that the “Teamsters and trial lawyers” are opposed to self-driving legislation and they “seem to own lock, stock and barrel the Democrats on this committee.”\nThe Teamsters did not immediately comment. The American Association for Justice, which represents plaintiffs lawyers said it “will continue to oppose any legislation that exempts the driverless car industry from basic safety standards, and allows auto and tech companies to avoid being held accountable through the use of forced arbitration clauses.”\nThe Self-Driving Coalition, which represents Ford Motor Co, Alphabet Inc’s Waymo, Volvo Cars and others said “the exclusion of AV legislation from current surface transportation reauthorization bills reflects yet another missed opportunity to save lives” but it vowed to work with lawmakers.\nLast month, Greg Regan, president of the Transportation Trades Department for the AFL-CIO, told U.S. lawmakers that autonomous vehicles place “millions of jobs at risk” and any self-driving legislation should not apply to commercial trucks.\nReuters reported in May that Waymo and rival Cruise, a unit of General Motors, have applied for permits to start charging for rides and delivery using autonomous vehicles in San Francisco.\nReporting by David Shepardson; Editing by Aurora Ellis\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the JSON-formatted response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"The news is relevant to self-driving technology malfunctions as it discusses the U.S. Senate Commerce Committee's rejection of attempts to lift regulations for autonomous vehicles. The committee's decision reflects concerns about safety standards and recent incidents involving driver assistance systems, such as Tesla crashes. The article also mentions opposition from the 'Teamsters and trial lawyers' who express concerns about the exemption of driverless cars from basic safety standards. These elements collectively highlight the ongoing debate and challenges surrounding self-driving technology and its potential malfunctions.\"\n}\nThis news is relevant to self-driving technology malfunctions because it addresses the regulatory and safety concerns associated with autonomous vehicles, including recent incidents and opposition from various groups.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-usa-autos-self-driving-idCAKCN2DS29P","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":82,"Time":"SEPTEMBER 7, 2017","Title":"Lyft to offer rides in self-driving cars in San Francisco area","Content":"By Heather Somerville\nRecruiters for Lyft wait for the opening of a job fair in Golden, Colorado, June 7, 2017. REUTERS\/Rick Wilking\nSAN FRANCISCO (Reuters) - A self-driving car will soon be one ride option available from Lyft in the San Francisco Bay Area, as the ride-services company ramps up its efforts to become a serious player in autonomous vehicle technology.\nLyft said on Thursday that self-driving cars will soon be dispatched to certain passengers who request a ride through the app in the area. The cars will come from Drive.ai, a Mountain View, California, startup that builds software to turn cars into autonomous vehicles.\nIt is the latest in a string of partnerships between Lyft and an autonomous car company, but it is the one with the most immediate impact to Lyft passengers. There will be initially a small number of cars available, said Drive.ai Co-founder and President Carol Reiley, each with a trained driver in the front seat in case something goes wrong.\n“We want to make sure the experience feels as much like an autonomous vehicle experience as possible,” Reiley said.\nPassengers must choose to opt into the program and the rides are free. Reiley declined to disclose the car model being used or precisely when the self-driving Lyft rides would start. Lyft declined to comment further.\nThe program allows Lyft to test how its passengers react to self-driving cars and Drive.ai, a two-year-old company, to log more miles and tweak its software. Reiley said Drive.ai will use its own mapping data for the trips.\nThe program is Lyft’s latest push into autonomous cars since announcing in July a new self-driving car division, including a facility in Palo Alto, California with hundreds of engineers who will work on autonomous technology and collaborate with other autonomous vehicle companies.\nLyft has previously announced partnerships with Alphabet Inc's GOOGL.O self-driving division, Waymo, technology company Nutonomy, and automakers General Motors Co GM.N and Jaguar Land Rover. Lyft has previously said it will launch a pilot with Nutonomy in Boston by year-end.\nAlthough Lyft is a late entry into the field of autonomous cars, the partnership gives the company something of a victory over its chief competitor, Uber Technologies Inc [UBER.UL]. While Uber was first to offer rides in self-driving cars, using its own autonomous technology, with programs in Pennsylvania and Arizona, it does not yet offer them to passengers in the Bay Area.\nReporting by Heather Somerville; Editing by Cynthia Osterman\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving technology as it announces Lyft's plan to offer rides in self-driving cars in the San Francisco Bay Area. Lyft has partnered with Drive.ai, a startup specializing in autonomous vehicle software. The report mentions that there will be a small number of self-driving cars available initially, each with a trained driver in the front seat as a safety precaution. The program aims to provide passengers with an autonomous vehicle experience, allowing Lyft to test passenger reactions to self-driving cars and Drive.ai to gather more data for software improvement. This news directly pertains to self-driving technology and its integration into Lyft's ride-sharing platform.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/cbusiness-us-lyft-selfdriving-idCAKCN1BI1R0-OCABS","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":84,"Time":"MAY 14, 2020","Title":"GM self-driving unit Cruise cuts jobs as pandemic hits sector","Content":"By Jane Lanhee Lee, David Shepardson\n(Reuters) - General Motors Co's GM.N self-driving car unit Cruise told staff on Thursday it has decided to lay off workers, the latest start-up in the nascent industry to cut jobs during the coronavirus pandemic.\nSlideshow ( 3 images )\nCruise officials acknowledged they were cutting jobs, but declined to confirm the number. According to an internal email partially read to Reuters, about 8% of the staff would be cut, which amounts to more than 140 people.\nThe outbreak has caused funding to dry up in the autonomous driving industry. That is only the latest challenge in a sector whose promise of large-scale rollouts of so-called robotaxis has been pushed out by many years.\n“In this time of great change, we’re fortunate to have a crystal clear mission and billions in the bank. The actions we took today reflect us doubling down on our engineering work and engineering talent,” Cruise spokesperson Milin Mehta told Reuters.\nThe cuts at Cruise, which had 1,800 full-time employees, come just a week after GM Chief Executive Officer Mary Barra said the No. 1 U.S. automaker’s commitment to Cruise was “unwavering” and it was growing the business.\n“Cruise is well capitalized,” she told analysts on a May 6 conference call after GM posted stronger-than-expected first-quarter profit. “We have and will continue to grow our team by recruiting and retaining the very best engineering and leadership talent.”\nThe email said layoffs at Cruise include staff at an engineering team in Pasadena, California, that works on Lidar, a sensor technology that uses pulsed laser light to sense objects, similar to the way radar uses radio waves. The email was sent by Cruise CEO Dan Ammann on Thursday.\nLidar start-ups had mushroomed when self-driving car tech drew billions of dollars in investments.\nThe cuts at Cruise echo pain felt elsewhere in the industry.\nStarsky Robotics, a five-year-old San Francisco start-up focused on automating long-haul freight delivery, closed its doors in mid-March due to lack of funding. And Zoox, a self-driving tech firm based in Silicon Valley, also has laid off staff during the pandemic.\nReporting By Jane Lanhee Lee in Oakland and David Shepardson in Washington D.C. and Ben Klayman in Detroit; Editing by Chizu Nomiyama, David Gregorio and Ben Klayman\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions as it discusses General Motors Co's self-driving car unit Cruise laying off workers during the coronavirus pandemic. Although it doesn't directly mention malfunctions in self-driving technology, the layoffs could be indicative of challenges or setbacks in the development and deployment of self-driving technology. The article also mentions the impact of the pandemic on funding in the autonomous driving industry, which could potentially affect the progress of self-driving technology. Additionally, the mention of staff cuts in an engineering team working on Lidar technology, a crucial component of self-driving systems, further suggests challenges or issues in this technology sector.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-gm-autonomous-layoffs-idCAKBN22Q34W","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":85,"Time":null,"Title":"Uber seeks OK to resume self-driving tests in Pennsylvania","Content":"By David Shepardson\nWASHINGTON (Reuters) - Uber Technologies Inc has asked Pennsylvania for permission to resume self-driving car testing on public roads and has improved the autonomous vehicle software, the company said on Friday, more than seven months after it suspended testing following a deadly crash in Arizona.\nFILE PHOTO: A car passes the location where a woman pedestrian was struck and killed by an Uber self-driving sport utility vehicle in Tempe, Arizona, U.S. March 19, 2018. REUTERS\/Rick Scuteri\/File Photo\nThe company disclosed in a report to the U.S. National Highway Traffic Safety Administration that it would resume testing with two employees in the front seat, enable an automatic braking system at all times, and more strictly monitor safety employees.\nUber has been testing self-driving cars in manual mode on public streets.\nIn June, police in Tempe, Arizona, said a back-up driver behind the wheel of a self-driving Uber was distracted and streaming a television show on her phone right up until about the time the car struck and killed a pedestrian walking across a street, deeming the crash that rocked the nascent industry “entirely avoidable.”\nThe crash was the first death attributed to a self-driving vehicle and was seen as a significant setback for the industry, which is racing to get vehicles into commercial use.\nUber Chief Executive Dara Khosrowshahi said in a statement Friday the company would resume road tests only after implementing “improved processes.”\nUber said it now has real-time third party monitoring of back-up safety drivers, sets limits on the time drivers can work per day and has improved training.\nIn July, Pennsylvania’s Department of Transportation said it was issuing new guidelines asking companies to submit details about testing, and that it expected companies would comply.\nA spokeswoman for the agency said Friday it had received the application and would review it.\nThe state said it would send companies an authorization letter after approving their submissions. Uber said it would not resume testing in Pittsburgh until it received that letter.\nAlphabet Inc's Waymo GOOGL.O unit plans to launch a commercial ride-hailing service in Arizona this year, while General Motors Co GM.N is on track to roll out a similar service next year with vehicles without steering wheels or brake pedals.\nAuthorities in Pittsburgh, where Uber debuted its self-driving vehicles in 2016, have said they welcome the cars back on city streets.\n“I think we have increased confidence,” said Karina Ricks, director of the city’s Department of Mobility and Infrastructure, citing the addition of a second employee in cars being tested and more transparency from Uber.\nFollowing the Tempe accident, Pittsburgh Mayor Bill Peduto balked at allowing Uber to resume autonomous driving before a full federal investigation of the crash.\n“One of the problems with Uber was they appeared to be moving just a bit too fast,” Ricks said. “They got a little cocky about their technology.”\nThe crash raised significant questions about the performance of Uber’s software.\nUber said a key recommendation of an internal review after the Tempe crash was to improve the self-driving vehicles’ “overall software system design.” Uber said in its safety report released Friday the vehicle had improved “system latency,” allowing it to detect objects and actors sooner and execute safe reactions faster.\nThe National Transportation Safety Board, which has an ongoing investigation, said in May that Uber registered observations of the pedestrian about six seconds before impact, but the system did not determine that emergency braking was needed until 1.3 seconds before impact.\nThe NTSB said the vehicle had registered the pedestrian who was walking a bike across the road first as an unknown object, then a vehicle and lastly a bicycle.\nGRAPHIC-How self-driving cars see the road - tmsnrt.rs\/2FToUeq\nUber said it had a new approach to “handling uncertainty within the self-driving system.” Uber also has a new separate systems engineering testing team and plans a self-driving safety advisory board of outside experts. If a vehicle is uncertain about something in its view the software is now better positioned to “reason over many possible outcomes to ultimately come to a safe response,” it added.\nThe NTSB also said Uber had disabled an manufacturer-installed automatic emergency braking system in the 2017 Volvo XC90 while the car was under computer control in order to “reduce the potential for erratic vehicle behavior.”\nThat braking system is now active whenever Uber is testing on public roads, it said on Friday. It has also filed a voluntary safety assessment with the National Highway Traffic Safety Administration on its self-driving efforts and made public a report from an outside law firm that reviewed the safety culture at Uber. The agency has an ongoing probe of the Tempe crash.\nReporting by David Shepardson. Additional reporting by Heather Somerville in San Francisco.; Editing by Nick Zieminski and Richard Chang\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Based on the provided news article, here is the JSON-formatted response:\njson\nCopy code\n{\n   \"Is_relevant\": true,\n   \"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it discusses Uber's request to resume self-driving car testing on public roads after suspending testing due to a deadly crash in Arizona. The crash, which resulted in a pedestrian's death, raised significant concerns about the performance of Uber's self-driving software. The article mentions that Uber has improved its autonomous vehicle software, implemented an automatic braking system at all times, and increased safety monitoring. It also highlights the crash as the first death attributed to a self-driving vehicle and the subsequent setbacks for the self-driving industry.\"\n}\nThis response confirms that the news is relevant to self-driving technology malfunctions and provides specific information from the article that supports this relevance.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/cbusiness-us-uber-selfdriving-idCAKCN1N71XJ-OCABS","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":86,"Time":null,"Title":"RPT-Self-driving 'arms race' complicates supplier alliances","Content":"By Edward Taylor, Paul Lienert\n(Repeats for wider distribution)\nFRANKFURT\/DETROIT, April 13 (Reuters) - The race to develop and exploit autonomous vehicle technology is reshaping the hierarchy of the automotive industry, replacing traditional top-down manufacturing relationships with complex webs of alliances and acquisitions.\nDealmaking in the automotive and technology industry is driven by the rapid transition of self-driving vehicles from research projects to major elements of near-term product plans at several of the world’s biggest automakers.\nThat shift is behind deals like one announced last week between Robert Bosch and Daimler AG’s Mercedes. Bosch and Mercedes said they will collaborate on development of self-driving vehicles, with Bosch in a broad role as a systems integrator — sort of a copilot with the automaker in speeding up deployment of self-driving vehicles. Bosch also expects to sell the jointly developed systems to other companies.\nSeparately, Silicon Valley chipmaker Intel Corp acquired automotive vision technology leader Mobileye NV , and has a deal to help German luxury car maker BMW AG develop autonomous vehicles around Intel and Mobileye systems.\nThe first fully self-driving cars are expected to go into production by 2020-2021. Analysts have said self-driving cars will not be in wide use before 2030.\n“Everybody is trying to understand what skill sets are required to be first in the game (and) if they don’t have it, they’re going to partner, invest or purchase,” said Xavier Mosquet, a senior partner at Boston Consulting Group and an authority on autonomous vehicles.\nMajor auto companies are rich in engineers schooled in the physics of combustion and collisions, materials science and mechanical systems. The development of self-driving cars demands experts in artificial intelligence, robotics, computer programming and digital networks who work mainly outside the auto industry.\nAutomakers are following different paths to acquire engineering talent. Some are relying on partnerships like the Bosch-Mercedes pact. Others such as General Motors Co are going it alone, buying self-driving vehicle startups and building technology in-house.\nAlphabet Inc’s Waymo and auto supplier Delphi Automotive Plc are offering turn-key systems to companies such as Fiat Chrysler Automobiles that are choosing not to invest in their own autonomous driving systems.\nCOPILOT APPROACH\nSome of the car companies and large suppliers could wind up as competitors. BMW has said it wants to sell its self-driving systems to other manufacturers, as does Delphi, which is developing a system of its own. Intel and Mobileye are partners in both ventures.\nThe Dutch provider of high-definition maps, HERE, has taken a position at the center for several supplier webs. HERE is jointly owned by Daimler, BMW, and Volkswagen AG’s Audi. Intel owns a minority stake in HERE, and rival chipmaker Nvidia Corp has a partnership deal.\nNvidia itself wants to be a provider of powerful computer chips and “deep learning” software for self-driving cars to a broad array of customers, including rivals such as Mercedes and Tesla Inc, competing mega-suppliers such as Bosch and ZF Friedrichshafen AG and Chinese tech companies Baidu Inc and Tencent Holdings Ltd (For a graphic on self-driving vehicles see: tmsnrt.rs\/2nYv7gc)\nThe vehicle manufacturers are divided on how much self-driving development and integration to farm out to the parts makers, or whether to keep most of that in-house - as they have done for decades with much of their core engine technology.\n“At the moment, the carmaker is at an advantage since it knows how the components all fit together,” said Mercedes executive Christoph von Hugo.\nBCG’s Mosquet believes the industry may not settle on a single template for collaboration, given the complexity of autonomous vehicles and their underlying technology.\n“These different approaches will have to pass the test of time,” he said. “In two or three years, we will see who has been successful with which approach.”\nReporting by Edward Taylor in Frankfurt and Paul Lienert in Detroit; Editing by Matthew Lewis\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving technology malfunctions as it discusses the race to develop autonomous vehicle technology and the challenges faced by automakers in reshaping their relationships and forming alliances. While the news does not directly mention self-driving technology malfunctions, it highlights the rapid transition of self-driving vehicles from research projects to near-term product plans, indicating the significance of the technology. The need for collaborations and acquisitions in this field suggests the complexity and potential risks associated with self-driving technology, including malfunctions or safety concerns.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/autos-selfdriving-suppliers-idCNL1N1HK206","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":87,"Time":null,"Title":"Alphabet looks to snowy Michigan to test self-driving cars","Content":"By David Shepardson\nWASHINGTON (Reuters) - Alphabet Inc's GOOGL.O self-driving car unit Waymo is expanding winter testing as it works to address a potential blind spot for autonomous vehicles: snowy and icy conditions.\nWaymo unveils a self-driving Chrysler Pacifica minivan during the North American International Auto Show in Detroit, Michigan, U.S., January 8, 2017. REUTERS\/Brendan McDermid\nWaymo said Thursday that Michigan is the sixth state where the self-driving car project will test autonomous vehicles. The company chose the state to see how vehicles will respond in snow, sleet, and ice.\nWaymo has been testing vehicles in Texas, Arizona, Washington State, Nevada and California and starts in Michigan next week on public roads with a backup safety driver sitting in the driver seat if necessary.\n“For human drivers, the mix of winter conditions affects how well you can see and the way your vehicle handles the road. The same is true for self-driving cars,” said Waymo chief executive John Krafcik in a blog post. “Our ultimate goal is for our fully self-driving cars to operate safely and smoothly in all kinds of environments.”\nWaymo said it has conducted cold weather testing since 2012, including some around the Nevada and California border of Lake Tahoe, but the new testing in Metro Detroit may pose tougher challenges navigating snow slicked or icy roads crowded with commuters.\nWaymo opened a development center in Novi, Michigan in May 2016, working with Fiat Chrysler Automobiles NV FCHA.MI to integrate Waymo’s self-driving technology into Chrysler Pacifica Hybrid minivans.\nThe new winter testing is in the back yard of Detroit’s Big Three automakers. U.S. automakers have touted plans to get self-driving cars on the roads.\nGeneral Motors Co GM.N chief executive Mary Barra said Tuesday the Detroit automaker is working hard on self-driving cars and expects to deploy cars without backup drivers in \"quarters not years.\"\nWaymo has been mapping areas in Metro Detroit and wants to assess how its sensors perform in wet, cold conditions. One hurdle is self-driving car mapping software may have challenges in winter weather.\nOne challenge for Waymo may be human drivers. Many self-driving vehicles from numerous companies have been struck by inattentive or careless drivers in California, according to reports to filed with the state.\nReporting by David Shepardson; editing by Grant McCool\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the JSON-formatted response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"The news is relevant to self-driving technology malfunctions because Alphabet Inc's self-driving car unit Waymo is expanding winter testing to address a potential blind spot for autonomous vehicles in snowy and icy conditions. Waymo acknowledges that winter conditions can affect visibility and the vehicle's handling of the road, which is a crucial concern for self-driving cars. The company specifically mentions testing in snow, sleet, and ice in Michigan, a location known for its challenging winter weather. Waymo has also noted that the testing may pose tougher challenges in navigating snow-slicked or icy roads crowded with commuters. Additionally, the news highlights concerns about self-driving car mapping software facing challenges in winter weather conditions, which is a key aspect of self-driving technology. Overall, the news provides specific information about how Waymo is addressing and testing self-driving technology in challenging winter conditions, making it relevant to self-driving technology malfunctions.\"\n}\nThe news is indeed relevant to self-driving technology malfunctions as it discusses how Waymo is addressing issues related to snowy and icy conditions, which can be challenging for autonomous vehicles.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/cbusiness-us-autos-selfdriving-waymo-idCAKBN1CV1SN-OCABS","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":93,"Time":"JULY 10, 2019","Title":"Volvo solves autonomous revenue riddle with package deals","Content":"By Esha Vaish\nSTOCKHOLM (Reuters) - Swedish truckmaker AB Volvo’s first commercial autonomous truck deal shows how it is bundling services to generate revenue from a technology that is years away from wide deployment.\nFILE PHOTO: A self-driving Volvo electric truck with no cab called Vera is seen during a presentation in Berlin, Germany, September 12, 2018. REUTERS\/Emma Thomasson\/File Photo\nDriverless transportation has been hailed as a transformative revenue opportunity, with the Boston Consulting Group expecting connected high-tech vehicles to generate about $150 billion of new profits for the auto sector by 2035.\nBut regulatory, technological and infrastructure roadblocks stand in the way of deploying fully autonomous vehicles on public roads and the journey is proving long and costly.\nVolvo, the world’s second biggest truckmaker behind Daimler, has decided for now to only deploy driverless trucks in pilots for customers, aiming to perform specific jobs on a limited, repetitive and controlled route, often on enclosed customer sites.\n“There’s a lot of uncertainties and that’s why we believe the right way to develop autonomous is with commercial pilots where we partner up with customers, go for real implementations and learn from that,” Sasko Cuklev, Volvo Trucks’ autonomous solutions director, told Reuters in an interview.\nThe truck maker said last month that its first commercial autonomous transport package will involve seven trucks transporting limestone for Norway’s Broennoey Kalk AS from a mine to a nearby port starting this winter.\n“We are in the early stages when it comes to implementing autonomous solutions, so we’re trying to learn and we’re open to different setups. But in general it is more and more talk about services and solutions that is coming into play,” Cuklev said.\nThe deal with Broennoey bundles together the provision of the autonomous trucks with a virtual driver, control tower system, maintenance, repair and insurance, with Volvo paid per tonne transported.\n“We see autonomous as more of a complement to today’s business and limited to dedicated specific applications where it really makes sense,” Cuklev said.\nHe said Volvo was targeting autonomous vehicles for mining operations and hub-to-hub transport on a highway road or regional hauling over shorter distances such as between ports and warehouses using its cabinless truck Vera.\nVolvo’s Vera and some other commercial vehicles from rivals are experimenting with using self-driving trucks on public roads, often limiting speeds, picking less busy industrial roads or having people in the cabin in case the technology fails.\nNvidia-backed startup TuSimple said in May that it would deploy its self-driving trucks to haul mail between U.S. Postal Service facilities in Phoenix and Dallas in the southwestern United States.\nSweden’s Einride is testing its cabinless trucks to haul freight between a warehouse and a terminal on public roads in Sweden.\nTEAMING UP\nCarmakers BMW and Daimler this month teamed up to spread the costs of developing automated driving technology as cooperation within the industry becomes more widespread.\nVolkswagen and Ford are in the final stage of talks on a strategic alliance to jointly develop self-driving and electric cars, while Renault and Fiat Chrysler Automobiles attempted and failed to merge.\nAutomakers’ investment in autonomous transportation comes as traditional sales are dented by the economic uncertainty caused by the U.S.-China trade war, with analysts worried that truck cycles might have peaked and margins might fall.\nVolvo has forecast lower demand in China and Europe this year and its trucks order intake has fallen for two consecutive quarters this year, missing forecasts.\nVolvo, which produces trucks under the Mack, Renault and UD Trucks brands, is facing a push to cooperate with others from China’s Geely [GEELY.UL], which became a top shareholder in both Volvo and its main rival Daimler last year.\nCuklev declined to comment on the subject, but highlighted the example of Volvo’s recent tie-up with Nvidia to develop artificial intelligence for self-driving trucks as the type of cooperation that Volvo was keen on.\n“When it comes to automation we’re open to looking into different partnerships in the entire autonomous area,” Cuklev said.\n(Removes extraneous ‘s in headline)\nReporting by Esha Vaish in Stockholm; Editing by Elaine Hardcastle\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it discusses the challenges and limitations in deploying fully autonomous vehicles on public roads. It mentions roadblocks such as regulatory, technological, and infrastructure issues that stand in the way of widespread deployment. Additionally, it highlights that some self-driving trucks, including Volvo's Vera, are experimenting with using self-driving technology on public roads while having people in the cabin in case the technology fails. This indicates concerns and precautions related to self-driving technology malfunctions.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/cbusiness-us-volvo-autonomous-idCAKCN1U51DP-OCABS","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":96,"Time":"JULY 6, 2016","Title":"Canada's Ontario gets no applications for self-driving car plan","Content":"By Ethan Lou\nTORONTO (Reuters) - Ontario’s program to allow testing of self-driving cars on public roads has not received any applications since it launched Jan. 1, the Canadian province’s government said on Wednesday, as a recent U.S. accident increased scrutiny of the technology.\nVisitors look at a self-driving car by Google displayed at the Viva Technology event in Paris, France, July 1, 2016. REUTERS\/Benoit Tessier\nOntario, home to nearly all of Canada’s auto industry, said last year it would be the first province to allow testing of self-driving cars. Ontario’s auto sector has struggled as investment shifted to cheaper jurisdictions, and the government has sought to attract higher paying research and development work.\nBut the province is a late-comer compared with parts of the United States, where Alphabet Inc's GOOGL.O subsidiary Google has been testing self-driving prototypes since 2009.\nDebate about the safety of driverless cars heated up last week after driver of a Tesla Motors Inc TSLA.O car in autopilot mode was killed in a collision.\nA spokesman from Ontario’s Ministry of Transportation said the self-driving car program has a 10-year time frame and has received a lot of interest despite the lack of applications.\nOntario confirmed the lack of applications after Reuters learned about it through a freedom-of-information request.\nThe province said last year nearly 100 companies and other organizations were already working in the connected vehicle and automated vehicle industry in Ontario, and its driverless-car program would enable them to keep research local.\nBut some car companies with plants in Ontario are still favoring the United States.\nHonda Motor Co Ltd 7267.T, which does research on autonomous vehicles in the United States, said it had no plans to participate.\nCanadian auto parts supplier Magna International Inc MG.TO said that in most cases it is testing self-driving car technology on closed circuits.\nGeneral Motors Co GM.N said in June it would expand its Canadian engineering base to add 700 jobs in Ontario as it boosts research spending on connected and driverless cars. It did not immediately respond to questions on whether will participate in Ontario's program.\nA spokesman for Google, which has agreed to work with Fiat Chrysler Automobiles NV FCHA.MIFCAU.N to build a fleet of 100 self-driving mini vans, said the company is not participating in Ontario's program, though he said the province was taking the right steps. Chrysler declined to comment.\nEditing by Steve Orlofsky\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it mentions a recent U.S. accident involving a Tesla car in autopilot mode, which has increased scrutiny of the technology. While the news primarily focuses on the lack of applications for Ontario's program to allow testing of self-driving cars, the reference to the Tesla accident highlights concerns about the safety and reliability of self-driving technology. This incident involving a Tesla car in autopilot mode aligns with self-driving technology malfunctions and raises questions about the technology's effectiveness and safety.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/cbusiness-us-canada-autos-idCAKCN0ZM2CB","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":99,"Time":"FEBRUARY 11, 2020","Title":"U.S. lawmakers to hear calls for requiring new safeguards for self-driving cars","Content":"By David Shepardson\nWASHINGTON, Feb 11 (Reuters) - A U.S. House panel on Tuesday will hear calls for requiring new safeguards as part of any effort to speed the adoption of self-driving cars on U.S. roads.\nAt the same hearing, groups representing automakers and tech companies will tout benefits of autonomous vehicles and warn the United States risks falling behind China and others without new legal backing. U.S. lawmakers have been divided for years over what consumer and legal protections should be added to any self-driving legislation.\nJeffrey Tumlin, director of transportation for San Francisco’s Municipal Transportation Agency will tell the Energy and Commerce subcommittee that Congress should require manufacturers to include event data recorders in autonomous vehicles to “preserve all information from sensors before a collision.”\nTumlin’s written testimony, reviewed by Reuters, adds Congress should “ensure that every safety incident involving an autonomous vehicle is documented in a national database that is available to researchers and the public.”\nCalifornia requires companies testing self-driving cars to disclose all crashes involving the vehicles on public roads, even when under manual control. Those reports are made public as is data on disengagements of self-driving cars. Other states do not require those disclosures.\nJohn Bozzella, who heads an auto trade association representing General Motors Co, Toyota Motor Corp , Volkswagen AG and others, will testify “the worst outcome would be for Congress to delay the enactment of meaningful legislation that would establish the needed federal framework to realize these safety and mobility solutions.”\nCurrent regulations essentially bar deployment of vehicles without steering wheels and human controls unless they get approval from the National Highway Traffic Safety Administration (NHTSA). The agency last week granted its first-ever temporary exemption for a fleet of riderless delivery vehicles from SoftBank Group Corp backed Nuro Inc.\nAutomakers oppose requiring airplane-style certification of new autonomous vehicle technologies before they are deployed on U.S. roads and note NHTSA can demand removal of unsafe vehicles.\nDaniel Hinkle, a counsel at a trial lawyers group, says in his testimony that “those who are injured or harmed by automated driving must be able to hold the driver manufacturer accountable.”\nRepresentative Cathy McMorris Rodgers, the top Republican on the panel, cast the issue in geopolitical terms, noting Chinese self-driving car developers in 2019 “logged the second-most miles of any country testing” in California.\n“China is using our infrastructure, testing on our roads, collecting information on our citizens, and stealing our technology to beat us. There is a global race to (autonomous vehicles). Do we want China to win that race, or do we want to lead?” she said.\nCathy Chase, president of Advocates for Highway and Auto Safety, will urge Congress to require tests for self-driving cars to ensure they can properly detect other vehicles and hazards and call for a mandatory manual override in all self-driving cars.\nChase argues it is misleading to argue the United States was falling behind in commercial deployment. “We are behind in establishing comprehensive safeguards to ensure that this progress happens without jeopardizing or diminishing public safety,” her testimony says. (Reporting by David Shepardson’ Editing by Lincoln Feast.)\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to self-driving technology malfunctions because it discusses the need for new safeguards in the development and deployment of self-driving cars. Specifically, it mentions the call to require manufacturers to include event data recorders in autonomous vehicles to preserve sensor information before a collision, the creation of a national database to document safety incidents involving autonomous vehicles, and the concern about the deployment of vehicles without steering wheels and human controls. These topics all relate to safety and potential malfunctions in self-driving technology.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/autos-autonomous-congress-idUKL1N2AA1F2","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":81,"Time":"7:36 PM GMT+2•August 1, 2023","Title":"NHTSA probes 280,000 new Teslas over loss of steering control","Content":"cameraImage Credits: Tesla\nThe National Highway Traffic Safety Administration (NHTSA) is investigating 280,000 Tesla vehicles over reports of loss of steering control and power steering. The malfunctioning has frequently been accompanied by driver-facing messages indicating that power steering assist is reduced or disabled, according to the report.\nThe preliminary evaluation into 2023 Model 3 and Model Y vehicles comes after regulators received 12 complaints from owners, according to NHTSA’s investigation. NHTSA has put Tesla under the microscope many times over the years, largely in response to the automaker’s advanced driver assistance systems, Autopilot and Full Self-Driving (FSD). It’s not clear if this investigation relates directly to either ADAS, and NHTSA did not respond in time to TechCrunch’s request for more information.\nA driver in Woodbridge, Virginia complained that their Tesla Performance Model 3 began showing problems two weeks after delivery on June 15.\n“Today while driving and making a left turn, the steering wheel locked up while showing an error code UI_a020 stating steering assist is disabled,” the driver wrote in a complaint. “Vehicle steering gets locked, and once you get out and get back in, the car code disappears; however after driving a mile or 2 code reappears, and the steering gets very stiff, making it almost impossible to turn in either direction. Very unsafe and unreliable to drive on highways at high speeds. Please investigate this matter, as I am not the only one with this issue.”\nAnother Model 3 driver reported in May the “car steering felt stuck and slid off the road which resulted in crashing into a tree.”\nA complaint from a Model Y owner in April said the “wheel jerked hard right and made a thud” and that Tesla had moved their service visit from May 2 to May 25 due to a backlog of parts.\nThe preliminary evaluation is the first step to a formal investigation to determine if the problem poses an unreasonable safety risk. If NHTSA were to demand a recall, it would need to upgrade the probe to an engineering analysis.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to self-driving technology malfunctions because it reports on the National Highway Traffic Safety Administration (NHTSA) investigating 280,000 Tesla vehicles due to reports of loss of steering control and power steering issues. This malfunctioning has frequently been accompanied by driver-facing messages indicating that power steering assist is reduced or disabled. Several complaints from Tesla owners describe instances where the steering wheel locked up, displayed error codes, and made the steering very stiff, leading to unsafe driving conditions. These issues directly relate to the malfunction of the steering system in these vehicles, which is a crucial component of self-driving technology. The investigation by NHTSA is the first step to determine if this problem poses an unreasonable safety risk, and it highlights potential malfunctions in the advanced driver assistance systems (ADAS) utilized by Tesla, such as Autopilot and Full Self-Driving (FSD).\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2023\/08\/01\/nhtsa-probes-280000-new-teslas-over-loss-of-steering-control\/","query_name":"updated_file_self-driving_technology_malfunctions.xlsx"}
{"num":10,"Time":"12:53 AM GMT+2•May 19, 2022","Title":"NHTSA probes Tesla Autopilot crash that killed three people","Content":"cameraImage Credits: NTSB\nA U.S. federal agency is investigating a crash involving a 2022 Tesla Model S that may have been operating in Autopilot during a crash that killed three people.\nAutopilot is Tesla’s advanced driver-assistance system (ADAS) that performs automated functions such as steering, accelerating and automatic braking. Bloomberg first reported on the news.\nThe accident, which happened earlier this month, occurred in Newport Beach, California when the Tesla hit a curb and slammed into construction equipment, leaving the car totaled. This is one of more than 30 crashes being investigated by the National Highway Traffic Safety Administration (NHTSA), all of which potentially have involved Autopilot. Out of the 35 special crash investigations into Tesla since 2016 involving the electric vehicle company’s ADAS, Autopilot has been ruled out only in three.\nA total of 14 crash deaths have been reported in those investigations.\nThis month’s collision is the 42nd included in NHTSA’s special crash investigation of ADAS systems like Autopilot, a probe that began in 2016 after a fatal accident in Florida involving another Tesla Model S that had Autopilot activated.\nWhile Tesla’s website says that “Current Autopilot features require active driver supervision and do not make the vehicle autonomous,” the company’s branding has been accused of misleading drivers of their vehicles’ capabilities. Simply by choosing names like Autopilot and “full self-driving software,” which is Tesla’s newer, more advanced ADAS, the company lulls drivers into a false sense of security despite the fact that its technology is nowhere near full self-driving.\nMore TechCrunch\nMistral AI makes its first large language model free for everyone\nVR is dead\nTinder goes ultra-premium, Amazon invests in Anthropic and Apple explains its new AirPods\nInvestors taking 30% of a startup in a round are being short-sighted","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to Tesla Autopilot accidents. The National Highway Traffic Safety Administration (NHTSA) is investigating a crash involving a 2022 Tesla Model S that may have been operating in Autopilot during the accident, which resulted in the tragic death of three people. This incident occurred in Newport Beach, California, when the Tesla hit a curb and slammed into construction equipment, totaling the car. The NHTSA is currently investigating more than 30 crashes, all of which potentially involved Autopilot. Out of the 35 special crash investigations into Tesla since 2016 related to the company's advanced driver-assistance system (ADAS), Autopilot has been ruled out only in three cases. Additionally, a total of 14 crash deaths have been reported in these investigations. This news highlights ongoing concerns about the safety and effectiveness of Tesla's Autopilot system, as well as the potential for driver misinterpretation of the system's capabilities, despite Tesla's warnings that it requires active driver supervision and is not fully autonomous.\"\n}\nThe news is indeed about a Tesla Autopilot accident, and the specific information provided in the news article supports this conclusion.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/05\/18\/nhtsa-probes-tesla-autopilot-crash-that-killed-three-people\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":3,"Time":"9:07 PM GMT+2•June 15, 2022","Title":"NHTSA report shows Tesla Autopilot led the pack in crashes, but the data has gaps","Content":"cameraImage Credits: Scott Olson \/ Getty Images\nOn the face of it, data from the first year of the National Highway Traffic Safety Administration’s project to track the safety of advanced driver assistance systems look terrible for Tesla. Its electric vehicles were involved in 70% of the reported crashes involving Level 2 technologies, 60% of those resulting in serious injuries and nearly 85% of the fatal ones.\nThe data released early Wednesday were collected under the federal regulator’s Standing General Order issued last June, requiring automakers to report the most serious crashes that involved Level 2 ADAS, which need a human driver to remain fully engaged in the driving task. NHTSA is also tracking crashes involving fully automated vehicles — none of which are currently available to consumers.\nThere are five levels of automation under standards created by SAE International. Level 2 means two functions such as adaptive cruise and lane keeping are automated and still have a human driver in the loop at all times. Level 2 is an advanced driver assistance system, and has become increasingly common in new vehicles. \nTesla topped the ADAS list for all the wrong reasons: 273 reported crashes, three with serious injuries, and five deaths. Honda trailed far behind Tesla with 90 crashes and one fatality, while most other manufacturers reported just a handful. Nissan reported none at all.\nSo does that mean Tesla owners should trade in their Model 3 with Autopilot for a Nissan Leaf, and its own Level 2 ADAS, called ProPilot?\nIt’s a more complicated question than one might think. The way the Order is worded, the technologies that Tesla has deployed, and the sheer number of Tesla vehicles on the road mean that its vehicles may not be quite as dangerous as the numbers suggest.\nFor a start, there are more ADAS-equipped Teslas on the road (about 830,000) than vehicles from other manufacturers, although Nissan isn’t far off, with 560,000.\nTesla’s Autopilot can also be used on a variety of roads, unlike Nissan’s ProPilot and GM’s SuperCruise systems, which are limited to highways. Without knowing the number of miles driven with each ADAS system in operation, and where, it is impossible to compare their relative levels of safety — or how each might contrast with accident rates under full human control.\nThe Order required manufacturers to report all the incidents they knew about, but most vehicles on the road do not have remote telematics that send vehicle data back to the factory. Manufacturers of these cars were reliant on consumer complaints (which comprised the majority of reports), law enforcement contacts or media stories, all of which may not have accurately reported whether their ADAS systems were in use.\nTesla, on the other hand, knows exactly which vehicles were using Autopilot when they crashed, as its vehicles have cellular and Wi-Fi connections that automatically report vehicle data when a crash occurs. Almost all its crash reports were sourced from such telematics, compared to just nine from Subaru, four from GM, three from Lucid and one from Honda.\nFinally, the Order required makers to include data on crashes that they were made aware of beginning 10 days after the Order was served last June. In Tesla’s case, that apparently included crashes stretching back to 2019, including three of its five fatal crashes, and all three of its serious ones. (It’s unclear why Tesla was only notified of those accidents months or years after they happened.) Apart from Tesla, only Honda reported a couple of crashes from before June 2021.\nWhile all these variables seem to point in the same direction — a relative over-reporting of Tesla crash data, and under-reporting of crashes involving other car-makers — their impact is impossible to quantify from the NHTSA data alone. Perhaps all Level 2 systems are more dangerous than human drivers alone, due to driver inattention. Or it could be the case that Tesla’s Autopilot as deployed is in fact less competent and more dangerous than rival ADAS technologies.\n“The data released today is a good start, but it doesn’t provide an apples-to-apples comparison of advanced vehicle safety,” said National Transportation Safety Board Chair Jennifer Homendy. “What NHTSA provided was a ‘fruit bowl’ of data with a lot of caveats, making it difficult for the public and experts alike to understand what is being reported. Independent analysis of the data is key to identifying any safety gaps and potential remedies.”\nThe final word on Autopilot will have to wait for NHTSA’s separate, ongoing and recently expanded investigation into Autopilot, which could potentially lead to a recall. In the meantime, drivers with Level 2 systems in their cars would be well advised to heed NHTSA’s advice — “no commercially available motor vehicles today are capable of driving themselves.”\nMore TechCrunch\nMistral AI makes its first large language model free for everyone\nVR is dead\nAs AI porn generators get better, the stakes get higher\nVR is dead","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news article:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": [\n    \"The news is relevant to Tesla Autopilot accidents as it discusses Tesla's involvement in crashes related to Level 2 advanced driver assistance systems.\",\n    \"The article states that Tesla's electric vehicles were involved in 70% of reported crashes involving Level 2 technologies.\",\n    \"Tesla was also involved in 60% of the Level 2 crashes resulting in serious injuries and nearly 85% of the fatal crashes.\",\n    \"The data was collected under the National Highway Traffic Safety Administration's project to track the safety of advanced driver assistance systems, which includes Level 2 ADAS systems like Tesla's Autopilot.\",\n    \"The article mentions that Tesla's Autopilot was involved in 273 reported crashes, three with serious injuries, and five deaths.\",\n    \"Tesla's Autopilot is compared to other manufacturers' systems like Nissan's ProPilot.\",\n    \"It is noted that Tesla has more ADAS-equipped vehicles on the road than other manufacturers.\",\n    \"Tesla's ability to collect data from its vehicles with cellular and Wi-Fi connections is discussed as a factor in understanding crash data.\",\n    \"The news article suggests that the impact of the data on Tesla's Autopilot safety compared to other systems is complex and not easy to quantify.\",\n    \"The National Transportation Safety Board Chair mentions that the data provided lacks an 'apples-to-apples' comparison of advanced vehicle safety.\",\n    \"The article also mentions an ongoing investigation into Autopilot by the NHTSA, which could potentially lead to a recall.\"\n  ]\n}\nThe news is relevant to Tesla Autopilot accidents because it discusses Tesla's involvement in crashes related to Level 2 advanced driver assistance systems, particularly its Autopilot system. It provides data on the number of crashes, injuries, and fatalities involving Tesla vehicles equipped with Autopilot, and it highlights the complexities in comparing safety data among different systems. Additionally, the article mentions an ongoing investigation by the NHTSA into Autopilot, which indicates its relevance to the topic of Tesla Autopilot accidents.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/06\/15\/tesla-autopilot-nhtsa-crashes-fatalities\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":8,"Time":"Thu 9 Jun 2022 20.53 CEST","Title":"Tesla investigation deepens after more than a dozen US ‘Autopilot’ crashes","Content":"US federal regulators are deepening their investigation into Tesla’s Autopilot function after more than a dozen Tesla cars crashed into parked first-responder vehicles over a period of four years.\nThe National Highway Traffic Safety Administration (NHTSA) said on Thursday it was upgrading its preliminary investigation, which launched last August, to an “engineering analysis”, which is taken before the agency determines a recall.\nTwitter set to comply with Elon Musk demand for data on fake accounts\nRead more\nThe investigation covers all four Tesla vehicles – Models Y, X, S and 3 – representing about 830,000 vehicles that have been sold in the US.\nThe investigation is focused on Tesla’s Autopilot feature, which is supposed to help drivers navigate roads through artificial intelligence, which detects other vehicles. The company instructs drivers to pay attention to the road and keep their hands on the steering wheel while using Autopilot, though some drivers have used Autopilot drunk or sitting in the backseat of the car.\nThe 16 crashes at the base of the investigation took place between January 2018 and January 2022 and resulted in 15 injuries and one death. In documents posted to its website, NHTSA said forensic data indicates that the majority of the drivers had their hands on the steering wheel before impact and were complying with the system as it was designed to be used.\nThe agency said this as the investigation specifically looks into whether the Autopilot feature ultimately undermines “the effectiveness of driver’s supervision”.\nNHTSA, in a separate investigation, is looking into a separate batch of complaints that have been filed against Tesla vehicles that suddenly brake at high speeds, otherwise known as “phantom braking”. The agency has received more than 750 complaints relating to the problem, though there have been no crashes or injuries reported. The agency has asked Tesla for more information about its awareness of the issue.\nThe agency has long been wrestling with Tesla, specifically the company’s outspoken founder, Elon Musk, for years. Anonymous officials and former regulators detailed to the Washington Post earlier this year the heated reaction workers at the agency have received from Musk, and noted that regulators have had to learn to deal with a combative company.\nIn a letter to Musk last fall, Jennifer Homendy, chair of the National Transportation Safety Board (NTSB) – a separate federal agency that investigates crashes and makes recommendations to other agencies such as the NHTSA – urged Musk to respond to its safety recommendations for Autopilot that it issued to Tesla back in 2017. The NTSB recommended that Tesla develop safeguards that would not allow drivers to use vehicles in ways inconsistent with its design.\n“If you are serious about putting safety front and center in Tesla vehicle design, I invite you to complete action on the safety recommendations we issued four years ago,” Homendy wrote in a letter to Musk.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to Tesla Autopilot accidents. The National Highway Traffic Safety Administration (NHTSA) is deepening its investigation into Tesla’s Autopilot function after more than a dozen Tesla cars crashed into parked first-responder vehicles over a period of four years. This investigation covers all four Tesla vehicles – Models Y, X, S, and 3 – representing about 830,000 vehicles that have been sold in the US. The investigation is specifically focused on Tesla’s Autopilot feature and its potential role in these crashes. The 16 crashes that are part of the investigation took place between January 2018 and January 2022, resulting in 15 injuries and one death. NHTSA is examining whether the Autopilot feature undermines the effectiveness of driver supervision, which is a key concern in Autopilot-related accidents. This information clearly indicates that the news is about Tesla Autopilot accidents.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2022\/jun\/09\/tesla-autopilot-crashes-investigation-nhtsa","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":4,"Time":null,"Title":"U.S. opens probe into Tesla’s Autopilot over emergency vehicle crashes","Content":"WASHINGTON, Aug 16 (Reuters) - U.S. auto safety regulators on Monday opened a formal safety probe into Tesla Inc's (TSLA.O) driver assistance system Autopilot after a series of crashes involving Tesla models and emergency vehicles.\nThe National Highway Traffic Safety Administration (NHTSA) said it had identified 11 crashes since January 2018 in which Teslas \"have encountered first responder scenes and subsequently struck one or more vehicles involved with those scenes.\"\nAdvertisement · Scroll to continue\nThe probe will take in 765,000 U.S. vehicles with Autopilot built since 2014. Tesla shares closed down 4.3% on the news.\nNHTSA, which closed an earlier investigation into Autopilot in 2017 without taking any action, has come under fire for failing to ensure the safety of the system that handles some driving tasks and allows drivers to keep their hands off the wheel for extended periods.\nAfter the new probe, the auto safety agency could opt to take no action, or it could demand a recall, which might effectively impose limits on how, when and where Autopilot operates.\nAdvertisement · Scroll to continue\nAny restrictions could narrow the competitive gap between Tesla's system and similar advanced driver assistance systems offered by established automakers.\nTesla did not immediately respond to a request for comment. Chief Executive Elon Musk has repeatedly defended Autopilot and in April tweeted that \"Tesla with Autopilot engaged now approaching 10 times lower chance of accident than average vehicle.\"\nAdvertisement · Scroll to continue\nNHTSA said it had reports of 17 injuries and one death in the 11 crashes, including the December 2019 crash of a Tesla Model 3 that left a passenger dead after the vehicle collided with a parked fire truck in Indiana.\nThe 11 crashes included four this year and it had opened a preliminary evaluation of Autopilot in the 2014-2021 Tesla Models Y, X, S, and 3. The crashes involved vehicles \"all confirmed to have been engaged in either Autopilot or Traffic Aware Cruise Control,\" it said.\nAdvertisement · Scroll to continue\nBefore NHTSA could demand a recall, it must first upgrade an investigation into an engineering analysis. The two-step investigative process often takes a year or more.\nAFTER DARK\nNHTSA said most of the 11 crashes took place after dark and crash scenes included measures like emergency vehicle lights, flares or road cones.\nIts investigation will assess technologies \"used to monitor, assist, and enforce the driver's engagement\" with driving when using Autopilot operation.\nThe logo of Tesla is seen in Taipei, Taiwan August 11, 2017. REUTERS\/Tyrone Siu Acquire Licensing Rights\nMusk tweeted last month that Tesla's advanced camera-only driver assistance system, known as \"Tesla Vision,\" will soon \"capture turn signals, hazards, ambulance\/police lights & even hand gestures.\"\nAutopilot was operating in at least three fatal Tesla U.S. crashes since 2016, the National Transportation Safety Board (NTSB) has said.\nThe NTSB has criticized Tesla's lack of system safeguards for Autopilot and NHTSA's failure to ensure the safety of Autopilot.\nNTSB chair Jennifer Homendy on Monday praised the new probe. She said the board has urged the agency to develop standards for drive monitoring systems and require automakers to \"incorporate system safeguards that limit the use of automated vehicle control systems to those conditions for which they were designed.\"\nIn February 2020, Tesla's director of autonomous driving technology, Andrej Karpathy, identified a challenge for its Autopilot system: how to recognize when a parked police car's emergency flashing lights are turned on.\n\"This is an example of a new task we would like to know about,\" Karpathy said at a conference.\nKEY CONCERNS\nBryant Walker Smith, a law professor at the University of South Carolina, said the parked emergency crashes \"really seem to illustrate in vivid and even tragic fashion some of the key concerns with Tesla's system.\"\nNHTSA, he said, \"has been far too deferential and timid, particularly with respect to Tesla.\"\nDemocratic Senators Richard Blumenthal and Ed Markey, who have previously questioned the Autopilot system, urged a thorough and transparent probe that would lead to improvements in the safety of Tesla's automated driving and driver assistance technology and \"prevent future crashes.\"\nNHTSA said Monday it has sent teams to review 31 Tesla crashes involving 10 deaths since 2016 where it suspected advanced driver assistance systems use. It ruled out Autopilot in three of the crashes.\nIt noted that \"no commercially available motor vehicles today are capable of driving themselves\" and said drivers must use them correctly and responsibly.\nNHTSA has been without a Senate-confirmed administrator since January 2017. President Joe Biden has yet to nominate anyone for the post.\nReporting by David Shepardson in Washington and Hyunjoo Jin in Oakland, California Editing by David Holmes, Nick Zieminski and Richard Pullin\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a formal safety probe initiated by U.S. auto safety regulators into Tesla Inc.'s Autopilot system following a series of crashes involving Tesla models and emergency vehicles. The National Highway Traffic Safety Administration (NHTSA) is investigating 11 crashes since January 2018 in which Teslas encountered first responder scenes and subsequently struck one or more vehicles involved with those scenes. The investigation covers 765,000 U.S. vehicles with Autopilot built since 2014. NHTSA has reports of 17 injuries and one death in these crashes. The crashes occurred while vehicles were engaged in either Autopilot or Traffic Aware Cruise Control. The investigation will assess technologies used to monitor, assist, and enforce the driver's engagement when using Autopilot operation. This news is directly related to Tesla Autopilot accidents, as it involves multiple crashes with emergency vehicles and raises concerns about the safety of the Autopilot system.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/us-opens-formal-safety-probe-into-tesla-autopilot-crashes-2021-08-16\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":17,"Time":"Published 3:55 PM EDT, Mon October 17, 2022","Title":"Tesla Autopilot’s safety questioned after latest fatal motorcycle crash","Content":"Washington, DC\nCNN\n — \nTesla’s Autopilot was involved in a third fatal motorcycle crash this summer, raising questions about the driver-assist system’s ability to operate safely.\nThe National Highway Traffic Safety Administration has already launched investigations into the first two crashes and gathered information on the third crash. More details of the latest crash surfaced Monday.\nThe three fatal crashes occurred in a 51-day span this summer and follow a similar line of events: A person driving a Tesla in the early morning hours with Autopilot active strikes a motorcycle.\nThe crashes renew questions about whether users of the systems are kept sufficiently engaged and prepared to fully control the vehicle when needed. Research has shown that drivers glance away from the road more frequently while using Autopilot, and that many Autopilot users believe their cars drive themselves.\nTesla’s Autopilot system keeps the vehicle in its lane while traveling at a set speed, and drivers are instructed to keep their hands on the steering wheel at all times. The automaker says it detects torque on the wheel and uses a camera near the rear-view mirror to determine driver inattentiveness, and uses alerts to remind drivers to keep their eyes on the road.\nIngrid Eva Noon was riding her motorcycle in Palm Beach County, Florida at 2:11 a.m. on Aug. 26 when an impaired driver using Tesla’s Autopilot impacted the rear of Noon’s motorcycle, throwing her onto the Tesla’s windshield and killing her, according to the Palm Beach County Sheriff’s office. Driver-assist crash data that automakers like Tesla must report to NHTSA was published Monday and revealed that Autopilot was engaged.\nUtah resident Landon Embry was killed while riding his Harley-Davidson on July 24 at approximately 1:09 am when a Tesla driver using Autopilot collided with the back of his motorcycle.\nA Utah motorcycle rider was killed in July after being struck by a Tesla driver using Autopilot.\nUtah Department of Safety\nA Tesla driver using Autopilot struck a motorcycle lying on a road on July 7 at 4:47 a.m in Riverside, California. The motorcyclist, who had already fallen off the bike after hitting a dividing wall, was killed, according to California Highway Patrol. The Tesla did not strike the rider, who had already been ejected, California Highway Patrol said.\nThe recent crashes suggest the Tesla system is insufficient, according to motorcycle advocates.\nMotorcycle safety advocates say they’re concerned that the software fails to see motorcycles and lulls Tesla drivers into a sense of complacency and inattentiveness. The advocates say that the government’s vehicle safety regulations do not adequately protect motorcycle riders and that steps should be taken to better protect them, including testing driver-assist systems like Autopilot for motorcycle detection.\n“Motorcyclists have long been told by crash-causing inattentive drivers, ‘Sorry, I didn’t see you.’ Now we are hearing, ‘Sorry, my car didn’t see you.’ This is unacceptable,” Rob Dingman, President and CEO, American Motorcyclist Association said.\n“If it can’t see a motorcycle, can it see a pedestrian? Can it see a small child? Can it see an animal?” Eric Stine, treasure of the Utah chapter of ABATE, which advocates for motorcycle riders.\nNHTSA said in a statement Monday that no commercially available vehicles today can drive themselves and encouraged drivers to use assistance technologies appropriately.\n“Certain advanced driving assistance features can promote safety by helping drivers avoid crashes and mitigate the severity of crashes that occur, but as with all technologies and equipment on motor vehicles, drivers must use them correctly and responsibly,” NHTSA said.\nThis summer Tesla fans rushed to defend the automaker after a prominent critic released a video showing one of its cars with driver-assist technology plowing into child-size mannequins.\nTesla did not respond to a request for comment for this story.\nIt is not alone in apparent challenges identifying things at night.\nThe Insurance Institute for Highway Safety found that 19 of 23 vehicles tested for pedestrian detection earned a “superior” or “advanced” rating during the daytime, but only four received a “superior” rating at night. More than half earned a basic score or no credit.\nVisibility is a challenge for humans and machines at night as there’s less light reflecting off things on the road. Tesla cautions in its vehicle owners’ manuals that many factors can impact Autopilot’s performance, including poor visibility.\n“Never depend on these [Autopilot] components to keep you safe,” Tesla says. “It is the driver’s responsibility to stay alert, drive safely, and be in control of the vehicle at all times.”\nTesla has said it relies exclusively on cameras to detect objects on the road and to determine if a driver has their eyes on the road. Tesla competitors General Motors and Ford use infrared sensors in their vehicles, which can see some objects better when there’s less visible light, in order to better see a driver’s face and detect distracted driving in low light conditions.\nThe American Motorcyclist Association says that driver-assist technology that reliably detects motorcycles can prevent crashes. For years it has urged the National Highway Traffic Safety Administration to test for motorcycle detection as it assesses the safety of new vehicles, including their driver-assist technologies. (NHTSA declined to comment on why it does not do so.) Europe’s vehicle safety programs test if driver-assist systems identify motorcycles.\nThe American Motorcyclist Association has cautioned for years about the risks of emerging driving technologies not adequately detecting motorcyclists.\n“If this issue is not addressed early in developing automated vehicles,” it wrote to NHTSA last year, “The consequences will prove disastrous for motorcyclists.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n\"Tesla’s Autopilot was involved in a third fatal motorcycle crash this summer\",\n\"The three fatal crashes occurred in a 51-day span this summer and follow a similar line of events: A person driving a Tesla in the early morning hours with Autopilot active strikes a motorcycle.\",\n\"The crashes renew questions about whether users of the systems are kept sufficiently engaged and prepared to fully control the vehicle when needed.\",\n\"Ingrid Eva Noon was riding her motorcycle in Palm Beach County, Florida at 2:11 a.m. on Aug. 26 when an impaired driver using Tesla’s Autopilot impacted the rear of Noon’s motorcycle, throwing her onto the Tesla’s windshield and killing her.\",\n\"Utah resident Landon Embry was killed while riding his Harley-Davidson on July 24 at approximately 1:09 am when a Tesla driver using Autopilot collided with the back of his motorcycle.\",\n\"A Utah motorcycle rider was killed in July after being struck by a Tesla driver using Autopilot.\",\n\"The recent crashes suggest the Tesla system is insufficient, according to motorcycle advocates.\",\n\"Motorcycle safety advocates say they’re concerned that the software fails to see motorcycles and lulls Tesla drivers into a sense of complacency and inattentiveness.\",\n\"The advocates say that the government’s vehicle safety regulations do not adequately protect motorcycle riders and that steps should be taken to better protect them, including testing driver-assist systems like Autopilot for motorcycle detection.\",\n\"NHTSA said in a statement Monday that no commercially available vehicles today can drive themselves and encouraged drivers to use assistance technologies appropriately.\",\n\"The American Motorcyclist Association says that driver-assist technology that reliably detects motorcycles can prevent crashes. For years it has urged the National Highway Traffic Safety Administration to test for motorcycle detection as it assesses the safety of new vehicles, including their driver-assist technologies.\"\n]\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2022\/10\/17\/business\/tesla-motorcycle-crashes-autopilot\/index.html","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":21,"Time":"Updated 3:36 PM EDT, Thu June 9, 2022","Title":"Federal investigators expand probe into Tesla Autopilot crashes","Content":"CNN\n — \nFederal investigators are expanding their probe of Teslas that have slammed into parked first responders’ vehicles.\nThe National Highway Traffic Safety Administration said Thursday its investigation now encompasses 830,000 vehicles with similar driver assist technology. Investigators have reviewed approximately 200 Tesla crashes and are focusing on 16 crashes involving emergency or road work vehicles.\nThe probe, officially known as an engineering analysis, could lead to a safety recall.\nNHTSA’s work is specifically reviewing whether Tesla’s automated technologies make driving less safe, leaving drivers unprepared to take control and prevent a collision.\nUS government investigates Tesla cars after reports of unexpected braking\nTesla says its automated driver assist technology, called Autopilot, is “designed to assist you with the most burdensome parts of driving,” combining elements of automated cruise control and lane keeping assist to keep the vehicle in position on the highway. It still requires an actively engaged driver.\nThe system is separate from Tesla’s Full Self Driving Beta, a test program rolled out to approximately 100,000 Tesla drivers that aims to one day remove any need for a driver at all. Tesla CEO Elon Musk has repeatedly pushed back estimates of when that system will function as promised. In today’s disclosure, NHTSA says it has asked Tesla for information on FSD crashes and non-disclosure agreements, although it is unclear if any FSD crashes are part of the active investigation.\nNHTSA said in many of the 16 crashes, the drivers’ hands were on the steering wheel, but the drivers did not take “evasive action between 2-5 seconds prior to impact.”\n“NHTSA reminds the public that no commercially available motor vehicles today are capable of driving themselves,” the agency said in a statement.\nTesla did not respond to CNN’s request for comment.\nFederal documents say investigators began their work in 2021 after observing “an accumulation of crashes in which Tesla vehicles, operating with Autopilot engaged, struck stationary in-road or roadside first responder vehicles tending to pre-existing collision scenes.” The 16 crashes include on fatality and 15 injuries.\nBy upgrading its investigation NHTSA says it will be able to “extend its existing crash analysis, evaluate additional data sets, and perform vehicle evaluations.”\nA NHTSA document shared with CNN in May showed the agency had more than 30 open investigations involving Tesla vehicles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n\"The news is relevant to Tesla Autopilot accidents as it discusses the expansion of a federal investigation into Tesla Autopilot-related crashes.\",\n\"The National Highway Traffic Safety Administration (NHTSA) is expanding its probe to encompass 830,000 vehicles with similar driver assist technology.\",\n\"Investigators are specifically focusing on 16 crashes involving emergency or road work vehicles.\",\n\"The investigation is officially known as an engineering analysis, which could lead to a safety recall.\",\n\"NHTSA is reviewing whether Tesla's automated technologies, including Autopilot, make driving less safe and leave drivers unprepared to take control and prevent collisions.\",\n\"In many of the 16 crashes under investigation, the drivers had their hands on the steering wheel but did not take evasive action seconds prior to impact.\",\n\"The investigation started in 2021 after an accumulation of crashes where Tesla vehicles operating with Autopilot engaged struck stationary first responder vehicles.\",\n\"The investigation includes one fatality and 15 injuries.\",\n\"The news highlights concerns about the safety of Tesla's Autopilot technology and its potential impact on driving safety.\"\n]\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2022\/06\/09\/cars\/tesla-autopilot-federal-probe-expands\/index.html","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":16,"Time":"9:02 PM GMT+2•September 11, 2016","Title":"Tesla Autopilot 8.0 uses radar to prevent accidents like the fatal Model S crash","Content":"cameraImage Credits: Tesla\nToday, Tesla revealed Version 8 of its Autopilot software, going live in one to two weeks. Version 8.0 includes updates to the signal processing tech used to interpret images received from the onboard radar. The update focuses primarily on the radar component of the Autopilot sensor system, turning it from a supplementary part of the overall tech, designed to complement the cameras, into a primary control sensor that, according to Elon Musk himself, should prevent accidents like the one that resulted in Josh Brown’s death.\n“We’re making much more effective use of radar,” Musk explained on a press call regarding the updates. “We weren’t confident that we could resolve false positives where the radar would think that it should brake, but it shouldn’t.”\nBut after getting upgraded components from Tesla’s suppliers, they found they could combine that with their real-time fleet learning system and almost entirely eliminate false positives. That means radar alone can initiate a braking event, rather than requiring the camera component to also provide a positive image recognition result of a potential object.\nMusk said that the new system should detect “any thing that’s metallic, anything that’s large and dense,” and then brake the vehicle, “as long as it’s not large and fluffy.” If Autopilot is active, the system will be better able to predict the car’s path and then produce a gradual brake that Musk said should be “much more comfortable” and able to “brake entirely” in most cases, but even if Autosteer is not engaged, it should also result in a emergency brake that will “mitigate” rather than completely avoid collisions in most cases.\nThe system is able to brake even when it doesn’t recognize what the object is, and that’s the key to why he suggests it would’ve avoided the accident that resulted in Brown’s death. It could recognize anything from unusual vehicles, to “an alien spaceship,” “to a pile of junk metal” or even a multi-car pileup. It can also detect it through obfuscation like rain, fog, or haze.\nTesla’s new Autopilot can also bounce its radar signal off of the roadway or its surroundings and receive an echo signal back to provide a view around any cars you’re driving behind, which means it should have “superhuman” object detection abilities, according to Musk. It’ll also register false positives using Tesla’s network of fleet vehicles, geocoding stuff that’s confusing the system like billboards to prevent them causing unnecessary braking events.\nThe entire Autosteer upgrade will be done via over-the-air software, free to existing  Tesla owners, and will work with Tesla vehicles from 2014 and on, since those all include the necessary radar hardware. Fitting this software and the new processing power onto the existing computing hardware in the vehicle will be a special “challenge,”","Relevant":true,"Overview":"Here's the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"News_topic\": \"Tesla Autopilot 8.0 update\",\n        \"Explanation\": \"The news is relevant to Tesla Autopilot because it discusses the release of Version 8.0 of Tesla's Autopilot software, which includes updates to the radar component. Elon Musk specifically mentions that the radar is now a primary control sensor designed to prevent accidents. The article discusses how the updated radar system can detect various objects, even in challenging conditions like rain or fog, and initiate braking events, which is directly related to improving safety and preventing accidents. Additionally, the entire Autosteer upgrade is mentioned as being available through over-the-air software updates to existing Tesla owners, further highlighting the relevance of the news to Tesla's Autopilot system.\"\n    }\n}\nThe provided news is relevant to Tesla Autopilot as it discusses significant updates to the Autopilot software, specifically focusing on radar technology and its role in preventing accidents, which aligns with Tesla's efforts to enhance safety through its Autopilot system.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2016\/09\/11\/tesla-autopilot-8-0-uses-radar-to-prevent-accidents-like-the-fatal-model-s-crash\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":18,"Time":"June 18, 2021","Title":"U.S. safety agency probes 10 Tesla crash deaths since 2016","Content":"Companies\nTesla Inc\nFollow\nCarparts.Com Inc\nFollow\nUber Technologies Inc\nFollow\nWASHINGTON, June 17 (Reuters) - U.S. auto safety regulators said on Thursday they have opened 30 investigations into Tesla (TSLA.O) crashes involving 10 deaths since 2016 where advanced driver assistance systems were suspected of use.\nThe National Highway Traffic Safety Administration (NHTSA) released a list offering details about crashes under review by its Special Crash Investigations programs.\nThe agency had already confirmed some specific Tesla crash investigations but had not previously released to Reuters a full accounting of all Tesla crashes investigated where Tesla's Autopilot system was suspected of being involved.\nAdvertisement · Scroll to continue\nOf the 30 Tesla crashes, NHTSA has ruled out Tesla's Autopilot in three and published reports on two of the crashes.\nTesla did not immediately respond to a request for comment.\nReuters first sought a full list from NHTSA more than a year ago under a public records request. The list only includes the state and month the crashes occurred.\nPreviously, NHTSA said it had opened 28 special crash investigations into Tesla crashes, with 24 pending. The spreadsheet shows a February 2019 crash where Autopilot use was undetermined.\nAdvertisement · Scroll to continue\nAutopilot, which handles some driving tasks, was operating in at least three Tesla vehicles involved in fatal U.S. crashes since 2016, the National Transportation Safety Board (NTSB) has said. The NTSB has criticized Tesla's lack of system safeguards for Autopilot, which allows drivers to keep their hands off the wheel for extended periods.\nThe spreadsheet shows NHTSA has opened eight investigations into Tesla crashes since March.\nAdvertisement · Scroll to continue\nThe issue has gotten new attention after a fiery April 17 Texas crash that killed two men in Texas in which police had said they believed no one was behind the wheel.\n[1\/2]A Tesla logo is seen in Los Angeles, California U.S. January 12, 2018. REUTERS\/Lucy Nicholson Acquire Licensing Rights\nThe National Transportation Safety Board said in May that testing suggested the vehicle's automated steering system was \"not available\" on the road where the accident occurred.\nOn Wednesday, Senate Commerce Committee chair Maria Cantwell cited Tesla crashes as the panel voted against moving ahead with regulations to speed the adoption of self-driving cars.\nAdvertisement · Scroll to continue\n\"It seems like every other week we're hearing about a new vehicle that crashed when it was on Autopilot\" Cantwell said.\nSeparately, NHTSA has conducted no new tests after it withdrew its designation last month that some newer Tesla Model 3 and Model Y vehicles as having four advanced safety features after the automaker said it was removing radar sensors to transition to a camera-based Autopilot system.\nThe agency said Thursday that after discussions with Tesla it restored the lane departure warning designation after Tesla confirmed the technology was unaffected.\nNHTSA said in a statement it has \"not yet finalized the list of Model Year 2022 vehicles\" for testing.\nThe spreadsheet also notes NHTSA has opened six other investigations into six other crashes involving driver assistance systems, including two involving Cadillac vehicles in which there were no reported injuries.\nThe other four include two involved a 2012 Lexus RX450H and 2017 Navya Arma in which there were no reported injuries.\nThe remaining two involved 2017 Volvo XC90 vehicles, including an Uber Technologies self-driving test vehicle that struck and killed a woman in Arizona in 2018.\nUber made a series of development decisions that contributed to the crash's cause and deactivated the automatic emergency braking systems in the Volvo XC90 vehicle, safety investigators found.\nReporting by David Shepardson Editing by Chris Reese\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Based on the provided news article, I will analyze whether it is about Tesla Autopilot accidents and provide the relevant JSON-format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is about the U.S. safety agency investigating 30 Tesla crashes involving 10 deaths since 2016, where advanced driver assistance systems, including Tesla's Autopilot, were suspected of use. The National Highway Traffic Safety Administration (NHTSA) released a list detailing these crashes, and it confirms that Autopilot was operating in at least three Tesla vehicles involved in fatal U.S. crashes since 2016. The article also mentions the National Transportation Safety Board's criticism of Tesla's Autopilot system and recent attention due to a fatal Texas crash where police believed no one was behind the wheel. Additionally, it discusses the Senate Commerce Committee's consideration of regulations related to self-driving cars and NHTSA's testing status for newer Tesla Model 3 and Model Y vehicles.\"\n}\nIn summary, the news is relevant to Tesla Autopilot accidents because it discusses multiple Tesla crashes involving Autopilot and the investigations surrounding them, as well as the broader context of autonomous driving systems' safety.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/us-safety-agency-says-it-has-opened-probes-into-10-tesla-crash-deaths-since-2016-2021-06-17\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":14,"Time":"Mon 16 Aug 2021 16.56 CEST","Title":"Tesla’s Autopilot faces US investigation after crashes with emergency vehicles","Content":"The US government has opened a formal investigation into Tesla’s driver-assistance system known as Autopilot after a series of collisions with parked emergency vehicles.\nThe investigation covers 765,000 vehicles, almost everything that Tesla has sold in the US since the start of the 2014 model year. Of the crashes identified by the National Highway Traffic Safety Administration (NHTSA) as part of the investigation, 17 people were injured and one was killed.\nThirty Tesla crashes linked to assisted driving system under investigation in US\nRead more\nNHTSA says it has identified 11 crashes since 2018 in which Teslas on Autopilot or Traffic Aware Cruise Control have hit vehicles at scenes where first responders used flashing lights, flares, an illuminated arrow board or cones warning of hazards. The agency announced the action on Monday in a posting on its website.\n“Most incidents took place after dark and the crash scenes encountered included scene control measures such as first responder vehicle lights, flares, an illuminated arrow board, and road cones,” the agency said.\nThe investigation covers Tesla’s entire current model lineup, the Models Y, X, S and 3 from the 2014 through 2021 model years.\nThe National Transportation Safety Board (NTSB), which also has investigated some of the Tesla crashes, has recommended that NHTSA and Tesla limit Autopilot’s use to areas where it can safely operate.\nThe NTSB also recommended that NHTSA require Tesla to have a better system to make sure drivers are paying attention. NHTSA has not taken action on any of the recommendations. The NTSB has no enforcement powers and can only make recommendations to other federal agencies such as NHTSA.\nAutopilot has frequently been misused by Tesla drivers, who have been caught driving drunk or even riding in the back seat while a car rolled down a California highway.\nThe investigation is the latest in a series launched by NHTSA which has opened at least 30 crash investigations involving Tesla cars that it suspected were linked to Autopilot. One investigation into a 2016 crash cleared Tesla’s Autopilot of any blame.\nTesla and other manufacturers warn that drivers using the systems must be ready to intervene at all times. Teslas using the system have crashed into semis crossing in front of them, stopped emergency vehicles and a roadway barrier.\nA message was left early on Monday seeking comment from Tesla, which has disbanded its media relations office. Earlier this month Tesla tweeted that “a Tesla with Autopilot engaged experienced 0.2 accidents per million miles driven, while the US average was 9x higher”.\nThe crashes into emergency vehicles cited by NHTSA began on 22 January 2018 in Culver City near Los Angeles when a Tesla using Autopilot struck a firetruck that was parked partially in the travel lanes with its lights flashing. Crews were handling another crash at the time.\nSince then, the agency said there were crashes in Laguna Beach, California; Norwalk, Connecticut; Cloverdale, Indiana; West Bridgewater, Massachusetts; Cochise county, Arizona; Charlotte, North Carolina, Montgomery county, Texas; Lansing, Michigan; and Miami, Florida.\n“The investigation will assess the technologies and methods used to monitor, assist and enforce the driver’s engagement with the dynamic driving task during Autopilot operation,” NHTSA said in investigation documents.\nIn addition, the investigation will cover object and event detection by the system, as well as where it is allowed to operate. NHTSA says it will examine “contributing circumstances” to the crashes, as well as similar crashes.\nAn investigation could lead to a recall or other enforcement action by NHTSA.\n“NHTSA reminds the public that no commercially available motor vehicles today are capable of driving themselves,” the agency said in a statement. “Every available vehicle requires a human driver to be in control at all times, and all state laws hold human drivers responsible for operation of their vehicles.”\nThe agency said it has “robust enforcement tools” to protect the public and investigate potential safety issues, and it will act when it finds evidence “of noncompliance or an unreasonable risk to safety”.\nIn June NHTSA ordered all automakers to report any crashes involving fully autonomous vehicles or partially automated driver assist systems.\nThe measures show the agency has started to take a tougher stance on automated vehicle safety than in the past. It has been reluctant to issue any regulations of the new technology for fear of hampering adoption of the potentially life-saving systems.\nShares of Tesla, based in Palo Alto, California, fell nearly 2% before the opening bell.\nAgencies contributed to this story","Relevant":true,"Overview":"Here's the JSON-format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": [\n    \"The news is about Tesla's Autopilot system facing a US government investigation.\",\n    \"The investigation is prompted by a series of collisions involving Tesla vehicles with parked emergency vehicles.\",\n    \"The investigation covers 765,000 Tesla vehicles sold in the US since the start of the 2014 model year.\",\n    \"The collisions occurred at scenes where first responders used flashing lights, flares, an illuminated arrow board, or road cones warning of hazards.\",\n    \"A total of 17 people were injured, and one person was killed in these incidents.\",\n    \"The National Transportation Safety Board (NTSB) has also recommended limitations on Autopilot's use and better driver attention monitoring.\",\n    \"The investigation is part of a series launched by NHTSA involving Tesla cars suspected of being linked to Autopilot in at least 30 crash investigations.\",\n    \"The investigation will assess technologies used to monitor driver engagement with Autopilot, object and event detection by the system, and where it is allowed to operate.\",\n    \"The investigation could lead to a recall or other enforcement action by NHTSA.\"\n  ]\n}\nThe news is relevant to Tesla Autopilot accidents because it discusses a formal investigation into Tesla's driver-assistance system, Autopilot, due to a series of collisions with parked emergency vehicles. The investigation covers a significant number of Tesla vehicles sold in the US, and it provides specific details about the incidents, injuries, and the actions taken by regulatory agencies.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2021\/aug\/16\/teslas-autopilot-us-investigation-crashes-emergency-vehicles","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":7,"Time":"Published 9:05 AM GMT+2, January 3, 2020","Title":"3 crashes, 3 deaths raise questions about Tesla’s Autopilot","Content":"DETROIT (AP) — Three crashes involving Teslas that killed three people have increased scrutiny of the company’s Autopilot driving system just months before CEO Elon Musk has planned to put fully self-driving cars on the streets.\nOn Sunday, a Tesla Model S sedan left a freeway in Gardena, California, at a high speed, ran a red light and struck a Honda Civic, killing two people inside, police said.\nOn the same day, a Tesla Model 3 hit a parked firetruck on an Indiana freeway, killing a passenger in the Tesla.\nAnd on Dec. 7, yet another Model 3 struck a police cruiser on a Connecticut highway, though no one was hurt.\nThe special crash investigation unit of the National Highway Traffic Safety Administration is looking into the California crash. The agency hasn’t decided whether its special-crash unit will review the crash that occurred Sunday near Terre Haute, Indiana. In both cases, authorities have yet to determine whether Tesla’s Autopilot system was being used.\nNHTSA also is investigating the Connecticut crash, in which the driver told police that the car was operating on Autopilot, a Tesla system designed to keep a car in its lane and a safe distance from other vehicles. Autopilot also can change lanes on its own.\nTesla has said repeatedly that its Autopilot system is designed only to assist drivers, who must still pay attention and be ready to intervene at all times. The company contends that Teslas with Autopilot are safer than vehicles without it, but cautions that the system does not prevent all crashes.\nEven so, experts and safety advocates say a string of Tesla crashes raises serious questions about whether drivers have become too reliant on Tesla’s technology and whether the company does enough to ensure that drivers keep paying attention. Some critics have said it’s past time for NHTSA to stop investigating and to take action, such as forcing Tesla to make sure drivers pay attention when the system is being used.\nNHTSA has started investigations into 13 Tesla crashes dating to at least 2016 in which the agency believes Autopilot was operating. The agency has yet to issue any regulations, though it is studying how it should evaluate similar “advanced driver assist” systems.\n“At some point, the question becomes: How much evidence is needed to determine that the way this technology is being used is unsafe?” said Jason Levine, executive director of the nonprofit Center for Auto Safety in Washington. “In this instance, hopefully these tragedies will not be in vain and will lead to something more than an investigation by NHTSA.”\nLevine and others have called on the agency to require Tesla to limit the use of Autopilot to mainly four-lane divided highways without cross traffic. They also want Tesla to install a better system to monitor drivers to make sure they’re paying attention all the time. Tesla’s system requires drivers to place their hands on the steering wheel. But federal investigators have found that this system lets drivers zone out for too long.\nTesla plans to use the same cameras and radar sensors, though with a more powerful computer, in its fully self-driving vehicles. Critics question whether those cars will be able to drive themselves safely without putting other motorists in danger.\nDoubts about Tesla’s Autopilot system have long persisted. In September, the National Transportation Safety Board, which investigates transportation accidents, issued a report saying that a design flaw in Autopilot and driver inattention combined to cause a Tesla Model S to slam into a firetruck parked along a Los Angeles-area freeway in January 2018. The board determined that the driver was overly reliant on the system and that Autopilot’s design let him disengage from driving for too long.\nIn addition to the deaths on Sunday night, three U.S. fatal crashes since 2016 — two in Florida and one in Silicon Valley — involved vehicles using Autopilot.\nDavid Friedman, vice president of advocacy for Consumer Reports and a former acting NHTSA administrator, said the agency should have declared Autopilot defective and sought a recall after a 2016 crash in Florida that killed a driver. Neither Tesla’s system nor the driver had braked before the car went underneath a semi-trailer that had turned in front of the car.\n“We don’t need any more people getting hurt for us to know that there is a problem and that Tesla and NHTSA have failed to address it,” Friedman said.\nIn addition to NHTSA, states can regulate autonomous vehicles, though many have decided they want to encourage testing.\nIn the 2016 crash, NHTSA closed its investigation without seeking a recall. Friedman, who was not at NHTSA at the time, said the agency determined that the problem didn’t happen frequently. But he said that argument has since been debunked.\nFriedman said it’s foreseeable some drivers will not pay attention to the road while using Autopilot, so the system is defective.\n“The public is owed some explanation for the lack of action,” he said. “Simply saying they’re continuing to investigate — that line has worn out its usefulness and its credibility.”\nIn a statement, NHTSA said it relies on data to make decisions, and if it finds any vehicle poses an unreasonable safety risk, “the agency will not hesitate to take action.” NHTSA also has said it doesn’t want to stand in the way of technology given its life-saving potential.\nMessages were left Thursday seeking comment from Tesla.\nRaj Rajkumar, an electrical and computer engineering professor at Carnegie Mellon University, said it’s likely that the Tesla in Sunday’s California crash was operating on Autopilot, which has become confused in the past by lane lines. He speculated that the lane line was more visible for the exit ramp, so the car took the ramp because it looked like a freeway lane. He also suggested that the driver might not have been paying close attention.\n“No normal human being would not slow down in an exit lane,” he said.\nIn April, Musk said he expected to start converting the company’s electric cars to fully self-driving vehicles in 2020 to create a network of robotic taxis to compete against Uber and other ride-hailing services.\nAt the time, experts said the technology isn’t ready and that Tesla’s camera and radar sensors weren’t good enough for a self-driving system. Rajkumar and others say additional crashes have proved that to be true.\nMany experts say they’re not aware of fatal crashes involving similar driver-assist systems from General Motors, Mercedes and other automakers. GM monitors drivers with cameras and will shut down the driving system if they don’t watch the road.\n“Tesla is nowhere close to that standard,” he said.\nHe predicted more deaths involving Teslas if NHTSA fails to take action.\n“This is very unfortunate,” he said. “Just tragic.”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n\"Title: 3 crashes, 3 deaths raise questions about Tesla’s Autopilot.\",\n\"Three crashes involving Teslas that killed three people have increased scrutiny of the company’s Autopilot driving system.\",\n\"On Sunday, a Tesla Model S sedan left a freeway in Gardena, California, at a high speed, ran a red light and struck a Honda Civic, killing two people inside, police said.\",\n\"On the same day, a Tesla Model 3 hit a parked firetruck on an Indiana freeway, killing a passenger in the Tesla.\",\n\"The special crash investigation unit of the National Highway Traffic Safety Administration is looking into the California crash. The agency hasn’t decided whether its special-crash unit will review the crash that occurred Sunday near Terre Haute, Indiana. In both cases, authorities have yet to determine whether Tesla’s Autopilot system was being used.\",\n\"NHTSA also is investigating the Connecticut crash, in which the driver told police that the car was operating on Autopilot, a Tesla system designed to keep a car in its lane and a safe distance from other vehicles. Autopilot also can change lanes on its own.\",\n\"Tesla has said repeatedly that its Autopilot system is designed only to assist drivers, who must still pay attention and be ready to intervene at all times. The company contends that Teslas with Autopilot are safer than vehicles without it, but cautions that the system does not prevent all crashes.\",\n\"Even so, experts and safety advocates say a string of Tesla crashes raises serious questions about whether drivers have become too reliant on Tesla’s technology and whether the company does enough to ensure that drivers keep paying attention.\"\n]\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/ca5e62255bb87bf1b151f9bf075aaadf","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":24,"Time":"Updated 6:09 AM EDT, Tue August 17, 2021","Title":"Tesla is under investigation because its cars keep hitting emergency vehicles","Content":"New York\nCNN Business\n — \nFederal safety regulators are investigating at least 11 accidents involving Tesla cars using Autopilot or other self-driving features that crashed into emergency vehicles when coming upon the scene of an earlier crash.\nThe National Highway Transportation Safety Administration said seven of these accidents resulted 17 injuries and one death.\nAll of the Teslas in question had the self-driving Autopilot feature or the traffic-aware cruise control engaged as they approached the crashes, the NHTSA said.\nTesla (TSLA) stock fell 5% in morning trading following news of the probe.\nThe accidents under investigation occurred between January 22, 2018, and July 10, 2021, across nine different states. They took place mostly at night, and the post-accident scenes all included control measures like first responder vehicle lights, flares, an illuminated arrow board and road cones.\nTesla did not immediately respond to a request for comment about the probe.\nThis Jan. 22, 2018, file photo shows a Tesla Model S electric car that has crashed into a fire engine on Interstate 405 in Culver City, Calif. This is one of 11 accidents involving Tesla vehicles using Autopilot and emergency vehicles.\nKCBS-TV\/AP\nThe safety of Tesla’s Autopilot feature has been questioned before. The National Transportation Safety Board, a separate agency that also investigates plane crashes and other fatal accidents, found Autopilot partly to blame in a 2018 fatal crash in Florida that killed a Tesla driver.\nPolice in a Houston suburb said there was no one in the driver’s seat of a Tesla that crashed and killed two people in the car earlier this year, a charge that Tesla has denied. But Lars Moravy, Tesla’s vice president of vehicle engineering, confirmed in April in comments to investors that Tesla’s adaptive cruise control was engaged and accelerated to 30 mph before that car crashed.\nTesla has been seeking to offer full self-driving technology to its drivers. But while it says that its data shows cars using Autopilot have fewer accidents per mile than cars being driven by drivers, it does warn “current Autopilot features require active driver supervision and do not make the vehicle autonomous.”\nElon Musk says Tesla is pushing ahead with 'full self-driving' a month after fatal crash\nThe safety agency said its investigation will allow it to “better understand the causes of certain Tesla crashes,” including “the technologies and methods used to monitor, assist, and enforce the driver’s engagement with driving while Autopilot is in use.” It will also look into any contributing factors in the crashes.\n“NHTSA reminds the public that no commercially available motor vehicles today are capable of driving themselves,” said the agency in a statement. “Every available vehicle requires a human driver to be in control at all times, and all state laws hold human drivers responsible for operation of their vehicles. Certain advanced driving assistance features can promote safety by helping drivers avoid crashes and mitigate the severity of crashes that occur, but as with all technologies and equipment on motor vehicles, drivers must use them correctly and responsibly.”\nThe investigation involves the Tesla Y, X, S and 3 with model years 2014 to 2021.\nGordon Johnson, an analyst and vocal critic of Tesla, wrote in a note to clients Monday that the issue isn’t just about Autopilot users — but also other non-Tesla drivers on the road who could be injured by cars using the feature.\n“NHTSA is zeroing in on a particular danger that Tesla creates for people outside the vehicle — ie, those who never agreed to be Autopilot ‘guinea pigs,’” Johnson wrote. “Thus, to simply say ‘Tesla drivers accept Autopilot’s risks,’ as has been used in the past, does not appear to be a defense here.”\nSelf-driving options such as Tesla’s Autopilot or more widely available adaptive cruise control, available on a wide range of automakers’ vehicles, do a good job of slowing a vehicle down when the car in front is slowing down, said Sam Abuelsamid, an expert in self-driving vehicles and principal analyst at Guidehouse Insights.\nBut Abuelsamid said those vehicles are designed to ignore stationary objects when traveling at more than 40 mph so they don’t slam on the brakes when approaching overpasses or other stationary objects on the side of the road, such as a car stopped on the shoulder. Fortunately most of these vehicles with some kind of automatic braking do stop for stationary objects when they’re moving more slowly, Abuelsamid said.\nThe real problem he said is that many more Tesla owners assume their cars can, in fact, drive themselves than do drivers of other cars with automatic braking and other safety features. And the cues that a driver would see when approaching an accident site, such as road flares or flashing lights, make more sense to a human than they might to an auto drive system.\n“When it works, which can be most of the time, it can be very good,” said Abuelsamid, about Tesla’s Autopilot feature. “But it can easily be confused by things that humans would have no problem with. Machine visions are not as adaptive as humans. And the problem is that all machine systems sometimes make silly errors.”","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to Tesla Autopilot accidents. Federal safety regulators are investigating at least 11 accidents involving Tesla cars using Autopilot or other self-driving features that crashed into emergency vehicles when coming upon the scene of an earlier crash. The National Highway Transportation Safety Administration reported seven of these accidents resulted in 17 injuries and one death. All of the Teslas in question had the self-driving Autopilot feature or the traffic-aware cruise control engaged as they approached the crashes.\"\n}\nThe news is relevant to Tesla Autopilot accidents because it discusses multiple accidents involving Tesla vehicles using Autopilot or self-driving features, resulting in collisions with emergency vehicles at the scene of earlier crashes. The specific information provided in the response highlights the number of accidents, injuries, and deaths involved, as well as the engagement of Autopilot or traffic-aware cruise control in the affected Tesla vehicles.","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2021\/08\/16\/business\/tesla-autopilot-federal-safety-probe\/index.html","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":22,"Time":"3:31 PM GMT+2•August 16, 2021","Title":"US safety regulator opens investigation into Tesla Autopilot following crashes with parked emergency vehicles","Content":"cameraImage Credits: Spencer Platt \/ Getty Images\nU.S. auto regulators have opened a preliminary investigation into Tesla’s Autopilot advanced driver assistance system, citing 11 incidents in which vehicles crashed into parked first responder vehicles while the system was engaged.\nThe Tesla vehicles involved in the collisions were confirmed to have either have had engaged Autopilot or a feature called Traffic Aware Cruise Control, according to investigation documents posted on the National Highway Traffic and Safety Administration’s website. Most of the incidents took place after dark and occurred despite “scene control measures,” such as emergency vehicle lights, road cones and an illuminated arrow board signaling drivers to change lanes.\n“The investigation will assess the technologies and methods used to monitor, assist, and enforce the driver’s engagement with the dynamic driving task during Autopilot operation,” the document says.\nThe investigation covers around 765,000 Tesla vehicles that span all currently available models: Tesla Model Y, Model X, Model S and Model 3. The 11 incidents or fires resulted in 17 injuries and one fatality. They occurred between January 2018 and July 2021.\nThis is not the first time Tesla’s Autopilot has fallen under the scrutiny of NHTSA, the country’s top vehicle safety regulator. In 2017, the agency investigated an incident that resulted in a fatal crash in 2016, though the EV maker was found to be at no-fault in the accident. NHSTA has investigated a further 25 crashes involving Tesla’s ADAS since, the Associated Press reported when it broke the story Monday.\nIn June, NHTSA issued an order requiring automakers to report crashes involving vehicles equipped with ADAS or Levels 3-5 of automated driving systems.\n“NHTSA reminds the public that no commercially available motor vehicles today are capable of driving themselves,” an agency spokesperson told TechCrunch on Monday.\nTechCrunch has reached out to Tesla, which has dissolved its media relations division, for comment, and will update the story if the company responds.\nUpdate: In the wake of the announcement of a formal investigation into Tesla Autopilot, special interest group Advocates for Highway and Auto Safety released a statement calling for “minimum performance standards”","Relevant":true,"Overview":"Based on the provided news article, here's the JSON-format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is about a US safety regulator opening a preliminary investigation into Tesla's Autopilot advanced driver assistance system. The investigation was prompted by 11 incidents in which Tesla vehicles crashed into parked first responder vehicles while the Autopilot system was engaged. These incidents resulted in 17 injuries and one fatality. The investigation will assess the technologies and methods used to monitor, assist, and enforce the driver's engagement with the dynamic driving task during Autopilot operation. The investigation covers around 765,000 Tesla vehicles of various models, and it spans incidents that occurred between January 2018 and July 2021. This news article provides specific information related to Tesla Autopilot accidents, including the incidents, the investigation, and the potential consequences for Tesla.\"\n}\nThe news is indeed about Tesla Autopilot accidents, as it discusses a preliminary investigation into these accidents and provides specific details related to the incidents and the investigation.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2021\/08\/16\/u-s-safety-regulator-opens-investigation-into-tesla-autopilot-following-crashes-with-parked-emergency-vehicles\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":28,"Time":"9 December 2019","Title":"Tesla Model 3 on Autopilot crashes into police car","Content":"Tesla Model 3 on Autopilot crashes into police car\nPublished\n9 December 2019\nShare\nIMAGE SOURCE,\nCONNECTICUT STATE POLICE\nImage caption,\nThe Tesla Model 3 failed to avoid a traffic incident on a highway in Connecticut.\nA Tesla has crashed into a police car and another vehicle while on Autopilot in the US.\nBoth the police car and the other vehicle, which had broken down, were stationary at the time of the incident.\nThe driver told police that he put his Tesla Model 3 on Autopilot so that he could check on his dog in the back.\nThe crash occurred on Interstate 95 in Norwalk, Connecticut, in the early hours of Saturday morning local time.\nThe police car was parked behind the broken down vehicle with its blue lights flashing, as it waited for a tow truck.\nThe Tesla, which starts from £42,000 in the UK and $36,000 in the US, struck the rear of the police car and continued travelling forward, leading it to hit the car in front.\nPolice said no one was injured.\nIMAGE SOURCE,\nCONNECTICUT STATE POLICE\nImage caption,\nThe front of the Tesla Model 3 following the incident.\n\"The operator of the Tesla stated that he had his vehicle on 'auto-pilot' and explained that he was checking on his dog which was in the back seat,\" wrote Connecticut State Police on Facebook.\nThe police force added: \"Regardless of your vehicle's capabilities, when operating a vehicle your full attention is required at all times to ensure safe driving.\"\nThe driver was charged with reckless driving and reckless endangerment.\nThis is not the first time a Tesla has crashed while on Autopilot. In total, there have been at least five fatalities worldwide involving Tesla vehicles on Autopilot.\nThe National Transportation Safety Board (NTSB) in the US is currently investigating accidents caused by Autopilot.\nTesla did not immediately respond to a request for comment. However, the firm does not recommend that drivers remove their hands from the wheel while using the Autopilot feature.\nMore on this story\nTesla Autopilot engaged during fatal crash\nPublished\n17 May 2019\nTesla Autopilot design 'led to' crash\nPublished\n5 September 2019","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to Tesla Autopilot accidents because it reports an incident where a Tesla Model 3 on Autopilot crashed into a police car and another stationary vehicle on Interstate 95 in Norwalk, Connecticut. The driver mentioned that he put his Tesla Model 3 on Autopilot to check on his dog in the back seat. The news also mentions that this is not the first time a Tesla has crashed while on Autopilot, with at least five fatalities worldwide involving Tesla vehicles on Autopilot. The driver in this incident was charged with reckless driving and reckless endangerment. The National Transportation Safety Board (NTSB) in the US is currently investigating accidents caused by Autopilot. Tesla's recommendation not to remove hands from the wheel while using Autopilot is also highlighted.\"\n}","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-50713716","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":29,"Time":"Updated 3:49 PM EDT, Fri May 17, 2019","Title":"Tesla on Autopilot crashed when the driver’s hands were not detected on the wheel","Content":"New York\nCNN Business\n — \nIt’s happened again: A Tesla with its Autopilot feature engaged was involved in a fatal crash.\nThe crash, which happened March 1 in Florida, left the 50-year-old Tesla driver dead. Authorities revealed new details about the incident on Thursday. The driver turned on Autopilot about 10 seconds before the Model 3 sedan collided with a semi truck, according to the National Transportation Safety Board. The Tesla driver’s hands were not detected on the steering wheel for less than eight seconds prior. The truck driver was uninjured.\nThe NTSB’s report did not indicate the Tesla driver was at fault and said the investigation is ongoing. But the news raises more questions about Tesla’s marketing of Autopilot, the company’s semi-autonomous driving software.\nTesla CEO Elon Musk has long promised that full self-driving capability is on the way — but it’s not here yet. And critics argue that slapping the “Autopilot” name onto a driver-assistance feature can lull people into a false sense of security, making them less likely to stay fully alert and therefore be more vulnerable to a crash.\nTesla's new Autopilot is amazing. But please keep your eyes on the road\nTesla has consistently defended its Autopilot feature.\n“Tesla drivers have logged more than one billion miles with Autopilot engaged, and our data shows that, when used properly by an attentive driver who is prepared to take control at all times, drivers supported by Autopilot are safer than those operating without assistance,” the company said in a Thursday statement.\nA separate vehicle safety report from Tesla shows that in the first quarter of this year there was one accident for every 2.87 million miles driven in which drivers had Autopilot engaged. For those driving a Tesla without Autopilot, there was one accident for every 1.76 million miles driven.\nTesla hikes prices as it makes autopilot a standard feature on most cars\nSeveral crashes have occurred in the past while Tesla’s Autopilot function was engaged, including a few fatal incidents.\nFor example, one crash — also in Florida, and which involved a Tesla and a tractor-trailer — killed a Tesla driver in 2016. His Model S crashed while Autopilot was active, and authorities said the driver had not tried to control the car in two minutes.\nEarlier this month, the family of an Apple engineer said it is suing the automaker. They blamed the Autopilot feature on his Tesla for his death. Tesla had said it determined the driver’s hands were not detected on the wheel for six seconds prior to the collision. The company declined to comment on the lawsuit.\nTesla’s Autopilot function has evolved in recent months and years, giving cars more advanced self-driving capabilities — but it’s still intended for use only with a fully attentive driver. It’s also not designed to work on certain roads, and drivers are expected to keep their hands on the wheel at all times.\nBut it is possible for a driver to touch the steering wheel without the car detecting it.\nThe Model 3 owner’s manual tells drivers that the car “detects your hands by recognizing light resistance as the steering wheel turns, or from you manually turning the steering wheel very lightly (without enough force to retake control). Engaging a turn signal or using any steering wheel button or scroll wheel also qualifies for your hands being detected.”\nCorrection: A previous version of this story said the driver's hands were not on the wheel. In fact, it's possible that the driver's hands were on the wheel but that the system did not detect them. Additionally, a previous version of this story incorrectly identified the model of Tesla involved in the 2016 crash.\nCNN Business’ Peter Valdes-Dapena contributed to this report.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is relevant to Tesla Autopilot accidents. The news reports a fatal crash involving a Tesla with its Autopilot feature engaged in Florida. The driver turned on Autopilot about 10 seconds before the Model 3 sedan collided with a semi-truck, and the Tesla driver's hands were not detected on the steering wheel for less than eight seconds prior to the crash. The National Transportation Safety Board (NTSB) report did not indicate the Tesla driver was at fault, and the investigation is ongoing. The article also discusses the concerns and criticisms surrounding Tesla's marketing of Autopilot as a semi-autonomous driving software, with critics arguing that the use of the term 'Autopilot' can lead to a false sense of security among drivers. It mentions previous incidents where crashes occurred while Tesla's Autopilot function was engaged, including fatal accidents. The article also includes statements from Tesla defending its Autopilot feature's safety record and usage guidelines. Additionally, it clarifies that the system may not always detect the driver's hands on the steering wheel, as indicated by the Model 3 owner's manual.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2019\/05\/16\/cars\/tesla-autopilot-crash\/index.html","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":30,"Time":"31 March 2018","Title":"Tesla in fatal California crash was on Autopilot","Content":"Tesla in fatal California crash was on Autopilot\nPublished\n31 March 2018\nShare\nIMAGE SOURCE,\nREUTERS\nImage caption,\nThe driver of the Tesla Model X died shortly after the crash\nElectric carmaker Tesla says a vehicle involved in a fatal crash in California was in Autopilot mode, raising further questions about the safety of self-driving technology.\nOne of the company's Model X cars crashed into a roadside barrier and caught fire on 23 March.\nTesla says Autopilot was engaged at the time of the accident involving the driver, 38, who died soon afterwards.\nBut they did not say whether the system had detected the concrete barrier.\nWill we be able to trust self-driving cars?\nTesla Autopilot update seeks better safety\nUber barred from self-drive trial\n\"The driver had received several visual and one audible hands-on warning earlier in the drive,\" a statement on the company's website said.\n\"The driver's hands were not detected on the wheel for six seconds prior to the collision.\"\n\"The driver had about five seconds and 150m (490ft) of unobstructed view of the concrete divider... but the vehicle logs show that no action was taken,\" the statement added.\nTesla's Autopilot system does some of the things a fully autonomous machine can do. It can brake, accelerate and steer by itself under certain conditions, but it is classified as a driver assistance system, is not intended to operate independently and as such the driver is meant to have their hands on the wheel at all times.\nIn 2016, a Tesla driver was killed in Florida when his car failed to spot a lorry crossing its path.\nIt led the company to introduce new safety measures, including turning off Autopilot and bringing the car to a halt if the driver lets go of the wheel for too long.\nFederal investigators said last year that Tesla \"lacked understanding\" of the semi-autonomous Autopilot's limitations.\nMedia caption,\nUber dashcam footage shows moment before fatal impact\nThe accident in California comes at a difficult time for self-driving technology.\nEarlier this month, Uber was forbidden from resuming self-driving tests in the US state of Arizona.\nIt followed a fatal crash in the state in which an autonomous vehicle hit a woman who was walking her bike across the road.\nIt was thought to be the first time an autonomous car had been involved in a fatal collision with a pedestrian.\nThe company suspended all self-driving tests in North America after the accident.\nMore on this story\nUber barred from resuming self-drive trial\nPublished\n27 March 2018\nUber halts self-driving tests after death\nPublished\n20 March 2018\nToyota suspends US driverless car tests\nPublished\n20 March 2018\nVideo shows fatal Uber crash. Video, 00:00:29\nVideo shows fatal Uber crash\nPublished\n22 March 2018\n0:29\nTesla Autopilot 'partly to blame' for crash\nPublished\n12 September 2017\nWill we ever be able to trust self-driving cars?\nPublished\n19 January 2018\nTesla tries to make Autopilot safer\nPublished\n12 September 2016","Relevant":true,"Overview":"Here's the JSON-format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"The news is about a fatal crash in California involving a Tesla vehicle that was on Autopilot mode. The crash occurred on 23 March 2018 when a Tesla Model X car crashed into a roadside barrier and caught fire. Tesla confirmed that Autopilot was engaged at the time of the accident. The driver, aged 38, died shortly after the crash. The news mentions that the driver received several warnings to have hands on the wheel before the collision, but the driver's hands were not detected on the wheel for six seconds prior to the collision. It also highlights that the vehicle logs showed no action was taken by the driver despite having an unobstructed view of the concrete divider for about five seconds. This incident raises questions about the safety of self-driving technology and the use of Tesla's Autopilot system.\"\n}\nThe news is indeed about a Tesla Autopilot accident, and the specific information provided above explains why it is relevant to Tesla Autopilot accidents.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/world-us-canada-43604440","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":32,"Time":"Published 6:16 PM GMT+2, February 11, 2020","Title":"NTSB releases details in 2 crashes involving Tesla Autopilot","Content":"SAN FRANCISCO (AP) — An Apple engineer who died when his Tesla Model X slammed into a concrete barrier had previously complained about the SUV malfunctioning on that same stretch of Silicon Valley freeway.\nHis complaints were detailed in a trove of documents released Tuesday by federal investigators in two Tesla crashes involving Autopilot, one in California and the other in Florida.\nThe National Transportation Safety Board is investigating the March 2018 crash that killed Walter Huang near Mountain View, California. It’s also probing a crash in Delray Beach, Florida, that happened about a year later and killed driver Jeremy Banner.\nThe documents say Huang told his wife that Autopilot had previously veered his SUV toward the same barrier on U.S. 101 near Mountain View where he later crashed. Huang died at a hospital from his injuries.\n“Walter said the car would veer toward the barrier in the mornings when he went to work,” the Huang family’s attorney wrote in a response to NTSB questions.\nRecords from an iPhone recovered from the crash site showed that Huang may have been using it before the accident. Records obtained from AT&T showed that data had been used while the vehicle was in motion, but the source of the transmissions couldn’t be determined, the NTSB wrote. One transmission was less than a minute before the crash.\nHuang had described Autopilot’s previous malfunctioning to his brother, the Huang family attorney wrote, in addition to talking with a friend who owns a Model X. Huang, a software engineer, discussed with the friend how a patch to the Autopilot software affected its performance and made the Model X veer, according to the attorney.\nThe Huang family is suing Tesla and California’s Department of Transportation for allegedly failing to maintain the highway.\nAutopilot is a partially automated system designed to keep a vehicle in its lane and keep a safe distance from vehicles in front of it. It also can change lanes with driver approval. Tesla says Autopilot is intended to be used for driver assistance and that drivers must be ready to intervene at all times.\nThe full NTSB board is scheduled to hold a hearing on the Mountain View crash on Feb. 25. At that time, it will determine a cause and make safety recommendations.\nNTSB staff members have already recommended that California transportation officials move faster to repair highway safety barriers damaged by vehicles.\nA report from the agency says California officials failed to fix the barrier that was damaged in a crash 11 days before Huang was killed. In that incident, a 2010 Toyota Prius traveling over 75 mph (120 kmh) crashed against the attenuator, a cushion that protects vehicles from hitting the end of concrete lane dividers.\nThe California Highway Patrol responded to the March 12 crash but did not notify the state Department of Transportation of the damage as required, the NTSB said.\nHuang’s 2017 Tesla Model X was traveling at 71 mph (114 kph) when it crashed against the same attenuator, which the NTSB determined had been damaged and repaired more frequently than any other left-exit in Caltrans’ District 4, which includes all of the San Francisco Bay Area.\nIn the three years before the Tesla crash, the device was struck at least five times, including one crash that resulted in fatalities. A car struck it again on May 20, 2018, about two months after the Tesla crash, the NTSB said.\nNTSB first released some details from its investigation in September.\nThe California Department of Transportation said in a statement Tuesday that it has “identified and is implementing several steps to enhance monitoring and tracking of the repair of damage” to highway infrastructure.\n“These efforts include updates to its policies and maintenance manual, training of staff, and enhanced reporting on the timely repair of high priority traffic safety devices,” Caltrans said.\nIn the Florida crash, Banner turned on the Autopilot function of his Model 3 sedan 10 seconds before the crash, then took his hands off the steering wheel, NTSB documents said. The car then drove underneath a tractor-trailer that was crossing in front of it, sheering off the car’s roof and killing Banner. It was eerily similar to another Florida crash in 2016 in which a Tesla on Autopilot went beneath a semi trailer.\nThe NTSB said in a preliminary report that it still hasn’t determined the cause of the crash. According to the report, traffic was light on the four-lane highway and dawn was breaking when Banner, 50, set his speed at 69 mph (111 kph) and activated the autopilot as he headed to work. The speed limit was 55 mph (88 kph). Seconds later, a tractor-trailer driven by Richard Wood, 45, pulled from a driveway and began to cross to the other side of the highway.\nWood said he saw two sets of car headlights coming toward him, but he thought he had time to make it across. “It was dark and it looked like the cars was back further than they was,” Wood told NTSB investigators four days after the crash.\nA photo taken by the NTSB from Tesla’s front-end video camera showed Wood’s trailer fully blocking the road 1.5 seconds before the crash. Data from the Tesla’s computer shows that Banner hit his brakes less than a second before the crash, but the car went under the trailer. Wood says he saw a second car but it didn’t hit the trailer.\n____\nKrisher reported from Detroit. Terry Spencer contributed from Orlando, Florida.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about two crashes involving Tesla Autopilot. One crash in California resulted in the death of Walter Huang, who complained that Autopilot had previously veered his SUV toward the same barrier on U.S. 101 where he later crashed. Records from an iPhone recovered from the crash site suggested that Huang may have been using it before the accident. Another crash in Florida involved a Model 3 sedan where the driver turned on the Autopilot function, took his hands off the steering wheel, and the car drove underneath a tractor-trailer, resulting in a fatal accident. The NTSB is investigating both crashes and their connection to Tesla's Autopilot system. The news provides specific details related to these accidents and their association with Tesla's Autopilot system.\"\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/ntsb-releases-details-in-2-crashes-involving-tesla-autopilot-e2eabac31019a5a750dc8ef67791f62b","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":33,"Time":"August 10, 2023","Title":"US opens investigation into fatal Tesla crash in Virginia","Content":"WASHINGTON, Aug 10 (Reuters) - U.S. auto safety regulators said Thursday they are opening a special crash investigation into a fatal accident in Virginia involving a Tesla Model Y suspected of relying on advanced driver assistance systems and striking a heavy truck.\nThe National Highway Traffic Safety Administration (NHTSA) is probing a July 19 fatal crash in which the driver of a Tesla died after striking a tractor-trailer truck in Warrenton, Virginia.\nThe Fauquier County Sheriff's Office said the 57-year Tesla driver was killed after the tractor trailer was attempting to turn onto a highway from a truck stop. The Tesla struck the side and went underneath the tractor trailer and the driver was pronounced deceased on the scene. The driver of the tractor trailer was issued a summons for reckless driving.\nSince 2016, the U.S. auto safety regulator has opened more than three dozen Tesla special crash investigations in cases where driver systems such as Autopilot were suspected of being used, with 23 crash deaths reported to date.\nAdvertisement · Scroll to continue\nAutopilot is a feature intended to steer, accelerate and brake cars automatically within their lane, while enhanced Autopilot can assist in changing lanes on highways. Tesla, which did not respond to requests for comment, says the system requires active human supervision.\nNHTSA last month opened another special crash probe into a July 5 fatal accident in California involving a 2018 Tesla Model 3. The South Lake Tahoe crash killed the 17-year-old driver of a 2013 Subaru Impreza after a head-on collision with the Tesla Model 3 and fatally injured a three-month-old passenger in the Tesla who died several days later.\nAdvertisement · Scroll to continue\nNHTSA typically opens more than 100 \"special\" crash investigations annually into emerging technologies and other potential auto safety issues that have, for instance, previously helped to develop safety rules on air bags.\nThose are separate from defect investigations opened by the agency to determine if a safety recall is warranted.\nIn June 2022, NHTSA upgraded to an engineering analysis its defect probe into 830,000 Tesla vehicles with driver assistance system Autopilot and crashes with parked emergency vehicles, including fire trucks. The probe was first opened in August 2020.\nAdvertisement · Scroll to continue\nTransportation Secretary Pete Buttigieg last month declined to offer an update on the status of the Tesla Autopilot investigation but told Reuters \"we're moving as quickly as we responsibly can.\"\nReporting by David Shepardson Editing by Alexandra Hudson\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to Tesla Autopilot accidents because it reports a fatal accident in Virginia involving a Tesla Model Y suspected of relying on advanced driver assistance systems. The National Highway Traffic Safety Administration (NHTSA) is conducting a special crash investigation into the incident. The driver of the Tesla struck a tractor-trailer truck, resulting in the driver's death. This incident is part of a series of investigations by NHTSA into Tesla accidents, some of which involve Autopilot and have resulted in crash deaths. The news also mentions that Autopilot is a feature designed for automatic steering, acceleration, and braking within a lane, and Tesla emphasizes the need for active human supervision, which is relevant to Autopilot technology.\"\n}\nThe news is indeed about a fatal Tesla crash, and it fits the criteria for Tesla Autopilot accidents, as it involves a Tesla Model Y suspected of relying on advanced driver assistance systems and the crash resulted in the driver's death.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/technology\/us-opens-new-investigation-into-fatal-tesla-crash-virginia-2023-08-10\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":35,"Time":"Published 4:08 PM EDT, Mon August 30, 2021","Title":"Another Tesla reportedly using Autopilot hits a parked police car","Content":"New York\nCNN Business\n — \nAnother Tesla has hit an emergency vehicle, apparently while using the Autopilot driver-assist feature, adding to a problem that is already the subject of a federal safety probe.\nThe Florida Highway Patrol reported the accident just before 5 in the morning Saturday along Interstate 4 in Orlando. No one was seriously injured in the crash, though the Tesla did narrowly miss hitting a state trooper as he left his car to assist another driver who had broken down on the highway.\nThe broken-down car was a Mercedes that had come to a stop in a travel lane. The police cruiser was stopped behind it with its emergency lights flashing. The left front of the Tesla Model 3 crashed into the side of the police car, and then hit the Mercedes.\n“The driver stated that [the Tesla] was in Autopilot mode,” said the report from the Florida Highway Patrol.\nThe National Highway Traffic Safety Administration disclosed earlier this month that it is investigating at least 11 accidents involving Teslas that have hit police cars, ambulances or other emergency vehicles while they were responding to traffic accidents. The crashes under investigation occurred between January 22, 2018, and July 10, 2021, across nine states. They took place mostly at night, and the accident response scenes were all outfitted with control measures such as emergency vehicle lights, flares, illuminated arrow boards and road cones, according to NHTSA.\nFlorida police said they would report the crash to the NHTSA and to Tesla.\nA Tesla Model 3, left, after it hit a Florida Highway Patrol cruiser, right, in an accident early on Aug. 28. The Tesla driver told police she had been using Autopilot at the time of the accident.\nFlorida Highway Patrol\nThe highway safety agency said that it is important that Tesla owners using Autopilot remain alert and ready to take control of the car in order to avoid obstacles.\n“NHTSA reminds the public that no commercially available motor vehicles today are capable of driving themselves,” the agency said in a statement. “Every available vehicle requires a human driver to be in control at all times, and all state laws hold human drivers responsible for operation of their vehicles.”\nTesla did not respond to a request for comment on the latest crash or on the NHTSA investigation. Although the company says that its data show cars using Autopilot have fewer accidents per mile than cars being driven by humans, it does warn that “current Autopilot features require active driver supervision and do not make the vehicle autonomous.”\nIn addition to the NHTSA probe, Senators Richard Blumenthal of Connecticut and Edward Markey of Massachusetts, Democrats who have been critical of Tesla in the past, have asked the Federal Trade Commission to launch an investigation into whether Tesla’s use of the term “Autopilot” and its claims about the car’s self-driving capabilities amount to misleading advertising. The FTC has not commented on whether it has launched the requested probe into Tesla’s claims.\nDriver-assist options such as Tesla’s Autopilot or adaptive cruise control, which is available in a wide range of vehicles from various automakers, do a good job of slowing a car down when traffic ahead slows, said Sam Abuelsamid, an expert in self-driving vehicles and principal analyst at Guidehouse Insights.\nBut Abuelsamid said those vehicles are designed to ignore stationary objects when traveling at more than 40 mph so they don’t slam on the brakes when approaching overpasses or other objects on the side of the road — such as a car stopped on the shoulder. Fortunately most of these automatic braking systems do stop for stationary objects when they are traveling at slower speeds.\nTesla is under investigation because its cars keep hitting emergency vehicles\nThe bigger problem, according to Abuelsamid, is that many more Tesla owners appear to assume their cars can, in fact, drive themselves than do drivers of other vehicles with automatic driver-assist features. Moreover, the cues that a driver would see when approaching an accident site, such as road flares or flashing lights, make more sense to a human than they might to an auto drive system.\n“When it works, which can be most of the time, it can be very good,” Abuelsamid said about Tesla’s Autopilot feature. “But it can easily be confused by things that humans would have no problem with. Machine visions are not as adaptive as humans’. And the problem is that all machine systems sometimes make silly errors.”","Relevant":true,"Overview":"Here is the JSON-format response for the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Summary\": \"Another Tesla has hit an emergency vehicle, apparently while using the Autopilot driver-assist feature, adding to a problem that is already the subject of a federal safety probe.\",\n    \"Details\": [\n      \"The news is about a Tesla incident involving the use of Autopilot.\",\n      \"The accident occurred along Interstate 4 in Orlando when a Tesla Model 3 crashed into a police car with its emergency lights flashing.\",\n      \"The driver stated that the Tesla was in Autopilot mode at the time of the accident, as reported by the Florida Highway Patrol.\",\n      \"The National Highway Traffic Safety Administration (NHTSA) is investigating 11 accidents involving Teslas that hit police cars or other emergency vehicles while responding to traffic accidents. These accidents occurred across nine states between January 22, 2018, and July 10, 2021, mostly at night and at scenes equipped with control measures.\",\n      \"The NHTSA emphasizes that Tesla owners using Autopilot should remain alert and ready to take control of the car.\",\n      \"Senators Richard Blumenthal and Edward Markey have asked the Federal Trade Commission to investigate Tesla's use of the term 'Autopilot' and its claims about self-driving capabilities.\",\n      \"An expert in self-driving vehicles, Sam Abuelsamid, mentioned that Tesla owners tend to assume their cars can drive themselves more than drivers of other vehicles with automatic driver-assist features.\"\n    ]\n  }\n}\nIn summary, the news is indeed relevant to Tesla Autopilot accidents because it describes an incident involving a Tesla using Autopilot that collided with a police car on a highway. Additionally, it mentions the ongoing NHTSA investigation into similar accidents involving Teslas and highlights concerns about the public's perception of Autopilot capabilities.","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2021\/08\/30\/business\/tesla-crash-police-car\/index.html","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":37,"Time":"January 9, 2023","Title":"'Extensive' Tesla Autopilot probe proceeding 'really fast', U.S. official says","Content":"WASHINGTON, Jan 9 (Reuters) - The acting head of the National Highway Traffic Safety Administration (NHTSA) told reporters on Monday that the regulatory agency is \"working really fast\" on the Tesla (TSLA.O) Autopilot investigation it opened in August 2021.\n\"We're investing a lot of resources,\" said NHTSA acting head Ann Carlson in comments to reporters on the sidelines of an event in Washington. She declined to commit to a particular timeframe on when the probe will be resolved. \"The resources require a lot of technical expertise, actually some legal novelty and so we're moving as quickly as we can, but we also want to be careful and make sure we have all the information we need.\"\nIn June, NHTSA upgraded to an engineering analysis its defect probe into 830,000 Tesla vehicles with driver assistance system Autopilot and involving crashes with parked emergency vehicles. That step was necessary before the agency could demand a recall.\nNHTSA is reviewing whether Tesla vehicles adequately ensure drivers are paying attention. Previously, the agency said evidence suggested drivers in most crashes under review had complied with Tesla's alert strategy that seeks to compel driver attention, raising questions about its effectiveness.\nAdvertisement · Scroll to continue\nCarlson confirmed the agency was in discussions with Tesla about a Dec. 31 tweet that Chief Executive Elon Musk wrote about a driver monitoring function.\nCarlson, who has been general counsel of the agency since early 2021, became acting administrator of NHTSA in September.\nA Dec. 31 tweet suggested drivers with more than 10,000 miles using Tesla's \"Full Self-Driving\" (FSD) software system should be able to disable the \"steering wheel nag,\" an alert that instructs drivers to hold the wheel to confirm they are paying attention. Musk responded: \"Agreed, update coming in Jan.\"\nAdvertisement · Scroll to continue\nCarlson said the agency has \"a very extensive investigation ongoing... We are in conversations with Tesla about this latest communication.\"\nTesla did not immediately comment.\nIn 2020, the National Transportation Safety Board criticized Tesla's \"ineffective monitoring of driver engagement\" after a 2018 fatal Autopilot crash and said NHTSA had provided \"scant oversight.\"\nTesla sells the $15,000 FSD software as an add-on which enables its vehicles to change lanes and park autonomously. That complements its standard \"Autopilot\" feature, which enables cars to steer, accelerate and brake within their lanes without driver intervention. Both systems use the steering wheel monitoring function.\nAdvertisement · Scroll to continue\nLast month, NHTSA said it had opened two new special investigations into crashes involving Tesla vehicles where advanced driver assistance systems are suspected to have been in use. Since 2016, NHTSA has opened more than three dozen Tesla special crash investigations where advanced driver assistance systems such as Autopilot were suspected of being used with 19 crash deaths reported.\nReporting by David Shepardson; Editing by Leslie Adler and David Gregorio\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to Tesla Autopilot accidents because it discusses the ongoing investigation by the National Highway Traffic Safety Administration (NHTSA) into Tesla's Autopilot system and its potential safety issues. The NHTSA is actively working on this investigation, which was opened in August 2021, and is investing a significant amount of resources in it. The investigation is focused on Tesla vehicles with Autopilot and their involvement in crashes, including those with parked emergency vehicles. The NHTSA is also reviewing whether Tesla's alert strategy for ensuring driver attention is effective. Additionally, the news mentions discussions between the NHTSA and Tesla regarding a December 31st tweet by Elon Musk about a driver monitoring function. This information is specific to the ongoing investigation into Tesla Autopilot and its potential safety concerns.\"\n}\nThe news is relevant to Tesla Autopilot accidents because it provides information about the ongoing investigation by the NHTSA into Tesla's Autopilot system and its potential safety issues. It includes details about the resources being invested, the upgrade to an engineering analysis, and discussions with Tesla regarding driver monitoring functions.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/technology\/us-agency-working-really-fast-nhtsa-autopilot-probe-2023-01-09\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":38,"Time":"December 22, 2022","Title":"U.S. safety agency probes two more Tesla crashes involving suspected driver assistance","Content":"Companies\nTesla Inc\nWASHINGTON, Dec 22 (Reuters) - The National Highway Traffic Safety Administration (NHTSA) said on Thursday it has opened two new special investigations into crashes involving Tesla (TSLA.O) vehicles where advanced driver assistance systems are suspected to have been in use.\nThe new crashes under investigation include an eight-vehicle crash on Nov. 24 on the Bay Bridge in San Francisco in which the driver of a 2021 Tesla Model S reported the Full-Self Driving feature had malfunctioned, according to a police report. The other recent crash involves a 2020 Model 3 in Ohio where a minor injury was reported.\nIn the Bay Bridge incident that led to two minor injuries, police said they were unable to determine if its \"Full-Self Driving\" system was activated.\nSince 2016, NHTSA has opened 41 special crash investigations involving Tesla vehicles and where advanced driver assistance systems such as Autopilot were suspected of being used, including eight investigations in 2022. A total of 19 crash deaths have been reported in those Tesla-related investigations.\nAdvertisement · Scroll to continue\nNHTSA typically opens more than 100 special crash investigations annually into emerging technologies and other potential auto safety issues that have, for instance, previously helped to develop safety rules on air bags.\nTesla did not respond to a request for comment.\nChief Executive Elon Musk has touted Tesla \"Full Self-Driving\" (FSD) software as a potential cash cow for the world's biggest electric carmaker. But Tesla's advanced driver assistance systems - and Musk's claims about them - face growing legal, regulatory and public scrutiny.\nAdvertisement · Scroll to continue\nTesla sells the $15,000 FSD software as an add-on which enables its vehicles to change lanes and park autonomously. That complements its standard \"Autopilot\" feature, which enables cars to steer, accelerate and brake within their lanes without driver intervention.\nIn June, NHTSA upgraded its defect probe into 830,000 Tesla vehicles with Autopilot and involving crashes with parked emergency vehicles, a required step before it could seek a possible recall.\nReporting by David Shepardson Editing by Chris Reese and Frances Kerry\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about two new special investigations opened by the National Highway Traffic Safety Administration (NHTSA) into crashes involving Tesla vehicles where advanced driver assistance systems are suspected to have been in use. One of the crashes involved an eight-vehicle crash on Nov. 24 on the Bay Bridge in San Francisco, in which the driver of a 2021 Tesla Model S reported the Full-Self Driving feature had malfunctioned. The other recent crash involved a 2020 Model 3 in Ohio where a minor injury was reported. Since 2016, NHTSA has opened 41 special crash investigations involving Tesla vehicles and advanced driver assistance systems, including Autopilot. A total of 19 crash deaths have been reported in those Tesla-related investigations. These details clearly indicate that the news is related to Tesla Autopilot accidents.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/us-safety-agency-probing-two-new-tesla-driver-assistance-crashes-2022-12-22\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":34,"Time":"Wed 13 Jul 2016 22.38 CEST","Title":"Fatal crash prompts federal investigation of Tesla self-driving cars","Content":"A letter from the National Highway and Transportation Safety Administration says that it is investigating all Tesla vehicles equipped with any version of its “Autopilot Technology Package” in the wake of a crash in May that killed a 40-year-old man.\nThe accident that killed Joshua Brown appears to have happened because autopilot – a self-driving mode still in its testing phase – could not distinguish between a white truck and a bright sky in this case, according to Tesla.\nTesla has no plans to disable autopilot mode as third recent crash is revealed\nRead more\nTesla explained the cause of the accident in mechanical terms on 30 June when news of the accident broke: “Neither Autopilot nor the driver noticed the white side of the tractor trailer against a brightly lit sky, so the brake was not applied,” a spokesperson wrote on the company website.\n“The high ride height of the trailer combined with its positioning across the road and the extremely rare circumstances of the impact caused the Model S [car] to pass under the trailer, with the bottom of the trailer impacting the windshield of the Model S.”\nIn the Nhtsa letter informing Tesla it was under investigation, Jeffrey L Quandt, chief of the vehicle control division in the office of defects investigation, wrote that the crash that killed Brown and two non-fatal accidents reported this month were all “alleged to have occurred because Forward Collision Warning (FCW) or Automatic Emergency Braking (AEB) [two safety mechanisms] did not occur when expected”.\nTesla has said it informed Nhtsa immediately about the crash that killed Brown but the regulator only announced that it had happened on 30 June.\nThe electric car company offloaded $2bn worth of stock between the crash and its announcement nearly two months later, a move the Securities and Exchange Commission (SEC) is reportedly looking into. According to the Wall Street Journal, the SEC is scrutinizing whether the incident was “material” information that Tesla should have disclosed in a formal regulatory filing. Tesla told the Guardian on Wednesday afternoon it still had not received any communication from the SEC.\nThat crash was the only fatality in three Tesla accidents that have been reported this month; all occurred while the driver was using the autopilot feature, which was only intended to be used with the driver’s hands on the wheel.\nOn Twitter and in interviews, Musk has said he has no plans to disable autopilot; he also hit back at Fortune magazine for its coverage of the stock sale.\nSomeone needs to explain the definition of \"material\" to Elon Musk. @alansmurray @FortuneMagazine pic.twitter.com\/0X6skK6LEI\n— defcon_5 (@defcon_5) July 6, 2016\nIn March, the US Senate told leading automakers the public could not be asked to trust corporate goodwill regarding safety regulations. “The credibility of this technology is exceedingly fragile if people can’t trust standards,” Senator Richard Blumenthal told General Motors’ Michael Ableson, “not necessarily for you, but for all the other actors that may come into this space at this point.”\nTesla declined to comment on the accident and subsequent Nhtsa investigation, citing privacy concerns.","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"The news is about Tesla Autopilot accidents. The National Highway and Transportation Safety Administration (NHTSA) is investigating Tesla vehicles equipped with the 'Autopilot Technology Package' following a fatal crash in May that killed a 40-year-old man named Joshua Brown. The accident occurred because the Autopilot mode, which was in its testing phase, could not distinguish between a white truck and a bright sky. Tesla explained that neither Autopilot nor the driver noticed the white side of the tractor trailer against a brightly lit sky, causing the accident. Additionally, the NHTSA letter mentioned that the crash and two non-fatal accidents were allegedly due to Forward Collision Warning (FCW) and Automatic Emergency Braking (AEB) not functioning as expected. It's worth noting that all these accidents happened while the driver was using the Autopilot feature, which was intended to be used with the driver's hands on the wheel.\"\n}\nThe provided news is relevant to Tesla Autopilot accidents, as it discusses an investigation by NHTSA into Tesla vehicles equipped with Autopilot and provides specific information about the fatal crash, the technical details of the accident, and the potential issues with safety mechanisms in Autopilot mode.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/jul\/13\/tesla-autopilot-investigation-fatal-crash","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":26,"Time":"Published 7:07 PM GMT+2, February 25, 2020","Title":"NTSB: Tesla Autopilot, distracted driver caused fatal crash","Content":"WASHINGTON (AP) — Tesla’s partially automated driving system steered an electric SUV into a concrete barrier on a Silicon Valley freeway because it was operating under conditions it couldn’t handle and because the driver likely was distracted by playing a game on his smartphone, the National Transportation Safety Board has found.\nThe board made the determination Tuesday in the fatal crash, and provided nine new recommendations to prevent partially automated vehicle crashes in the future. Among the recommendations is for tech companies to design smartphones and other electronic devices so they don’t operate if they are within a driver’s reach, unless it’s an emergency.\nChairman Robert Sumwalt said the problem of drivers distracted by smartphones will keep spreading if nothing is done.\n“If we don’t get on top of it, it’s going to be a coronavirus,” he said in calling for government regulations and company policies prohibiting driver use of smartphones.\nMuch of the board’s frustration was directed at the National Highway Traffic Safety Administration and to Tesla, which have not acted on recommendations the NTSB passed two years ago. The NTSB investigates crashes but only has authority to make recommendations. NHTSA can enforce the advice, and manufacturers also can act on it.\nBut Sumwalt said if they don’t, “then we are wasting our time. Safety will not be improved. We are counting on them to do their job.”\nFor Tesla, the board repeated previous recommendations that it install safeguards to stop its Autopilot driving system from operating in conditions it wasn’t designed to navigate. The board also wants Tesla to design a more effective system to make sure the driver is always paying attention.\nIf Tesla doesn’t add driver monitoring safeguards, misuse of Autopilot is expected “and the risk for future crashes will remain,” the board wrote in one of its findings.\nTuesday’s hearing focused on the March 2018 crash of a Tesla Model X SUV, in which Autopilot was engaged when the vehicle swerved and slammed into a concrete barrier dividing freeway and exit lanes in Mountain View, Calif., killing Apple engineer Walter Huang.\nJust before the crash, the Tesla steered to the left into a paved area between the freeway travel lanes and an exit ramp, the NTSB said. It accelerated to 71 mph and crashed into the end of the concrete barrier. The car’s forward collision avoidance system didn’t alert Huang, and its automatic emergency braking did not activate, the NTSB said.\nAlso, Huang did not brake, and there was no steering movement detected to avoid the crash, the board’s staff said.\nNTSB staff members said they couldn’t pinpoint exactly why the car steered into the barrier, but it likely was a combination of faded lane lines, bright sunshine that affected the cameras, and a closer-than-normal vehicle in the lane ahead of the Tesla.\nThe board also found that Huang likely would have lived if a cushion at the end of the barrier had been repaired by California transportation officials. That cushion had been damaged in a crash 11 days before Huang was killed.\nRecommendations to NHTSA included expanded testing to make sure partially automated systems can avoid running into common obstacles such as a barrier. The board also asks that NHTSA evaluate Autopilot to determine where it can safely operate and to develop and enforce standards for monitoring drivers so they pay attention while using the systems.\nNHTSA has told the NTSB it has investigations open into 14 Tesla crashes and would use its enforcement of safety defects to take action if needed.\nThe agency issued a statement saying it will review the NTSB’s report and that all commercially available vehicles require human drivers to stay in control at all times.\n“Distraction-affected crashes are a major concern, including those involving advanced driver assistance features,” the statement said.\nSumwalt said at the start of Tuesday’s hearing that systems like Autopilot cannot drive themselves, yet drivers continue to use them without paying attention.\n“This means that when driving in the supposed ‘self-driving’ mode, you can’t read a book, you can’t watch a movie or TV show, you can’t text and you can’t play video games,” he said.\nUnder questioning from board members, Robert Molloy, the NTSB’s director of highway safety, said the NHTSA is taking a hands-off approach to regulating new automated driving systems like Autopilot. Molloy called the approach “misguided,” and said nothing is more disappointing than seeing recommendations ignored by Tesla and NHTSA.\n“They need to do more,” he said of the federal highway safety agency.\nAutopilot is designed to keep a vehicle in its lane and keep a safe distance from vehicles in front of it. It also can change lanes with driver approval. Tesla says Autopilot is intended to be used for driver assistance and that drivers must be ready to intervene at all times.\nSumwalt said the board had made recommendations to six automakers in 2017 to stop the problem and only Tesla has failed to respond.\nTeslas can sense a driver applying force to the steering wheel, and if that doesn’t happen, it will issue visual and audio warnings. But monitoring steering wheel torque, “is a poor surrogate measure” of monitoring the driver, Ensar Becic, the NTSB’s human performance and automation highway safety expert told the board.\nMessages were left Tuesday seeking comment from Tesla.\nSumwalt said the NTSB had called for technology more than nine years ago to disable distracting functions of smartphones while the user is driving, but no action has been taken.\nDon Karol, the NTSB’s project manager for highway safety, told the board that the staff is recommending that cell phone companies program phones to automatically lock out distracting functions such as games and phone calls while someone is driving. The staff also recommends that companies enact policies to prevent use of company issued cell phones while workers are driving.\nTesla has said Autopilot was put out initially in “beta,” meaning it was being tested and improved as bugs were identified, Karol told the board.\nThat brought a response from Vice Chairman Bruce Landsburg, who said if the system has known bugs, “it’s probably pretty foreseeable that somebody’s going to have a problem with it. And then they (Tesla) come back and say ‘oh, we warned you.’”","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n\"The news is about a fatal crash involving a Tesla Model X SUV and the use of Tesla's Autopilot system.\",\n\"The National Transportation Safety Board (NTSB) found that the crash was caused by Tesla's partially automated driving system operating under conditions it couldn't handle and the driver being distracted by playing a game on his smartphone.\",\n\"The crash occurred in March 2018 when the Tesla Model X swerved and crashed into a concrete barrier, resulting in the death of the driver, Walter Huang.\",\n\"The NTSB highlighted factors such as faded lane lines, bright sunshine affecting cameras, and a closer-than-normal vehicle in the lane ahead as potential reasons for the crash.\",\n\"The NTSB made recommendations to prevent similar crashes in the future, including designing smartphones and electronic devices not to operate within a driver's reach unless it's an emergency.\",\n\"The NTSB also recommended that Tesla install safeguards to prevent Autopilot from operating in conditions it wasn't designed for and to implement a more effective driver monitoring system to ensure attention is always paid to driving.\",\n\"Tesla's Autopilot was described as being in 'beta' during the time of the crash, and concerns were raised about known bugs in the system.\",\n\"The NTSB expressed frustration with both the National Highway Traffic Safety Administration (NHTSA) and Tesla for not acting on previous recommendations related to safety.\"\n]\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/us-news-ap-top-news-ca-state-wire-government-regulations-transportation-d03d88fca7ef389ffbe3469f50e36dcf","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":43,"Time":"Published 6:16 PM EST, Sat November 13, 2021","Title":"US opens probe into Tesla’s Autopilot over emergency vehicle crashes","Content":"CNN\n — \nWASHINGTON, Nov 12 (Reuters) - The U.S. National Highway Traffic Safety Administration (NHTSA) said it is looking into a consumer report that a Tesla Model Y was involved in an accident while using the company’s Full Self-Driving (FSD) Beta software.\nThe owner of a 2021 Tesla Model Y reported to the auto safety agency that on Nov. 3 in Brea, California the vehicle was in FSD Beta mode “and while taking a left turn the car went into the wrong lane and I was hit by another driver in the lane next to my lane.”\nThe car “gave an alert half way through the turn” and the driver tried to assume control “but the car by itself took control and forced itself into the incorrect lane,” the report says. The car was severely damaged on the driver side, the owner added.\n“NHTSA is aware of the consumer complaint in question and is in communication with the manufacturer to gather additional information,” an NHTSA spokesperson told Reuters on Friday.\nTesla did not immediately comment.\nEarlier this month, Tesla recalled nearly 12,000 U.S. vehicles because of a communication error that could trigger a false collision warning or unexpected automatic emergency brake.\nThe recall was prompted after a software update to vehicles with FSD Beta. Tesla said more than 99.8% of the vehicles recalled as of Oct. 29 had installed a software update to address the issue and no further action was necessary.\nFSD is an advanced driver assistance system that handles some driving tasks but Tesla says does not make vehicles completely autonomous. The features “require a fully attentive driver,” it says.\nLast month, NHTSA raised concerns about how FSD was being used. “Despite Tesla’s characterization of FSD as ‘beta,’ it is capable of and is being used on public roads,” NHTSA said.\nNHTSA in August opened a formal safety probe into Tesla’s Autopilot, a different driver assistance software system, after a dozen crashes involving Tesla models and emergency vehicles.","Relevant":true,"Overview":"Here's the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to Tesla Autopilot accidents because it reports that the U.S. National Highway Traffic Safety Administration (NHTSA) is investigating an accident involving a Tesla Model Y that was using the company’s Full Self-Driving (FSD) Beta software. The owner of the vehicle reported that while in FSD Beta mode, the car went into the wrong lane during a left turn, leading to a collision with another driver. The car gave an alert during the turn, but the driver was unable to regain control as the car forced itself into the incorrect lane, resulting in severe damage to the driver's side. Additionally, the news mentions that NHTSA had previously raised concerns about Tesla's Autopilot system and its involvement in accidents with emergency vehicles.\"\n}\nThis response confirms that the news is about a Tesla Autopilot accident and provides specific details about the incident and the NHTSA's investigation into it.","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2021\/11\/13\/tech\/tesla-autopilot-emergency-crashes\/index.html","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":27,"Time":"Published 8:10 PM GMT+2, June 14, 2022","Title":"Data likely shows Teslas on Autopilot crash more than rivals","Content":"DETROIT (AP) — The government will soon release data on collisions involving vehicles with autonomous or partially automated driving systems that will likely single out Tesla for a disproportionately high number of such crashes.\nIn coming days, the National Highway Traffic Safety Administration plans to issue figures it has been gathering for nearly a year. The agency said in a separate report last week that it had documented more than 200 crashes involving Teslas that were using Autopilot, “Full Self-Driving,” Traffic-Aware Cruise Control or some other of the company’s partially automated systems.\nTesla’s figure and its crash rate per 1,000 vehicles was substantially higher than the corresponding numbers for other automakers that provided such data to The Associated Press ahead of NHTSA’s release. The number of Tesla collisions was revealed as part of a NHTSA investigation of Teslas on Autopilot that had crashed into emergency and other vehicles stopped along roadways.\nTesla does have many more vehicles with partly automated systems operating on U.S. roads than most other automakers do — roughly 830,000, dating to the 2014 model year. And it collects real-time data online from vehicles, so it has a much faster reporting system. Other automakers, by contrast, must wait for reports to arrive from the field and sometimes don’t learn about crashes for months.\nIn a June 2021 order, NHTSA told more than 100 automakers and automated vehicle tech companies to report serious crashes within one day of learning about them and to disclose less-serious crashes by the 15th day of the following month. The agency is assessing how the systems perform, whether they endanger public safety and whether new regulations may be needed.\nGeneral Motors said it reported three crashes while its “Super Cruise” or other partially automated systems were in use. The company said it has sold more than 34,000 vehicles with Super Cruise since its debut in 2017.\nNissan, with over 560,000 vehicles on the road using its ”ProPilot Assist,” didn’t have to report any crashes, the company said.\nStellantis, formerly Fiat Chrysler, said it reported two crashes involving its systems. Ford reported zero involving its “Blue Cruise” driver-assist system which went on sale in the spring, though Ford wouldn’t say if there were crashes with less-capable systems.\nGM said the three crashes weren’t the fault of Super Cruise. It also reported two crashes that happened before the June 2021 order, a spokesman said.\nSeveral automakers and tech companies, including Toyota and Honda, declined to release their numbers before the NHTSA data is revealed.\nA message was left seeking comment from Tesla, which has disbanded its media relations department. NHTSA wouldn’t comment on the data Tuesday.\nRaj Rajkumar, an electrical and computer engineering professor at Carnegie Mellon University who studies automated vehicles, said he wouldn’t be surprised if Tesla was found to have had a high number of crashes involving its driver-assist systems. Tesla, based in Austin, Texas, stopped using radar in its system and instead relies solely on cameras and computers — a system that Rajkumar calls “inherently unsafe.”\nThe system’s computer, he said, can recognize only what’s in its memory. Flashing lights on an emergency vehicle, Rajkumar said, might confuse the system, as would anything that the computer hasn’t seen before.\n“Emergency vehicles may look very different from all the data that the Tesla software had been trained on,” he said.\nIn addition to the publicly released crash data, NHTSA has sent investigative teams to far more incidents involving Teslas using electronic systems than other automakers. As part of a larger inquiry into crashes involving advanced driver assistance systems, the agency has sent teams to 34 crashes since 2016 in which the systems were thought to have been in use. Of the 34 crashes, 28 involved Teslas, according to a NHTSA document.\nNHTSA said in documents that it has received 191 reports of crashes involving Teslas on Autopilot and nonemergency vehicles, plus 16 more involving parked emergency vehicles or those with warning lights, for a total of 207. Of the 191, the agency removed 85 because of actions of other vehicles or insufficient data to make a firm assessment of the crashes. That left 106 that were included in the Autopilot investigation.\nIt wasn’t clear if 207 matched the total number of Tesla crashes reported to NHTSA under the order. A NHTSA spokeswoman wouldn’t comment.\nThe agency ordered automakers and tech companies to report crashes involving driver-assist systems, as well as fully autonomous driving systems.\nIn defending its partially automated systems, Tesla has said that Autopilot and “Full Self-Driving” cannot drive themselves, and that drivers should be ready to intervene at all times. The systems can keep cars in their lanes and away from other vehicles and objects. But in documents released last week, NHTSA raised questions about whether human drivers can intervene fast enough to prevent crashes.\nTesla’s “Full Self-Driving” is designed to complete a route on its own with human supervision, with the eventual aim of driving itself and running a fleet of autonomous robo-taxis. In 2019, Musk had pledged to have the robo-taxis running in 2020.\nTesla’s Autopilot driver-assist system detects hands on the steering wheel to make sure drivers are paying attention. But that’s inadequate, Rajkumar said. By contrast, systems such as GM’s monitor a driver’s eyes with a camera, he said, to make sure they’re looking forward.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to Tesla Autopilot accidents because it reports on government data that highlights a disproportionately high number of crashes involving Teslas using Autopilot and other partially automated systems. The National Highway Traffic Safety Administration (NHTSA) has gathered data over nearly a year, documenting more than 200 crashes involving Teslas using Autopilot, Full Self-Driving, Traffic-Aware Cruise Control, or similar systems. The article also mentions that Tesla's crash rate per 1,000 vehicles is substantially higher than that of other automakers who provided similar data to the Associated Press. Additionally, NHTSA has sent investigative teams to more Tesla incidents involving electronic systems compared to other automakers as part of a larger inquiry into crashes involving advanced driver assistance systems. The news also discusses concerns raised by experts about the safety of Tesla's approach, including the reliance on cameras and computers without radar and the potential limitations of the system in recognizing certain situations, such as emergency vehicles with flashing lights. Finally, NHTSA's scrutiny and questions about whether human drivers can intervene fast enough to prevent crashes with Tesla's Autopilot system are highlighted in the article.\"\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/technology-business-5e6c354622582f9d4607cc5554847558","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":46,"Time":"July 19, 2023","Title":"US opens special probe into fatal Tesla crash","Content":"Companies\nTesla Inc\nCarparts.Com Inc\nWASHINGTON, July 18 (Reuters) - U.S. auto safety regulators are opening a special crash investigation into a fatal accident in California involving a 2018 Tesla Model 3 suspected of relying on advanced driver assistance systems, the government agency said on Tuesday.\nThe National Highway Traffic Safety Administration (NHTSA) is probing the July 5 crash in South Lake Tahoe killed the 17-year-old driver of a 2013 Subaru Impreza after a head-on collision with the Tesla Model 3 and fatally injured a three-month-old passenger in the Tesla who died several days later, the California Highway Patrol said.\nSince 2016, the U.S. auto safety regulator has opened more than three dozen Tesla special crash investigations in cases where systems such as Autopilot were suspected of being used, with 22 crash deaths reported through Tuesday.\nAutopilot is a feature intended to steer, accelerate and brake cars automatically within their lane, while enhanced Autopilot can assist in changing lanes on highways. Tesla, which did not respond to requests for comment, says the system requires active human supervision.\nAdvertisement · Scroll to continue\nThe Subaru was traveling at an estimated 55 miles per hour (88.51 kilometers per hour) while the Tesla was at 45 mph when they collided, a police report said. The Tesla driver suffered serious injuries and two other Tesla passengers suffered moderate injuries.\n\"There are no charges pending at this time. Vehicle and car seat inspections are currently being done,\" a California Highway Patrol spokesperson said.\nAdvertisement · Scroll to continue\nThis is the first new special crash investigation involving Tesla and the suspected use of driver assistance systems since two were opened in March, including one into the February fatal crash in California involving a 2014 Tesla (TSLA.O) Model S and a fire truck in Contra Costa County, California.\nA local California fire department said a Tesla struck one of its fire trucks and that the Tesla driver was pronounced dead at the scene.\nAdvertisement · Scroll to continue\nThe other investigation in March involves a 2022 Tesla Model Y that struck and seriously injured a 17-year-old student who got off a school bus in North Carolina.\nNHTSA typically opens more than 100 \"special\" crash investigations annually into emerging technologies and other potential auto safety issues that have, for instance, previously helped to develop safety rules on air bags.\nThose are separate from defect investigations opened by the agency to determine if a safety recall is warranted.\nAdvertisement · Scroll to continue\nIn June, NHTSA upgraded to an engineering analysis its defect probe into 830,000 Tesla vehicles with driver assistance system Autopilot and crashes with parked emergency vehicles, including fire trucks.\nNHTSA said earlier this month it was seeking updated responses and current data from Tesla in the Autopilot probe by Wednesday.\nReporting by David Shepardson; Editing by Doina Chiacu, Chizu Nomiyama and Deepa Babington\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"The news is relevant to Tesla Autopilot accidents. The article reports a special crash investigation by the National Highway Traffic Safety Administration (NHTSA) into a fatal accident in California involving a 2018 Tesla Model 3. The investigation is suspecting the use of advanced driver assistance systems, including Autopilot, in the Tesla Model 3 at the time of the accident. The collision resulted in the death of the 17-year-old driver of a 2013 Subaru Impreza and fatally injured a three-month-old passenger in the Tesla Model 3. It also mentions that the U.S. auto safety regulator has opened more than three dozen special crash investigations into Tesla vehicles suspected of using Autopilot, with 22 crash deaths reported through Tuesday. The article provides details about the speed of both vehicles involved in the crash and mentions that Autopilot is designed to steer, accelerate, and brake cars automatically within their lane, requiring active human supervision. Additionally, it highlights that this is the first new special crash investigation involving Tesla and the suspected use of driver assistance systems since earlier investigations into Tesla accidents in March.\"\n}\nThe news is relevant to Tesla Autopilot accidents because it discusses a fatal accident involving a Tesla Model 3, the suspected use of advanced driver assistance systems, and the ongoing special crash investigation by NHTSA.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/us-opens-new-special-probe-into-fatal-tesla-crash-2023-07-18\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":48,"Time":null,"Title":"Tesla says Autopilot not to blame in crash with bus in Germany","Content":"By Reuters Staff\nFRANKFURT\/SAN FRANCISCO (Reuters) - A Tesla Motors Inc car operating under Autopilot collided with a bus in northern Germany, but the electric car maker said a collision was unavoidable because the bus swerved into the vehicle’s lane.\nThe logo of U.S. car manufacturer Tesla is seen in Zurich, Switzerland July 14, 2016. REUTERS\/Arnd Wiegmann\/File Photo\nThe incident occurred on Wednesday in the town of Ratzeburg, German police said.\nThe car’s driver told Tesla following the crash that he was using Autopilot when the incident occurred but that it was unrelated to the accident, a Tesla spokeswoman said on Thursday.\n“We have spoken to our customer, who confirmed that Autopilot was functioning properly and that his use of Autopilot was unrelated to the accident,” Tesla said in a statement.\nReuters could not confirm independently what the car driver told Tesla and the circumstances that caused the bus to swerve.\nTesla’s Autopilot, introduced last October, helps drivers steer and stay in lanes and is intended to assist, but not replace, drivers. It has been the focus of intense scrutiny since it was revealed in July that a Tesla Model S driver was killed while using the technology in a May 7 collision with a truck in Florida.\nSince then, there have been various reports of accidents involving Teslas whose drivers were using Autopilot, including a fatality in China, but it is not clear what role, if any, the driving assistance system played. There have also been reports of serious accidents prevented because of Autopilot.\nPolice in Ratzeburg said in a statement on Thursday that the 50-year-old driver from Brandenburg drove into the back of a Danish tour bus as it was returning to the inside lane after overtaking. Police said they planned to investigate “why (Autopilot) did not work.”\nThe Tesla driver was slightly injured but none of the 29 bus passengers were hurt, the statement added. Authorities were not immediately reachable by telephone after hours on Thursday.\nTesla denied that Autopilot was at fault, saying the bus swerved into the car’s lane and side-swiped the Tesla, making a collision “unavoidable,” the spokeswoman said.\n“We can only do so much to prevent an accident,” she said, adding that Tesla was in contact with German police.\nReuters was not immediately able to reach German police for comment.\nThe Silicon Valley automaker said this month it was updating the semi-autonomous driving system with new limits on hands-off driving and other improvements.\nReporting by Georgina Prodhan in Frankfurt and Alexandria Sage in San Francisco; Editing by Emma Thomasson, Matthew Lewis and Lisa Shumaker\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the JSON-format response for the provided news article:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"A Tesla Motors Inc car operating under Autopilot collided with a bus in northern Germany, but the electric car maker said a collision was unavoidable because the bus swerved into the vehicle’s lane. The car’s driver told Tesla following the crash that he was using Autopilot when the incident occurred but that it was unrelated to the accident. Tesla’s Autopilot, introduced last October, helps drivers steer and stay in lanes and is intended to assist, but not replace, drivers. It has been the focus of intense scrutiny since it was revealed in July that a Tesla Model S driver was killed while using the technology in a May 7 collision with a truck in Florida. Police in Ratzeburg said in a statement on Thursday that the 50-year-old driver from Brandenburg drove into the back of a Danish tour bus as it was returning to the inside lane after overtaking. Police said they planned to investigate 'why (Autopilot) did not work.' Tesla denied that Autopilot was at fault, saying the bus swerved into the car’s lane and side-swiped the Tesla, making a collision 'unavoidable,' the spokeswoman said.\"\n}\nThis news article is relevant to Tesla Autopilot accidents because it discusses an incident involving a Tesla vehicle operating under Autopilot that collided with a bus in Germany. Tesla states that the collision was unavoidable because the bus swerved into the vehicle's lane. The driver confirmed using Autopilot but claimed that it was unrelated to the accident. The article also mentions the scrutiny on Tesla's Autopilot system since a fatal accident in Florida and reports of other accidents involving Autopilot. Police plan to investigate why Autopilot did not work in this case, and Tesla denied fault, attributing the collision to the bus swerving into the car's lane.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-tesla-germany-crash-idUSKCN11Z2IW","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":45,"Time":null,"Title":"U.S. has 'real concern' about Tesla Autopilot driver interaction","Content":"WASHINGTON, May 23 (Reuters) - U.S. Transportation Secretary Pete Buttigieg said Tuesday there are concerns about the interaction between Tesla's advanced driver assistance system Autopilot and drivers that is the subject of an ongoing government investigation.\nSince August 2021, the National Highway Traffic Safety Administration (NHTSA) has been investigating a series of Tesla crashes involving Autopilot and with parked emergency vehicles and whether Tesla vehicles adequately ensure drivers are paying attention.\n\"There is a real concern that's not limited to the technology itself but the interaction between the technology and the driver,\" Buttigieg told reporters when asked about the Tesla Autopilot probe.\nButtigieg said advanced driver assistance systems can benefit drivers. \"The question is not are they absolutely free of problems or 1000% foolproof,\" Buttigieg said. \"The question is, how can we be sure that they will lead to a better set of safety outcomes ... This technology has a lot of promise. We just have to make sure it unfolds in a responsible fashion.\"\nAdvertisement · Scroll to continue\nTesla did not respond to a request for comment.\nIn January, Acting NHTSA Administrator Ann Carlson said the agency was \"working really fast\" on the Tesla Autopilot probe.\nIn June 2022, NHTSA upgraded to an engineering analysis its defect probe into 830,000 Tesla vehicles with Autopilot and involving crashes with parked emergency vehicles.\nNHTSA has said evidence suggested drivers in most crashes under review had complied with Tesla's alert strategy that seeks to compel driver attention, raising questions about its effectiveness.\nAdvertisement · Scroll to continue\nIn 2020, the National Transportation Safety Board criticized Tesla's \"ineffective monitoring of driver engagement\" after a 2018 fatal Autopilot crash, and said NHTSA had provided \"scant oversight.\"\nNHTSA in April opened an probe into whether Autopilot or other advanced driver systems were in use when a Tesla struck a 17-year-old student who exited a school bus in North Carolina.\nSince 2016, NHTSA has opened 40 Tesla special crash investigations where driver assistance systems like Autopilot were suspected of being used, with 20 crash deaths reported. The agency has ruled out Tesla Autopilot use in three other special crash investigations.\nReporting by David Shepardson\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Based on the provided news article, here is the JSON-format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is about concerns regarding the interaction between Tesla's advanced driver assistance system Autopilot and drivers, which is the subject of an ongoing government investigation. Since August 2021, the National Highway Traffic Safety Administration (NHTSA) has been investigating a series of Tesla crashes involving Autopilot, particularly those with parked emergency vehicles, and whether Tesla vehicles adequately ensure drivers are paying attention. U.S. Transportation Secretary Pete Buttigieg expressed 'real concern' about this interaction between the technology and the driver. This news is directly related to Tesla Autopilot accidents and the ongoing investigation into their safety and driver interaction.\"\n}\nThe news is relevant to Tesla Autopilot accidents because it discusses concerns about the interaction between Tesla's Autopilot system and drivers, specifically mentioning ongoing government investigations and the series of Tesla crashes involving Autopilot.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/us-says-real-concern-about-tesla-autopilot-driver-interaction-2023-05-23\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":51,"Time":"June 16, 2022","Title":"Tesla leads in driver-assisted technology crashes -U.S. data","Content":"Companies\nTesla Inc\nFollow\nHonda Motor Co Ltd\nFollow\nAlphabet Inc\nFollow\nShow more companies\nWASHINGTON, June 15 (Reuters) - Tesla Inc (TSLA.O) reported 273 vehicle crashes since July involving advanced driving assistance systems, more than any other automaker, according to data U.S. auto safety regulators released on Wednesday.\nAutomakers and tech companies reported more than 500 crashes since June 2021, when the National Highway Traffic Safety Administration (NHTSA) issued an order requiring the information. Two U.S. Senators called for a deeper probe of the \"out-of-control industry,\" and a U.S. safety board said the data was not standardized so it was hard to evaluate performance of each carmaker's system.\nCar companies are rushing to add driver assistance systems, saying these improve safety by handling some maneuvers. U.S. regulators are trying to understand the practical effect of the changes. But automakers collect and report data in different ways, making it difficult to evaluate systems performance.\nDemocratic Senators Ed Markey and Richard Blumenthal told NHTSA in a letter that \"publicizing the data alone is not enough. We urge NHTSA to ... shed needed light on this out-of-control industry and impose guardrails to prevent more deadly crashes.\"\nAdvertisement · Scroll to continue\nThe current report contains \"a ‘fruit bowl’ of data with a lot of caveats, making it difficult\" to understand, National Transportation Safety Board (NTSB) Chair Jennifer Homendy said in a statement. \"Tesla collects a huge amount of high-quality data, which may mean they’re overrepresented in NHTSA’s release.\"\nTesla's advanced driver assistant software dubbed \"Full Self Driving\" has also created confusion about vehicle capabilities.\nAdvertisement · Scroll to continue\nThe senators raised alarm about the high number of Tesla crashes. \"We worry that some drivers today are using the technology as a convenience feature and are placing themselves and other road users in danger,\" Markey and Blumenthal wrote.\nNHTSA ordered companies to quickly report all crashes involving advanced driver assistance systems (ADAS) and vehicles equipped with automated driving systems being tested on public roads.\nAdvertisement · Scroll to continue\nOf the 392 such crashes reported by a dozen automakers since July, six deaths were reported and five serious injuries. Honda Motor (7267.T) identified 90 crashes.\nCompanies also reported 130 crashes involving prototype automated driving systems, while 108 involved no injuries and one was a serious injury crash.\nNHTSA said Alphabet Inc's (GOOGL.O) self-driving car unit Waymo reported 62 crashes involving automated driving systems, while General Motors' (GM.N) Cruise had 23.\nAdvertisement · Scroll to continue\nWaymo said its crashes were not high severity and one-third were in manual mode. Airbags deployed in only two crashes.\nCruise said it \"has logged millions of miles in one of the most complex urban driving environments because saving lives is our chief aim.\"\nThe NTSB reiterated a five-year-old recommendation that NHTSA require automakers to provide standardized crash and usage data. It made the recommendation after the a 2016 crash killed a Tesla driver using the company's Autopilot system, which the company website says \"enables your car to steer, accelerate and brake automatically within its lane\" but still requires the driver's attention.\nNHTSA said the first batch of data has already been used to trigger investigations and recalls and helped inform existing defect probes.\nThe data \"will help our investigators quickly identify potential defect trends,\" NHTSA Administrator Steven Cliff said, cautioning the raw number of incidents reported per manufacturer \"is by itself inadequate to draw conclusions.\"\nThe agency emphasized crashes are tracked by individual automakers in different ways and discouraged comparisons in part because there are no comprehensive metrics on how widely each system is used.\nTesla did not respond to requests for comment.\nHonda told Reuters it had found no defects in the systems and its crash reports were based on unverified customer statements \"to comply with NHTSA’s 24-hour reporting deadline.\"\nNo other automaker reported more than 10 ADAS crashes during the period.\nDespite the limitations, NHTSA said the data was essential to quickly spot potential defects or safety trends. Incidents that occur when an advanced system was engaged within 30 seconds of a crash must be reported within 24 hours to NHTSA.\nThe agency plans to release new data monthly.\nNHTSA has been scrutinizing Autopilot and said last week it was upgrading its probe into 830,000 Tesla vehicles with the system, a required step before it could seek a recall. The regulator had opened a preliminary evaluation to assess the performance of Autopilot after about a dozen crashes in which Tesla vehicles struck stopped emergency vehicles. read more\nSeparately, NHTSA has opened 35 special crash investigations involving Tesla vehicles in which ADAS was suspected of being used. A total of 14 crash deaths have been reported in those Tesla investigations, including a May California crash that killed three people.\nReporting by David Shepardson; Editing by Muralikumar Anantharaman and David Gregorio\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n   \"Is_relevant\": true,\n   \"Specific_information\": \"The news is relevant to Tesla Autopilot accidents because it reports that Tesla Inc has reported 273 vehicle crashes involving advanced driving assistance systems, more than any other automaker, according to data released by U.S. auto safety regulators. This indicates a significant number of accidents related to advanced driver assistance systems, including Tesla's Autopilot, which falls under the category of Tesla Autopilot accidents.\"\n}\nThis response confirms that the news is indeed relevant to Tesla Autopilot accidents, as it discusses the high number of crashes involving advanced driving assistance systems, including Tesla's Autopilot.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/tesla-honda-among-those-report-advanced-driving-systems-vehicle-crashes-us-2022-06-15\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":42,"Time":"Fri 30 Sep 2016 20.31 CEST","Title":"Tesla Autopilot not to blame for bus accident in Germany, company says","Content":"Tesla claims that one its cars operating in Autopilot mode was not to blame for a collision with a bus in Germany on Wednesday.\nThe company claims the accident in Ratzeburg was unavoidable because the bus swerved into the vehicle’s lane.\nThe car’s driver told Tesla following the crash that he was using Autopilot when the incident occurred but that it was unrelated to the accident, a Tesla spokeswoman said on Thursday.\n“We have spoken to our customer, who confirmed that Autopilot was functioning properly and that his use of Autopilot was unrelated to the accident,” Tesla said in a statement.\nReuters could not confirm independently what the car driver told Tesla and the circumstances that caused the bus to swerve.\nAutopilot supplier disowns Tesla for 'pushing the envelope on safety'\nRead more\nTesla’s Autopilot, introduced last October, helps drivers steer and stay in lanes and is intended to assist, but not replace, drivers. It has been the focus of intense scrutiny since it was revealed in July that a Tesla Model S driver was killed while using the technology in a 7 May collision with a truck in Florida.\nSince then, there have been various reports of accidents involving Teslas whose drivers were using Autopilot, including a fatality in China, but it is not clear what role, if any, the driving assistance system played. There have also been reports of serious accidents prevented because of Autopilot.\nPolice in Ratzeburg said in a statement on Thursday that the 50-year-old driver from Brandenburg drove into the back of a Danish tour bus as it was returning to the inside lane after overtaking. Police said they planned to investigate “why [Autopilot] did not work”.\nThe Tesla driver was slightly injured but none of the 29 bus passengers were hurt, the statement added. Authorities were not immediately reachable by telephone after hours on Thursday.\nTesla denied that Autopilot was at fault, saying the bus swerved into the car’s lane and side-swiped the Tesla, making a collision “unavoidable”, the spokeswoman said. “We can only do so much to prevent an accident,” she said, adding that Tesla was in contact with German police.\nReuters was not immediately able to reach German police for comment.\nThe Silicon Valley automaker said this month it was updating the semi-autonomous driving system with new limits on hands-off driving and other improvements.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n\"Tesla Autopilot not to blame for bus accident in Germany, company says.\",\n\"Tesla claims that one of its cars operating in Autopilot mode was not to blame for a collision with a bus in Germany.\",\n\"The company claims the accident in Ratzeburg was unavoidable because the bus swerved into the vehicle’s lane.\",\n\"The car’s driver told Tesla following the crash that he was using Autopilot when the incident occurred but that it was unrelated to the accident, a Tesla spokeswoman said on Thursday.\",\n\"“We have spoken to our customer, who confirmed that Autopilot was functioning properly and that his use of Autopilot was unrelated to the accident,” Tesla said in a statement.\",\n\"Police in Ratzeburg said in a statement on Thursday that the 50-year-old driver from Brandenburg drove into the back of a Danish tour bus as it was returning to the inside lane after overtaking. Police said they planned to investigate 'why [Autopilot] did not work.'\",\n\"Tesla denied that Autopilot was at fault, saying the bus swerved into the car’s lane and side-swiped the Tesla, making a collision 'unavoidable,' the spokeswoman said.\"\n]\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2016\/sep\/30\/tesla-autopilot-bus-crash-germany","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":52,"Time":"April 29, 2021","Title":"U.S. safety board to release report on Texas Tesla crash within a month","Content":"Companies\nTesla Inc\nApril 28 (Reuters) - The U.S. transport safety board is working to publish as soon as possible a preliminary report on a recent Tesla (TSLA.O) vehicle crash in Texas in which local police said no one was in the driver's seat, a senior official told Reuters, acknowledging public confusion over the fatal accident.\n\"A preliminary report typically comes out in a month, but we are working hard to get it out as soon as possible because we recognize there is this confusion out there,\" Kristin Poland, Deputy Director of the U.S. National Transportation Safety Board (NTSB), said in an interview with Reuters.\nEarlier this month, a Tesla Model S smashed into a tree in Texas and burst into flames, killing one occupant found in the front passenger seat and the owner in the back seat. Police said it was apparent that there was no one in the driver's seat at the time of the crash. read more\nOn Monday, Tesla refuted police's claims, saying a deformed steering wheel suggested that someone was likely in the driver’s seat. read more\nAdvertisement · Scroll to continue\n\"We recognize that that’s not an ideal circumstance,\" Poland said, referring to public confusion over what happened during the accident. \"The sooner we can get the preliminary report out the better it will be for everybody to provide as much clarity as possible,\" she said, saying the report will be based on solid facts.\nThe veteran NTSB official declined to discuss any findings so far, saying that the probe by its experienced investigators is underway.\nAdvertisement · Scroll to continue\nThe NTSB has criticized Tesla's Autopilot after a series of investigations into fatal crashes, including a lack of adequate driver monitoring systems. read more\nTesla Chief Executive Elon Musk tweeted earlier that data downloaded by Tesla indicates the vehicle was not operating on Autopilot, its semi-autonomous driving system.\nTesla said it was not able to recover the data from an SD card from the burnt car at the time of the impact, but local authorities are working on that.\nAdvertisement · Scroll to continue\n\"We were able to knock down the bulk of the fire within 3 to 4 minutes,\" Palmer Buck, Fire Chief of The Woodlands Fire Department, told Reuters.\nFire officers cooled the battery area while an initial accident investigation took place, as a chemical chain reaction in the lithium ion battery pack continued to cause heat and occasional flames from the underside of the car.\nThe fire was fully extinguished once the investigation was complete, he said.\nReporting by Hyunjoo Jin in BERKELEY, California and David Shepardson in WASHINGTON Editing by Marguerita Choy\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Summary\": \"The news is relevant to Tesla Autopilot accidents.\",\n    \"Details\": [\n      \"The news reports on a recent Tesla vehicle crash in Texas in which local police stated that no one was in the driver's seat at the time of the accident.\",\n      \"The U.S. transport safety board is working to release a preliminary report on this Tesla crash as soon as possible due to public confusion surrounding the incident.\",\n      \"The report mentions that the Tesla Model S crashed into a tree and burst into flames, resulting in a fatality in the front passenger seat and the owner in the back seat.\",\n      \"Tesla refuted the police's claims, suggesting that someone was likely in the driver's seat based on the deformed steering wheel.\",\n      \"The NTSB has criticized Tesla's Autopilot system in the past, including concerns about driver monitoring systems.\",\n      \"Tesla CEO Elon Musk stated that data downloaded from the vehicle indicated that it was not operating on Autopilot, the company's semi-autonomous driving system.\",\n      \"The report mentions efforts to recover data from the burnt car's SD card by local authorities.\",\n      \"The fire department responded to the accident, and the fire was extinguished after an initial investigation.\"\n    ]\n  }\n}\nThe news is relevant to Tesla Autopilot accidents as it discusses a Tesla vehicle crash in Texas, concerns about whether someone was in the driver's seat, and the ongoing investigation by the U.S. transport safety board. It also mentions Tesla's response to the incident and the involvement of the fire department.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/amid-confusion-ntsb-release-report-texas-tesla-crash-soon-possible-2021-04-28\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":54,"Time":"Updated 9:16 AM EDT, Fri May 14, 2021","Title":"Elon Musk says Tesla is pushing ahead with ‘full self-driving’ a month after fatal crash","Content":"Washington, DC\nCNN\n — \nTesla seems ready to move forward with a broader roll out of its autonomous driving software after a federal government investigation raised questions about initial, headline-grabbing findings from a fatal Tesla crash in Spring, Texas, last month.\nTesla CEO Elon Musk teased upgrades and the expansion of his company’s driver assistance software in a series of tweets Wednesday.\nLocal police said immediately after the April crash that they were certain no one was in the Tesla’s driver’s seat at the time of the crash, which killed two men. But the federal government released a preliminary investigation into the April 17 crash this week, and said security camera footage showed the Tesla’s owner getting into the driver’s seat at the beginning of the trip. The car traveled about 550 feet before crashing, the report said.\nIt’s unclear if the driver would even have had time to climb out of the driver’s seat before the crash, or if the driver climbed into the rear seat following the crash.\nConstable Mark Herman, who leads the Harris County police precinct that initially investigated the crash and concluded the driver’s seat was empty, did not respond to multiple requests for comment.\nHerman’s initial claim had raised questions about the safety of Tesla’s Autopilot, its suite of driver assistance features.\nTesla owners warned of 'full self-driving' risks even before fatal crash\nTesla tells its drivers to remain in the driver’s seat and remain attentive when Autopilot is active. The software is only capable of handling limited driving tasks, and drivers need to be prepared to take over at any moment. But some drivers have posted clips of themselves exiting the driver’s seat, creating dangerous situations. Drivers have also died in crashes in which they had Autopilot active and were not paying enough attention to the road.\nThe National Transportation Safety Board, which is conducting the investigation into the fatal crash in Texas, has not concluded if Autopilot was active at the time of the crash. The safety agency may not complete and release its report for a year or longer. The National Highway Traffic Safety Administration (NHTSA) is also investigating, and has 28 active investigations into Tesla’s advanced driver assistance system, according to a spokesperson.\nThe NTSB told CNN Business that it tested a Model S P100D, the same Tesla model involved in the wreck, on the road where the crash occurred and found that adaptive cruise control, one of Autopilot’s feature, could engage, but Autosteer, the other Autopilot feature, which steers a vehicle, could not.\nMeanwhile, Tesla continues to develop its driver assistance systems. Musk said Wednesday that subscriptions for a more advanced version of Autopilot, “full self-driving,” will roll out in about a month.\n“I think we’re maybe a month or two away from wide beta. But these things are hard to predict accurately,” Musk tweeted.\nAmerica's electric cars need lithium so badly it may wipe out this species\nMusk is known for setting aggressive deadlines and sometimes missing them. He said in March that a button to download the beta “full self-driving” software would be added in 10 days.\nTesla owners have continued to wait for “full self-driving” since 2016. A small group has been allowed to test the software since last fall. Drivers who test the software have said they’ve seen significant improvements.\nMusk said Wednesday that there were “gigantic” changes to the “full self-driving” beta as Tesla polished the software. He also said the upgraded system would no longer brake unexpectedly when driving through shadows under bridges and overpasses, an apparent bug many drivers have dubbed “phantom braking.”\nWhile the high-profile Texas crash has not appeared to hamper Tesla’s ability to roll out a long-promised feature, Tesla’s autonomous driving software has faced other controversies.\nCalifornia Highway Police said they arrested 25-year-old Tesla driver Param Sharma this week for reckless driving after he rode in the backseat of a driverless Tesla on a highway outside San Francisco. Days after his arrest ,Sharma appeared undeterred, again riding in the backseat of his Tesla, according to video from the Bay Area television station Fox 2.\nNHTSA is also investigating a May 5 crash in which a Tesla crashed into an overturned semi truck on a state road near Fontana, California, killing the Tesla driver. It’s unclear if Autopilot was active in the crash.\nWhile the Autopilot system is not perfect, it’s popular with many Tesla drivers. They say they feel less tired after using Autopilot on long road trips.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n\"The news is relevant to Tesla Autopilot accidents because it discusses Tesla's plans to move forward with its 'full self-driving' software.\",\n\"It mentions a fatal Tesla crash in Spring, Texas, last month, in which two men were killed.\",\n\"The federal government's preliminary investigation into the April 17 crash revealed that the Tesla's owner got into the driver's seat at the beginning of the trip, raising questions about the safety of Tesla's Autopilot.\",\n\"The article highlights that some Tesla drivers have been exiting the driver's seat while Autopilot is active, creating dangerous situations, and that drivers have died in such circumstances.\",\n\"The National Transportation Safety Board (NTSB) is conducting an investigation into the fatal crash in Texas to determine if Autopilot was active at the time of the crash.\",\n\"The National Highway Traffic Safety Administration (NHTSA) has 28 active investigations into Tesla's advanced driver assistance system.\",\n\"The news also mentions Tesla's plans to roll out subscriptions for a more advanced version of Autopilot, 'full self-driving,' in about a month.\"\n]\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2021\/05\/14\/cars\/tesla-autopilot-crashes-fsd\/index.html","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":55,"Time":"10:18 PM GMT+1•February 20, 2023","Title":"NHTSA requests info after Tesla crashes into fire truck","Content":"cameraImage Credits: Contra Costa County Fire Department\nThe National Highway Traffic Safety Administration (NHTSA) said Monday it has asked Tesla to provide more information after one of its vehicles crashed into a fire truck in California, Bloomberg reported.\nThe agency did not confirm to TechCrunch what kind of information it is seeking, but NHTSA likely wants to determine whether one of Tesla’s advanced driver assistance systems (ADAS) — Autopilot or Full Self-Driving (FSD) beta — was engaged at the time of the crash.\nThe Contra Costa County fire department tweeted about the incident Saturday, asking road users to slow down and move over when approaching emergency vehicles.\n“Truck 1 was struck by a Tesla while blocking I-680 lanes from a previous accident,” the tweet said. “Driver pronounced dead on-scene; passenger was extricated & transported to hospital. Four firefighters also transported for evaluation.”\nThe tweet included photos of the accident, including several of a completely totaled Tesla.\nSlow down and move over when approaching emergency vehicles. Truck 1 was struck by a Tesla while blocking I-680 lanes from a previous accident. Driver pronounced dead on-scene; passenger was extricated & transported to hospital. Four firefighters also transported for evaluation. pic.twitter.com\/YCGn8We1bK\n— Con Fire PIO (@ContraCostaFire) February 18, 2023\nNHTSA has opened dozens of special crash investigations (SCI) into Tesla vehicles where Autopilot was suspected of being used. Of the 48 SCIs that have been opened and closed between June 2016 and July 2022, 39 involved Teslas. And of those 39, only three were confirmed to have not involved Autopilot. NHTSA still has many open investigations into crashes involving Teslas, some of which were fatal. The agency doesn’t comment on open investigations.\nThis latest fatal crash comes a few days after Tesla issued a recall for 362,758 vehicles to update its FSD software after regulators said FSD could allow the vehicles to act unsafe around intersections and cause crashes. The recall followed a Super Bowl ad taken out by Tesla’s biggest hater, The Dawn Project, that called on regulators to ban FSD until critical safety defects are fixed.\nTesla has come under scrutiny from a range of federal and state regulators for the safety of its ADAS. Earlier this month, the Department of Justice requested information from Tesla on Autopilot and FSD, potentially as part of a criminal investigation into the company.\nDrivers sue Tesla for alleged false advertising of Autopilot and FSD software","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to Tesla Autopilot accidents because it reports a Tesla vehicle crashing into a fire truck in California. The National Highway Traffic Safety Administration (NHTSA) has requested more information to determine if Tesla's advanced driver assistance systems (ADAS), such as Autopilot or Full Self-Driving (FSD) beta, were engaged at the time of the crash. The incident resulted in the death of the driver, the hospitalization of the passenger, and the transportation of four firefighters for evaluation. NHTSA has opened numerous special crash investigations (SCIs) involving Tesla vehicles suspected of using Autopilot. Additionally, the news mentions a recent Tesla recall related to FSD software and ongoing scrutiny from federal and state regulators regarding the safety of Tesla's ADAS systems.\"\n}","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2023\/02\/20\/nhtsa-requests-info-after-tesla-crashes-into-fire-truck\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":41,"Time":"Published 2:18 AM GMT+2, August 6, 2022","Title":"US agency probes Tesla crashes that killed 2 motorcyclists","Content":"DETROIT (AP) — Two crashes involving Teslas apparently running on Autopilot are drawing scrutiny from federal regulators and point to a potential new hazard on U.S. freeways: The partially automated vehicles may not stop for motorcycles.\nThe National Highway Traffic Safety Administration sent investigation teams to two crashes last month in which Teslas collided with motorcycles on freeways in the darkness. Both were fatal.\nThe agency suspects that Tesla’s partially automated driver-assist system was in use in each. The agency says that once it gathers more information, it may include the crashes in an broader probe of Teslas striking emergency vehicles parked along freeways. NHTSA also is investigating over 750 complaints that Teslas can brake for no reason.\nThe first crash involving a motorcyclist happened at 4:47 a.m. July 7 on State Route 91, a freeway in Riverside, California. A white Tesla Model Y SUV was traveling east in the high occupancy vehicle lane. Ahead of it was a rider on a green Yamaha V-Star motorcycle, the California Highway Patrol said in a statement.\nAt some point, the vehicles collided, and the unidentified motorcyclist was ejected from the Yamaha. He was pronounced dead at the scene by the Fire Department.\nWhether or not the Tesla was operating on Autopilot remains under investigation, a CHP spokesman said.\nThe second crash happened about 1:09 a.m. July 24 on Interstate 15 near Draper, Utah. A Tesla Model 3 sedan was behind a Harley-Davidson motorcycle, also in an HOV lane. “The driver of the Tesla did not see the motorcyclist and collided with the back of the motorcycle, which threw the rider from the bike,” the Utah Department of Public Safety said in a prepared statement.\nThe rider, identified as Landon Embry, 34, of Orem, Utah, died at the scene. The Tesla driver told authorities that he had the vehicle’s Autopilot setting on, the statement said.\nMichael Brooks, acting executive director of the nonprofit Center for Auto Safety, called on NHTSA to recall Tesla’s Autopilot because it is not recognizing motorcyclists, emergency vehicles or pedestrians.\n“It’s pretty clear to me, and it should be to a lot of Tesla owners by now, this stuff isn’t working properly and it’s not going to live up to the expectations, and it is putting innocent people in danger on the roads,” Brooks said.\nSince 2016, NHTSA has sent teams to 39 crashes in which automated driving systems are suspected of being in use, according to agency documents. Of those, 30 involved Teslas, including crashes that caused 19 deaths.\nBrooks criticized the agency for continuing to investigate but not taking action. “What the Hell are they doing while these crashes continue to occur?” he asked. “Drivers are being lured into thinking this protects them and others on the roads, and it’s just not working.”\nTesla CEO Elon Musk has eliminated use of radar from his systems and relies solely on cameras and computer memory. Brooks and other safety advocates say the lack of radar hurts vision in the darkness.\nMessages were left seeking comment from Tesla, which has disbanded its media relations department.\nTesla has said that Autopilot and “Full Self-Driving” cannot drive themselves, and that drivers should be ready to intervene at all times.\nThe Los Angeles Times, meanwhile, on Friday reported that California’s Department of Motor Vehicles has accused Tesla of false advertising in its promotion of Autopilot and Full Self-Driving. The accusations came in complaints filed with the state’s Office of Administrative Hearings on July 28, the Times reported.\nIn a June interview, new NHTSA Administrator Steven Cliff said the agency is intensifying efforts to understand risks posed by automated vehicles so it can decide what regulations may be necessary to protect drivers, passengers and pedestrians. There are no federal regulations that directly cover either self-driving vehicles or those with partially automated driver-assist systems such as Autopilot.\nThe agency also says the technology holds great promise of reducing traffic crashes.\nNHTSA also has ordered all automakers and tech companies with automated driving systems to report all crashes. The agency released the first batch of data in June showing that nearly 400 crashes were reported over a 10-month period, including 273 with Teslas. But it cautioned against making comparisons, saying that Tesla’s telematics allow it to gather data in real time, much faster than other companies.\nTesla’s Autopilot keeps cars in their lane and a distance behind other vehicles. The company also is using selected owners to test “Full Self-Driving” software, which is designed to complete a route on its own with human supervision. Eventually, Musk says the cars will drive themselves, enabling a fleet of autonomous robotaxis that will boost Tesla’s earnings. In 2019, Musk had pledged to have the robotaxis running in 2020.\nHe said at the company’s annual shareholders’ meeting Thursday that “Full Self-Driving” is greatly improved, and he expects to make the software available by the end of the year to all owners who request it.\n_______\nAP News Researcher Rhonda Shafner in New York and writer Stefanie Dazio in Los Angeles contributed to this report.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n\"Two crashes involving Teslas apparently running on Autopilot are drawing scrutiny from federal regulators and point to a potential new hazard on U.S. freeways.\",\n\"The National Highway Traffic Safety Administration sent investigation teams to two crashes last month in which Teslas collided with motorcycles on freeways in the darkness. Both were fatal.\",\n\"The agency suspects that Tesla’s partially automated driver-assist system was in use in each.\",\n\"The second crash happened about 1:09 a.m. July 24 on Interstate 15 near Draper, Utah. A Tesla Model 3 sedan was behind a Harley-Davidson motorcycle, also in an HOV lane. “The driver of the Tesla did not see the motorcyclist and collided with the back of the motorcycle, which threw the rider from the bike,” the Utah Department of Public Safety said in a prepared statement. The rider, identified as Landon Embry, 34, of Orem, Utah, died at the scene. The Tesla driver told authorities that he had the vehicle’s Autopilot setting on, the statement said.\",\n\"Michael Brooks, acting executive director of the nonprofit Center for Auto Safety, called on NHTSA to recall Tesla’s Autopilot because it is not recognizing motorcyclists, emergency vehicles, or pedestrians.\",\n\"Since 2016, NHTSA has sent teams to 39 crashes in which automated driving systems are suspected of being in use, according to agency documents. Of those, 30 involved Teslas, including crashes that caused 19 deaths.\",\n\"The Los Angeles Times, meanwhile, on Friday reported that California’s Department of Motor Vehicles has accused Tesla of false advertising in its promotion of Autopilot and Full Self-Driving. The accusations came in complaints filed with the state’s Office of Administrative Hearings on July 28, the Times reported.\"\n]\n}","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/technology-12ba38bd863e2b128a8e2914179049e0","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":59,"Time":"26 February 2020","Title":"Tesla Autopilot crash driver 'was playing video game'","Content":"Tesla Autopilot crash driver 'was playing video game'\nPublished\n26 February 2020\nShare\nIMAGE SOURCE,\nREUTERS\nImage caption,\nThe driver of the Tesla Model X died shortly after the crash\nAn Apple employee who died after his Tesla car hit a concrete barrier was playing a video game at the time of the crash, investigators believe.\nThe US National Transportation Safety Board (NTSB) said the car had been driving semi-autonomously using Tesla's Autopilot software.\nTesla instructs drivers to keep their hands on the wheel in Autopilot mode.\nBut the NTSB said more crashes were foreseeable if Tesla did not implement changes to its Autopilot system.\nThe authority has published the results of a two-year investigation, following the crash in March 2018.\nTesla's Autopilot software steered the vehicle into the triangular \"gore area\" at a motorway intersection, and accelerated into a concrete barrier.\nImage caption,\nThe Tesla entered the gore area from Highway 101 and hit the barrier\nThe front of the Tesla separated from the rear, causing two other drivers to crash.\nThe Tesla driver, 38-year-old Walter Huang, was taken to hospital but died of his injuries. The other drivers survived.\nThe NTSB said:\nThe Tesla driver had not taken control of the car because he had been distracted by a smartphone video game\nThe Tesla's collision avoidance system was \"not designed to detect the crash [barrier]\"\nTesla's Autopilot system did not \"provide an effective means of monitoring the driver's engagement\"\nThe use of Tesla's Autopilot software has been implicated in several crashes.\nThe system lets the car operate semi-autonomously, changing lanes and adjusting its speed.\nBut critics say the \"Autopilot\" branding makes some drivers think the car is driving fully autonomously.\nThe NTSB said the driver had been \"over-reliant\" on the software.\nTesla does instruct drivers to keep their hands on the wheel when using Autopilot, and an audible warning sounds if they fail to do so.\nBut the NTSB said \"monitoring of driver-applied steering wheel torque is an ineffective surrogate measure of driver engagement\".\n\"If Tesla does not incorporate system safeguards that limit the use of the Autopilot system to those conditions for which it was designed, continued use of the system beyond its operational design domain is foreseeable and the risk for future crashes will remain,\" it said.\nRecommendations\nThe NTSB ended its report with several recommendations including:\nimproving collision avoidance systems to include common obstacles such as traffic barriers\nevaluating Tesla's Autopilot to determine whether the ability to operate it \"outside the intended operational design\" posed an unreasonable risk to safety\npreventing automation complacency in drivers\nrequiring all new passenger vehicles with semi-autonomous features to be equipped with a driver monitoring system that meets new standards\nIt also suggested smartphone manufacturers should develop a \"distracted driving lockout mechanism\" to \"disable any driver-distracting functions when a vehicle is in motion but that allows the device to be used in an emergency\".\nAnd it urged Apple to \"implement a company policy that bans the non-emergency use of portable electronic devices while driving by all employees and contractors\".\nThe NTSB also found a impact-absorbing crash barrier hit by the Tesla had been \"in a damaged and non-operational condition at the time of the collision\".\nIt said the California Highway Patrol had failed to report damage following a previous crash and it was \"likely\" the Tesla driver would have survived the crash if the barrier had been replaced.\nBBC News has contacted Tesla for its response.\nMore on this story\nTesla was on Autopilot in fatal crash\nPublished\n31 March 2018","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": [\n        \"The news is about a Tesla Autopilot crash where the driver was playing a video game at the time of the crash.\",\n        \"The US National Transportation Safety Board (NTSB) confirmed that the Tesla car was driving semi-autonomously using Tesla's Autopilot software.\",\n        \"The Tesla Autopilot software steered the vehicle into a concrete barrier at a motorway intersection, resulting in a fatal crash.\",\n        \"The driver, Walter Huang, was distracted by a smartphone video game and did not take control of the car in time to prevent the crash.\",\n        \"The NTSB criticized Tesla's Autopilot system for not providing an effective means of monitoring the driver's engagement and for the lack of safeguards to limit its use to intended operational conditions.\",\n        \"The NTSB made recommendations, including improving collision avoidance systems, evaluating the use of Autopilot outside its intended operational design, and requiring driver monitoring systems in vehicles with semi-autonomous features.\",\n        \"The news article mentions that Tesla had been involved in previous incidents where the Autopilot system was implicated in crashes.\"\n    ]\n}\nThis news is relevant to Tesla Autopilot accidents because it describes a fatal crash involving a Tesla vehicle operating in Autopilot mode, with the driver being distracted by a video game, which led to the collision with a concrete barrier. The report also highlights concerns and recommendations related to the Autopilot system's safety and driver engagement.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-51645566","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":58,"Time":"Published 8:09 PM EST, Tue February 25, 2020","Title":"Tesla and regulators need to do more to prevent Autopilot deaths, safety agency says","Content":"Washington, DC\nCNN Business\n — \nThe National Transportation Safety Board called for Tesla, regulators and others to improve the safety of automated driving technologies. The recommendations follow an investigation into the fatal crash of a distracted Tesla driver who was using the company’s Autopilot system two years ago.\nWalter Huang had a game active on his smartphone when his Tesla Model X crashed into a highway barrier in Mountain View, California, in March 2018, NTSB investigators found. At the time, Huang was using Autopilot, Tesla’s partial self-driving system which features cruise control and lanekeeping assistance technology. He did not brake or steer away from the barrier.\n'Terrifying but fantastic:' New Tesla feature sparks awe and mayhem\nTesla drivers are supposed to monitor the Autopilot system and take control of the vehicle if the car does anything unsafe. And Tesla is supposed to alert the driver if the system detects they are not engaged.\nIn this case, the SUV steered toward the barrier without providing an alert to the driver, the NTSB found. The car’s forward collision warning system did not issue an alert before striking the barrier and automatic emergency braking did not activate, the agency found.\nThe NTSB previously called on Tesla to make changes to its Autopilot technology following a 2016 fatal crash. It reiterated those recommendations Tuesday, advising Tesla to limit the use of Autopilot to the roads it was designed for and to develop a better way to sense if drivers are distracted while using the technology.\nCurrently, Tesla’s Autopilot technology detects when a driver is engaged by whether or not they have recently turned the steering wheel, but the NTSB said that’s not enough to determine whether the driver is paying attention.\nWalter Huang was killed when his Tesla struck a highway barrier.\nAP\nThe NTSB said it never heard back from Tesla after its 2017 recommendations.\nRobert Molloy, a director of highway safety at NTSB, and other officials at the agency, also criticized the US Department of Transportation’s National Highway Traffic Safety Administration for failing to regulate partial autonomous driving systems, such as Tesla’s Autopilot. It called for more thorough testing of these systems to ensure they’re safe.\n“There’s nothing more disappointing than investigating a crash, coming up with a good solution and Tesla’s response is no response, NHTSA’s response is we don’t plan to do anything and another crash happens,” said Molloy.”They need to do more.”\nTesla did not immediately respond to a request for comment. An NHTSA spokesman said in a statement that crashes that involve distracted drivers are a major concern and said it’s reviewing NTSB’s report.\nIn 2018, there were 2,841 fatalities due to crashes involving distracted drivers, a 12.4% decrease from the year before, according to the most recent data from the NHTSA.\nNTSB officials also discussed the role employers can play in improving road safety.\nThe agency specifically called out Apple, where Huang worked as an engineer.\nSelf-driving cars: Hype-filled decade ends on sobering note\n“Apple has yet to recognize their own responsibility as an employer,” said NTSB chairman Robert Sumwalt. “They have failed to say of our over 135,000 employees, that we care about you. And we don’t want you to go out and kill yourself or others on the roadway.”\n“We expect our employees to follow the law,” an Apple spokesman said when asked for comment.\nThe NTSB also called on smartphone makers to develop a mechanism that automatically disables functions that could distract a driver when a vehicle is moving.\nIn addition, it recommended that the Occupational Safety and Health Administration revise its distracted driving initiatives to increase employers’ awareness of the need for prohibiting cell phone use while driving.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to Tesla Autopilot accidents because it discusses the National Transportation Safety Board's (NTSB) recommendations following an investigation into a fatal Tesla crash involving the use of Tesla's Autopilot system. The crash occurred in March 2018 when a Tesla Model X, driven by Walter Huang, crashed into a highway barrier while Huang was using Autopilot. The NTSB found that the SUV steered toward the barrier without providing an alert to the driver, and the car's forward collision warning and automatic emergency braking systems did not activate. The NTSB reiterated its recommendations for Tesla to improve Autopilot technology and ensure better driver engagement. It also criticized the US Department of Transportation's National Highway Traffic Safety Administration for failing to regulate partial autonomous driving systems like Tesla's Autopilot.\"\n}","From":"cnn.com","Url":"https:\/\/www.cnn.com\/2020\/02\/25\/tech\/tesla-autopilot-safety-ntsb\/index.html","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":60,"Time":"13 July 2016","Title":"Tesla says autopilot involved in second car crash","Content":"Tesla says autopilot involved in second car crash\nPublished\n13 July 2016\nShare\nIMAGE SOURCE,\nREUTERS\nImage caption,\nTesla cars use radar and other sensors to detect other vehicles and road markings\nBy Leo Kelion\nTechnology desk editor\nTesla has admitted that its autopilot feature was activated when one of its cars crashed on Sunday.\nHowever, the electric carmaker has suggested that the function was not being used correctly at the time.\nThe motorist survived the accident, but another Tesla owner died in an earlier crash that occurred when the driver-assist function failed to detect another vehicle in its path.\nChief executive Elon Musk said Tesla had no plans to disable autopilot.\nHowever, he told the Wall Street Journal that his company would publish a blog highlighting how drivers should make use of the technology.\nHe also tweeted that it was right that Tesla should be \"taking the heat for customer safety\".\nIMAGE SOURCE,\nTWITTER\nThe California-based carmaker has previously blogged that \"customers using autopilot are statistically safer than those not using it at all\".\nTesla's deployment of the technology is being investigated by the US road safety watchdog.\n'Ignored alerts'\nThe latest crash, near Cardwell, Montana, saw a Model X car swerve to hit wooden rails next to a two-lane road.\n\"This vehicle was being driven along an undivided mountain road shortly after midnight with autosteer enabled,\" a spokeswoman told the BBC, referring to autopilot's steering function.\n\"The data suggests that the driver's hands were not on the steering wheel, as no force was detected on the steering wheel for over two minutes after autosteer was engaged - even a very small amount of force, such as one hand resting on the wheel, will be detected.\nIMAGE SOURCE,\nTESLA\nImage caption,\nThe crash in Montana involved a Model X car\n\"This is contrary to the terms of use that are agreed to when enabling the feature and the notification presented in the instrument cluster each time it is activated.\n\"As road conditions became increasingly uncertain, the vehicle again alerted the driver to put his hands on the wheel.\n\"He did not do so, and shortly thereafter the vehicle collided with a post on the edge of the roadway.\n\"Autosteer... is best suited for highways with a centre divider.\n\"We specifically advise against its use at high speeds on undivided roads.\"\nCNN reported that the car had lost one of its wheels in the crash, but neither the driver nor his passenger had been injured.\nIt said the man - who had identified himself only as Pang - had told a police officer the alerts given by the car had been in English but he spoke Mandarin.\nMultiple probes\nTesla released details of the incident more quickly than it had done for the earlier crash in Florida.\nThe US Securities and Exchange Commission said on Monday that it was investigating why the company took until 30 June to publicly disclose the fatality - 54 days after it happened and nine days after it first informed the National Highway Traffic Safety Administration.\nIMAGE SOURCE,\nAP\nImage caption,\nJoshua Brown died after his Tesla S crashed while using autopilot\nTesla had raised funds from investors in the interim.\n\"I didn't know there had been an autopilot incident at the time of the fundraising,\" Mr Musk has said. \"What we told NHTSA was just that somebody died - it wasn't that there was an autopilot incident.\"\nOn Tuesday, the traffic safety body released a letter it had sent to Tesla detailing its own investigation into the accident.\nIt has asked for details about all updates made to autopilot since it was enabled last year, as well as information about forthcoming changes.\nThe watchdog is also considering whether autopilot was in use during a third collision involving a Tesla car that occurred on 1 July.\nProPilot unveiled\nAlthough Tesla says motorists should remain \"engaged and aware\" while using autopilot, critics have suggested that it was \"inevitable\" that users would become distracted.\nIMAGE SOURCE,\nAP\nImage caption,\nNissan is introducing a self-steering facility in minivans that go on sale in July\nNevertheless, other car makers are pursuing a similar strategy.\nNissan has unveiled ProPilot - lane-detecting technology that allows a car to follow the vehicle ahead, braking and accelerating as necessary without human direction.\nJapan will be the first market to get the innovation when it is included in a minivan released next month and Nissan planned to offer ProPilot in European models next year.\nAs a safety measure, the company has designed the facility to turn itself off if the driver removed his or her hands from the wheel for more than a brief period.\nIMAGE SOURCE,\nJAGUAR LAND ROVER\nImage caption,\nLand Rover is testing a technology that can detect different off-road driving conditions\nIn another related development, Jaguar Land Rover has announced plans to test more than 100 self-driving research vehicles on roads near Coventry, England before the end of 2020.\nMore on this story\nTesla stock rises on masterplan tweet\nPublished\n11 July 2016\nSecond Tesla crash probed in US\nPublished\n7 July 2016\nUS investigates fatal Tesla crash\nPublished\n1 July 2016\nRelated Internet Links\nTesla\nNissan\nJaguar Land Rover\nThe BBC is not responsible for the content of external sites.","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"Tesla has admitted that its autopilot feature was activated when one of its cars crashed on Sunday. The motorist survived the accident, but another Tesla owner died in an earlier crash that occurred when the driver-assist function failed to detect another vehicle in its path. Chief executive Elon Musk said Tesla had no plans to disable autopilot. However, he told the Wall Street Journal that his company would publish a blog highlighting how drivers should make use of the technology.\"\n}\nThe news is relevant to Tesla Autopilot accidents because it mentions two key points:\nThe autopilot feature was activated when one of Tesla's cars crashed.\nThere was a previous Tesla accident where the driver-assist function failed to detect another vehicle, resulting in a fatality.\nThese details indicate a connection to Tesla Autopilot accidents.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-36783345","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":63,"Time":"15 December 2021","Title":"Tesla Model 3: Paris' largest taxi firm suspends cars after fatal crash","Content":"Tesla Model 3: Paris' largest taxi firm suspends cars after fatal crash\nPublished\n15 December 2021\nShare\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nFrance's transport minister said there was no suggestion that there was a technical issue with the car\nParis' largest taxi firm, G7, has suspended the use of Tesla Model 3 cars in its fleet, after one was involved in a fatal accident over the weekend.\nOne person was killed and another 20 injured, after a driver lost control of the vehicle.\nTesla has denied any technical problem with the car, which has self-driving features such as automatic steering.\nParis prosecutors have opened an investigation into charges of manslaughter and unintentional injury.\nOn Wednesday, France's Transport Minister Jean-Baptiste Djebbari also said there was no suggestion, at this stage, that the accident was linked to a technical problem.\nTesla Europe's chief executive told the minister there had been no safety alerts about the model when they spoke on Tuesday night.\nTesla Autopilot 'tricked' to run without driver\nTesla crash driver 'was playing video game'\nThe accident involved an off-duty taxi driver who had been taking his family to a restaurant, reports say.\nFrench media said the vehicle struck two pedestrians, a traffic light and a van.\nTesla's assistive technology allows its vehicles to automatically steer, accelerate and brake.\nBut the firm has been accused of being misleading, since the technology does not automatically drive the car, and drivers are required to maintain control and attention at all times.\nMedia caption,\nThis driver received a ban after being caught in the passenger seat in England\nTesla has marketed the feature as an \"Autopilot\" and promised \"full self-driving\", which is now available to some users in a beta version.\nUsers have abused the system frequently in the past, with examples ranging from using their phones while the car drives unattended to switching car seats and leaving no driver at the wheel.\nEarlier this year, US authorities opened an official investigation into Autopilot system, following 11 Tesla crashes since 2018 involving emergency vehicles.\nRelated Topics\nCar Technology\nFrance\nTesla\nDriverless cars\nTaxis\nParis","Relevant":true,"Overview":"Based on the provided news article, here's the JSON-format response:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Reason\": \"The news is relevant to Tesla Autopilot accidents because it involves a fatal accident with a Tesla Model 3, a vehicle equipped with self-driving features such as automatic steering.\",\n    \"Detailed_information\": \"Paris' largest taxi firm, G7, suspended the use of Tesla Model 3 cars in its fleet after one of these vehicles was involved in a fatal accident. The accident resulted in one fatality and 20 injuries when the driver lost control of the vehicle. Tesla, however, denied any technical problem with the car, highlighting its self-driving features like automatic steering. Paris prosecutors have opened an investigation into charges of manslaughter and unintentional injury. The news also mentions that Tesla's assistive technology allows its vehicles to automatically steer, accelerate, and brake. It points out that Tesla has marketed this feature as 'Autopilot' and promised 'full self-driving.' Additionally, it mentions that US authorities had previously opened an investigation into the Autopilot system following multiple Tesla crashes since 2018.\"\n  }\n}\nThe news is relevant to Tesla Autopilot accidents because it involves a fatal accident with a Tesla Model 3, which is equipped with self-driving features like automatic steering. The detailed information provided in the news article supports this relevance.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/world-europe-59647069","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":57,"Time":null,"Title":"Tesla says fatal crash involved Autopilot","Content":"cameraImage Credits: Tesla\nTesla has provided another update to last week’s fatal crash. As it turns out, Tesla said the driver had Autopilot on with the adaptive cruise control follow-distance set to minimum. However, it seems the driver ignored the vehicle’s warnings to take back control.\n“The driver had received several visual and one audible hands-on warning earlier in the drive and the driver’s hands were not detected on the wheel for six seconds prior to the collision,” Tesla wrote in a blog post. “The driver had about five seconds and 150 meters of unobstructed view of the concrete divider with the crushed crash attenuator, but the vehicle logs show that no action was taken.”\nThe promise of Tesla’s Autopilot system is to reduce car accidents. In the company’s blog post, Tesla notes Autopilot reduces crash rates by 40 percent, according to an independent review by the U.S. government. Of course, that does not mean the technology is perfect in preventing all accidents.\nAs Tesla previously noted, the crash was so severe because the middle divider on the highway had been damaged in an earlier accident. Tesla also cautioned that Autopilot does not prevent all accidents, but it does make them less likely to occur.\nNo one knows about the accidents that didn’t happen, only the ones that did. The consequences of the public not using Autopilot, because of an inaccurate belief that it is less safe, would be extremely severe. There are about 1.25 million automotive deaths worldwide. If the current safety level of a Tesla vehicle were to be applied, it would mean about 900,000 lives saved per year. We expect the safety level of autonomous cars to be 10 times safer than non-autonomous cars.\nIn the past, when we have brought up statistical safety points, we have been criticized for doing so, implying that we lack empathy for the tragedy that just occurred. Nothing could be further from the truth. We care deeply for and feel indebted to those who chose to put their trust in us. However, we must also care about people now and in the future whose lives may be saved if they know that Autopilot improves safety. None of this changes how devastating an event like this is or how much we feel for our customer’s family and friends. We are incredibly sorry for their loss.\nThis development, of course, comes in light of a fatal accident involving one of Uber’s self-driving cars in Tempe, Arizona.\nMore TechCrunch\nTinder goes ultra-premium, Amazon invests in Anthropic and Apple explains its new AirPods\nVenture capital is opening the gates for defense tech\nMistral AI makes its first large language model free for everyone\nPitch Deck Teardown: Point.me's $10M Series A deck","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Summary\": \"Tesla says fatal crash involved Autopilot.\",\n    \"Details\": [\n      \"The news is relevant to Tesla Autopilot accidents because it mentions a fatal crash involving Autopilot.\",\n      \"Tesla confirmed that the driver had Autopilot engaged with the adaptive cruise control follow-distance set to the minimum.\",\n      \"The driver received multiple warnings to take control of the vehicle, including visual and audible alerts, but failed to do so.\",\n      \"The driver had a clear view of the concrete divider but took no action, as indicated by vehicle logs.\",\n      \"While Tesla promotes Autopilot as a technology that reduces crash rates, it acknowledges that it does not prevent all accidents.\",\n      \"Tesla highlighted the importance of using Autopilot correctly and expressed concern about the potential consequences if people avoid using it due to misconceptions about its safety.\",\n      \"The news indirectly compares Tesla's safety record to traditional non-autonomous cars, suggesting that autonomous cars could be ten times safer.\"\n    ]\n  }\n}\nIn summary, this news is relevant to Tesla Autopilot accidents because it discusses a fatal crash involving Autopilot, provides details about the driver's actions, mentions Tesla's safety claims regarding Autopilot, and highlights the importance of using Autopilot correctly.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2018\/03\/30\/tesla-says-fatal-crash-involved-autopilot\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":65,"Time":"12 September 2017","Title":"Tesla Autopilot 'partly to blame' for crash","Content":"Tesla Autopilot 'partly to blame' for crash\nPublished\n12 September 2017\nShare\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nThe US National Transportation Safety Board says Tesla was partly responsible for the Model S crash\nThe US National Transportation Safety Board (NTSB) has found that Tesla's Autopilot system was partly to blame for a fatal accident in which a Model S collided with a lorry.\nFederal investigators say Tesla \"lacked understanding\" of the semi-autonomous Autopilot's limitations.\nThe NTSB recommended that car manufacturers and regulators take steps to ensure such systems are not misused.\nIt said the collision should never have happened.\nThe crash, in May 2016, led to the death of Tesla driver Joshua Brown, 40. The driver of the truck, which was pulling a trailer, was unhurt.\n\"In this crash, Tesla's system worked as designed, but it was designed to perform limited tasks in a limited range of environments,\" Christopher Hart, a member of the NTSB said in a meeting to discuss the findings of its investigation.\n\"Tesla allowed the driver to use the system outside of the environment for which it was designed, and the system gave far more leeway to the driver to divert his attention to something other than driving.\"\nSemi-autonomous driving\nIn June, the NTSB released a 500-page report stating that in 37 minutes of driving, Mr Brown had his hands on the wheel for just 25 seconds.\nAt the time, the lorry driver told the Associated Press that Mr Brown had been watching a Harry Potter film when the collision occurred.\nHowever, the NTSB said today that there was no evidence that Mr Brown was watching a film while he was driving.\nThe safety board added that the lorry driver refused to be interviewed for its investigation.\nPrimary factors\nThe NTSB found that both Mr Brown and the lorry driver had sufficient sight distance to afford time for either party to have acted to prevent the collision.\nThe investigation found that Mr Brown's inattention and the lorry driver's unwillingness to give way were both primary factors of the crash.\nThe investigators could not find a reason why Mr Brown was inattentive.\nHowever, they confirmed that neither driver was fatigued or impaired by alcohol and drugs, and neither driver was distracted by a mobile device.\nFollowing the meeting, the NTSB's report included seven safety recommendations requiring car manufacturers to add safeguards to prevent automated vehicle control systems from being used outside the conditions for which they were designed.\n\"At Tesla, the safety of our customers comes first, and one thing is very clear: Autopilot significantly increases safety, as NHTSA has found that it reduces accident rates by 40%,\" the car-maker said in a statement.\n\"We appreciate the NTSB's analysis of last year's tragic accident and we will evaluate their recommendations as we continue to evolve our technology.\n\"We will also continue to be extremely clear with current and potential customers that Autopilot is not a fully self-driving technology and drivers need to remain attentive at all times.\"\nMore on this story\nFatal Tesla crash driver 'given warnings'\nPublished\n20 June 2017\nElon Musk rolls out cheapest Tesla yet\nPublished\n29 July 2017","Relevant":true,"Overview":"Based on the provided news article, here is the JSON-format response:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"The US National Transportation Safety Board (NTSB) has found that Tesla's Autopilot system was partly to blame for a fatal accident in which a Model S collided with a lorry. Federal investigators say Tesla 'lacked understanding' of the semi-autonomous Autopilot's limitations. The NTSB recommended that car manufacturers and regulators take steps to ensure such systems are not misused. It said the collision should never have happened. The crash, in May 2016, led to the death of Tesla driver Joshua Brown, 40. 'In this crash, Tesla's system worked as designed, but it was designed to perform limited tasks in a limited range of environments,' Christopher Hart, a member of the NTSB said in a meeting to discuss the findings of its investigation. 'Tesla allowed the driver to use the system outside of the environment for which it was designed, and the system gave far more leeway to the driver to divert his attention to something other than driving.' The NTSB found that both Mr. Brown and the lorry driver had sufficient sight distance to afford time for either party to have acted to prevent the collision. The investigation found that Mr. Brown's inattention and the lorry driver's unwillingness to give way were both primary factors of the crash. Following the meeting, the NTSB's report included seven safety recommendations requiring car manufacturers to add safeguards to prevent automated vehicle control systems from being used outside the conditions for which they were designed.\"\n}\nThis news is about a Tesla Autopilot-related accident, and the specific information provided explains the NTSB's findings regarding the accident, Tesla's Autopilot system's limitations, and the recommended safety measures.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-41242884","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":67,"Time":"MAY 29, 2018","Title":"Tesla hits parked California police vehicle; driver blames 'Autopilot'","Content":"By David Shepardson\nWASHINGTON (Reuters) - The driver of a Tesla Inc TSLA.O Model S crashed into an unoccupied, parked police vehicle in Laguna Beach, California, on Tuesday and the driver told investigators the Tesla was in \"Autopilot\" mode at the time, police said.\nThe driver suffered minor injuries, Laguna Beach Sergeant Jim Cota said, who posted photos of the crash scene showing extensive damage to the front end of the Tesla and the rear side of the police vehicle.\nAutopilot is a semi-autonomous technology that the company says is a form of advanced cruise control.\n“Tesla has always been clear that Autopilot doesn’t make the car impervious to all accidents,” the company said in a statement after the accident and could not immediately confirm the driver’s report that the vehicle was in Autopilot mode.\nSeveral crashes and fire incidents involving Tesla vehicles this year has been a near constant headache for Chief Executive Elon Musk, who boasts that his company’s vehicles are among the safest in the industry.\nSlideshow ( 2 images )\nEarlier this month, the U.S. National Highway Traffic Safety Administration (NHTSA) said it was sending a team to investigate the crash of a Tesla vehicle in South Jordan, Utah. The driver was traveling at 60 miles (97 km) per hour when the Model S smashed into a fire truck stopped at a red light, according to police.\nPolice in Utah said data from Tesla showed that the driver enabled Autopilot about 1 minute and 22 seconds before the crash. The report said she took her hands off the steering wheel “within two seconds” of engaging the system and then did not touch the steering wheel for the next 80 seconds, until the crash happened.\nNHTSA is also investigating a fatal crash in March that involved a Tesla Model X using Autopilot that struck a highway divider. The agency is also probing the January crash of a Tesla vehicle apparently traveling in Autopilot that struck a parked fire truck. Both of those incidents were also in California.\nThe National Transportation Safety Board is also probing four Tesla crashes that have occurred since last year, including three under review by NHTSA.\nTesla’s Model S owner’s manual warns some Autopilot functions “cannot detect all objects and may not brake\/decelerate for stationary vehicles or objects especially when traveling over 50 mph (80 kph)” and when a vehicle ahead of the driver “moves out of your driving path and a stationary vehicle or object is in front of you.”\nReporting by David Shepardson; Editing by Lisa Shumaker\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Tesla hits parked California police vehicle; driver blames 'Autopilot'\",\n        \"Description\": \"The news is relevant to Tesla Autopilot accidents as it reports an incident where a Tesla Model S crashed into an unoccupied, parked police vehicle in Laguna Beach, California. The driver claimed that the Tesla was in 'Autopilot' mode at the time of the accident.\",\n        \"Details\": {\n            \"Driver's Claim\": \"The driver told investigators that the Tesla was in 'Autopilot' mode at the time of the accident.\",\n            \"Injuries\": \"The driver suffered minor injuries in the crash.\",\n            \"Tesla's Response\": \"Tesla acknowledged the accident and mentioned that Autopilot doesn't make the car impervious to all accidents.\",\n            \"Previous Incidents\": \"The news also mentions several other crashes and fire incidents involving Tesla vehicles using Autopilot, which have been a concern for Tesla's CEO Elon Musk and are under investigation by authorities.\"\n        }\n    }\n}\nIn summary, this news is relevant to Tesla Autopilot accidents as it reports an incident where a Tesla vehicle crashed while in Autopilot mode, and it provides details about the driver's claim, injuries, Tesla's response, and previous similar incidents involving Tesla vehicles using Autopilot.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-tesla-autopilot-idUSKCN1IU2SZ","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":70,"Time":"MAY 25, 2018","Title":"Tesla agrees to settle class action over Autopilot billed as 'safer'","Content":"By Tina Bellon\nNEW YORK (Reuters) - Tesla Inc on Thursday reached an agreement to settle a class action lawsuit with buyers of its Model S and Model X cars who alleged that the company’s assisted-driving Autopilot system was “essentially unusable and demonstrably dangerous.”\nSlideshow ( 2 images )\nThe lawsuit said Tesla misrepresented on its website that the cars came with capabilities designed to make highway driving “safer.”\nThe Tesla owners said they paid an extra $5,000 to have their cars equipped with the Autopilot software with additional safety features such as automated emergency braking and side collision warning.\nThe features were “completely inoperable,” according to the complaint.\nUnder the proposed agreement, class members, who paid to get the Autopilot upgrade between 2016 and 2017, will receive between $20 and $280 in compensation. Tesla has agreed to place more than $5 million into a settlement fund, which will also cover attorney fees.\nThe case has been closely watched in the automotive and legal communities, as it was the only known court challenge Tesla has faced with regard to its assisted-driving technology.\nTesla’s Autopilot system has come under increased scrutiny in recent months after two Tesla drivers died in crashes in which Autopilot was engaged. The most recent crash, in March, is being investigated by safety regulators.\nTesla said in a statement it “wanted to do right” by its customers and, as part of the proposed deal, agreed to compensate car owners who had purchased the 2.0 version of Autopilot and “had to wait longer than we expected” for the driving features to become active.\n“Since rolling out our second generation of Autopilot hardware in October 2016, we have continued to provide software updates that have led to a major improvement in Autopilot functionality,” the company said. Even though the settlement only covers U.S. customers, Tesla said it would compensate “all customers globally in the same way.”\nThe proposed settlement does not mention the safety allegations but focuses on the delay in making the promised features available to consumers.\nSteve Berman, a lawyer for the car owners, did not immediately respond to a request for comment.\nThe agreement, announced in a filing in San Jose federal court late Thursday, must be approved by U.S. District Judge Beth Labson Freeman.\nAutopilot, released in 2015, is an enhanced cruise-control system that partially automates steering and braking. Tesla has said the use of Autopilot results in 40 percent fewer crashes, a claim the U.S. National Highway Traffic Safety Administration repeated in a 2017 report on the first fatality, which occurred in May 2016. Earlier this month, however, the agency said regulators had not assessed the effectiveness of the technology.\nThe 2017 lawsuit in San Jose federal court named six Tesla Model S and Model X owners from Colorado, Florida, New Jersey and California who alleged the company had engaged in fraud by concealment, and had violated various state consumer protection and unfair competition laws.\nThey sought to represent a nationwide class of consumers.\nReporting by Tina Bellon; editing by Noeleen Walder and David Gregorio\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to Tesla Autopilot accidents because it discusses a class action lawsuit against Tesla by buyers of its Model S and Model X cars. The lawsuit alleges that Tesla's Autopilot system was 'essentially unusable and demonstrably dangerous.' It mentions that Tesla owners paid extra for the Autopilot software with safety features like automated emergency braking and side collision warning, which were 'completely inoperable' according to the complaint. Additionally, the news mentions that Tesla's Autopilot system has come under scrutiny after two Tesla drivers died in crashes where Autopilot was engaged. The settlement proposed in the news does not mention safety allegations but focuses on the delay in making promised features available to consumers. Thus, this news is directly related to Tesla Autopilot issues.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-tesla-autopilot-lawsuit-idUSKCN1IQ1SH","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":69,"Time":"June 9, 2022","Title":"Tesla Autopilot concerns are on U.S. agency's 'radar,' chair says","Content":"Companies\nTesla Inc\nFollow\nWASHINGTON, June 9 (Reuters) - The chair of the Federal Trade Commission (FTC) said concerns raised by lawmakers about Tesla Inc's (TSLA.O) driver assistance system known as Autopilot are \"on our radar.\"\nFollowing agency policy, FTC Chair Lina Khan, speaking in an interview on Tuesday, would not confirm nor deny a probe.\n\"It's absolutely true that, you know, this is a issue on which many members of Congress have focused and written to us about, so it's certainly something that's on our radar,\" Khan said.\nTesla, which disbanded its press department, did not respond to a request for comment.\nIn August, Senators Ed Markey and Richard Blumenthal urged the FTC to probe Tesla, saying the automaker misled consumers and endangered the public by marketing its driving automation systems as fully self-driving.\nAn FTC investigation could potentially lead to a lawsuit seeking to force the company to change how it describes Autopilot's capabilities. That might damage Tesla's reputation.\nAdvertisement · Scroll to continue\nThe August letter came soon after the National Highway Transportation Safety Administration (NHTSA) opened a probe into Tesla’s Autopilot and crashes involving parked emergency vehicles.\nNHTSA has opened special investigations into 35 crashes since 2016 involving Tesla vehicles in which advanced driver assistance systems like Autopilot were suspected of being in use. To date, 14 deaths have been reported in those incidents, including three who were killed in a California crash last month.\nAdvertisement · Scroll to continue\nTesla says Autopilot assists drivers by enabling vehicles to steer, accelerate and brake automatically but the features \"require active driver supervision and do not make the vehicle autonomous.\"\nIn a 2018 letter, NHTSA said Tesla had made \"misleading statements\" about the Tesla Model 3 five-star safety rating and associated data. The agency referred the issue to the FTC to investigate whether Tesla’s claims constituted “unfair or deceptive acts or practices.\"\nAdvertisement · Scroll to continue\nTwo U.S. consumer advocacy groups in 2018 also urged the FTC to investigate Tesla's naming of Autopilot. The FTC previously declined comment on NHTSA’s referral and has taken no public action on it.\nReporting by Diane Bartz and David Shepardson; Editing by Cynthia Osterman\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab\nDiane Bartz\nThomson Reuters\nFocused on U.S. antitrust as well as corporate regulation and legislation, with experience involving covering war in Bosnia, elections in Mexico and Nicaragua, as well as stories from Brazil, Chile, Cuba, El Salvador, Nigeria and Peru.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": [\n\"The news is relevant to Tesla Autopilot concerns as it discusses concerns raised by lawmakers about Tesla Inc's driver assistance system known as Autopilot.\",\n\"FTC Chair Lina Khan mentioned that these concerns are 'on our radar,' indicating that they are being monitored by the Federal Trade Commission.\",\n\"Senators Ed Markey and Richard Blumenthal urged the FTC to probe Tesla, alleging that the automaker misled consumers and endangered the public by marketing its driving automation systems as fully self-driving.\",\n\"The National Highway Transportation Safety Administration (NHTSA) has opened a probe into Tesla’s Autopilot and crashes involving parked emergency vehicles, with 35 crashes involving Tesla vehicles suspected of being in use of advanced driver assistance systems like Autopilot, resulting in 14 reported deaths.\",\n\"Tesla's description of Autopilot is mentioned, stating that it 'requires active driver supervision and does not make the vehicle autonomous.'\",\n\"The news also mentions that NHTSA had referred the issue of Tesla's misleading statements to the FTC to investigate whether they constituted 'unfair or deceptive acts or practices.'\",\n\"Two U.S. consumer advocacy groups in 2018 urged the FTC to investigate Tesla's naming of Autopilot.\"\n]\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/tesla-autopilot-concerns-are-us-agencys-radar-chair-says-2022-06-09\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":64,"Time":"April 19, 2021","Title":"Texas police to demand Tesla crash data as Musk denies Autopilot use","Content":"WASHINGTON, April 19 (Reuters) - Texas police will serve search warrants on Tesla Inc (TSLA.O) on Tuesday to secure data from a fatal vehicle crash, a senior officer told Reuters on Monday, after CEO Elon Musk said company checks showed the car’s Autopilot driver assistance system was not engaged.\nMark Herman, Harris County Constable Precinct 4, said evidence including witness statements clearly indicated there was nobody in the driver's seat of the Model S when it crashed into a tree, killing two people, on Saturday night.\nAdvertisement · Scroll to continue\nHerman said a tweet by Musk on Monday afternoon, saying that data logs retrieved by the company so far ruled out the use of the Autopilot system, was the first officials had heard from the company.\n\"If he is tweeting that out, if he has already pulled the data, he hasn’t told us that,\" Herman told Reuters. \"We will eagerly wait for that data.\"\nThe crash is the 28th Telsa accident to be investigated by the National Highway Traffic Safety Administration (NHTSA), which regulates vehicle safety.\nAdvertisement · Scroll to continue\nIt is also being probed by the National Transportation Safety Board (NTSB), which removed Tesla as a party to an earlier investigation into a fatal crash in 2018 after the company made public details of the probe without authorisation.\nIn Saturday's accident, the 2019 Tesla Model S was traveling at high-speed near Houston when it failed to negotiate a curve and went off the road, crashing into a tree and bursting into flames, Herman said.\nAdvertisement · Scroll to continue\nAuthorities found the bodies of two men in the car, one in the front passenger seat and the owner of the car in the backseat.\n\"We have witness statements from people that said they left to test drive the vehicle without a driver and to show the friend how it can drive itself,\" Herman said.\nTesla's Autopilot is a driver assistance system that handles some driving tasks and allows drivers to take their hands off the steering wheel at times, but Tesla says its features \"require active driver supervision and do not make the vehicle autonomous.\"\nAdvertisement · Scroll to continue\nIn his tweet, Musk rejected the idea that the vehicle's semi-automated driving software was to blame: \"Data logs recovered so far show Autopilot was not enabled & this car did not purchase FSD,\" in a reference to Full Self-Driving, Tesla's separate beta semi-automated driver assistance system that still requires driver supervision.\nMusk added that \"standard Autopilot would require lane lines to turn on, which this street did not have,\" referring to road markers that need to be captured by a vehicle's cameras to enable autopilot.\nThe remains of a Tesla vehicle are seen after it crashed in The Woodlands, Texas, April 17, 2021, in this still image from video obtained via social media. Video taken April 17, 2021. SCOTT J.... Acquire Licensing Rights Read more\nTesla has access to operational and diagnostic data delivered to its servers at \"regular intervals\" from the car, which has been impounded by police. It is unclear whether investigators will be able to retrieve data directly from the event data recorder in the severely burned vehicle.\nJust hours before the crash, Musk had tweeted: \"Tesla with Autopilot engaged now approaching 10 times lower chance of accident than average vehicle.\"\nloading\nloading\n'SHOULDN'T BE A DEATH RISK'\nTesla's Autopilot system, which was operating in at least three Tesla vehicles involved in fatal U.S. crashes since 2016, has come under increasing scrutiny.\nNHTSA told Reuters last month it had opened 27 special investigations into crashes of Tesla vehicles, 23 of which remain active, in crashes believed to have been tied to Autopilot use.\nloading\nTesla did not respond to a request for comment. Its shares closed down 3.4% Monday before picking up 1.5% in after hours trading following Musk's tweet.\nSenator Richard Blumenthal, a Democrat, wrote on Twitter that \"using Tesla's driverless system — or any other — shouldn't be a death risk. Advancements in driving technology must first & foremost be safe.\"\nThe NTSB, which makes safety recommendations but cannot compel recalls, said its investigation into the Texas crash would focus \"on the vehicle's operation and the post-crash fire.\"\nFire officials said it took four hours to completely extinguish the fire because of the car's lithium ion battery.\nReporting by David Shepardson Editing by Chizu Nomiyama\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Based on the provided news article, here is the JSON-format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": [\n        \"Texas police are serving search warrants on Tesla to secure data from a fatal vehicle crash.\",\n        \"The crash involved a Tesla Model S, and two people were killed when it crashed into a tree.\",\n        \"Elon Musk stated that the car's Autopilot system was not engaged at the time of the accident.\",\n        \"Witness statements indicate that there was nobody in the driver's seat of the Model S when it crashed.\",\n        \"The National Highway Traffic Safety Administration (NHTSA) is investigating the accident, marking it as the 28th Tesla accident under investigation.\",\n        \"The National Transportation Safety Board (NTSB) is also investigating the crash.\",\n        \"The news article discusses Tesla's Autopilot system, its capabilities, and the controversy surrounding its use.\",\n        \"Elon Musk mentioned that data logs show Autopilot was not enabled in this case and referred to the absence of lane lines on the road where the crash occurred.\"\n    ]\n}\nThis news is relevant to Tesla Autopilot accidents because it discusses a fatal crash involving a Tesla vehicle, the investigation by law enforcement, the absence of a driver in the driver's seat, and Elon Musk's comments regarding the Autopilot system.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/us-probes-fatal-tesla-crash-believed-be-driverless-2021-04-19\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":39,"Time":null,"Title":"Fatal Tesla crash in California draws federal investigators to site of head-on collision","Content":"A fatal crash involving a Tesla in California has drawn the attention of federal investigators, who sent a team to the site of the collision in South Lake Tahoe.\nA Tesla Model 3 and Subaru Impreza collided head on during the evening of July 5, according to state police, and the driver of the Subaru died a short time later. Local media reports say that an infant that had been traveling in the Tesla died last week.\nThe National Highway Traffic Safety Administration has been looking into a string of accidents involving Teslas that are believed to have had automated driving technology installed.\nThe NHTSA last sent investigators to a March 15 crash in Halifax County, North Carolina, that injured a 17-year-old student. The State Highway Patrol said at the time that the driver of the 2022 Tesla Model Y, a 51-year-old male, failed to stop for the bus, which was displaying all of its activated warning devices.\nSending special investigation teams to crashes means that the agency suspects the Teslas were operating systems that can handle some aspects of driving, including Autopilot and “Full Self-Driving.” Despite the names used for the technology, Tesla says these are driver-assist systems and that drivers must be ready to intervene at all times.\nThe NHTSA has sent investigative teams to at least 30 crashes since 2016 involving Teslas suspected of using partially automated driving systems. At least 16 people have died in the crashes.","Relevant":true,"Overview":"Here's the JSON-format response based on the provided news article:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Fatal Tesla crash in California draws federal investigators to site of head-on collision\",\n        \"Description\": \"A fatal crash involving a Tesla in California has drawn the attention of federal investigators, who sent a team to the site of the collision in South Lake Tahoe. A Tesla Model 3 and Subaru Impreza collided head-on during the evening of July 5, according to state police, and the driver of the Subaru died a short time later. Local media reports say that an infant that had been traveling in the Tesla died last week. The National Highway Traffic Safety Administration has been looking into a string of accidents involving Teslas that are believed to have had automated driving technology installed. The NHTSA last sent investigators to a March 15 crash in Halifax County, North Carolina, that injured a 17-year-old student. The State Highway Patrol said at the time that the driver of the 2022 Tesla Model Y, a 51-year-old male, failed to stop for the bus, which was displaying all of its activated warning devices. Sending special investigation teams to crashes means that the agency suspects the Teslas were operating systems that can handle some aspects of driving, including Autopilot and “Full Self-Driving.” Despite the names used for the technology, Tesla says these are driver-assist systems and that drivers must be ready to intervene at all times. The NHTSA has sent investigative teams to at least 30 crashes since 2016 involving Teslas suspected of using partially automated driving systems. At least 16 people have died in the crashes.\"\n    }\n}\nThis news article is relevant to Tesla Autopilot accidents because it reports on a fatal crash involving a Tesla Model 3, where the National Highway Traffic Safety Administration (NHTSA) is investigating. The article also mentions that the NHTSA is looking into accidents involving Teslas suspected of using automated driving technology, including Autopilot. Therefore, it fits the criteria for Tesla Autopilot accidents.","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/tesla-nhtsa-crash-fatal-autopilot-selfdriving-7773fa62468523ac8a9f88a3501f242c","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":61,"Time":"Fri 8 Jun 2018 01.19 CEST","Title":"Tesla fatal crash: 'autopilot' mode sped up car before driver killed, report finds","Content":"A Tesla driving in “autopilot” mode crashed in March when the vehicle sped up and steered into a concrete barrier, according to a new report on the fatal collision, raising fresh concerns about Elon Musk’s technology.\nThe National Transportation Safety Board (NTSB) said that four seconds before the 23 March crash on a highway in Silicon Valley, which killed Walter Huang, 38, the car stopped following the path of a vehicle in front of it. Three seconds before the impact, it sped up from 62mph to 70.8mph, and the car did not brake or steer away, the NTSB said.\nWalter Huang, who died in the Tesla crash, with his wife. Photograph: Hand-out\/Minami Tamaki LLP\nThe report – which said the Tesla battery was breached, causing the car to be engulfed in flames – comes after after the company has repeatedly sought to deflect blame on to the driver and the local highway conditions. Musk has also aggressively attacked journalists writing about this crash and other recent autopilot collisions, complaining that the negative attention would discourage people from using his technology.\nThe NTSB report, however, has once again raised serious safety questions about the limits and performance of the autopilot technology, which is meant to assist drivers and has faced growing scrutiny from experts and regulators. Mark Fong, an attorney for Huang’s family, also said the report appeared to “contradict Tesla’s characterization” of the collision.\nFollowing numerous embarrassing autopilot crashes, including Teslas colliding into a police vehicle and firetruck, the company has pointed to its manual which warns that the technology cannot detect all objects and that drivers should remain attentive.\nAfter the fatal crash in the city of Mountain View, Tesla noted that the driver had received multiple warnings to put his hands on the wheel and said he did not intervene during the five seconds before the car hit the divider.\nEmergency personnel at the scene of the crash in Mountain View, California. Photograph: AP\nBut the NTSB report revealed that these alerts were made more than 15 minutes before the crash. In the 60 seconds prior to the collision, the driver also had his hands on the wheel on three separate occasions, though not in the final six seconds, according to the agency. As the car headed toward the barrier, there was no “precrash braking” or “evasive steering movement”, the report added.\nFong said in a statement: “The NTSB report provides facts that support our concerns that there was a failure of both the Tesla Autopilot and the automatic braking systems of the car.”\n“The Autopilot system should never have caused this to happen,” he added.\n“There’s clearly a technology failure,” said Erick Guerra, an assistant professor in city and regional planning at the University of Pennsylvania. “The technology is just not up to doing as much as people hope it can do.”\nTesla declined to comment on the NTSB report. In previous statements, the company emphasized that the highway safety barrier had been damaged in an earlier crash, contributing to the severity of the collision. The NTSB confirmed this previous damage.\nElon Musk: as business fortunes dip, he starts a war with the media\nRead more\nSome critics have argued that the Tesla “autopilot” branding and Musk’s hype about his technology can be misleading and problematic given that the current capability of his cars continues to be fairly limited. Experts say the development of autonomous technology is entering a particularly dangerous phase when drivers are lulled into a false sense of security but expected to intervene to avoid crashes and other problems.\n“In an ideal automated system, it should really be taking over when the driver fails … rather than forcing the driver to take over when it fails,” said Guerra, who was not involved with the NTSB report.\nThe NTSB, which has publicly feuded with Tesla over the release of information during the investigation, said it intended to issue safety recommendations to prevent similar crashes.\nThe problems with the damaged highway divider do not “absolve Tesla of responsibility”, said Ryan Calo, a University of Washington law professor and expert in autonomous cars. “That doesn’t mean they are off the hook.”\nTesla’s designers may not have anticipated this specific kind of crash, he added: “The technology is being deployed before there is a clear sense of … what is adequately safe.”","Relevant":true,"Overview":"Here's the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": [\n        {\n            \"News_title\": \"Tesla fatal crash: 'autopilot' mode sped up car before driver killed, report finds\",\n            \"Description\": \"A Tesla driving in “autopilot” mode crashed in March when the vehicle sped up and steered into a concrete barrier, according to a new report on the fatal collision.\",\n            \"Key_details\": [\n                \"The National Transportation Safety Board (NTSB) reported that four seconds before the crash, the Tesla stopped following the path of a vehicle in front of it.\",\n                \"Three seconds before the impact, it sped up from 62mph to 70.8mph, and the car did not brake or steer away.\",\n                \"The NTSB report indicated that the Tesla battery was breached, causing the car to be engulfed in flames.\",\n                \"The NTSB report has raised serious safety questions about the limits and performance of the autopilot technology, contradicting Tesla's characterization of the collision.\",\n                \"The driver received multiple warnings to put his hands on the wheel, but these alerts were made more than 15 minutes before the crash.\",\n                \"In the 60 seconds prior to the collision, the driver had his hands on the wheel on three separate occasions, though not in the final six seconds.\",\n                \"There was no 'precrash braking' or 'evasive steering movement' as the car headed toward the barrier, according to the agency.\"\n            ]\n        }\n    ]\n}\nThe provided news is relevant to Tesla Autopilot accidents because it discusses a Tesla vehicle in \"autopilot\" mode crashing into a concrete barrier, resulting in a fatal collision. The National Transportation Safety Board's report reveals details about the events leading up to the crash, including the car's acceleration, lack of braking or steering away, and the breach of the Tesla battery causing a fire. The report also highlights concerns about the performance of Tesla's autopilot technology and contradicts Tesla's characterization of the incident.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/jun\/07\/tesla-fatal-crash-silicon-valley-autopilot-mode-report","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":62,"Time":"Wed 26 Feb 2020 16.51 CET","Title":"Tesla driver who died in 'autopilot' crash was playing on phone, inquiry finds","Content":"A Tesla driver killed in a Silicon Valley crash was playing a video game on his smartphone at the time of his fatal crash, investigators said on Tuesday.\nThe National Transport and Safety Board (NTSB) investigation found that Walter Huang, a 38-year-old Apple software engineer and game developer, made no attempts to stop his vehicle as it sped towards a crash barrier before the 2018 crash.\nHuang’s Tesla Model X was in “Autopilot” mode and traveling at about 70 miles per hour when it crashed into a safety barrier and was struck by two other vehicles. He died in hospital from his injuries.\n“If you own a car with partial automation, you do not own a self-driving car. So don’t pretend that you do,” said the NTSB chairman, Robert Sumwalt. “This means that when driving in the supposed self-driving mode you can’t sleep. You can’t read a book. You can’t watch a movie or TV show. You can’t text. And you can’t play video games. Yet that’s precisely what we found that this driver was doing.”\nNTSB officials said the crash was very similar to other Tesla crashes it is investigating and called on car companies, government regulators, mobile phone companies and employers to do more to prevent accidents caused by distracted driving.\nSumwalt took aim at government regulators who he said have given “scant oversight” of the industry as it develops self-driving technologies and noted that Tesla had yet to respond to its safety recommendations which he said had been sent to the company 881 days ago “and we have heard nothing”.\nThe NTSB has previously criticized Tesla’s Autopilot after a 2016 fatal crash in Florida and called on the company to make its driver-assist systems more resilient to misuse by inattentive drivers.\nAt the Washington hearing NTSB experts discussed “automation complacency” an issue when drivers placed too much trust in technology. Robert Molloy, director of the NTSB’s office of highway safety, said: “This is not a unique problem to Tesla, this is a problem related to automation and how people work with automation.”\nTesla has claimed its internal data shows that drivers using Autopilot crash less frequently while using the technology than while driving manually.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is about a Tesla driver, Walter Huang, who died in a crash while the vehicle was in 'Autopilot' mode. The investigation found that Huang was playing a video game on his smartphone at the time of the crash. The National Transport and Safety Board (NTSB) investigated the incident and stated that Huang made no attempts to stop his vehicle as it approached a crash barrier. The crash occurred in 2018, and Huang's Tesla Model X was traveling at about 70 miles per hour when it crashed into a safety barrier and was struck by two other vehicles. The NTSB chairman, Robert Sumwalt, emphasized that owners of cars with partial automation should not consider them self-driving cars and listed activities that should not be done while in self-driving mode, including playing video games. The NTSB also criticized Tesla's Autopilot system after a previous fatal crash in 2016 and called for improvements to prevent accidents caused by distracted driving. Additionally, the news mentions the concept of 'automation complacency,' where drivers place too much trust in technology, as discussed by NTSB experts. Tesla has claimed that its internal data shows drivers using Autopilot crash less frequently than when driving manually.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2020\/feb\/25\/tesla-driver-autopilot-crash","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":77,"Time":"MARCH 18, 2021","Title":"U.S. safety agency reviewing 23 Tesla crashes, three from recent weeks","Content":"By David Shepardson\nWASHINGTON (Reuters) - The U.S. auto safety agency disclosed on Thursday it has opened 27 investigations into crashes of Tesla vehicles, 23 of which remain active, and at least three of the crashes occurred in recent weeks.\nFILE PHOTO: The logo of car manufacturer Tesla is seen at a branch office in Bern, Switzerland, October 28, 2020. REUTERS\/Arnd Wiegmann\/File Photo\nThe National Highway Traffic Safety Administration (NHTSA) confirmed Thursday that it will send a team to investigate a recent Tesla crash in the Houston area. Four of the 27 NHTSA investigations have been completed and the results published.\nEarlier this week, NHTSA said it was sending its special crash investigation team to probe two crashes in Michigan, including a crash early Wednesday involving a Tesla suspected of being in Autopilot mode when it struck a parked Michigan State Police patrol car.\nTesla did not immediately comment.\nNHTSA said in July that its “(Special Crash Investigations team) has looked into 19 crashes involving Tesla vehicles where it was believed some form of advanced driver assistance system was engaged at the time of the incident.”\nMichigan State Police said a parked patrol car was struck by a Tesla apparently in Autopilot mode while investigating a traffic crash near Lansing on Interstate-96. No one was injured and the 22-year-old Tesla driver was issued traffic citations.\nOn Monday, NHTSA said it was sending another team to investigate a “violent” March 11 crash in Detroit in which a Tesla became wedged underneath a tractor-trailer and left a passenger in critical condition.\nDetroit police said Tuesday they do not believe that Autopilot was in use.\nThe Autopilot feature was operating in at least three Tesla vehicles involved in fatal U.S. crashes since 2016.\nTesla advises drivers they must keep their hands on the steering wheel and pay attention while using Autopilot. However, some Tesla drivers say they are able to avoid putting their hands on the wheel for extended periods when using Autopilot.\nNHTSA’s Special Crash Investigation team typically looks at more than 100 crashes a year with a focus on emerging technologies. Issues in recent years include performance of alternative fueled vehicles, child restraint systems, adaptive controls, safety belts, vehicle-pedestrian interactions, and potential safety defects.\nSeparately, the agency said it had been briefed on Tesla’s “full self-driving” (FSD) software. Tesla Chief Executive Elon Musk wrote on Twitter last week that the beta FSD software had been expanded to about 2,000 owners while other drivers had access to the program revoked.\nThe agency said it “will monitor the new technology closely and will not hesitate to take action to protect the public against risks to safety.”\nNHTSA said the system does not make the Tesla “capable of driving itself. The most advanced vehicle technologies available for purchase today provide driver assistance and require a fully attentive human driver at all times performing the driving task and monitoring the surrounding environment.”\nReporting by David Shepardson; Editing by Aurora Ellis and Richard Pullin\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here's the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to Tesla Autopilot accidents. The National Highway Traffic Safety Administration (NHTSA) is currently reviewing 23 Tesla crashes, and at least three of these crashes occurred in recent weeks. Additionally, the news mentions that a Tesla suspected of being in Autopilot mode struck a parked Michigan State Police patrol car. It also discusses a 'violent' March 11 crash in Detroit involving a Tesla, where a passenger was left in critical condition. While the Detroit police do not believe Autopilot was in use in that specific case, it is clear that the article discusses Tesla accidents and investigations related to Autopilot functionality. Therefore, the news fits the category of Tesla Autopilot accidents.\"\n}\nThe provided news is relevant to Tesla Autopilot accidents because it discusses multiple Tesla crashes, including one suspected of being in Autopilot mode and another where Autopilot's involvement is not confirmed but still related to Tesla accidents.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-tesla-crash-idUSKBN2BA2ML","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":78,"Time":"JULY 6, 2016","Title":"Tesla told regulators about Autopilot crash nine days after accident","Content":"By Alexandria Sage, Paul Lienert\nSAN FRANCISCO\/DETROIT (Reuters) - Tesla Motors TSLA.O alerted regulators to a fatality in one of its electric cars in partial self-driving Autopilot mode nine days after it crashed, the company said on Tuesday, defending its decision not to make the accident public before a federal investigation was announced.\nA Tesla Model S involved in the fatal crash on May 7, 2016 is shown with the top third of the car sheared off by the impact of the collision of the Tesla with a tractor-trailer truck on nearby highway and came to rest in the yard of Robert and Chrissy VanKavelaar in Williston, Florida, U.S. on May 7, 2016. Courtesy Robert VanKavelaar\/Handout via REUTERS\nTesla learned about the crash of the Model S sedan in Florida “shortly” after the May 7 crash, and on May 16 it disclosed the incident to the government. The National Highway Traffic Safety Administration (NHTSA) on June 30 announced a probe.\nThe news comes as the company faces pressure on several fronts. Its bid to buy rooftop solar power company SolarCity has been questioned by investors, and over the U.S. July 4 holiday weekend, it disclosed that second-quarter vehicle production missed company targets.\nAutopilot is one of the most advanced and most promoted Tesla technologies and is still in beta or test mode. That has spurred questions - including in an article by Fortune magazine - over whether the company and regulators should have informed the public earlier of the fatality.\nOn Tuesday, Chief Executive Elon Musk tweeted in response to the article about the timing of the disclosure that the May fatality “wasn’t material” to Tesla.\nTesla raised at least $1.46 billion from investors on May 18-19 with a stock offering, as the Autopilot investigation was unfolding.\nThe company knew of the crash by the time of the capital raising. But its own investigation was not yet complete and it had not yet been informed by the government of its probe, according to a timeline described by a Tesla spokeswoman.\nThe windshield was ripped off the Model S after it plowed into the side of a truck on a divided highway, and the damage meant the car was unable to transmit data to Tesla. Tesla learned of the accident “shortly thereafter” from local authorities, the spokeswoman said.\nThe company was obligated to disclose the fatality to regulators during its third quarter but notified them earlier, on May 16, as it was investigating.\nRELATED COVERAGE\nFactbox: Tesla Autopilot crash timeline\n“Tesla then provided NHTSA with additional details about the accident over the following weeks as it worked to complete its investigation, which it ultimately concluded during the last week of May,” the spokeswoman said.\nNHTSA spokesman Bryan Thomas said the investigation was “active” and the agency would not comment further.\nTESLA SAYS DISCLOSURE WAS NOT NECESSARY\nAsked why the company did not disclose the incident ahead of the share sale and ahead of its recently announced bid to acquire SolarCity SCTY.O, Tesla issued the following statement:\n“Tesla does not find it necessary, nor does any automaker, to share the details of every accident that occur in a Tesla vehicle. More than a million people die globally every year in car accidents, but automakers do not disclose each of these accidents to investors, let alone before those investigations are complete and without regard to what the results of those investigations end up being.”\nTesla shares fell as much as 4 percent on the first trading day after the accident was disclosed but then ended the day up 2 percent.\nThe spokeswoman said the reaction showed investors agreed with the company. “The market apparently understands this,” she said.\nTesla have swung widely this year and are down about 11 percent so far. The stock dived on news of Tesla’s SolarCity bid, falling more than the value of its offer. Its shares ended down 1.16 percent on Tuesday, the first trading day after disclosing the production shortfall.\nSlideshow ( 2 images )\nPeter Henning, a law professor at Wayne State University in Detroit, said Tesla probably should have informed investors of the crash before its stock offering and SolarCity offer.\n“The materiality issue is not about the death itself, but more about the circumstances of the crash and calling into question a technology that’s important to Tesla’s future,” Henning said.\n“Those are issues that investors want to know, so you could make a reasonable argument that it crossed the (materiality) line. When it’s that close, the (U.S.) Securities and Exchange Commission expects disclosure.”\nThe SEC did not immediately respond to an after-hours request for comment.\nEditing by Peter Henderson and Cynthia Osterman\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news article is relevant to Tesla Autopilot accidents. It discusses Tesla's response to a fatal accident that occurred on May 7, 2016, involving a Tesla Model S in partial self-driving Autopilot mode. Tesla learned about the crash shortly after it happened and disclosed the incident to the government on May 16, 2016. The National Highway Traffic Safety Administration (NHTSA) subsequently announced a probe on June 30, 2016, related to this incident. The article also mentions that Autopilot is a highly promoted Tesla technology that was in beta or test mode at the time of the accident, raising questions about whether the company and regulators should have informed the public earlier about the fatality. Additionally, the article discusses Tesla's stock offering and the timing of the disclosure of the accident, which had an impact on Tesla's stock price.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-tesla-autopilot-disclosure-idUSKCN0ZL2UC","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":76,"Time":"September 4, 2021","Title":"U.S. probing fatal Tesla crash that killed pedestrian","Content":"WASHINGTON, Sept 3 (Reuters) - U.S. auto safety regulators are investigating a July 26 fatal crash in New York involving a Tesla (TSLA.O)vehicle that might have been using an advanced driver assistance system, they disclosed on Friday.\nNew York City police confirmed on Friday an ongoing investigation into the July 26 death of a 52-year-old man attempting to fix a flat tire on his vehicle on the Long Island Expressway when he was struck by a Tesla.\nA National Highway Traffic Safety Administration (NHTSA) spokeswoman told Reuters on Friday the agency was aware of the \"July 26 incident involving a Tesla vehicle on the Long Island Expressway in New York, and has launched a Special Crash Investigation team to investigate the crash.\"\nNHTSA's probe into the New York crash was first reported by Reuters.\nTesla did not immediately respond to a request for comment.\nAdvertisement · Scroll to continue\nThe U.S. investigations come amid increased scrutiny of Tesla's Autopilot and other driver assistance systems. Tesla's Autopilot handles some driving tasks and allows drivers to keep their hands off the wheel for extended periods.\nLast month, NHTSA said it opened a formal safety probe into Autopilot after 11 crashes involving Tesla models and emergency vehicles.\nOn Wednesday, NHTSA identified a 12th crash involving a Tesla vehicle using an advanced driver assistance system that struck an emergency vehicle in Orlando, Florida.\nAdvertisement · Scroll to continue\nOn Friday, NHTSA released an updated list of special crash investigation crashes that it is reviewing, in which advanced driver assistance systems are suspected of being used including the New York crash involving a 2021 Tesla Model Y as well as a crash in Florida.\nNHTSA has opened 33 investigations into Tesla crashes involving 11 deaths since 2016, in which use of advanced driver assistance systems was suspected. NHTSA has ruled out Autopilot use in three of those non-fatal crashes.\nReporting by David Shepardson; Editing by Richard Chang\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": [\n    \"U.S. auto safety regulators are investigating a July 26 fatal crash in New York involving a Tesla vehicle that might have been using an advanced driver assistance system.\",\n    \"New York City police confirmed an ongoing investigation into the July 26 death of a 52-year-old man who was struck by a Tesla while attempting to fix a flat tire on his vehicle on the Long Island Expressway.\",\n    \"A National Highway Traffic Safety Administration (NHTSA) spokeswoman stated that the agency has launched a Special Crash Investigation team to investigate the crash.\",\n    \"NHTSA's probe into the New York crash was reported by Reuters.\",\n    \"The U.S. investigations come amid increased scrutiny of Tesla's Autopilot and other driver assistance systems.\",\n    \"Last month, NHTSA opened a formal safety probe into Autopilot after 11 crashes involving Tesla models and emergency vehicles.\",\n    \"On Wednesday, NHTSA identified a 12th crash involving a Tesla vehicle using an advanced driver assistance system that struck an emergency vehicle in Orlando, Florida.\",\n    \"On Friday, NHTSA released an updated list of special crash investigation crashes that it is reviewing, including the New York crash involving a 2021 Tesla Model Y as well as a crash in Florida.\",\n    \"NHTSA has opened 33 investigations into Tesla crashes involving 11 deaths since 2016, in which the use of advanced driver assistance systems was suspected.\"\n  ]\n}\nThe provided news is relevant to Tesla Autopilot accidents because it discusses a fatal crash involving a Tesla vehicle that may have been using an advanced driver assistance system (Autopilot). It also mentions ongoing investigations by U.S. auto safety regulators, increased scrutiny of Tesla's Autopilot and other driver assistance systems, and multiple incidents involving Tesla vehicles and emergency vehicles.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/us-probing-fatal-tesla-crash-that-killed-pedestrian-2021-09-03\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":74,"Time":"Fri 18 Jun 2021 02.41 CEST","Title":"Thirty Tesla crashes linked to assisted driving system under investigation in US","Content":"US safety regulators have opened 30 investigations into Tesla crashes involving 10 deaths since 2016 where an advanced driver assistance system was suspected to have been in use.\nThe National Highway Traffic Safety Administration (NHTSA) released a list offering details about crashes under review by its special crash investigations programs.\nTesla Model 3 becomes most popular battery electric car on UK roads\nRead more\nThe agency, which has previously confirmed some specific Tesla crash investigations, had not previously released to Reuters a full accounting of all Tesla crashes investigated where Tesla’s Autopilot system was suspected of being involved.\nOf the 30 Tesla crashes, NHTSA has ruled out Tesla’s Autopilot in three and published reports on two of the crashes.\nTesla did not immediately respond to a request for comment.\nNHTSA has said previously it had opened 28 special crash investigations into Tesla crashes, with 24 pending. The spreadsheet shows a February 2019 crash where Autopilot use was undetermined.\nAutopilot, which handles some driving tasks, was operating in at least three Tesla vehicles involved in fatal US crashes since 2016, investigators have said.\nThe spreadsheet shows NHTSA has opened eight investigations into Tesla crashes since March.","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"The news is relevant to Tesla Autopilot accidents. The National Highway Traffic Safety Administration (NHTSA) has opened 30 investigations into Tesla crashes involving 10 deaths since 2016 where an advanced driver assistance system (Tesla's Autopilot) was suspected to have been in use. This information clearly indicates a connection between Tesla vehicles equipped with Autopilot and accidents, with investigations ongoing to determine the extent of Autopilot's involvement in these incidents.\"\n}\nThe news discusses investigations by the NHTSA into Tesla crashes where Autopilot was suspected to have been in use. It provides specific information about the number of investigations, fatalities, and the agency's focus on Tesla's Autopilot system. Therefore, it is relevant to Tesla Autopilot accidents.","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2021\/jun\/18\/thirty-tesla-crashes-linked-to-assisted-driving-system-under-investigation-in-us","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":72,"Time":"JULY 1, 2016","Title":"Tesla crash raises concerns about autonomous vehicle regulation","Content":"By David Shepardson, Bernie Woodall\n(Reuters) - The fatal crash of a Tesla Motors Inc Model S in Autopilot mode has turned up pressure on auto industry executives and regulators to ensure that automated driving technology is deployed safely.\n The first such known accident, which occurred in Florida in May, has highlighted tensions surrounding efforts to turn over responsibility for braking, steering and driving judgments to machines. It may delay the U.S. government’s plan to outline guidelines for self-driving cars this month.\nThe cause of the Model S crash is still under investigation by federal and Florida state authorities, which are looking into whether the driver was distracted before his 2015 Model S went under a truck trailer.\nShares of Tesla and Mobileye NV, the maker of the camera vision system used in the Model S, rose on Friday as analysts said the accident was likely a short-term setback. The stocks fell in after-hours trading on Thursday after an investigation of the crash was made known.\nAdvocates of automating driving point to research that shows 90 per cent of accidents are caused by human mistakes. But machines can also make mistakes, or encounter situations they are not designed to handle.\nOn Friday, the U.S. National Highway Traffic Safety Administration (NHTSA) said U.S. traffic deaths rose by 7.7 percent to 35,200 in 2015 - the highest annual tally since 2008 and biggest single-year jump since 1966. Federal officials and industry executives say that toll could be cut by technology such as brakes that automatically engage when sensors detect an impending crash.\nIn March, 20 automakers agreed with regulators to make automatic emergency braking standard on nearly all U.S. vehicles by 2022, a move that could prevent thousands of rear-end crashes annually.\nBut automakers have issued numerous recalls for problems with such systems. Honda Motor Co recalled nearly 50,000 Acura SUVs and cars in June 2015 because the system can apply the brakes when it detects a vehicle accelerating and is driving along a metal fence or guardrail.\nU.S. Transportation Secretary Anthony Foxx told Reuters earlier this year he planned to propose regulatory guidelines by mid-July to clear the way for wider deployment of automated driving systems.\nRELATED COVERAGE\nTesla mixes warnings and bravado about hands-free driving\nDVD player found in Tesla car in fatal May crash\nSee more stories\n“This technology is coming,” Foxx said. “Ready or not, it’s coming.”\nOn Wednesday, Foxx said the guidelines could take more time, and cautioned there are questions “that are probably unanswerable at this point.”\nNHTSA said in a report in March that there are relatively few hurdles to fully autonomous vehicles being used on U.S. roads, as long as vehicle design “allows a human driver to operate the vehicle with a wheel and pedals.”\nAt a conference in Detroit last month, NHTSA chief Mark Rosekind said he would accept technology that was “two times” better than conventional vehicles at preventing collisions.\nHours before the crash became public knowledge on Thursday, U.S. National Transportation Safety Board Chairman Christopher Hart said driverless cars will not be perfect.\n“There will be fatal crashes, that’s for sure,” Hart told the audience at the National Press Club in Washington, but added that will not derail the move toward driverless cars, even if the vehicles are not ready: “This train has left the station.”\nAlphabet Inc’s Google unit and other companies are racing to get self-driving cars on U.S. roads. Google has logged about 1.5 million miles of test driving, but has not said when it would offer its technology for sale.\n“We have a responsibility to get this out there as soon as we can and really as soon as we have data that says we’re better than the current system of flawed human drivers,” Google’s self-driving car CEO John Krafcik said last month in Washington.\nSlideshow ( 3 images )\nFormer NHTSA chief David Strickland, who is heading a self-driving coalition including Google and Ford Motor Co, told Reuters on Friday he does not “think this crash is going to change the arc for the entire industry ... and our pathway toward full self-driving.”\nAutomakers have wide latitude to install systems that intervene when drivers are not attentive - from lane-keeping systems to automatic emergency braking - and do not need prior approval from regulators, even if the systems are described as in “beta”, or public testing mode.\nFormer NHTSA chief Joan Claybrook said in an interview the agency needs to set performance standards for electronic systems like Autopilot. “It’s the like Wild West. The regulatory system is not being used,” Claybrook said.\nTesla’s Autopilot system “is explicitly denoted as a beta product,” said Jason Corso, an associate professor of electrical and computer engineering at the University of Michigan. The accident is a “wake-up call that significant further study is needed to model the sensors and the underlying recognition technologies on which these systems rely,” he said.\nSlideshow ( 3 images )\nTimothy Carone, a business professor at the University of Notre Dame, said there will be more of these types of eventsas more automated cars, planes, trains and weapons are put into use.\nDeaths will start to rise, but will then decline, he said, as “artificial intelligence, big data, and sensors for collecting data begin to mature and become capable of handling unusual situations that are difficult to simulate in test environments.”\nReporting by Narottam Medhora in Bengaluru, Bernie Woodall in Detroit and David Shepardson in Washington; Editing by Bill Rigby and Jeffrey Benkoe\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to Tesla Autopilot accidents because it discusses a fatal crash of a Tesla Model S in Autopilot mode that occurred in Florida in May. The article mentions that this is the first known accident of its kind, highlighting concerns and pressure on the auto industry and regulators regarding the safe deployment of automated driving technology. It also mentions that federal and state authorities are investigating whether the driver was distracted before the Model S collided with a truck trailer. The article further discusses the impact on Tesla's stock and the debate around the safety of autonomous vehicles. Therefore, this news article is directly related to Tesla Autopilot accidents.\"\n}","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-tesla-autopilot-idUSKCN0ZH4VO","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":71,"Time":"9:58 PM GMT+2•October 26, 2022","Title":"Tesla said to face criminal investigation by the Department of Justice over self-driving claims","Content":"cameraImage Credits: Tesla\nTesla is said to be facing a criminal investigation launched by the U.S. Department of Justice facing claims made by the company regarding its “Autopilot” capabilities, Reuters reports, citing “three people familiar with the matter.” The inquiry was launched last year per the sources and was initiated following over a dozen accidents involving the active use of Tesla’s Autopilot system, some resulting in fatalities.\nTesla, and in particular CEO Elon Musk have been bold in their claims regarding Autopilot’s capabilities: The company’s so-called “Full Self-Driving” or FSD (which is not that at all, by the way, even by the admission of the company’s own materials) beta launched in October of 2020, and now has over 100,000 members enrolled from the larger global Tesla owner population, according to the most recent public numbers.\nThe automaker still cautions users of “Autopilot,” “Enhanced Autopilot” and “Full Self-Driving Capability” that they must remain “alert,” with their “hands on the steering wheel at all times” and that they “maintain control of [their] car.”\nThat said, Musk himself has suggested FSD could be “safer than a human” before the end of this year in an earrings call from January. It was a reiteration of a claim from a year prior he made on Twitter, noting that FSD would “work at a safety level well above that of the average driver this year.”\nNote that just because the DoJ is investigating doesn’t mean criminal charges will necessarily result — they could opt to pursue civil action, do nothing at all or level charges.\nMore TechCrunch\nTinder goes ultra-premium, Amazon invests in Anthropic and Apple explains its new AirPods\nVR is dead\nApple revamps iCloud.com with more features for drive, mail and notes\nVR is dead","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"The news is relevant to Tesla Autopilot accidents because it reports that Tesla is facing a criminal investigation by the U.S. Department of Justice (DoJ) regarding its claims about its 'Autopilot' capabilities. The investigation was initiated following over a dozen accidents involving the active use of Tesla's Autopilot system, some of which resulted in fatalities. This investigation suggests a connection between Tesla's Autopilot system and accidents, making it relevant to Tesla Autopilot accidents.\"\n}\nThe news discusses a criminal investigation by the Department of Justice into Tesla's Autopilot system and its claims, linking it to accidents involving the system, which aligns with the topic of Tesla Autopilot accidents.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2022\/10\/26\/tesla-criminal-investigation-doj-autopilot-self-driving\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":73,"Time":"October 21, 2021","Title":"EXCLUSIVE Dutch forensic lab says it has decoded Tesla's driving data","Content":"Companies\nTesla Inc\nFollow\nLONDON, Oct 21 (Reuters) - The Dutch government's forensic lab said on Thursday it had decrypted electric carmaker Tesla Inc's (TSLA.O) closely guarded driving data-storage system, uncovering a wealth of information that could be used to investigate serious accidents.\nIt was already known that Tesla cars store data from accidents, but the Netherlands Forensic Institute (NFI) said it had discovered far more data than investigators had previously been aware of.\nThe NFI said the decrypted data showed Tesla vehicles store information about the operation of its driver assistance system, known as Autopilot. The vehicles also record speed, accelerator pedal position, steering wheel angle and brake usage, and depending on how the vehicle is used, that data can be stored for over a year.\n\"These data contain a wealth of information for forensic investigators and traffic accident analysts and can help with a criminal investigation after a fatal traffic accident or an accident with injury,\" Francis Hoogendijk, a digital investigator at the NFI, said in a statement.\nAdvertisement · Scroll to continue\nTesla did not immediately respond to a request for comment.\nThe Dutch lab said rather than seek the data from Tesla, it had \"reverse engineered\" data logs - a process where software is deconstructed to extract information - present in Tesla vehicles \"in order to objectively investigate them.\"\n'WHO IS RESPONSIBLE?'\nThe NFI investigated a collision involving a Tesla driver using Autopilot and a car in front of it that suddenly braked hard.\nAdvertisement · Scroll to continue\nThe investigation showed the Tesla driver reacted within the expected response time to a warning to resume control of the car, but the collision occurred because the Tesla was following the other vehicle too closely in busy traffic.\n\"That makes it interesting, because who is responsible for the following distance: the car or the driver?\" said NFI investigator Aart Spek.\nThe NFI said Tesla encrypts its coded driving data to keep its technology secure from other manufacturers and protect driver privacy. Car owners can request their data, including camera footage, in the event of an accident.\nAdvertisement · Scroll to continue\nEarlier this year, Tesla said it had established a site in China to store car data locally, as automakers came under growing scrutiny over how they handle information collected by vehicle cameras and sensors. read more\nDECRYPTION REVEALED MORE DATA\nThe NFI found that Tesla had complied with data requests from the Dutch authorities, but left out a lot of data that could have proven useful.\n\"Tesla however only supplies a specific subset of signals, only the ones requested, for a specific timeframe, whereas the log files contain all the recorded signals,\" the NFI's report said.\nAdvertisement · Scroll to continue\nReport this ad\nBy decrypting Tesla's code, the NFI now knows more about what kind of data the carmaker is storing and for how long, allowing for more detailed data requests, Hoogendijk said.\n\"You cannot claim what you do not know, so it's helpful that we now know what else is being stored,\" he said.\nHoogendijk added this also applies to other carmakers, as investigators simply do not know how much and what kind of data manufacturers store and for how long.\nTesla has remote access to the data, the lab said, which is periodically uploaded from cars and used by the company for product improvements or to fix malfunctions.\nThe NFI said it had obtained data from Tesla models S, Y, X and the mass-market Model 3 and shared the results at a conference of the European Association for Accident Research so that other accident analysts can use it.\nIn August, the U.S. National Highway Traffic Safety Administration (NHTSA) opened a formal safety probe into Tesla's Autopilot system in 765,000 U.S. vehicles after a series of crashes involving Tesla models and emergency vehicles.\nTo date, NHTSA has identified 12 crashes that involved Tesla vehicles using the advanced driver-assistance systems and emergency vehicles. NHTSA said most of the incidents took place after dark.\nReporting by Nick Carey; Editing by Bernadette Baum\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Based on the provided news article, here's the JSON-format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to Tesla Autopilot accidents because it discusses the Dutch government's forensic lab decrypting Tesla's closely guarded driving data-storage system. The decrypted data reveals that Tesla vehicles store information about the operation of its driver assistance system, known as Autopilot, along with other details such as speed, accelerator pedal position, steering wheel angle, and brake usage. This information can be crucial in investigating accidents involving Tesla vehicles that may have been operating on Autopilot. Additionally, the article mentions an investigation into a collision involving a Tesla driver using Autopilot, providing further relevance to Tesla Autopilot accidents.\"\n}\nThe news contains specific information about Tesla's driving data, including its use in investigations, the data points collected, and its relevance to Autopilot accidents.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/dutch-forensic-lab-says-it-has-decoded-teslas-driving-data-2021-10-21\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":86,"Time":"15 July 2020","Title":"Tesla's 'Autopilot' misleading, Germany rules","Content":"Tesla's 'Autopilot' misleading, Germany rules\nPublished\n15 July 2020\nShare\nIMAGE SOURCE,\nGETTY IMAGES\nImage caption,\nAdvertising for Tesla's Model 3 (left) did not go over well in Germany\nTesla's claims about its Autopilot driver-assist feature are misleading, a German court has ruled.\nThe use of the word \"autopilot\", along with other marketing material, suggested the vehicle could drive on its own, the court said.\nBut the technology still requires the driver to remain alert at all times.\nTesla has been banned from repeating the misleading claims. German law does not allow fully autonomous driving.\nGermany's Wettbewerbszentrale fair-competition group had objected to Tesla's website promising, in July 2019, \"full potential for autonomous driving\" including automatic driving on motorways.\nUnder the heading \"by the end of the year\", Tesla added its cars would be able to recognise traffic lights and automatically stop and start driving in urban areas.\nMedia caption,\nThis car is on Autopilot. What happens next?\nThe court, in Munich, said: \"By using the term 'autopilot' and other wording, the defendant suggests that their vehicles are technically able to drive completely autonomously.\"\nIn 2018, two US consumer groups also complained about the \"deceptive and misleading\" naming of the Autopilot feature.\nTesla's strong position in the electric-vehicle market has made it a challenger to German carmakers.\nAnd it has chosen Berlin as the site of its first major European factory.\n'Typically bold'\nResponding to a tweet pointing out pilots do not stop working when a plane's autopilot mode is on, chief executive Elon Musk tweeted: \"Tesla Autopilot was literally named after the term used in aviation.\n\"Also, what about 'autobahn'?\" he added, referencing the German word for motorway.\n\"Auto\" is a German word for car, similar to the English word \"automobile\".\nLast week, Mr Musk said completely autonomous driving was \"very close\" and Tesla would crack the technology by the end of this year - a claim one analyst called \"typically bold\".\nPolice car\nThere have been several examples of drivers misusing the Autopilot system in dangerous circumstances.\nA British driver was disqualified from driving for 18 months after turning the system on and climbing into the passenger seat, on the motorway.\nEarlier this year, investigators said another Tesla driver, who died in a crash, in March 2018, had been playing a video game when his Autopilot-enabled car had hit a concrete barrier.\nThey also warned more crashes could happen if Tesla did not change the system.\nAnd in another case, an Autopilot-mode Tesla crashed into a stationary police car.\nMore on this story\nThis car is on Autopilot. What happens next? Video, 00:01:33\nThis car is on Autopilot. What happens next?\nPublished\n12 June 2018\n1:33\nUber 'not criminally liable' for car death\nPublished\n6 March 2019\n'Berlin rocks,' says Elon Musk as he picks factory\nPublished\n13 November 2019\nTesla 'very close' to full self-driving, Musk says\nPublished\n9 July 2020","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"The news is relevant to Tesla Autopilot accidents because it discusses a German court ruling that found Tesla's claims about its Autopilot driver-assist feature to be misleading. The court stated that the use of the term 'autopilot' and other marketing material suggested that the vehicle could drive on its own, but in reality, the technology still requires the driver to remain alert at all times. Tesla has been banned from repeating these misleading claims. Additionally, the article mentions instances of drivers misusing the Autopilot system, including a British driver who climbed into the passenger seat on the motorway while the system was active and a Tesla driver who was playing a video game when his Autopilot-enabled car crashed into a concrete barrier, highlighting the potential dangers associated with the feature.\"\n}\nThe news is relevant to Tesla Autopilot accidents as it discusses both the misleading claims made by Tesla regarding its Autopilot system and real-world incidents where drivers misused the system, resulting in accidents.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-53418069","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":79,"Time":"Thu 17 May 2018 19.21 CEST","Title":"Tesla confirms autopilot involved in Utah crash but seeks to blame driver","Content":"A Tesla car was driving in “autopilot” mode when it crashed into a stopped firetruck in Utah, the company said in a report to police that repeatedly cast blame on the driver, not its semi-autonomous driving system.\nThe confirmation that the vehicle’s technology failed to prevent it from colliding with a stopped object in its path came the same day that the US National Highway Traffic Safety Administration (NHTSA) announced it was sending a team to investigate the 11 May crash in Utah.\nTesla shares fall more than 7% after company reports record loss\nRead more\nTesla officials told police on Wednesday that the driver, who suffered a broken ankle when her Tesla Model S crashed, had turned on the “autosteer” and “cruise control” features about 80 seconds before the crash and taken her hands off the wheel, Tesla officials told police on Wednesday.\nIn recent weeks, Elon Musk’s electric car company has faced fresh scrutiny over the safety of its autopilot feature, which is supposed to assist drivers in navigating the road but cannot drive the cars on its own. In response to numerous high-profile autopilot crashes, including a fatal collision in California, the CEO has insisted that his technology is safer than traditional cars. Yet some experts have warned that the semi-autonomous features give drivers a false sense of security, allowing them to become easily distracted.\nThe 28-year-old Utah driver was looking at her phone before the collision and was given a traffic citation for “failure to keep proper lookout”, police said Wednesday.\nTesla’s report to police said there were more than a dozen instances of the driver taking her hands off the wheel for more than one minute at a time and that she only re-engaged when she was provided visual alerts.\n“Drivers are repeatedly advised Autopilot features do not make Tesla vehicle ‘autonomous’ and that the driver absolutely must remain vigilant with their eyes on the road, hands on the wheel and they must be prepared to take any and all action necessary to avoid hazards on the road,” the company wrote.\nThe Tesla Model S is seen after it hit the back of a firetruck in South Jordan, Utah. Photograph: Handout\/Reuters\nThe car was traveling at 60 miles per hour when the crash happened, Tesla said, adding that the driver manually pressed the vehicle brake fractions of a second before the collision. The driver’s use of autopilot was “contrary to proper use”, Tesla said, because she “did not pay attention to the road at all times” and used the feature on a street with no center median and with intersections controlled by stoplights.\nAsked why the autopilot technology did not prevent the collision, a spokesperson told the Guardian in an email: “Tesla told police that the crash occurred because the driver did not pay attention to the road at all times, and that Autopilot is not intended for use without a fully attentive driver.”\nThe spokesperson also pointed to Tesla’s manual, which warns its drivers that the cruise control feature cannot detect all objects and may not brake or slow down for stationary vehicles and that its “automatic emergency braking” is not a substitute for drivers maintaining a safe distance from the cars in front of them.\nTesla also declined to comment on the NHTSA investigation. In recent weeks, the company has publicly feuded with US investigators over continuing federal inquiries related to autopilot.\nMusk also criticized journalists for writing about the crash in a series of tweets earlier this week.\nThe first fatal autopilot crash happened in 2016 when Tesla’s software failed to “see” the white side of a tractor-trailer in its path against the backdrop of a white sky.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"This news is about a Tesla car that was driving in 'autopilot' mode when it crashed into a stopped firetruck in Utah. Tesla confirmed that the vehicle's technology failed to prevent the collision, and the company repeatedly cast blame on the driver, not its semi-autonomous driving system. The driver had turned on the 'autosteer' and 'cruise control' features about 80 seconds before the crash, took her hands off the wheel, and was looking at her phone before the collision. Tesla's report mentioned instances where the driver took her hands off the wheel for more than one minute at a time. The car was traveling at 60 miles per hour when the crash happened, and the driver manually pressed the vehicle brake fractions of a second before the collision. The spokesperson for Tesla stated that the crash occurred because the driver did not pay attention to the road at all times, and Autopilot is not intended for use without a fully attentive driver. This news report provides detailed information about a Tesla Autopilot-related accident.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/may\/16\/tesla-autopilot-utah-crash-confirms-investigation","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":87,"Time":"MARCH 2, 2019","Title":"U.S. agency probing two fatal Tesla crashes in Florida since last Sunday","Content":"By David Shepardson\nWASHINGTON (Reuters) - U.S. authorities are investigating a fatal Tesla Inc Model S crash in Florida last Sunday that killed the driver and caused a massive fire, the second fatal Tesla crash in the state this week being probed, the National Highway Traffic Safety Administration (NHTSA) said on Saturday.\nFILE PHOTO - The logo is seen on a new all-wheel-drive version of the Tesla Model S car in Hawthorne, California October 9, 2014.REUTERS\/Lucy Nicholson\nThe agency and the National Transportation Safety Board said late on Friday they were sending teams to investigate the other fatal crash Friday in Delray Beach, Florida of a 2018 Model 3 that crashed into a semi-trailer.\nA NHTSA spokesman confirmed Saturday the agency has an “ongoing investigation” into the Sunday Tesla crash in Davie, Florida, and “will take additional actions if appropriate.”\nTesla did not immediately comment Saturday.\nThe South Florida Sun Sentinel reported Monday the 2016 Tesla Model S caught fire and burned the 48-year-old driver beyond recognition. The newspaper said the Tesla battery repeatedly caught fire after being transported to a towing facility.\nNHTSA, the auto safety regulator, can demand a recall if it believes a defect poses an unreasonable safety risk, while the NTSB makes safety recommendations.\nNHTSA and NTSB are investigating a number of crashes since 2017 that focus on two primary issues: the role of Tesla’s driver assistance technology Autopilot in accidents and some significant battery fires in the electric vehicles after crashes, including cases of batteries reigniting.\nA report on Friday’s crash released by the Palm Beach County Sheriff’s Department did not indicate if Autopilot was engaged in the crash that killed the 50-year-old Tesla Model 3 owner.\nThe report said the Tesla struck a tractor trailer and the roof was sheared off as it passed underneath the trailer and stopped three-tenths of a mile south of the collision. The driver was pronounced dead at the scene.\nNTSB is sending a team of three to conduct a safety investigation, while NHTSA is sending a field team.\nSome Tesla drivers say they are able to avoid putting their hands on the wheel for extended periods when using Autopilot, while Tesla advises drivers that they must keep their hands on the steering wheel and pay attention while using Autopilot.\nNHTSA is also probing the January 2018 crash of a Tesla vehicle apparently traveling in Autopilot that struck a fire truck in Culver City, California, a May 2018 crash in Utah of a Tesla in Autopilot mode and a May 2018 Tesla accident in Florida that killed two teenagers and injured another but was not in Autopilot.\nThe NTSB is investigating three earlier Tesla incidents being reviewed by NHTSA, as well as an August 2017 Tesla battery fire in California, in which an owner ran into his garage.\nFriday’s crash is similar to the first fatal Tesla crash linked to Autopilot.\nIn May 2016, a Tesla Model S driver was killed near Williston, Florida, using Autopilot when he slammed into a tractor trailer that also sheared off the vehicle roof.\nThe NTSB said in 2017 Tesla lacked proper safeguards allowing the driver “to use the system outside of the environment for which it was designed and the system gave far too much leeway to the driver to divert his attention.”\nIn January 2017, NHTSA said its review found no evidence of defects in the 2016 fatal Autopilot crash that would require a recall.\nTesla says its Autopilot software reduces a driver’s “overall workload” and will “steer, accelerate and brake for you within almost any lane. It will also automatically change lanes on most highways to overtake other cars or navigate to interchanges and exits.”\nTesla says the feature “should not be used on highways that have very sharp turns or lane markings that are absent, faded, or ambiguous.”\nReporting by David Shepardson; Editing by Chizu Nomiyama and David Gregorio\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here's the JSON-format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": {\n    \"Summary\": \"The news is relevant to Tesla Autopilot accidents.\",\n    \"Details\": [\n      \"The news reports on two fatal Tesla crashes in Florida.\",\n      \"One of the crashes involved a Tesla Model S and resulted in the driver's death and a massive fire.\",\n      \"The National Highway Traffic Safety Administration (NHTSA) is investigating both of these fatal crashes.\",\n      \"The investigation involves examining the role of Tesla's driver assistance technology, Autopilot, in accidents.\",\n      \"The news mentions previous investigations into Tesla accidents, some of which involved Autopilot.\",\n      \"The NTSB is also involved in investigating these incidents.\",\n      \"There have been concerns about Tesla drivers not keeping their hands on the wheel while using Autopilot.\",\n      \"Tesla's Autopilot software is mentioned, which claims to handle steering, acceleration, braking, and lane changes on highways.\"\n    ]\n  }\n}\nThe news is relevant to Tesla Autopilot accidents because it discusses two fatal Tesla crashes in Florida, with a focus on investigating the role of Tesla's driver assistance technology, Autopilot, in these accidents. The report also mentions previous investigations into Tesla accidents involving Autopilot and the concerns about drivers not maintaining control while using Autopilot.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-tesla-crash\/u-s-agency-probing-two-fatal-tesla-crashes-in-florida-since-last-sunday-idUSKCN1QJ0MC","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":85,"Time":"10:35 PM GMT+1•January 31, 2023","Title":"DOJ requests Autopilot, FSD documents from Tesla","Content":"cameraImage Credits: Christopher Goodney\/Bloomberg \/ Getty Images\nThe U.S. Department of Justice has asked Tesla for documents related to its branded Full Self-Driving and Autopilot advanced driver-assistance systems, the automaker disclosed in a securities filing.\nTesla said in the filing it “has received requests from the DOJ for documents related to Tesla’s Autopilot and FSD features. . . . To our knowledge no government agency in any ongoing investigation has concluded that any wrongdoing occurred,” Tesla noted in the 10-K filing that was posted Monday.\nTesla has been under investigation by the DOJ for at least a year, Reuters reported last fall, citing three people familiar with the matter. It’s unclear if the DOJ’s request for documents is connected to that investigation, which was launched in late 2021 following more than a dozen accidents involving the active use of Tesla’s Autopilot system.\nTesla vehicles come standard with a driver-assistance system branded as Autopilot. For an additional $15,000, owners can buy “full self-driving,” or FSD — a feature that CEO Elon Musk has repeatedly promised will one day deliver full autonomous driving capabilities.\nNeither one of these systems are self-driving. Autopilot and FSD are advanced driver-assistance systems that automate some driving tasks and still require the driver to be ready to take over at any moment. Autopilot keeps the vehicle centered in the lane, can automatically change lanes and maintains the proper distance from other vehicles in traffic. FSD has those features and more, including an active guidance system that navigates a car from a highway on-ramp to off-ramp and can navigate interchanges and make lane changes.\nMusk’s claims and promises of these systems, as well as the branding, has caught the attention of regulators. The DOJ’s inquiry reflects an uptick in regulator scrutiny of Tesla.\nThe Securities and Exchange Commission (SEC) began investigating Musk after specific comments and efforts were made to promote the vehicle’s “self-driving” capabilities. The investigation follows a testimony from a Tesla engineer claiming that a 2016 video purporting to show a Tesla vehicle driving itself was in fact staged, and that Musk directed the video.\nTesla has been investigated and sued by several agencies and individuals for its claims of self-driving. The National Highway Traffic Safety Administration (NHTSA) has opened a number of special investigations into Tesla for crashes involving Autopilot; the California Department of Motor Vehicles has accused Tesla of falsely advertising its ADAS; and drivers have sued the company for deceitful marketing.\nAll of the attention hasn’t thwarted Musk. During Tesla’s fourth-quarter 2022 earnings call, Musk said “full self-driving is obviously getting better very rapidly.” In the past he has boasted that Tesla was close to “solving” full self-driving.\nMore TechCrunch\nMistral AI makes its first large language model free for everyone\nVR is dead\nTinder goes ultra-premium, Amazon invests in Anthropic and Apple explains its new AirPods\nVR is dead","Relevant":true,"Overview":"Based on the provided news article, here is the JSON-format response:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to Tesla Autopilot accidents because it discusses the U.S. Department of Justice (DOJ) requesting documents from Tesla related to its branded Full Self-Driving (FSD) and Autopilot advanced driver-assistance systems. The article mentions that the DOJ's request for documents is connected to an investigation that was launched following more than a dozen accidents involving the active use of Tesla's Autopilot system. It also highlights that Tesla's Autopilot and FSD systems are advanced driver-assistance systems, not self-driving, and that they still require the driver to be ready to take over at any moment. Additionally, the article mentions Tesla's history of regulatory scrutiny and investigations related to its claims of self-driving capabilities, including investigations by the Securities and Exchange Commission (SEC), the National Highway Traffic Safety Administration (NHTSA), and lawsuits by individuals for deceitful marketing.\"\n}\nThe provided news article is relevant to Tesla Autopilot accidents because it discusses the DOJ's investigation into Tesla's Autopilot and FSD systems, which have been associated with accidents, and highlights the regulatory scrutiny and investigations surrounding Tesla's self-driving claims.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2023\/01\/31\/doj-requests-autopilot-fsd-documents-from-tesla\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":89,"Time":"SEPTEMBER 3, 2019","Title":"Tesla Autopilot engaged in 2018 California crash; driver's hands off wheel: NTSB","Content":"By David Shepardson\nWASHINGTON (Reuters) - The National Transportation Safety Board (NTSB) said on Tuesday that a Tesla Model S was in Autopilot mode and the driver’s hands were off the wheel when it struck a fire truck in Culver City, California, one of a series of crashes the board is investigating involving Tesla’s driver assistance system.\nSlideshow ( 2 images )\nThe 2014 Model S Autopilot system was engaged continuously for the final 13 minutes 48 seconds of the trip before the vehicle struck a fire truck parked on Interstate-405 in January 2018, the NTSB safety board said in documents posted on Tuesday.\nThe board plans to issue a report detailing the crash’s probable cause on Wednesday.\nThe driver kept his hands off the wheel for all but 51 seconds of the final drive segment and received numerous alerts to place his hands back on the wheel, the documents showed. Tesla Inc did not immediately comment.\nThe fire truck was unoccupied and the driver was not injured in the incident.\n“I was having a coffee and a bagel. And all I remember, that truck, and then I just saw the boom in my face and that was it,” the man behind the wheel told the NTSB.\nThe report says the Tesla was following a vehicle for an extended period at a speed of around 21 miles per hour (33.8 kph) when the vehicle ahead changed lanes seconds before the crash\nAfter the vehicle in front shifted, the Tesla began accelerating toward the driver-set cruise control speed of 80 mph (129 kph) and hit the parked fire truck while going 30.9 mph. The system did not detect the driver’s hands on the wheel for the final 3 minutes and 41 seconds of the crash.\nHe did not remember precisely what he was doing at the time of the crash but could have been changing the radio or drinking coffee. Records show the driver was not using his cellphone to text or talk at the time of the crash, the NTSB said.\nTesla’s Autopilot was engaged during at least three fatal U.S. crashes, including fatal March 2018 crashes of a 2018 Model 3 in Delray Beach, Florida and in Mountain View, California of a Model X. Both of those crashes remain under investigation by the NTSB and National Highway Traffic Safety Administration. The agencies are also investigating other crashes and battery fires involving Tesla vehicles.\nThe crashes raised questions about the driver-assistance system’s ability to detect hazards and has sparked concern about the safety of systems that can perform driving tasks for extended stretches of time with little or no human intervention, but which cannot completely replace human drivers.\nWhile Tesla drivers say they are able to avoid holding the steering wheel for extended periods while using Autopilot, Tesla advises drivers to keep their hands on the wheel and pay attention while using the system.\nIn May 2016, a Tesla Model S driver was killed near Williston, Florida, while Autopilot was engaged, when he slammed into a tractor trailer.\nThe NTSB said in 2017 that Tesla lacked proper safeguards allowing the Florida driver “to use the system outside of the environment for which it was designed and the system gave far too much leeway to the driver to divert his attention.”\nReporting by David Shepardson; Editing by Bill Berkrot\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": {\n        \"Title\": \"Tesla Autopilot engaged in 2018 California crash; driver's hands off wheel: NTSB\",\n        \"Description\": \"The news is relevant to Tesla Autopilot accidents because it discusses a Tesla Model S being in Autopilot mode and the driver's hands being off the wheel when it struck a fire truck in Culver City, California, in January 2018. The Autopilot system was engaged continuously for the final 13 minutes 48 seconds of the trip before the accident. The driver also kept his hands off the wheel for all but 51 seconds of the final drive segment, despite receiving alerts to place his hands back on the wheel. This incident raises concerns about the safety of Tesla's driver-assistance system and its ability to detect hazards.\",\n        \"Source\": \"Reuters\",\n        \"Date\": \"Date: <insert date here>\"\n    }\n}\nThe provided news is relevant to Tesla Autopilot accidents because it describes an incident involving a Tesla Model S in Autopilot mode with the driver's hands off the wheel, resulting in a collision with a fire truck. This incident occurred in 2018 and highlights concerns about the safety of Tesla's Autopilot system and its ability to detect hazards. The news article is sourced from Reuters. Please replace \"<insert date here>\" with the actual date of the news article for the complete JSON response.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/us-tesla-crash-idUSKCN1VO22E","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":91,"Time":"MAY 29, 2018","Title":"Tesla in 'Autopilot' mode crashes into parked police vehicle","Content":"By Reuters Staff\nWASHINGTON, May 29 (Reuters) - The driver of a Tesla Inc vehicle crashed into an unoccupied, parked police vehicle in Laguna Beach, California, on Tuesday and the driver told investigators the Tesla was in “Autopilot” mode at the time, a police spokesman said.\nThe driver suffered minor injuries, Laguna Beach Sergeant Jim Cota said. Asked about Tuesday’s crash, Tesla said, “When using Autopilot, drivers are continuously reminded of their responsibility to keep their hands on the wheel and maintain control of the vehicle at all times. Tesla has always been clear that Autopilot doesn’t make the car impervious to all accidents.” (Reporting by David Shepardson Editing by Leslie Adler)\nOur Standards: The Thomson Reuters Trust Principles.","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"The news is relevant to Tesla Autopilot accidents. The driver of a Tesla Inc vehicle crashed into an unoccupied, parked police vehicle in Laguna Beach, California, while the driver told investigators the Tesla was in “Autopilot” mode at the time. Tesla's response also mentioned that when using Autopilot, drivers are continuously reminded of their responsibility to keep their hands on the wheel and maintain control of the vehicle at all times, and that Autopilot doesn’t make the car impervious to all accidents.\"\n}\nThe news is relevant to Tesla Autopilot accidents because it mentions a Tesla vehicle being in \"Autopilot\" mode during a crash into a parked police vehicle, and Tesla's statement regarding the use of Autopilot in such incidents.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/article\/tesla-autopilot\/tesla-in-autopilot-mode-crashes-into-parked-police-vehicle-idUSL2N1T020V","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":94,"Time":"23 April 2021","Title":"Tesla's Autopilot 'tricked' to operate without driver","Content":"Tesla's Autopilot 'tricked' to operate without driver\nPublished\n23 April 2021\nShare\nIMAGE SOURCE,\nTESLA\nBy Cody Godwin\nBBC News, San Francisco\nThe Autopilot feature in Tesla vehicles can be tricked into operating without a driver, an influential consumer magazine in the US has found.\nConsumer Reports engineers looked into claims that Autopilot can operate without a driver present.\nThey tested the Model Y on a closed track and concluded the system could be \"easily tricked\".\nIt comes days after a fatal Tesla crash in Texas. Police believe no one was in the driver's seat.\nTesla's Autopilot is an advanced driver assistance system that Tesla says \"enhances safety and convenience behind the wheel\".\n'Clearly insufficient'\nThe Consumer Reports team said they successfully - and repeatedly - tricked the car into driving their test track with no one in the driver's seat.\n\"In our evaluation, the system not only failed to make sure the driver was paying attention, but it also couldn't tell if there was a driver there at all,\" said Consumer Reports auto testing director Jake Fisher.\n\"It was a bit frightening when we realized how easy it was to defeat the safeguards, which we proved were clearly insufficient.\"\nTesla's website says the Autopilot system requires a \"fully attentive driver\" and using the system does not make the car autonomous.\nSome of the safety requirements for Autopilot mode include: keeping the driver's hands on the steering wheel, buckling the seatbelt and not opening any doors.\nCrash investigations\nOn Saturday, two men were killed after a Tesla car crashed into a tree and caught fire in Texas.\nIMAGE SOURCE,\nSCOTT J ENGLE\/REUTERS\nThe victims were found in the front passenger seat and in the back seat of the vehicle, leading police to believe nobody was in the driver's seat.\nHowever, Tesla boss Elon Musk said that \"data logs recovered so far show Autopilot was not enabled\".\n\"Moreover, standard Autopilot would require lane lines to turn on, which this street did not have,\" Mr Musk tweeted.\nSkip twitter post by Elon Musk\nAllow Twitter content?\nThis article contains content provided by Twitter. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Twitter’s cookie policy\n, external\nand privacy policy\n, external\nbefore accepting. To view this content choose ‘accept and continue’.\nAccept and continue\nThe BBC is not responsible for the content of external sites.\nEnd of twitter post by Elon Musk\nOn Thursday, two Democratic Senators sent a letter to the US National Highway Traffic Safety Administration (NHTSA) asking them to investigate the fatal Tesla accident in Texas.\nSenator Blumenthal and Senator Markey also requested a report outlining ways to prevent future accidents.\nThe NHTSA has opened investigations into 28 crashes involving Tesla vehicles.\nTesla has not responded to the BBC's request for comment.\nFollow Cody Godwin on Twitter at @MsCodyGodwin\nRelated Topics\nElon Musk\nTesla\nMore on this story\nMusk suggests Autopilot not to blame for crash\nPublished\n19 April 2021\nTesla recalls vehicles over failing touchscreens\nPublished\n2 February 2021","Relevant":true,"Overview":"Based on the provided news article, here is the JSON-format response:\njson\nCopy code\n{\n  \"Is_relevant\": true,\n  \"Specific_information\": \"Consumer Reports engineers tested Tesla's Autopilot feature and found that it could be 'easily tricked' into operating without a driver. They successfully and repeatedly tricked the car into driving their test track with no one in the driver's seat. The engineers expressed concerns about the system's inability to ensure driver attention and detect if a driver was present. This news is relevant to Tesla Autopilot accidents because it highlights a potential safety issue with Tesla's Autopilot system.\"\n}\nThe news is relevant to Tesla Autopilot accidents because it discusses concerns and testing related to the Autopilot system's operation without a driver.","From":"bbc.com","Url":"https:\/\/www.bbc.com\/news\/technology-56854417","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":88,"Time":"8:02 PM GMT+2•July 6, 2016","Title":"Another crashed Tesla was allegedly in Autopilot mode","Content":"Tesla’s Autopilot feature might be a bigger problem than the company realizes. A Detroit art gallery owner informed police that he had his 2016 Tesla Model X in Autopilot mode when it crashed and rolled over on the Pennsylvania Turnpike last Friday, as reported by the Detroit Free Press on Wednesday morning.\nAlthough the Model X driver, Albert Scaglione, and his son-in-law, Tim Yanke, both survived Friday’s crash, which occurred just over 100 miles east of Pittsburgh, the scope of scrutiny on Tesla’s Autopilot mode is likely to only grow. Thus far, Tesla released a statement to Electrek, saying that it currently has “no data to suggest that Autopilot was engaged at the time of the incident” and that “until the customer responds, we are unable to further investigate.”\nAccording to the Free Press, a Pennsylvania State Police report from Friday’s scene noted that the Model X hit a guard rail “off the right side of the roadway” and “it then crossed over the eastbound lanes and hit the concrete median,” tumbling onto its roof in the middle eastbound lane. Scaglione reportedly told police that his Model X was in Autopilot at the time of the accident.\nLast Friday’s Autopilot-related accident came just a day after the National Highway Traffic Safety Administration launched an investigation into the fatal May 7 crash in Florida, in which Tesla Model S owner Joshua D. Brown had the same feature activated. On Tuesday, Tesla claimed to have informed federal regulators about the May 7 crash nine days after it occurred .\nJust like the NHTSA will have to thoroughly investigate the Autopilot-related fatality in Florida, Pennsylvania State Police will have to dig deeper to confirm whether the Model X’s Autopilot feature suffered a malfunction. The reporting officer at the scene, Dale Vukovich, told the Free Press that he will likely cite Scaglione for the accident, which also involved another car, but didn’t specify the charge.\nBefore learning about the Model X crash, Musk offered his condolences over Brown’s death, before retweeting this:\n1.3 million people die a year in car accidents. Yet, 1 person dies in a Tesla on autopilot and people decry driverless cars as unsafe.\n— Nick Bilton (@nickbilton) July 2, 2016\nWhat will he say now?","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": \"The news is relevant to Tesla Autopilot accidents. The article reports an incident involving a 2016 Tesla Model X in Autopilot mode that crashed and rolled over on the Pennsylvania Turnpike. The driver, Albert Scaglione, reportedly claimed that his Model X was in Autopilot at the time of the accident. This incident highlights a potential issue with Tesla's Autopilot feature. It also mentions that a similar investigation was launched by the National Highway Traffic Safety Administration (NHTSA) into a fatal crash in Florida, where the Autopilot feature was activated. The article discusses the need for further investigation into the malfunction of the Autopilot feature in the Model X.\"\n}\nThe provided news is relevant to Tesla Autopilot accidents because it discusses an accident involving a Tesla Model X in Autopilot mode and the subsequent investigations and concerns about the Autopilot feature's safety.","From":"techcrunch.com","Url":"https:\/\/techcrunch.com\/2016\/07\/06\/another-tesla-crashes-in-autopilot-mode\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":95,"Time":"January 25, 2023","Title":"Testing group says Tesla Autopilot slips in driver assistance ratings","Content":"WASHINGTON, Jan 25 (Reuters) - Tesla's (TSLA.O) advanced driver assistance system Autopilot fell in a ranking of 12 major systems, Consumer Reports said Wednesday.\nOf 12 systems tested by Consumer Reports, a nonprofit organization that evaluates products and services, Ford Motor's (F.N) BlueCruise was first, followed by General Motors' (GM.N) Cadillac Super Cruise and Mercedes-Benz (MBGn.DE) Driver Assistance.\nTesla, which was second in Consumer Reports ratings in 2020 behind Super Cruise, fell to seventh, the group said. The electric vehicle maker did not respond to a request for comment.\nTesla hasn’t changed Autopilot’s basic functionality, instead adding more features to it, the group said.\n\"After all this time, Autopilot still doesn’t allow collaborative steering and doesn’t have an effective driver monitoring system,\" said Consumer Reports Auto Testing director Jake Fisher in a statement.\nThe National Highway Traffic Safety Administration (NHTSA) is investigating 830,000 Tesla vehicles with Autopilot after a series of crashes with parked emergency vehicles.\nAdvertisement · Scroll to continue\nNHTSA is reviewing whether Tesla vehicles adequately ensure drivers are paying attention. In June, the agency said evidence suggested drivers in most crashes with emergency vehicles under review had complied with Tesla's alert strategy that seeks to compel driver attention, raising questions about its effectiveness.\nNHTSA Acting Administrator Ann Carlson said earlier this month the agency was \"working really fast\" on its Autopilot probe. \"We're moving as quickly as we can, but we also want to be careful and make sure we have all the information we need,\" she said.\nAdvertisement · Scroll to continue\nIn 2020, the National Transportation Safety Board criticized Tesla's \"ineffective monitoring of driver engagement\" after a 2018 fatal Autopilot crash.\nAutopilot enables cars to steer, accelerate and brake within their lanes without driver intervention but Tesla says the feature requires \"active driver supervision and do not make the vehicle autonomous.\"\nNHTSA has separately opened more than three dozen Tesla special crash investigations where advanced systems including Autopilot were suspected of being used with 19 crash deaths reported.\nReporting by David Shepardson Editing by Shri Navaratnam\nOur Standards: The Thomson Reuters Trust Principles.\nAcquire Licensing Rights\n, opens new tab","Relevant":true,"Overview":"Here's the JSON-format response based on the provided news:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": [\n        \"The news is relevant to Tesla Autopilot accidents because it discusses Tesla's Autopilot system and its ranking among driver assistance systems.\",\n        \"Consumer Reports ranked Tesla Autopilot seventh among 12 major driver assistance systems, indicating a potential issue or slip in its performance.\",\n        \"The news mentions that Tesla hasn't added collaborative steering and lacks an effective driver monitoring system, highlighting concerns about its safety features.\",\n        \"The National Highway Traffic Safety Administration (NHTSA) is investigating 830,000 Tesla vehicles with Autopilot after a series of crashes with parked emergency vehicles, which directly relates to Autopilot accidents.\",\n        \"NHTSA is reviewing whether Tesla vehicles ensure drivers are paying attention and questions the effectiveness of Tesla's alert strategy in preventing accidents.\",\n        \"In 2020, the National Transportation Safety Board criticized Tesla's 'ineffective monitoring of driver engagement' after a fatal Autopilot crash, reinforcing the relevance of Autopilot accidents in the news.\",\n        \"NHTSA has opened more than three dozen Tesla special crash investigations where advanced systems, including Autopilot, were suspected of being used with 19 crash deaths reported, indicating the association of Autopilot with accidents.\"\n    ]\n}\nThis news is relevant to Tesla Autopilot accidents as it discusses issues with Tesla's Autopilot system, its ranking, safety concerns, investigations by the NHTSA, and previous criticism by safety organizations.","From":"reuters.com","Url":"https:\/\/www.reuters.com\/business\/autos-transportation\/testing-group-says-tesla-autopilot-slips-driver-assistance-ratings-2023-01-25\/","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":93,"Time":"Tue 15 May 2018 01.24 CEST","Title":"Tesla driver says car was in autopilot when it crashed at 60mph","Content":"The driver of a Tesla car that failed to stop at a red light and collided with a firetruck told investigators that the vehicle was operating on “autopilot” mode when it crashed, police said.\nA Tesla Model S was traveling at 60mph when it collided with the emergency vehicle in South Jordan, Utah, on Friday, causing minor injuries to both drivers, officials said Monday. The Tesla driver’s claim that the car was using the autopilot technology has raised fresh questions about the electric car company’s semi-autonomous system, which is supposed to assist drivers in navigating the road.\nElon Musk says Tesla planning 'thorough reorganization'\nRead more\nThe exact cause of the crash, which left the driver with a broken ankle, remains unknown, with Tesla saying it did not yet have the car’s data and could not comment on whether autopilot was engaged. South Jordan police also said the 28-year-old driver “admitted that she was looking at her phone prior to the collision” and that witnesses said the car did not brake or take any action to avoid the crash.\n“As a reminder for drivers of semi-autonomous vehicles, it is the driver’s responsibility to stay alert, drive safely, and be in control of the vehicle at all times,” the police department said in a statement.\nThe scene of the crash in Utah. Photograph: Courtesy of the South Jordan police department\nWhile driverless technology is expected to make the roads significantly safer by reducing human error and crashes, companies like Tesla are currently in a transition period that some experts say has created unique risks. That’s because semi-autonomous features, research has shown, can lull drivers into a false sense of security and make it hard for them to remain alert and intervene as needed.\nTesla has faced backlash for its decision to brand the technology “autopilot”, given that the drivers are expected not to depend on the feature to keep them safe.\nAfter a Tesla autopilot crash in March resulted in the driver’s death, the company issued a series of lengthy statements blaming the victim for “not paying attention”.\nOn Monday, Tesla’s CEO Elon Musk complained about an article on the Utah crash, writing on Twitter: “It’s super messed up that a Tesla crash resulting in a broken ankle is front page news and the ~40,000 people who died in US auto accidents alone in past year get almost no coverage.”\nHe also wrote that it was “actually amazing” the collision at 60mph only resulted in a broken ankle: “An impact at that speed usually results in severe injury or death.”\nMusk has on numerous occasions forcefully chastised journalists investigating Tesla crashes, arguing that the unflattering news coverage was dissuading people from using the technology and thus “killing people” in the process. After Tesla recently labeled an award-winning news outlet an “extremist organization”, some critics compared the company’s hyperbolic denouncements of the press to the anti-media strategy of Donald Trump.","Relevant":true,"Overview":"{\n\"Is_relevant\": true,\n\"Specific_information\": \"The news is relevant to Tesla Autopilot accidents because it reports an incident involving a Tesla car operating in 'autopilot' mode that crashed at 60mph. The Tesla driver failed to stop at a red light and collided with a firetruck. The driver claimed that the vehicle was using autopilot technology at the time of the crash, raising questions about Tesla's semi-autonomous system. However, the exact cause of the crash is still unknown, as Tesla had not yet accessed the car's data. Additionally, it was mentioned that the driver admitted to looking at her phone before the collision, and witnesses stated that the car did not brake or take any action to avoid the crash. The news also highlights the debate surrounding the safety and responsibility of using semi-autonomous features like Tesla's 'autopilot,' which some experts argue can lead to driver complacency and reduced alertness.\"\n}","From":"theguardian.com","Url":"https:\/\/www.theguardian.com\/technology\/2018\/may\/14\/tesla-crash-utah-driver-autopilot-mode-elon-musk","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
{"num":84,"Time":"Published 6:32 PM GMT+2, March 8, 2023","Title":"Feds suspect Tesla using automated system in firetruck crash","Content":"DETROIT (AP) — U.S. investigators suspect that a Tesla was operating on an automated driving system when it crashed into a firetruck in California last month, killing the driver and critically injuring a passenger.\nThe National Highway Traffic Safety Administration said Wednesday it has dispatched a special crash investigation team to look into the Feb. 18 crash in Northern California where emergency responders had to cut open the Tesla to remove the passenger. Four Contra Costa County firefighters had minor injuries.\nThe probe is part of a larger investigation by the agency into multiple instances of Teslas on Autopilot crashing into parked emergency vehicles that are tending to other crashes.\nThe $1.4 million ladder truck was damaged in the crash on Interstate 680. The truck was parked to shield a crew clearing another accident, fire officials said.\nOTHER NEWS\nEvacuated Illinois residents return home after ammonia leak from truck crash that killed 5 people\nA bus carrying children overturns in northwest England, killing the driver and a 14-year-old girl\nHundreds attend funeral for high school band director who died in bus crash\nThe driver of the the 2014 Tesla Model S was declared dead at the scene.\nNHTSA is investigating how Tesla’s Autopilot system detects and responds to emergency vehicles parked on highways. At least 15 Teslas have crashed into emergency vehicles nationwide while using the system.\nAuthorities said the truck had its lights on and was parked diagonally on northbound lanes of the freeway to protect responders to an earlier accident that did not result in injuries.\nThe fatal accident occurred around 4 a.m., and it took several hours to clear the freeway. The firetruck had to be towed away.\nThe Model S was among the nearly 363,000 vehicles Tesla recalled in February because of potential flaws in “Full Self-Driving” a more sophisticated partially automated driving system.\nThe recall, to be done with an online software update, is aimed at correcting possible problems at intersections and with speed limits.\nDespite their names, Tesla has said both systems are advanced driver assist systems and that human drivers must be ready to intervene at all times.\nMessages were left Wednesday seeking comment from Tesla, which has disbanded its public relations department.","Relevant":true,"Overview":"Here is the JSON-format response based on the provided news article:\njson\nCopy code\n{\n    \"Is_relevant\": true,\n    \"Specific_information\": [\n        \"U.S. investigators suspect that a Tesla was operating on an automated driving system when it crashed into a firetruck in California last month, killing the driver and critically injuring a passenger.\",\n        \"The National Highway Traffic Safety Administration has dispatched a special crash investigation team to look into the Feb. 18 crash in Northern California where emergency responders had to cut open the Tesla to remove the passenger.\",\n        \"The probe is part of a larger investigation by the agency into multiple instances of Teslas on Autopilot crashing into parked emergency vehicles that are tending to other crashes.\",\n        \"NHTSA is investigating how Tesla’s Autopilot system detects and responds to emergency vehicles parked on highways. At least 15 Teslas have crashed into emergency vehicles nationwide while using the system.\"\n    ]\n}\nThis news is relevant to Tesla Autopilot accidents because it reports a Tesla vehicle suspected to be operating on an automated driving system crashing into a firetruck in California. The article also mentions a larger investigation by the National Highway Traffic Safety Administration (NHTSA) into multiple instances of Teslas on Autopilot crashing into parked emergency vehicles. Therefore, it fits the category of Tesla Autopilot accidents.","From":"apnews.com","Url":"https:\/\/apnews.com\/article\/tesla-firetruck-autopilot-investigation-c6d64b941f546f7ae70fb8355d765cb3","query_name":"updated_file_Tesla_Autopilot_accidents.xlsx"}
